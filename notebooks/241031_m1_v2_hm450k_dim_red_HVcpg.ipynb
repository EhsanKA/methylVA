{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f18039-d090-4590-8147-7015aea29fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# %reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4e60-85f2-4786-b816-cd2ba384f459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25597bcb-917e-4df6-bc83-6e282da50a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Disables fast auto-tuning\n",
    "\n",
    "\n",
    "# Time tracking decorator\n",
    "def time_tracker(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting {func.__name__}...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "@time_tracker\n",
    "def sample_data(numerical_data_filtered, labels_encoded, split_size, random_state=42):\n",
    "    if split_size == 'shuffled_10000':\n",
    "        return generate_shuffled_data(numerical_data_filtered, 10000, random_state)\n",
    "    else:\n",
    "        size = int(split_size)\n",
    "        if size < len(numerical_data_filtered):\n",
    "            print(f\"Splitting data with size: {size}\")\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, train_size=size, random_state=random_state)\n",
    "            for train_idx, _ in splitter.split(numerical_data_filtered, labels_encoded):\n",
    "                return numerical_data_filtered.iloc[train_idx], [labels_encoded[i] for i in train_idx]\n",
    "        else:\n",
    "            return numerical_data_filtered, labels_encoded\n",
    "\n",
    "\n",
    "@time_tracker\n",
    "def train_val_test_split(data, labels, random_state=42):\n",
    "    print(\"Splitting data into training, validation, and test sets...\")\n",
    "    \n",
    "    # Convert labels list to pandas Series to use value_counts()\n",
    "    import pandas as pd\n",
    "    labels_series = pd.Series(labels, index=data.index)\n",
    "    \n",
    "    # Filter labels to exclude classes with fewer than 20 samples\n",
    "    label_counts = labels_series.value_counts()\n",
    "    valid_labels = label_counts[label_counts > 20].index\n",
    "\n",
    "    # Select only data and labels corresponding to classes with more than 20 samples\n",
    "    valid_indices = labels_series.isin(valid_labels)\n",
    "    data_filtered = data.loc[valid_indices]\n",
    "    labels_filtered = labels_series.loc[valid_indices]\n",
    "\n",
    "    # Perform the split with stratification on filtered data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "        data_filtered, labels_filtered, test_size=0.3, random_state=random_state, stratify=labels_filtered\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_remaining, y_remaining, test_size=0.4, random_state=random_state, stratify=y_remaining\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "@time_tracker\n",
    "def scale_and_save_data(train_val_test_splits, output_path, split_data_value=10000):\n",
    "    scaler = StandardScaler()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_splits\n",
    "    print(f\"Scaling and saving data...\")\n",
    "    \n",
    "    # Scaling the features\n",
    "    # X_train_scaled = scaler.fit_transform(X_train)\n",
    "    # X_val_scaled = scaler.transform(X_val)\n",
    "    # X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Convert labels to numpy arrays before converting to tensors\n",
    "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Creating TensorDataset objects\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    dataset_path = f'{output_path}/{split_data_value}/'\n",
    "    directory_path = Path(dataset_path)\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the datasets\n",
    "    torch.save(train_dataset.tensors, f'{dataset_path}train_dataset_tensors.pt')\n",
    "    torch.save(val_dataset.tensors, f'{dataset_path}val_dataset_tensors.pt')\n",
    "    torch.save(test_dataset.tensors, f'{dataset_path}test_dataset_tensors.pt')\n",
    "    print(f\"Data saved successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe76aba-1020-4d81-b766-94222f769d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if preprocessed data exists...\n",
      "Preprocessed data not found. Loading raw data and processing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# Load or prepare data\n",
    "numerical_data_path = 'data/v2/numerical_data_filtered.csv'\n",
    "metadata_path = 'data/v2/metadata_with_labels.csv'\n",
    "\n",
    "print(\"Checking if preprocessed data exists...\")\n",
    "\n",
    "print(\"Preprocessed data not found. Loading raw data and processing...\")\n",
    "data_files = [f'../data/v2_HM450/methyl_scores_v2_HM450k_{i}.pkl' for i in range(1, 12)]\n",
    "dataframes = [pd.read_pickle(file, compression=\"bz2\") for file in data_files]\n",
    "df = pd.concat(dataframes, axis=0)\n",
    "\n",
    "metadata_columns = [\n",
    "    'id', 'geo_accession', 'title', 'sex', 'age', 'race', 'tissue',\n",
    "    'geo_platform', 'inferred_age_Hannum', 'inferred_age_SkinBlood',\n",
    "    'inferred_age_Horvath353'\n",
    "]\n",
    "label_column = 'disease'\n",
    "sex_condition_column = 'inferred_sex'\n",
    "age_condition_column = 'inferred_age_MepiClock'\n",
    "\n",
    "numerical_data = df.drop(\n",
    "    metadata_columns + [label_column, sex_condition_column, age_condition_column],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Fix FutureWarning\n",
    "df[label_column] = df[label_column].fillna('no_label')\n",
    "\n",
    "# Fix PerformanceWarning\n",
    "labels_encoded = df[label_column].astype('category').cat.codes\n",
    "df = pd.concat([df, labels_encoded.rename('labels_encoded')], axis=1)\n",
    "df = df.reset_index()\n",
    "\n",
    "nan_percentage = numerical_data.isna().sum() / numerical_data.shape[0] * 100\n",
    "selected_columns = nan_percentage[nan_percentage < 10].index.tolist()\n",
    "numerical_data_filtered = numerical_data[selected_columns]\n",
    "print(\"Data is processed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77850704-5e4e-4de3-972d-b89ce5294273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd99d873-4b29-43c0-8448-6fc4f4f6390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_variances = numerical_data_filtered.var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cca1d6-4640-4d57-babd-3266cad0d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_variances = column_variances.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a83f3ec4-b407-4a5a-96c5-e82e5cf9d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Select the top N columns\n",
    "top_n = 20000  # adjust as needed\n",
    "top_variances = sorted_variances.head(top_n)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_variances.plot(kind='bar')\n",
    "plt.title(f'Top {top_n} Columns by Variance')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5d483a0-7777-4ef3-9c82-bbcbfed8e9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAIhCAYAAADdH1JpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSb0lEQVR4nO3de1wWdf7//+clh4tDcokQIImHUlHDMnFTtFZNBVO09LNfLRK1zNw1NRK3zVw3O6jlgdy0svx4yvO2ZtvaLuGhNFM8oNSaZOWaYEJ4QBBTQJjfH36cX5egAoKM8rjfbtx2r/e8ZuY903Dp0/fMe2yGYRgCAAAAAFhSnZruAAAAAADg8ghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyM0AYAAAAAFkZoAwAAAAALI7QBAAAAgIUR2gAAAADAwghtAHCdLV68WDabTbt37y5zeXR0tJo0aeLU1qRJEw0bNqxC+9m2bZsmT56sU6dOVa6jtdDq1at15513ytPTUzabTampqVes/+9//6vRo0erRYsW8vT0lJeXl+688079+c9/1k8//VTh/U+ePFk2m62Sva85zz77rGw2m7799tvL1kycOFE2m0179uypkn3abDZNnjy5SrYFAFZHaAOAG8DatWs1adKkCq2zbds2vfTSS4S2cjp27JhiY2N1xx13KDExUdu3b1eLFi0uW79u3TrdddddWrdunZ566imtW7fO/P///Oc/FR0dfR17X7OGDx8uSVq4cGGZy0tKSvT++++rbdu2ateuXZXsc/v27XryySerZFsAYHWuNd0BAMDV3XPPPTXdhQorKiqSzWaTq+uN8UfNd999p6KiIg0ePFhdunS5Yu2hQ4f0yCOPqEWLFvrss8/kcDjMZQ888IDGjh2rtWvXVneXLSMsLEz33nuvli5dqqlTp5b6b56UlKQjR47oT3/60zXtxzAMnTt3Tp6enurYseM1bQsAbiSMtAHADeDS2yNLSkr06quvKjQ0VJ6enqpXr57uuusu/fWvf5V04Ta7P/7xj5Kkpk2bymazyWaz6fPPPzfXnz59ulq2bCm73a6AgAANGTJER44ccdqvYRiaOnWqGjduLA8PD7Vv317r169X165d1bVrV7Pu888/l81m09KlSxUfH6/bbrtNdrtdP/zwg44dO6ZRo0apdevWuuWWWxQQEKAHHnhAX3zxhdO+fvzxR9lsNs2YMUOvv/66mjRpIk9PT3Xt2tUMVM8//7yCg4PlcDjUv39/ZWdnl+v8ffzxx4qIiJCXl5fq1q2rnj17avv27ebyYcOG6b777pMkDRo0SDabzen4LpWQkKAzZ87o7bffdgpsF9lsNg0YMMCpbeHChbr77rvl4eGh+vXrq3///kpLS7tq3y93G+Cl18TF2243bdqkESNGyM/PTz4+PhoyZIjOnDmjrKwsDRw4UPXq1VODBg00fvx4FRUVmetfPP8zZ85UQkKCmjZtqltuuUURERFKTk6+aj+HDx+urKws/fvf/y61bNGiRbLb7Xrsscd07tw5xcfHq23btnI4HKpfv74iIiL0j3/8o8xjHz16tObNm6dWrVrJbrdryZIlZZ6Xil5n5T3OHTt2qG/fvvLz85OHh4fuuOMOxcXFOdV8//33iomJUUBAgOx2u1q1aqW33nrLqeZqv7MAcCU3xj9/AsBNqLi4WOfPny/VbhjGVdedPn26Jk+erD//+c/67W9/q6KiIn377bfmrZBPPvmkTp48qTlz5ujDDz9UgwYNJEmtW7eWJP3hD3/Qe++9p9GjRys6Olo//vijJk2apM8//1x79uyRv7+/pAvPIU2bNk1PPfWUBgwYoIyMDD355JMqKioq89bBCRMmKCIiQvPmzVOdOnUUEBCgY8eOSZJefPFFBQUFKT8/X2vXrlXXrl21cePGUuHorbfe0l133aW33npLp06dUnx8vPr27asOHTrIzc1NCxcu1OHDhzV+/Hg9+eST+vjjj694rlasWKHHHntMkZGRWrlypQoKCjR9+nRz//fdd58mTZqke++9V08//bSmTp2qbt26ycfH57LbTEpKUmBgYLlHe6ZNm6YXXnhBjz76qKZNm6YTJ05o8uTJioiI0K5du9S8efNybac8nnzySQ0YMECrVq3S3r179cILL+j8+fM6cOCABgwYoKeeekobNmzQ66+/ruDgYI0bN85p/bfeekstW7bU7NmzJUmTJk1S7969dejQoTID6kWPPvqonn32WS1cuFB9+/Y123NycvSPf/xD/fv3l6+vr3Jzc3Xy5EmNHz9et912mwoLC7VhwwYNGDBAixYt0pAhQ5y2+9FHH+mLL77QX/7yFwUFBSkgIKDM/Z88eVJSxa6zqx3np59+qr59+6pVq1ZKSEhQo0aN9OOPPyopKcnczv79+9WpUyc1atRIs2bNUlBQkD799FONHTtWx48f14svvijp6r+zAHBFBgDgulq0aJEh6Yo/jRs3dlqncePGxtChQ83P0dHRRtu2ba+4nxkzZhiSjEOHDjm1p6WlGZKMUaNGObXv2LHDkGS88MILhmEYxsmTJw273W4MGjTIqW779u2GJKNLly5m22effWZIMn77299e9fjPnz9vFBUVGd27dzf69+9vth86dMiQZNx9991GcXGx2T579mxDktGvXz+n7cTFxRmSjNzc3Mvuq7i42AgODjbatGnjtM3Tp08bAQEBRqdOnUodwwcffHDVY/Dw8DA6dux41TrDMIycnBzD09PT6N27t1N7enq6YbfbjZiYGLPtxRdfNC79o1mS8eKLL5ba7qXXxMXrasyYMU51Dz/8sCHJSEhIcGpv27at0a5dO/PzxfPfpk0b4/z582b7zp07DUnGypUrr3qsQ4cONdzc3Iyff/7ZbJszZ44hyVi/fn2Z61y8HoYPH27cc889pY7d4XAYJ0+eLLXe5c7Lpdu93HVWnuO84447jDvuuMM4e/bsZfcTFRVlNGzYsNR1OHr0aMPDw8Pse3l+ZwHgcrg9EgBqyPvvv69du3aV+rl4m96V3Hvvvfrqq680atQoffrpp8rLyyv3fj/77DNJKjUb5b333qtWrVpp48aNkqTk5GQVFBRo4MCBTnUdO3YsNbvlRf/zP/9TZvu8efPUrl07eXh4yNXVVW5ubtq4cWOZtwf27t1bder8/388tWrVSpLUp08fp7qL7enp6Zc5UunAgQM6evSoYmNjnbZ5yy236H/+53+UnJysX3755bLrV4Xt27fr7Nmzpc53SEiIHnjgAfN8V5VLJ0C50vk7fPhwqfX79OkjFxcX8/Ndd90lSWXWXmr48OEqKirS0qVLzbZFixapcePG6t69u9n2wQcfqHPnzrrlllvM62HBggVlXg8PPPCAfH19r7pvqWLX2dWO87vvvtPBgwc1fPhweXh4lLm/c+fOaePGjerfv7+8vLx0/vx586d37946d+6cecvltfzOAgChDQBqSKtWrdS+fftSP1e6Be2iCRMmaObMmUpOTtaDDz4oPz8/de/e/bKvEfi1EydOSJJ5y+SvBQcHm8sv/m9gYGCpurLaLrfNhIQE/eEPf1CHDh20Zs0aJScna9euXerVq5fOnj1bqr5+/fpOn93d3a/Yfu7cuTL78utjuNyxlpSUKCcn57LrX06jRo106NChctWW93xXlYqcv7LOnZ+fn9Nnu90uSWX+t7rU/fffrxYtWmjRokWSpK+//lp79uzR448/br7K4MMPP9TAgQN12223admyZdq+fbt27dqlJ554osz+lHXeylLR6+xqx3nxtt6GDRtedp8nTpzQ+fPnNWfOHLm5uTn99O7dW5J0/PhxSdf2OwsAPNMGADcgV1dXjRs3TuPGjdOpU6e0YcMGvfDCC4qKilJGRoa8vLwuu+7Fv6xmZmaW+gvp0aNHzefZLtb9/PPPpbaRlZVV5mhbWe8YW7Zsmbp27ap33nnHqf306dNXPsgq8OtjvdTRo0dVp06dco/i/FpUVJTmzJmj5OTkqz7XdrU+XDzfl2O321VQUFCqvarDXlV54okn9Pzzz2vnzp1asWKF6tSp4zTKuGzZMjVt2lSrV692ul7KOkap7GuqLFV9nd16662SVGpynl/z9fWVi4uLYmNj9fTTT5dZ07RpU0nX9jsLAIy0AcANrl69evrd736np59+WidPntSPP/4o6fIjJA888ICkC3/J/bVdu3YpLS3NvI2tQ4cOstvtWr16tVNdcnJyuW6Vu8hms5l9uejrr792mr2xuoSGhuq2227TihUrnCZ4OXPmjNasWWPOKFlRzz77rLy9vTVq1Cjl5uaWWm4Yhjnlf0REhDw9PUud7yNHjmjTpk1Otw2WpUmTJvr666+d2jZt2qT8/PwK9/t6GDp0qFxdXfXuu+9q+fLl6t69uxo3bmwut9lscnd3dwpjWVlZZc4eWRFVfZ21aNFCd9xxhxYuXHjZQOnl5aVu3bpp7969uuuuu8ocOb90RE+6/O8sAFwOI20AcAPq27evwsLC1L59e9166606fPiwZs+ercaNG5szEbZp00aS9Ne//lVDhw6Vm5ubQkNDFRoaqqeeekpz5sxRnTp19OCDD5qzR4aEhOjZZ5+VdOF2unHjxmnatGny9fVV//79deTIEb300ktq0KCB0zNiVxIdHa1XXnlFL774orp06aIDBw7o5ZdfVtOmTcucPbMq1alTR9OnT9djjz2m6OhojRw5UgUFBZoxY4ZOnTql1157rVLbbdq0qVatWqVBgwapbdu2Gj16tPkuvf3792vhwoUyDEP9+/dXvXr1NGnSJL3wwgsaMmSIHn30UZ04cUIvvfSSPDw8zNkFLyc2NlaTJk3SX/7yF3Xp0kX79+/X3Llzy3UbbU0ICgpS7969tWjRIhmGYb54+6Lo6Gh9+OGHGjVqlH73u98pIyNDr7zyiho0aKDvv/++0vutjuvsrbfeUt++fdWxY0c9++yzatSokdLT0/Xpp59q+fLlki78ft133326//779Yc//EFNmjTR6dOn9cMPP+if//ynNm3aJKl8v7MAcDmENgC4AXXr1k1r1qzR//7v/yovL09BQUHq2bOnJk2aJDc3N0lS165dNWHCBC1ZskTz589XSUmJPvvsM/MWsjvuuEMLFizQW2+9JYfDoV69emnatGlOIwNTpkyRt7e35s2bp0WLFqlly5Z65513NHHiRNWrV69cfZ04caJ++eUXLViwQNOnT1fr1q01b948rV271nxvXHWKiYmRt7e3pk2bpkGDBsnFxUUdO3bUZ599pk6dOlV6u9HR0frPf/6jWbNmad68ecrIyFCdOnXUtGlT9erVS2PGjDFrJ0yYoICAAL355ptavXq1+f65qVOnXvUv7H/84x+Vl5enxYsXa+bMmbr33nv1t7/9TQ899FCl+17dhg8fro8//lj169fXww8/7LTs8ccfV3Z2tubNm6eFCxfq9ttv1/PPP2/+g0BlVcd1FhUVpS1btujll1/W2LFjde7cOTVs2FD9+vUza1q3bq09e/bolVde0Z///GdlZ2erXr16at68uflcm1S+31kAuBybYZTjhUAAAPyfQ4cOqWXLlnrxxRf1wgsv1HR3AAC46RHaAACX9dVXX2nlypXq1KmTfHx8dODAAU2fPl15eXnat2/fZWeRBAAAVYfbIwEAl+Xt7a3du3drwYIFOnXqlBwOh7p27aopU6YQ2AAAuE4YaQMAAAAAC2PKfwAAAACwMEIbAAAAAFgYoQ0AAAAALIyJSK6zkpISHT16VHXr1pXNZqvp7gAAAACoIYZh6PTp0woODladOpcfTyO0XWdHjx5VSEhITXcDAAAAgEVkZGSoYcOGl11OaLvO6tatK+nCfxgfH58a7g0AAACAmpKXl6eQkBAzI1wOoe06u3hLpI+PD6ENAAAAwFUfm2IiEgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAAACwMEIbAAAAAFiYa013ADUrPT1dx48fL1etv7+/GjVqVM09AgAAAPBrhLZaLD09XaEtW+nc2V/KVe/h6aUD36YR3AAAAIDriNBWix0/flznzv4iv+h4ufmFXLG26ESGTqybpePHjxPaAAAAgOuI0Aa5+YXIHtSsprsBAAAAoAxMRAIAAAAAFkZoAwAAAAALI7QBAAAAgIUR2gAAAADAwghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyM0AYAAAAAFkZoAwAAAAALI7QBAAAAgIUR2gAAAADAwghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyM0AYAAAAAFlajoW3Lli3q27evgoODZbPZ9NFHH5nLioqK9Kc//Ult2rSRt7e3goODNWTIEB09etRpGwUFBRozZoz8/f3l7e2tfv366ciRI041OTk5io2NlcPhkMPhUGxsrE6dOuVUk56err59+8rb21v+/v4aO3asCgsLnWr+85//qEuXLvL09NRtt92ml19+WYZhVOk5AQAAAIBfq9HQdubMGd19992aO3duqWW//PKL9uzZo0mTJmnPnj368MMP9d1336lfv35OdXFxcVq7dq1WrVqlrVu3Kj8/X9HR0SouLjZrYmJilJqaqsTERCUmJio1NVWxsbHm8uLiYvXp00dnzpzR1q1btWrVKq1Zs0bx8fFmTV5ennr27Kng4GDt2rVLc+bM0cyZM5WQkFANZwYAAAAALnCtyZ0/+OCDevDBB8tc5nA4tH79eqe2OXPm6N5771V6eroaNWqk3NxcLViwQEuXLlWPHj0kScuWLVNISIg2bNigqKgopaWlKTExUcnJyerQoYMkaf78+YqIiNCBAwcUGhqqpKQk7d+/XxkZGQoODpYkzZo1S8OGDdOUKVPk4+Oj5cuX69y5c1q8eLHsdrvCwsL03XffKSEhQePGjZPNZqvGMwUAAACgtrqhnmnLzc2VzWZTvXr1JEkpKSkqKipSZGSkWRMcHKywsDBt27ZNkrR9+3Y5HA4zsElSx44d5XA4nGrCwsLMwCZJUVFRKigoUEpKilnTpUsX2e12p5qjR4/qxx9/vGyfCwoKlJeX5/QDAAAAAOV1w4S2c+fO6fnnn1dMTIx8fHwkSVlZWXJ3d5evr69TbWBgoLKyssyagICAUtsLCAhwqgkMDHRa7uvrK3d39yvWXPx8saYs06ZNM5+lczgcCgkJqchhAwAAAKjlbojQVlRUpEceeUQlJSV6++23r1pvGIbT7Ypl3bpYFTUXJyG50q2REyZMUG5urvmTkZFx1f4DAAAAwEWWD21FRUUaOHCgDh06pPXr15ujbJIUFBSkwsJC5eTkOK2TnZ1tjoIFBQXp559/LrXdY8eOOdVcOlqWk5OjoqKiK9ZkZ2dLUqkRuF+z2+3y8fFx+gEAAACA8rJ0aLsY2L7//ntt2LBBfn5+TsvDw8Pl5ubmNGFJZmam9u3bp06dOkmSIiIilJubq507d5o1O3bsUG5urlPNvn37lJmZadYkJSXJbrcrPDzcrNmyZYvTawCSkpIUHBysJk2aVPmxAwAAAIBUw6EtPz9fqampSk1NlSQdOnRIqampSk9P1/nz5/W73/1Ou3fv1vLly1VcXKysrCxlZWWZwcnhcGj48OGKj4/Xxo0btXfvXg0ePFht2rQxZ5Ns1aqVevXqpREjRig5OVnJyckaMWKEoqOjFRoaKkmKjIxU69atFRsbq71792rjxo0aP368RowYYY6MxcTEyG63a9iwYdq3b5/Wrl2rqVOnMnMkAAAAgGpVo1P+7969W926dTM/jxs3TpI0dOhQTZ48WR9//LEkqW3btk7rffbZZ+ratask6Y033pCrq6sGDhyos2fPqnv37lq8eLFcXFzM+uXLl2vs2LHmLJP9+vVzejeci4uLPvnkE40aNUqdO3eWp6enYmJiNHPmTLPm4isInn76abVv316+vr4aN26c2WcAAAAAqA424+JsGrgu8vLy5HA4lJubW+PPt+3Zs0fh4eEKGjpb9qBmV6wtyPpBWUvilJKSonbt2l2nHgIAAAA3r/JmA0s/0wYAAAAAtR2hDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZWo6Fty5Yt6tu3r4KDg2Wz2fTRRx85LTcMQ5MnT1ZwcLA8PT3VtWtXffPNN041BQUFGjNmjPz9/eXt7a1+/frpyJEjTjU5OTmKjY2Vw+GQw+FQbGysTp065VSTnp6uvn37ytvbW/7+/ho7dqwKCwudav7zn/+oS5cu8vT01G233aaXX35ZhmFU2fkAAAAAgEvVaGg7c+aM7r77bs2dO7fM5dOnT1dCQoLmzp2rXbt2KSgoSD179tTp06fNmri4OK1du1arVq3S1q1blZ+fr+joaBUXF5s1MTExSk1NVWJiohITE5WamqrY2FhzeXFxsfr06aMzZ85o69atWrVqldasWaP4+HizJi8vTz179lRwcLB27dqlOXPmaObMmUpISKiGMwMAAAAAF7jW5M4ffPBBPfjgg2UuMwxDs2fP1sSJEzVgwABJ0pIlSxQYGKgVK1Zo5MiRys3N1YIFC7R06VL16NFDkrRs2TKFhIRow4YNioqKUlpamhITE5WcnKwOHTpIkubPn6+IiAgdOHBAoaGhSkpK0v79+5WRkaHg4GBJ0qxZszRs2DBNmTJFPj4+Wr58uc6dO6fFixfLbrcrLCxM3333nRISEjRu3DjZbLYyj6OgoEAFBQXm57y8vCo7fwAAAABufpZ9pu3QoUPKyspSZGSk2Wa329WlSxdt27ZNkpSSkqKioiKnmuDgYIWFhZk127dvl8PhMAObJHXs2FEOh8OpJiwszAxskhQVFaWCggKlpKSYNV26dJHdbneqOXr0qH788cfLHse0adPM2zIdDodCQkKu4awAAAAAqG0sG9qysrIkSYGBgU7tgYGB5rKsrCy5u7vL19f3ijUBAQGlth8QEOBUc+l+fH195e7ufsWai58v1pRlwoQJys3NNX8yMjKufOAAAAAA8Cs1entkeVx626FhGJe9FfFyNWXVV0XNxUlIrtQfu93uNDoHAAAAABVh2ZG2oKAgSaVHsbKzs80RrqCgIBUWFionJ+eKNT///HOp7R87dsyp5tL95OTkqKio6Io12dnZkkqPBgIAAABAVbFsaGvatKmCgoK0fv16s62wsFCbN29Wp06dJEnh4eFyc3NzqsnMzNS+ffvMmoiICOXm5mrnzp1mzY4dO5Sbm+tUs2/fPmVmZpo1SUlJstvtCg8PN2u2bNni9BqApKQkBQcHq0mTJlV/AgAAAABANRza8vPzlZqaqtTUVEkXJh9JTU1Venq6bDab4uLiNHXqVK1du1b79u3TsGHD5OXlpZiYGEmSw+HQ8OHDFR8fr40bN2rv3r0aPHiw2rRpY84m2apVK/Xq1UsjRoxQcnKykpOTNWLECEVHRys0NFSSFBkZqdatWys2NlZ79+7Vxo0bNX78eI0YMUI+Pj6SLrw2wG63a9iwYdq3b5/Wrl2rqVOnXnHmSAAAAAC4VjX6TNvu3bvVrVs38/O4ceMkSUOHDtXixYv13HPP6ezZsxo1apRycnLUoUMHJSUlqW7duuY6b7zxhlxdXTVw4ECdPXtW3bt31+LFi+Xi4mLWLF++XGPHjjVnmezXr5/Tu+FcXFz0ySefaNSoUercubM8PT0VExOjmTNnmjUOh0Pr16/X008/rfbt28vX11fjxo0z+wwAAAAA1cFmXJxNA9dFXl6eHA6HcnNzzVG8mrJnzx6Fh4craOhs2YOaXbG2IOsHZS2JU0pKitq1a3edeggAAADcvMqbDSz7TBsAAAAAgNAGAAAAAJZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZm6dB2/vx5/fnPf1bTpk3l6emp22+/XS+//LJKSkrMGsMwNHnyZAUHB8vT01Ndu3bVN99847SdgoICjRkzRv7+/vL29la/fv105MgRp5qcnBzFxsbK4XDI4XAoNjZWp06dcqpJT09X37595e3tLX9/f40dO1aFhYXVdvwAAAAAYOnQ9vrrr2vevHmaO3eu0tLSNH36dM2YMUNz5swxa6ZPn66EhATNnTtXu3btUlBQkHr27KnTp0+bNXFxcVq7dq1WrVqlrVu3Kj8/X9HR0SouLjZrYmJilJqaqsTERCUmJio1NVWxsbHm8uLiYvXp00dnzpzR1q1btWrVKq1Zs0bx8fHX52QAAAAAqJVca7oDV7J9+3Y99NBD6tOnjySpSZMmWrlypXbv3i3pwijb7NmzNXHiRA0YMECStGTJEgUGBmrFihUaOXKkcnNztWDBAi1dulQ9evSQJC1btkwhISHasGGDoqKilJaWpsTERCUnJ6tDhw6SpPnz5ysiIkIHDhxQaGiokpKStH//fmVkZCg4OFiSNGvWLA0bNkxTpkyRj4/P9T49AAAAAGoBS4+03Xfffdq4caO+++47SdJXX32lrVu3qnfv3pKkQ4cOKSsrS5GRkeY6drtdXbp00bZt2yRJKSkpKioqcqoJDg5WWFiYWbN9+3Y5HA4zsElSx44d5XA4nGrCwsLMwCZJUVFRKigoUEpKymWPoaCgQHl5eU4/AAAAAFBelh5p+9Of/qTc3Fy1bNlSLi4uKi4u1pQpU/Too49KkrKysiRJgYGBTusFBgbq8OHDZo27u7t8fX1L1VxcPysrSwEBAaX2HxAQ4FRz6X58fX3l7u5u1pRl2rRpeumllypy2AAAAABgsvRI2+rVq7Vs2TKtWLFCe/bs0ZIlSzRz5kwtWbLEqc5mszl9NgyjVNulLq0pq74yNZeaMGGCcnNzzZ+MjIwr9gsAAAAAfs3SI21//OMf9fzzz+uRRx6RJLVp00aHDx/WtGnTNHToUAUFBUm6MArWoEEDc73s7GxzVCwoKEiFhYXKyclxGm3Lzs5Wp06dzJqff/651P6PHTvmtJ0dO3Y4Lc/JyVFRUVGpEbhfs9vtstvtlTl8AAAAALD2SNsvv/yiOnWcu+ji4mJO+d+0aVMFBQVp/fr15vLCwkJt3rzZDGTh4eFyc3NzqsnMzNS+ffvMmoiICOXm5mrnzp1mzY4dO5Sbm+tUs2/fPmVmZpo1SUlJstvtCg8Pr+IjBwAAAIALLD3S1rdvX02ZMkWNGjXSnXfeqb179yohIUFPPPGEpAu3K8bFxWnq1Klq3ry5mjdvrqlTp8rLy0sxMTGSJIfDoeHDhys+Pl5+fn6qX7++xo8frzZt2pizSbZq1Uq9evXSiBEj9O6770qSnnrqKUVHRys0NFSSFBkZqdatWys2NlYzZszQyZMnNX78eI0YMYKZIwEAAABUG0uHtjlz5mjSpEkaNWqUsrOzFRwcrJEjR+ovf/mLWfPcc8/p7NmzGjVqlHJyctShQwclJSWpbt26Zs0bb7whV1dXDRw4UGfPnlX37t21ePFiubi4mDXLly/X2LFjzVkm+/Xrp7lz55rLXVxc9Mknn2jUqFHq3LmzPD09FRMTo5kzZ16HMwEAAACgtrIZhmHUdCdqk7y8PDkcDuXm5tb4CN2ePXsUHh6uoKGzZQ9qdsXagqwflLUkTikpKWrXrt116iEAAABw8ypvNrD0M20AAAAAUNsR2gAAAADAwghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACysUqHt0KFDVd0PAAAAAEAZKhXamjVrpm7dumnZsmU6d+5cVfcJAAAAAPB/KhXavvrqK91zzz2Kj49XUFCQRo4cqZ07d1Z13wAAAACg1qtUaAsLC1NCQoJ++uknLVq0SFlZWbrvvvt05513KiEhQceOHavqfgIAAABArXRNE5G4urqqf//++tvf/qbXX39dBw8e1Pjx49WwYUMNGTJEmZmZVdVPAAAAAKiVrim07d69W6NGjVKDBg2UkJCg8ePH6+DBg9q0aZN++uknPfTQQ1XVTwAAAAColVwrs1JCQoIWLVqkAwcOqHfv3nr//ffVu3dv1alzIQM2bdpU7777rlq2bFmlnQUAAACA2qZSoe2dd97RE088occff1xBQUFl1jRq1EgLFiy4ps4BAAAAQG1XqdD2/fffX7XG3d1dQ4cOrczmAQAAAAD/p1LPtC1atEgffPBBqfYPPvhAS5YsueZOAQAAAAAuqFRoe+211+Tv71+qPSAgQFOnTr3mTgEAAAAALqhUaDt8+LCaNm1aqr1x48ZKT0+/5k4BAAAAAC6oVGgLCAjQ119/Xar9q6++kp+f3zV3CgAAAABwQaVC2yOPPKKxY8fqs88+U3FxsYqLi7Vp0yY988wzeuSRR6q6jwAAAABQa1Vq9shXX31Vhw8fVvfu3eXqemETJSUlGjJkCM+0AQAAAEAVqlRoc3d31+rVq/XKK6/oq6++kqenp9q0aaPGjRtXdf8AAAAAoFarVGi7qEWLFmrRokVV9QUAAAAAcIlKhbbi4mItXrxYGzduVHZ2tkpKSpyWb9q0qUo6BwAAAAC1XaVC2zPPPKPFixerT58+CgsLk81mq+p+AQAAAABUydC2atUq/e1vf1Pv3r2ruj8AAAAAgF+p1JT/7u7uatasWVX3BQAAAABwiUqFtvj4eP31r3+VYRhV3R8AAAAAwK9U6vbIrVu36rPPPtO///1v3XnnnXJzc3Na/uGHH1ZJ5wAAAACgtqtUaKtXr5769+9f1X0BAAAAAFyiUqFt0aJFVd0PAAAAAEAZKvVMmySdP39eGzZs0LvvvqvTp09Lko4ePar8/Pwq6xwAAAAA1HaVGmk7fPiwevXqpfT0dBUUFKhnz56qW7eupk+frnPnzmnevHlV3U8AAAAAqJUqNdL2zDPPqH379srJyZGnp6fZ3r9/f23cuLHKOgcAAAAAtV2lZ4/88ssv5e7u7tTeuHFj/fTTT1XSMQAAAABAJUfaSkpKVFxcXKr9yJEjqlu37jV3CgAAAABwQaVCW8+ePTV79mzzs81mU35+vl588UX17t27qvoGAAAAALVepW6PfOONN9StWze1bt1a586dU0xMjL7//nv5+/tr5cqVVd1HAAAAAKi1KhXagoODlZqaqpUrV2rPnj0qKSnR8OHD9dhjjzlNTAIAAAAAuDaVCm2S5OnpqSeeeEJPPPFEVfYHAAAAAPArlQpt77///hWXDxkypFKdAQAAAAA4q1Roe+aZZ5w+FxUV6ZdffpG7u7u8vLwIbQAAAABQRSo1e2ROTo7TT35+vg4cOKD77ruPiUgAAAAAoApVKrSVpXnz5nrttddKjcIBAAAAACqvykKbJLm4uOjo0aNVuUkAAAAAqNUq9Uzbxx9/7PTZMAxlZmZq7ty56ty5c5V0DAAAAABQydD28MMPO3222Wy69dZb9cADD2jWrFlV0S8AAAAAgCoZ2kpKSqq6HwAAAACAMlTpM20AAAAAgKpVqZG2cePGlbs2ISGhMrsAAAAAAKiSoW3v3r3as2ePzp8/r9DQUEnSd999JxcXF7Vr186ss9lsVdNLAAAAAKilKhXa+vbtq7p162rJkiXy9fWVdOGF248//rjuv/9+xcfHV2knAQAAAKC2qtQzbbNmzdK0adPMwCZJvr6+evXVV5k9EgAAAACqUKVCW15enn7++edS7dnZ2Tp9+vQ1dwoAAAAAcEGlQlv//v31+OOP6+9//7uOHDmiI0eO6O9//7uGDx+uAQMGVHUfAQAAAKDWqtQzbfPmzdP48eM1ePBgFRUVXdiQq6uGDx+uGTNmVGkHAQAAAKA2q1Ro8/Ly0ttvv60ZM2bo4MGDMgxDzZo1k7e3d1X3DwAAAABqtWt6uXZmZqYyMzPVokULeXt7yzCMquoXAAAAAECVDG0nTpxQ9+7d1aJFC/Xu3VuZmZmSpCeffLLKp/v/6aefNHjwYPn5+cnLy0tt27ZVSkqKudwwDE2ePFnBwcHy9PRU165d9c033zhto6CgQGPGjJG/v7+8vb3Vr18/HTlyxKkmJydHsbGxcjgccjgcio2N1alTp5xq0tPT1bdvX3l7e8vf319jx45VYWFhlR4vAAAAAPxapULbs88+Kzc3N6Wnp8vLy8tsHzRokBITE6usczk5OercubPc3Nz073//W/v379esWbNUr149s2b69OlKSEjQ3LlztWvXLgUFBalnz55Os1jGxcVp7dq1WrVqlbZu3ar8/HxFR0eruLjYrImJiVFqaqoSExOVmJio1NRUxcbGmsuLi4vVp08fnTlzRlu3btWqVau0Zs0a3kkHAAAAoFpV6pm2pKQkffrpp2rYsKFTe/PmzXX48OEq6Zgkvf766woJCdGiRYvMtiZNmpj/3zAMzZ49WxMnTjRnrVyyZIkCAwO1YsUKjRw5Urm5uVqwYIGWLl2qHj16SJKWLVumkJAQbdiwQVFRUUpLS1NiYqKSk5PVoUMHSdL8+fMVERGhAwcOKDQ0VElJSdq/f78yMjIUHBws6cL76oYNG6YpU6bIx8enyo4bAAAAAC6q1EjbmTNnnEbYLjp+/Ljsdvs1d+qijz/+WO3bt9f/+3//TwEBAbrnnns0f/58c/mhQ4eUlZWlyMhIs81ut6tLly7atm2bJCklJUVFRUVONcHBwQoLCzNrtm/fLofDYQY2SerYsaMcDodTTVhYmBnYJCkqKkoFBQVOt2teqqCgQHl5eU4/AAAAAFBelQptv/3tb/X++++bn202m0pKSjRjxgx169atyjr33//+V++8846aN2+uTz/9VL///e81duxYc99ZWVmSpMDAQKf1AgMDzWVZWVlyd3eXr6/vFWsCAgJK7T8gIMCp5tL9+Pr6yt3d3awpy7Rp08zn5BwOh0JCQipyCgAAAADUcpW6PXLGjBnq2rWrdu/ercLCQj333HP65ptvdPLkSX355ZdV1rmSkhK1b99eU6dOlSTdc889+uabb/TOO+9oyJAhZp3NZnNazzCMUm2XurSmrPrK1FxqwoQJGjdunPk5Ly+P4AYAAACg3Co10ta6dWt9/fXXuvfee9WzZ0+dOXNGAwYM0N69e3XHHXdUWecaNGig1q1bO7W1atVK6enpkqSgoCBJKjXSlZ2dbY6KBQUFqbCwUDk5OVes+fnnn0vt/9ixY041l+4nJydHRUVFpUbgfs1ut8vHx8fpBwAAAADKq8KhraioSN26dVNeXp5eeuklrVu3Tv/617/06quvqkGDBlXauc6dO+vAgQNObd99950aN24sSWratKmCgoK0fv16c3lhYaE2b96sTp06SZLCw8Pl5ubmVJOZmal9+/aZNREREcrNzdXOnTvNmh07dig3N9epZt++febrDaQLE7LY7XaFh4dX6XEDAAAAwEUVvj3Szc1N+/btu+rth1Xh2WefVadOnTR16lQNHDhQO3fu1Hvvvaf33ntP0oXbFePi4jR16lQ1b95czZs319SpU+Xl5aWYmBhJksPh0PDhwxUfHy8/Pz/Vr19f48ePV5s2bczZJFu1aqVevXppxIgRevfddyVJTz31lKKjoxUaGipJioyMVOvWrRUbG6sZM2bo5MmTGj9+vEaMGMHoGQAAAIBqU6nbI4cMGaIFCxZUdV9K+c1vfqO1a9dq5cqVCgsL0yuvvKLZs2frscceM2uee+45xcXFadSoUWrfvr1++uknJSUlqW7dumbNG2+8oYcfflgDBw5U586d5eXlpX/+859ycXExa5YvX642bdooMjJSkZGRuuuuu7R06VJzuYuLiz755BN5eHioc+fOGjhwoB5++GHNnDmz2s8DAAAAgNrLZhiGUdGVxowZo/fff1/NmjVT+/bt5e3t7bQ8ISGhyjp4s8nLy5PD4VBubm6Nj9Dt2bNH4eHhCho6W/agZlesLcj6QVlL4pSSkqJ27dpdpx4CAAAAN6/yZoMK3R753//+V02aNNG+ffvMv7h/9913TjXX47ZJAAAAAKgtKhTamjdvrszMTH322WeSpEGDBunNN9+84uyJAAAAAIDKq9AzbZfeSfnvf/9bZ86cqdIOAQAAAAD+f5WaiOSiSjwOBwAAAACogAqFNpvNVuqZNZ5hAwAAAIDqU6Fn2gzD0LBhw2S32yVJ586d0+9///tSs0d++OGHVddDAAAAAKjFKhTahg4d6vR58ODBVdoZAAAAAICzCoW2RYsWVVc/AAAAAABluKaJSAAAAAAA1YvQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCXGu6A7ixpKWllbvW399fjRo1qsbeAAAAADc/QhvKpTg/R7LZNHjw4HKv4+HppQPfphHcAAAAgGtAaEO5lBTkS4Yhv+h4ufmFXLW+6ESGTqybpePHjxPaAAAAgGtAaEOFuPmFyB7UrKa7AQAAANQaTEQCAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZ2Q4W2adOmyWazKS4uzmwzDEOTJ09WcHCwPD091bVrV33zzTdO6xUUFGjMmDHy9/eXt7e3+vXrpyNHjjjV5OTkKDY2Vg6HQw6HQ7GxsTp16pRTTXp6uvr27Stvb2/5+/tr7NixKiwsrK7DBQAAAIAbJ7Tt2rVL7733nu666y6n9unTpyshIUFz587Vrl27FBQUpJ49e+r06dNmTVxcnNauXatVq1Zp69atys/PV3R0tIqLi82amJgYpaamKjExUYmJiUpNTVVsbKy5vLi4WH369NGZM2e0detWrVq1SmvWrFF8fHz1HzwAAACAWsu1pjtQHvn5+Xrsscc0f/58vfrqq2a7YRiaPXu2Jk6cqAEDBkiSlixZosDAQK1YsUIjR45Ubm6uFixYoKVLl6pHjx6SpGXLlikkJEQbNmxQVFSU0tLSlJiYqOTkZHXo0EGSNH/+fEVEROjAgQMKDQ1VUlKS9u/fr4yMDAUHB0uSZs2apWHDhmnKlCny8fG5zmflxpCWllbuWn9/fzVq1KgaewMAAADceG6I0Pb000+rT58+6tGjh1NoO3TokLKyshQZGWm22e12denSRdu2bdPIkSOVkpKioqIip5rg4GCFhYVp27ZtioqK0vbt2+VwOMzAJkkdO3aUw+HQtm3bFBoaqu3btyssLMwMbJIUFRWlgoICpaSkqFu3bmX2vaCgQAUFBebnvLy8KjknVlecnyPZbBo8eHC51/Hw9NKBb9MIbgAAAMCvWD60rVq1Snv27NGuXbtKLcvKypIkBQYGOrUHBgbq8OHDZo27u7t8fX1L1VxcPysrSwEBAaW2HxAQ4FRz6X58fX3l7u5u1pRl2rRpeumll652mDedkoJ8yTDkFx0vN7+Qq9YXncjQiXWzdPz4cUIbAAAA8CuWDm0ZGRl65plnlJSUJA8Pj8vW2Ww2p8+GYZRqu9SlNWXVV6bmUhMmTNC4cePMz3l5eQoJuXqIuVm4+YXIHtSsprsBAAAA3LAsPRFJSkqKsrOzFR4eLldXV7m6umrz5s1688035erqao58XTrSlZ2dbS4LCgpSYWGhcnJyrljz888/l9r/sWPHnGou3U9OTo6KiopKjcD9mt1ul4+Pj9MPAAAAAJSXpUNb9+7d9Z///EepqanmT/v27fXYY48pNTVVt99+u4KCgrR+/XpzncLCQm3evFmdOnWSJIWHh8vNzc2pJjMzU/v27TNrIiIilJubq507d5o1O3bsUG5urlPNvn37lJmZadYkJSXJbrcrPDy8Ws8DAAAAgNrL0rdH1q1bV2FhYU5t3t7e8vPzM9vj4uI0depUNW/eXM2bN9fUqVPl5eWlmJgYSZLD4dDw4cMVHx8vPz8/1a9fX+PHj1ebNm3M2SRbtWqlXr16acSIEXr33XclSU899ZSio6MVGhoqSYqMjFTr1q0VGxurGTNm6OTJkxo/frxGjBjB6BkAAACAamPp0FYezz33nM6ePatRo0YpJydHHTp0UFJSkurWrWvWvPHGG3J1ddXAgQN19uxZde/eXYsXL5aLi4tZs3z5co0dO9acZbJfv36aO3euudzFxUWffPKJRo0apc6dO8vT01MxMTGaOXPm9TtYAAAAALXODRfaPv/8c6fPNptNkydP1uTJky+7joeHh+bMmaM5c+ZctqZ+/fpatmzZFffdqFEjrVu3riLdBQAAAIBrYuln2gAAAACgtiO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWJhrTXcA+LW0tLRy1/r7+6tRo0bV2BsAAACg5hHaYAnF+TmSzabBgweXex0PTy8d+DaN4AYAAICbGqENllBSkC8Zhvyi4+XmF3LV+qITGTqxbpaOHz9OaAMAAMBNjdAGS3HzC5E9qFlNdwMAAACwDCYiAQAAAAALI7QBAAAAgIUR2gAAAADAwghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyM0AYAAAAAFkZoAwAAAAALI7QBAAAAgIW51nQHgGuRlpZW7lp/f381atSoGnsDAAAAVD1CG25Ixfk5ks2mwYMHl3sdD08vHfg2jeAGAACAGwqhDTekkoJ8yTDkFx0vN7+Qq9YXncjQiXWzdPz4cUIbAAAAbiiENtzQ3PxCZA9qVtPdAAAAAKoNE5EAAAAAgIUR2gAAAADAwghtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyM0AYAAAAAFsZ72lCrpKWllavO39+fl3ADAADAEghtqBWK83Mkm02DBw8uV72Hp5cOfJtGcAMAAECNI7ShVigpyJcMQ37R8XLzC7libdGJDJ1YN0vHjx8ntAEAAKDGEdpQq7j5hcge1KymuwEAAACUGxORAAAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABZGaAMAAAAACyO0AQAAAICFudZ0BwCrSktLK3etv7+/GjVqVI29AQAAQG1FaAMuUZyfI9lsGjx4cLnX8fD00oFv0whuAAAAqHKENuASJQX5kmHILzpebn4hV60vOpGhE+tm6fjx44Q2AAAAVDlCG3AZbn4hsgc1q+luAAAAoJZjIhIAAAAAsDBCGwAAAABYGKENAAAAACzM0qFt2rRp+s1vfqO6desqICBADz/8sA4cOOBUYxiGJk+erODgYHl6eqpr16765ptvnGoKCgo0ZswY+fv7y9vbW/369dORI0ecanJychQbGyuHwyGHw6HY2FidOnXKqSY9PV19+/aVt7e3/P39NXbsWBUWFlbLsQMAAACAZPHQtnnzZj399NNKTk7W+vXrdf78eUVGRurMmTNmzfTp05WQkKC5c+dq165dCgoKUs+ePXX69GmzJi4uTmvXrtWqVau0detW5efnKzo6WsXFxWZNTEyMUlNTlZiYqMTERKWmpio2NtZcXlxcrD59+ujMmTPaunWrVq1apTVr1ig+Pv76nAwAAAAAtZKlZ49MTEx0+rxo0SIFBAQoJSVFv/3tb2UYhmbPnq2JEydqwIABkqQlS5YoMDBQK1as0MiRI5Wbm6sFCxZo6dKl6tGjhyRp2bJlCgkJ0YYNGxQVFaW0tDQlJiYqOTlZHTp0kCTNnz9fEREROnDggEJDQ5WUlKT9+/crIyNDwcHBkqRZs2Zp2LBhmjJlinx8fMo8hoKCAhUUFJif8/Lyqvw8AQAAALh5WXqk7VK5ubmSpPr160uSDh06pKysLEVGRpo1drtdXbp00bZt2yRJKSkpKioqcqoJDg5WWFiYWbN9+3Y5HA4zsElSx44d5XA4nGrCwsLMwCZJUVFRKigoUEpKymX7PG3aNPOWS4fDoZCQq7/3CwAAAAAusvRI268ZhqFx48bpvvvuU1hYmCQpKytLkhQYGOhUGxgYqMOHD5s17u7u8vX1LVVzcf2srCwFBASU2mdAQIBTzaX78fX1lbu7u1lTlgkTJmjcuHHm57y8PILbTSotLa3ctf7+/ryIGwAAAOVyw4S20aNH6+uvv9bWrVtLLbPZbE6fDcMo1XapS2vKqq9MzaXsdrvsdvsV+4IbW3F+jmSzafDgweVex8PTSwe+TSO4AQAA4KpuiNA2ZswYffzxx9qyZYsaNmxotgcFBUm6MArWoEEDsz07O9scFQsKClJhYaFycnKcRtuys7PVqVMns+bnn38utd9jx445bWfHjh1Oy3NyclRUVFRqBA61S0lBvmQY8ouOl5vf1UdRi05k6MS6WTp+/DihDQAAAFdl6WfaDMPQ6NGj9eGHH2rTpk1q2rSp0/KmTZsqKChI69evN9sKCwu1efNmM5CFh4fLzc3NqSYzM1P79u0zayIiIpSbm6udO3eaNTt27FBubq5Tzb59+5SZmWnWJCUlyW63Kzw8vOoPHjccN78Q2YOaXfWnPMEOAAAAuMjSI21PP/20VqxYoX/84x+qW7eu+eyYw+GQp6enbDab4uLiNHXqVDVv3lzNmzfX1KlT5eXlpZiYGLN2+PDhio+Pl5+fn+rXr6/x48erTZs25mySrVq1Uq9evTRixAi9++67kqSnnnpK0dHRCg0NlSRFRkaqdevWio2N1YwZM3Ty5EmNHz9eI0aMuOzMkQAAAABwrSwd2t555x1JUteuXZ3aFy1apGHDhkmSnnvuOZ09e1ajRo1STk6OOnTooKSkJNWtW9esf+ONN+Tq6qqBAwfq7Nmz6t69uxYvXiwXFxezZvny5Ro7dqw5y2S/fv00d+5cc7mLi4s++eQTjRo1Sp07d5anp6diYmI0c+bMajp6AAAAALB4aDMM46o1NptNkydP1uTJky9b4+HhoTlz5mjOnDmXralfv76WLVt2xX01atRI69atu2qfAAAAAKCqWPqZNgAAAACo7QhtAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACzM0rNHAjeztLS0ctX5+/urUaNG1dwbAAAAWBWhDbjOivNzJJtNgwcPLle9h6eXDnybRnADAACopQhtwHVWUpAvGYb8ouPl5hdyxdqiExk6sW6Wjh8/TmgDAACopQhtQA1x8wuRPahZTXcDAAAAFsdEJAAAAABgYYQ2AAAAALAwQhsAAAAAWBihDQAAAAAsjNAGAAAAABbG7JHADaC8L+KWeBk3AADAzYbQBlhYRV/ELfEybgAAgJsNoQ2wsIq8iFviZdwAAAA3I0IbcAPgRdwAAAC1FxORAAAAAICFEdoAAAAAwMIIbQAAAABgYYQ2AAAAALAwJiIBbkK81w0AAODmQWgDbiK81w0AAODmQ2gDbiK81w0AAODmQ2gDbkK81w0AAODmwUQkAAAAAGBhhDYAAAAAsDBCGwAAAABYGKENAAAAACyMiUgA8F43AAAACyO0AbUY73UDAACwPkIbUIvxXjcAAADrI7QB4L1uAAAAFsZEJAAAAABgYYy0Aaiw8k5cwqQlAAAA147QBqDcKjpxCZOWAAAAXDtCG4Byq8jEJUxaAgAAUDUIbQAqjIlLAAAArh8mIgEAAAAAC2OkDUC1Ku+kJRITlwAAAJSF0AagWlR00hKJiUsAAADKQmgDUC0qMmmJxMQlAAAAl0NoA1CtmLQEAADg2hDaAFgKz8ABAAA4I7QBsASegQMAACgboQ2AJfAMHAAAQNkIbQAspaLPwHE7JQAAuNkR2gDckLidEgAA1BaENgA3JG6nBAAAtQWhDcANrbpup+RWSgAAYBWENgC1QkVvp+RWSgAAYBWENgC1QkVup7x4K+UXX3yhVq1alWv7jMwBAIDqQmgDUKuU53bKykxyYrd7aM2av6tBgwblqifkAQCA8iK0AcAlKjrJybkj3+jUpv9VdHR0ufdByAMAAOVFaAOAyyjvJCdFJzKqPeTxjB0AALUXoQ0Aqkh1hbyKPmPHqBwAADcXQhsA1JDyhryKPmNX0VsvCwoKZLfby1UrEQoBALjeCG2V8Pbbb2vGjBnKzMzUnXfeqdmzZ+v++++v6W4BuElV5Bm7ytx6KVsdySgpd3l1hkICIQAApRHaKmj16tWKi4vT22+/rc6dO+vdd9/Vgw8+qP379/MXDQDVqjwjcxW99fLsf3cr94tl1fo8XkVCYXWPElZnPYETAFBdCG0VlJCQoOHDh+vJJ5+UJM2ePVuffvqp3nnnHU2bNq2GewcAF1To+bqK1ldTKLweo4TVWW+lwGmlMFvd9dzeC6A2ILRVQGFhoVJSUvT88887tUdGRmrbtm1lrlNQUKCCggLzc25uriQpLy+v+jpaTvn5+ZKkgqwfVFJ47oq1F/9iV57aG72evlRNvZX6UtF6K/WlovXXqy8lRQXlqjfOF5a7vuSXXMkw5PObAXJx3HrVbRce/U5n9n9mifqiYz8q/6tPKxY4ZZNkVFN9dW7bavUV27a73UPLlr6vwMDActXXqVNHJSXlD/oVqa/ObVd3vZX6UtF6+lIz9VbqS1BQkIKCgsq97ep0MRMYxpW/x2zG1SpgOnr0qG677TZ9+eWX6tSpk9k+depULVmyRAcOHCi1zuTJk/XSSy9dz24CAAAAuIFkZGSoYcOGl13OSFsl2Gw2p8+GYZRqu2jChAkaN26c+bmkpEQnT56Un5/fZde5XvLy8hQSEqKMjAz5+PjUaF9Qe3DdoSZw3aEmcN2hpnDt3TgMw9Dp06cVHBx8xTpCWwX4+/vLxcVFWVlZTu3Z2dmXvc3CbreXute+Xr161dXFSvHx8eEXGtcd1x1qAtcdagLXHWoK196NweFwXLWmznXox03D3d1d4eHhWr9+vVP7+vXrnW6XBAAAAICqwkhbBY0bN06xsbFq3769IiIi9N577yk9PV2///3va7prAAAAAG5ChLYKGjRokE6cOKGXX35ZmZmZCgsL07/+9S81bty4prtWYXa7XS+++GKFpkoGrhXXHWoC1x1qAtcdagrX3s2H2SMBAAAAwMJ4pg0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHabiJvv/22mjZtKg8PD4WHh+uLL764Yv3mzZsVHh4uDw8P3X777Zo3b16pmjVr1qh169ay2+1q3bq11q5dW13dxw2qqq+7xYsXy2azlfo5d+5cdR4GbjAVue4yMzMVExOj0NBQ1alTR3FxcWXW8X2H8qjqa4/vPJRHRa67Dz/8UD179tStt94qHx8fRURE6NNPPy1Vx3fejYXQdpNYvXq14uLiNHHiRO3du1f333+/HnzwQaWnp5dZf+jQIfXu3Vv333+/9u7dqxdeeEFjx47VmjVrzJrt27dr0KBBio2N1VdffaXY2FgNHDhQO3bsuF6HBYurjutOknx8fJSZmen04+HhcT0OCTeAil53BQUFuvXWWzVx4kTdfffdZdbwfYfyqI5rT+I7D1dW0etuy5Yt6tmzp/71r38pJSVF3bp1U9++fbV3716zhu+8G5CBm8K9995r/P73v3dqa9mypfH888+XWf/cc88ZLVu2dGobOXKk0bFjR/PzwIEDjV69ejnVREVFGY888kgV9Ro3uuq47hYtWmQ4HI4q7ytuHhW97n6tS5cuxjPPPFOqne87lEd1XHt85+FqruW6u6h169bGSy+9ZH7mO+/Gw0jbTaCwsFApKSmKjIx0ao+MjNS2bdvKXGf79u2l6qOiorR7924VFRVdseZy20TtUl3XnSTl5+ercePGatiwoaKjo53+dRC1W2Wuu/Lg+w5XU13XnsR3Hi6vKq67kpISnT59WvXr1zfb+M678RDabgLHjx9XcXGxAgMDndoDAwOVlZVV5jpZWVll1p8/f17Hjx+/Ys3ltonapbquu5YtW2rx4sX6+OOPtXLlSnl4eKhz5876/vvvq+dAcEOpzHVXHnzf4Wqq69rjOw9XUhXX3axZs3TmzBkNHDjQbOM778bjWtMdQNWx2WxOnw3DKNV2tfpL2yu6TdQ+VX3ddezYUR07djSXd+7cWe3atdOcOXP05ptvVlW3cYOrju8mvu9QHlV9nfCdh/Ko7HW3cuVKTZ48Wf/4xz8UEBBQJdtEzSC03QT8/f3l4uJS6l9HsrOzS/0rykVBQUFl1ru6usrPz++KNZfbJmqX6rruLlWnTh395je/4V+dIaly11158H2Hq6mua+9SfOfh167lulu9erWGDx+uDz74QD169HBaxnfejYfbI28C7u7uCg8P1/r1653a169fr06dOpW5TkRERKn6pKQktW/fXm5ublesudw2UbtU13V3KcMwlJqaqgYNGlRNx3FDq8x1Vx583+FqquvauxTfefi1yl53K1eu1LBhw7RixQr16dOn1HK+825ANTP/CaraqlWrDDc3N2PBggXG/v37jbi4OMPb29v48ccfDcMwjOeff96IjY016//73/8aXl5exrPPPmvs37/fWLBggeHm5mb8/e9/N2u+/PJLw8XFxXjttdeMtLQ047XXXjNcXV2N5OTk6358sKbquO4mT55sJCYmGgcPHjT27t1rPP7444arq6uxY8eO6358sKaKXneGYRh79+419u7da4SHhxsxMTHG3r17jW+++cZczvcdyqM6rj2+83A1Fb3uVqxYYbi6uhpvvfWWkZmZaf6cOnXKrOE778ZDaLuJvPXWW0bjxo0Nd3d3o127dsbmzZvNZUOHDjW6dOniVP/5558b99xzj+Hu7m40adLEeOedd0pt84MPPjBCQ0MNNzc3o2XLlsaaNWuq+zBwg6nq6y4uLs5o1KiR4e7ubtx6661GZGSksW3btutxKLiBVPS6k1Tqp3Hjxk41fN+hPKr62uM7D+VRkeuuS5cuZV53Q4cOddom33k3Fpth/N8sAAAAAAAAy+GZNgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAqonNZtNHH31U090AANzgCG0AgFqtb9++6tGjR5nLtm/fLpvNpj179lRq25mZmXrwwQevpXsAABDaAAC12/Dhw7Vp0yYdPny41LKFCxeqbdu2ateuXYW2WVhYKEkKCgqS3W6vkn4CAGovQhsAoFaLjo5WQECAFi9e7NT+yy+/aPXq1Xr44Yf16KOPqmHDhvLy8lKbNm20cuVKp9quXbtq9OjRGjdunPz9/dWzZ09JpW+P/NOf/qQWLVrIy8tLt99+uyZNmqSioiJz+eTJk9W2bVstXbpUTZo0kcPh0COPPKLTp0+bNSUlJXr99dfVrFkz2e12NWrUSFOmTDGX//TTTxo0aJB8fX3l5+enhx56SD/++GPVnTAAwHVHaAMA1Gqurq4aMmSIFi9eLMMwzPYPPvhAhYWFevLJJxUeHq5169Zp3759euqppxQbG6sdO3Y4bWfJkiVydXXVl19+qXfffbfMfdWtW1eLFy/W/v379de//lXz58/XG2+84VRz8OBBffTRR1q3bp3WrVunzZs367XXXjOXT5gwQa+//romTZqk/fv3a8WKFQoMDJR0IWh269ZNt9xyi7Zs2aKtW7fqlltuUa9evczRPwDAjcdm/PpPKAAAaqFvv/1WrVq10qZNm9StWzdJUpcuXXTbbbdpxYoVper79OmjVq1aaebMmZIujLTl5uZq7969TnU2m01r167Vww8/XOZ+Z8yYodWrV2v37t2SLoy0zZgxQ1lZWapbt64k6bnnntOWLVuUnJys06dP69Zbb9XcuXP15JNPltrewoULNX36dKWlpclms0m6cKtmvXr19NFHHykyMrJyJwgAUKNca7oDAADUtJYtW6pTp05auHChunXrpoMHD+qLL75QUlKSiouL9dprr2n16tX66aefVFBQoIKCAnl7eztto3379lfdz9///nfNnj1bP/zwg/Lz83X+/Hn5+Pg41TRp0sQMbJLUoEEDZWdnS5LS0tJUUFCg7t27l7n9lJQU/fDDD07rS9K5c+d08ODBcp0LAID1ENoAANCFCUlGjx6tt956S4sWLVLjxo3VvXt3zZgxQ2+88YZmz56tNm3ayNvbW3FxcaVuN7w0xF0qOTlZjzzyiF566SVFRUXJ4XBo1apVmjVrllOdm5ub02ebzaaSkhJJkqen5xX3UVJSovDwcC1fvrzUsltvvfWK6wIArIvQBgCApIEDB+qZZ57RihUrtGTJEo0YMUI2m01ffPGFHnroIQ0ePFjShWD0/fffq1WrVhXa/pdffqnGjRtr4sSJZltZM1ZeSfPmzeXp6amNGzeWeXtku3bttHr1agUEBJQawQMA3LiYiAQAAEm33HKLBg0apBdeeEFHjx7VsGHDJEnNmjXT+vXrtW3bNqWlpWnkyJHKysqq8PabNWum9PR0rVq1SgcPHtSbb76ptWvXVmgbHh4e+tOf/qTnnntO77//vg4ePKjk5GQtWLBAkvTYY4/J399fDz30kL744gsdOnRImzdv1jPPPKMjR45UuM8AAGsgtAEA8H+GDx+unJwc9ejRQ40aNZIkTZo0Se3atVNUVJS6du2qoKCgy04sciUPPfSQnn32WY0ePVpt27bVtm3bNGnSpApvZ9KkSYqPj9df/vIXtWrVSoMGDTKfefPy8tKWLVvUqFEjDRgwQK1atdITTzyhs2fPMvIGADcwZo8EAAAAAAtjpA0AAAAALIzQBgAAAAAWRmgDAAAAAAsjtAEAAACAhRHaAAAAAMDCCG0AAAAAYGGENgAAAACwMEIbAAAAAFgYoQ0AAAAALIzQBgAAAAAWRmgDAAAAAAv7/wD7PkE7n22qrQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(column_variances, bins=60, edgecolor='k')\n",
    "plt.title('Histogram of Column Variances')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1664ddfe-6b59-4180-be81-9ce4accadf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30442\n",
      "2596\n"
     ]
    }
   ],
   "source": [
    "print((column_variances> 0.05).sum())\n",
    "print((column_variances> 0.1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab56167a-7d3e-486f-ae00-a27a35010788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2227\n",
      "0.0115\n",
      "6.884e-05\n"
     ]
    }
   ],
   "source": [
    "print(column_variances.max())\n",
    "print(column_variances.median())\n",
    "print(column_variances.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de0d52-c5c8-4296-be78-04461c4551dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdfbbe-576a-4d4f-8371-2fadda3cf0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c69b0a-a852-4578-aebd-7fd5baa2ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862daf6f-10b0-49a2-8fa3-93e9e9bf0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data_filtered[column_variances.index[(column_variances>0.1)]].to_csv('../data/dimension_reduction/highly_variable_features/numerical_data_filtered_0_10.csv')\n",
    "numerical_data_filtered[column_variances.index[(column_variances>0.05)]].to_csv('../data/dimension_reduction/highly_variable_features/numerical_data_filtered_0_05.csv')\n",
    "metadata_columns_with_labels = metadata_columns + [label_column, sex_condition_column, age_condition_column, 'labels_encoded']\n",
    "df_metadata = df[metadata_columns_with_labels]\n",
    "df_metadata.to_csv('../data/dimension_reduction/highly_variable_features/metadata_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6277fb7-a351-4bd7-be4e-09fb205bb576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg00011616</th>\n",
       "      <th>cg00015530</th>\n",
       "      <th>cg00017461</th>\n",
       "      <th>cg00025044</th>\n",
       "      <th>cg00025496</th>\n",
       "      <th>cg00032912</th>\n",
       "      <th>cg00035969</th>\n",
       "      <th>cg00041401</th>\n",
       "      <th>cg00041575</th>\n",
       "      <th>cg00041666</th>\n",
       "      <th>...</th>\n",
       "      <th>rs5931272</th>\n",
       "      <th>rs6546473</th>\n",
       "      <th>rs7660805</th>\n",
       "      <th>rs7746156</th>\n",
       "      <th>rs798149</th>\n",
       "      <th>rs877309</th>\n",
       "      <th>rs9292570</th>\n",
       "      <th>rs9363764</th>\n",
       "      <th>rs939290</th>\n",
       "      <th>rs951295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM2947470</th>\n",
       "      <td>0.394043</td>\n",
       "      <td>0.937988</td>\n",
       "      <td>0.578613</td>\n",
       "      <td>0.797852</td>\n",
       "      <td>0.036255</td>\n",
       "      <td>0.633789</td>\n",
       "      <td>0.746582</td>\n",
       "      <td>0.830566</td>\n",
       "      <td>0.895996</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947471</th>\n",
       "      <td>0.438721</td>\n",
       "      <td>0.897949</td>\n",
       "      <td>0.614258</td>\n",
       "      <td>0.731934</td>\n",
       "      <td>0.045563</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.897461</td>\n",
       "      <td>0.897461</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947472</th>\n",
       "      <td>0.398926</td>\n",
       "      <td>0.928223</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.042694</td>\n",
       "      <td>0.630371</td>\n",
       "      <td>0.722656</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>0.167480</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947473</th>\n",
       "      <td>0.373291</td>\n",
       "      <td>0.878906</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>0.793457</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.747559</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>0.884766</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947474</th>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.852051</td>\n",
       "      <td>0.590820</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.458740</td>\n",
       "      <td>0.745117</td>\n",
       "      <td>0.785645</td>\n",
       "      <td>0.846191</td>\n",
       "      <td>0.169556</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4565216</th>\n",
       "      <td>0.549316</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>0.951172</td>\n",
       "      <td>0.959473</td>\n",
       "      <td>0.958496</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.686035</td>\n",
       "      <td>0.074524</td>\n",
       "      <td>0.368896</td>\n",
       "      <td>0.031952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979980</td>\n",
       "      <td>0.518066</td>\n",
       "      <td>0.689941</td>\n",
       "      <td>0.979980</td>\n",
       "      <td>0.982910</td>\n",
       "      <td>0.546387</td>\n",
       "      <td>0.514160</td>\n",
       "      <td>0.970215</td>\n",
       "      <td>0.525879</td>\n",
       "      <td>0.210815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4565217</th>\n",
       "      <td>0.492676</td>\n",
       "      <td>0.970215</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>0.354248</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>0.711426</td>\n",
       "      <td>0.979980</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.490723</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>0.193726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4565218</th>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.961914</td>\n",
       "      <td>0.967285</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.976074</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>0.070923</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981934</td>\n",
       "      <td>0.579102</td>\n",
       "      <td>0.681641</td>\n",
       "      <td>0.979980</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>0.579102</td>\n",
       "      <td>0.496826</td>\n",
       "      <td>0.964844</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>0.190918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4565219</th>\n",
       "      <td>0.472168</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.964844</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.688477</td>\n",
       "      <td>0.081909</td>\n",
       "      <td>0.350098</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982422</td>\n",
       "      <td>0.568359</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.982422</td>\n",
       "      <td>0.983398</td>\n",
       "      <td>0.588379</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.580078</td>\n",
       "      <td>0.223755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4565220</th>\n",
       "      <td>0.494141</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.962402</td>\n",
       "      <td>0.950684</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.704102</td>\n",
       "      <td>0.096863</td>\n",
       "      <td>0.373291</td>\n",
       "      <td>0.034943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.982422</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.500488</td>\n",
       "      <td>0.965332</td>\n",
       "      <td>0.545410</td>\n",
       "      <td>0.224121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37067 rows × 2596 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cg00011616  cg00015530  cg00017461  cg00025044  cg00025496  \\\n",
       "GSM2947470    0.394043    0.937988    0.578613    0.797852    0.036255   \n",
       "GSM2947471    0.438721    0.897949    0.614258    0.731934    0.045563   \n",
       "GSM2947472    0.398926    0.928223    0.555664    0.820312    0.042694   \n",
       "GSM2947473    0.373291    0.878906    0.606934    0.793457    0.040649   \n",
       "GSM2947474    0.699219    0.852051    0.590820    0.833008    0.064209   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "GSM4565216    0.549316    0.978027    0.951172    0.959473    0.958496   \n",
       "GSM4565217    0.492676    0.970215    0.969727    0.966797    0.958984   \n",
       "GSM4565218    0.445312    0.978516    0.961914    0.967285    0.958008   \n",
       "GSM4565219    0.472168    0.974609    0.958008    0.964844    0.946289   \n",
       "GSM4565220    0.494141    0.977051    0.949219    0.962402    0.950684   \n",
       "\n",
       "            cg00032912  cg00035969  cg00041401  cg00041575  cg00041666  ...  \\\n",
       "GSM2947470    0.633789    0.746582    0.830566    0.895996    0.143555  ...   \n",
       "GSM2947471    0.640625    0.791016    0.897461    0.897461    0.125244  ...   \n",
       "GSM2947472    0.630371    0.722656    0.819824    0.865234    0.167480  ...   \n",
       "GSM2947473    0.612305    0.747559    0.782227    0.884766    0.175781  ...   \n",
       "GSM2947474    0.458740    0.745117    0.785645    0.846191    0.169556  ...   \n",
       "...                ...         ...         ...         ...         ...  ...   \n",
       "GSM4565216    0.980469    0.686035    0.074524    0.368896    0.031952  ...   \n",
       "GSM4565217    0.980957    0.703125    0.045166    0.354248    0.029083  ...   \n",
       "GSM4565218    0.976074    0.715820    0.070923    0.320801    0.027283  ...   \n",
       "GSM4565219    0.978516    0.688477    0.081909    0.350098    0.039124  ...   \n",
       "GSM4565220    0.980957    0.704102    0.096863    0.373291    0.034943  ...   \n",
       "\n",
       "            rs5931272  rs6546473  rs7660805  rs7746156  rs798149  rs877309  \\\n",
       "GSM2947470        NaN        NaN        NaN        NaN       NaN       NaN   \n",
       "GSM2947471        NaN        NaN        NaN        NaN       NaN       NaN   \n",
       "GSM2947472        NaN        NaN        NaN        NaN       NaN       NaN   \n",
       "GSM2947473        NaN        NaN        NaN        NaN       NaN       NaN   \n",
       "GSM2947474        NaN        NaN        NaN        NaN       NaN       NaN   \n",
       "...               ...        ...        ...        ...       ...       ...   \n",
       "GSM4565216   0.979980   0.518066   0.689941   0.979980  0.982910  0.546387   \n",
       "GSM4565217   0.980957   0.541016   0.711426   0.979980  0.984375  0.556152   \n",
       "GSM4565218   0.981934   0.579102   0.681641   0.979980  0.985840  0.579102   \n",
       "GSM4565219   0.982422   0.568359   0.730957   0.982422  0.983398  0.588379   \n",
       "GSM4565220   0.981445   0.535156   0.708496   0.980469  0.982422  0.554688   \n",
       "\n",
       "            rs9292570  rs9363764  rs939290  rs951295  \n",
       "GSM2947470        NaN        NaN       NaN       NaN  \n",
       "GSM2947471        NaN        NaN       NaN       NaN  \n",
       "GSM2947472        NaN        NaN       NaN       NaN  \n",
       "GSM2947473        NaN        NaN       NaN       NaN  \n",
       "GSM2947474        NaN        NaN       NaN       NaN  \n",
       "...               ...        ...       ...       ...  \n",
       "GSM4565216   0.514160   0.970215  0.525879  0.210815  \n",
       "GSM4565217   0.490723   0.966797  0.523438  0.193726  \n",
       "GSM4565218   0.496826   0.964844  0.582520  0.190918  \n",
       "GSM4565219   0.509766   0.969727  0.580078  0.223755  \n",
       "GSM4565220   0.500488   0.965332  0.545410  0.224121  \n",
       "\n",
       "[37067 rows x 2596 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_data_filtered[column_variances.index[(column_variances>0.1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c609d-97f8-4da7-9395-27b9abad1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save a subset of the numerical data filtered (first 20000 columns)\n",
    "numerical_data_filtered_subset = numerical_data_filtered.iloc[:, :20000]\n",
    "numerical_data_filtered_subset.to_csv('data/v2/numerical_data_filtered_subset.csv')\n",
    "# numerical_data_filtered.to_csv(numerical_data_path)\n",
    "metadata_columns_with_labels = metadata_columns + [label_column, sex_condition_column, age_condition_column, 'labels_encoded']\n",
    "df_metadata = df[metadata_columns_with_labels]\n",
    "df_metadata.to_csv(metadata_path)\n",
    "# print(\"Data saved successfully, including subset of first 20000 columns.\")\n",
    "\n",
    "# numerical_data_filtered.to_csv(numerical_data_path, index=False)\n",
    "# metadata_columns_with_labels = metadata_columns + [label_column, sex_condition_column, age_condition_column, 'labels_encoded']\n",
    "# df_metadata = df[metadata_columns_with_labels]\n",
    "# df_metadata.to_csv(metadata_path, index=False)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab27605-9859-4893-ba42-abe75439ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if preprocessed data exists...\n",
      "Loading preprocessed data from CSV files...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/v2/numerical_data_filtered_subset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking if preprocessed data exists...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading preprocessed data from CSV files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m numerical_data_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerical_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m df_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(metadata_path,  low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/v2/numerical_data_filtered_subset.csv'"
     ]
    }
   ],
   "source": [
    "# set_seed(42)\n",
    "\n",
    "\n",
    "# Load or prepare data\n",
    "numerical_data_path = 'data/v2/numerical_data_filtered_subset.csv'\n",
    "metadata_path = 'data/v2/metadata_with_labels.csv'\n",
    "\n",
    "print(\"Checking if preprocessed data exists...\")\n",
    "\n",
    "print(\"Loading preprocessed data from CSV files...\")\n",
    "numerical_data_filtered = pd.read_csv(numerical_data_path,  low_memory=False, index_col=0)\n",
    "df_metadata = pd.read_csv(metadata_path,  low_memory=False, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f404b3e-61a7-4d97-9561-3d0ac13a340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>tissue</th>\n",
       "      <th>geo_platform</th>\n",
       "      <th>inferred_age_Hannum</th>\n",
       "      <th>inferred_age_SkinBlood</th>\n",
       "      <th>inferred_age_Horvath353</th>\n",
       "      <th>disease</th>\n",
       "      <th>inferred_sex</th>\n",
       "      <th>inferred_age_MepiClock</th>\n",
       "      <th>labels_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_accession</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM2947470</th>\n",
       "      <td>116018</td>\n",
       "      <td>MTG_AD_rep1</td>\n",
       "      <td>male</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>38.997486</td>\n",
       "      <td>49.002998</td>\n",
       "      <td>69.367824</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>M</td>\n",
       "      <td>74.088153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947471</th>\n",
       "      <td>116019</td>\n",
       "      <td>MTG_AD_rep2</td>\n",
       "      <td>female</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>37.515462</td>\n",
       "      <td>36.846414</td>\n",
       "      <td>67.192135</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>F</td>\n",
       "      <td>72.482127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947472</th>\n",
       "      <td>116020</td>\n",
       "      <td>MTG_AD_rep3</td>\n",
       "      <td>female</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>40.120663</td>\n",
       "      <td>49.309851</td>\n",
       "      <td>73.118494</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>F</td>\n",
       "      <td>79.651463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947473</th>\n",
       "      <td>116021</td>\n",
       "      <td>MTG_Control_rep4</td>\n",
       "      <td>male</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>35.626721</td>\n",
       "      <td>47.058840</td>\n",
       "      <td>61.285652</td>\n",
       "      <td>control</td>\n",
       "      <td>M</td>\n",
       "      <td>62.677427</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947474</th>\n",
       "      <td>116022</td>\n",
       "      <td>MTG_Control_rep5</td>\n",
       "      <td>female</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>38.360221</td>\n",
       "      <td>53.644548</td>\n",
       "      <td>76.692224</td>\n",
       "      <td>control</td>\n",
       "      <td>F</td>\n",
       "      <td>79.044683</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id             title     sex   age race tissue  \\\n",
       "geo_accession                                                       \n",
       "GSM2947470     116018       MTG_AD_rep1    male  82.0  NaN    NaN   \n",
       "GSM2947471     116019       MTG_AD_rep2  female  75.0  NaN    NaN   \n",
       "GSM2947472     116020       MTG_AD_rep3  female  90.0  NaN    NaN   \n",
       "GSM2947473     116021  MTG_Control_rep4    male  79.0  NaN    NaN   \n",
       "GSM2947474     116022  MTG_Control_rep5  female  88.0  NaN    NaN   \n",
       "\n",
       "              geo_platform  inferred_age_Hannum  inferred_age_SkinBlood  \\\n",
       "geo_accession                                                             \n",
       "GSM2947470        GPL13534            38.997486               49.002998   \n",
       "GSM2947471        GPL13534            37.515462               36.846414   \n",
       "GSM2947472        GPL13534            40.120663               49.309851   \n",
       "GSM2947473        GPL13534            35.626721               47.058840   \n",
       "GSM2947474        GPL13534            38.360221               53.644548   \n",
       "\n",
       "               inferred_age_Horvath353              disease inferred_sex  \\\n",
       "geo_accession                                                              \n",
       "GSM2947470                   69.367824  Alzheimer's disease            M   \n",
       "GSM2947471                   67.192135  Alzheimer's disease            F   \n",
       "GSM2947472                   73.118494  Alzheimer's disease            F   \n",
       "GSM2947473                   61.285652              control            M   \n",
       "GSM2947474                   76.692224              control            F   \n",
       "\n",
       "               inferred_age_MepiClock  labels_encoded  \n",
       "geo_accession                                          \n",
       "GSM2947470                  74.088153               0  \n",
       "GSM2947471                  72.482127               0  \n",
       "GSM2947472                  79.651463               0  \n",
       "GSM2947473                  62.677427               6  \n",
       "GSM2947474                  79.044683               6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.read_csv(metadata_path, low_memory=False, index_col=\"geo_accession\")\n",
    "df_metadata = df_metadata.drop(columns=[\"Unnamed: 0\"])\n",
    "df_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a51bca5-4462-41e0-9955-cb8adff4ba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg00000029</th>\n",
       "      <th>cg00000108</th>\n",
       "      <th>cg00000109</th>\n",
       "      <th>cg00000236</th>\n",
       "      <th>cg00000292</th>\n",
       "      <th>cg00000321</th>\n",
       "      <th>cg00000363</th>\n",
       "      <th>cg00000622</th>\n",
       "      <th>cg00000658</th>\n",
       "      <th>cg00000714</th>\n",
       "      <th>...</th>\n",
       "      <th>cg01040890</th>\n",
       "      <th>cg01040960</th>\n",
       "      <th>cg01041222</th>\n",
       "      <th>cg01041239</th>\n",
       "      <th>cg01041256</th>\n",
       "      <th>cg01041284</th>\n",
       "      <th>cg01041405</th>\n",
       "      <th>cg01041455</th>\n",
       "      <th>cg01041681</th>\n",
       "      <th>cg01041703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM2947470</th>\n",
       "      <td>0.4397</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.7330</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.6084</td>\n",
       "      <td>0.5990</td>\n",
       "      <td>0.08440</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.10840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>0.9263</td>\n",
       "      <td>0.05734</td>\n",
       "      <td>0.0823</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.09000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947471</th>\n",
       "      <td>0.4788</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.7295</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.10394</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.11804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.9194</td>\n",
       "      <td>0.06073</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>0.2705</td>\n",
       "      <td>0.8447</td>\n",
       "      <td>0.3867</td>\n",
       "      <td>0.09800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947472</th>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.8840</td>\n",
       "      <td>0.6045</td>\n",
       "      <td>0.5405</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.10834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.07530</td>\n",
       "      <td>0.1345</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>0.7930</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.8490</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.09296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947473</th>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.7397</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.6284</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>0.015760</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.12177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.9346</td>\n",
       "      <td>0.07190</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.9410</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.8633</td>\n",
       "      <td>0.4175</td>\n",
       "      <td>0.11320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM2947474</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>0.6113</td>\n",
       "      <td>0.4622</td>\n",
       "      <td>0.10565</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.12770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7580</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.04020</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.3433</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.3872</td>\n",
       "      <td>0.07740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cg00000029  cg00000108  cg00000109  cg00000236  cg00000292  \\\n",
       "Unnamed: 0                                                               \n",
       "GSM2947470      0.4397      0.9424      0.7330      0.8590      0.6084   \n",
       "GSM2947471      0.4788      0.9470      0.7295      0.8735      0.6160   \n",
       "GSM2947472      0.5070      0.9487      0.7160      0.8840      0.6045   \n",
       "GSM2947473      0.5557      0.9250      0.7397      0.8945      0.6284   \n",
       "GSM2947474      0.5000      0.9340      0.7964      0.8700      0.6113   \n",
       "\n",
       "            cg00000321  cg00000363  cg00000622  cg00000658  cg00000714  ...  \\\n",
       "Unnamed: 0                                                              ...   \n",
       "GSM2947470      0.5990     0.08440    0.014110       0.757     0.10840  ...   \n",
       "GSM2947471      0.5160     0.10394    0.019090       0.754     0.11804  ...   \n",
       "GSM2947472      0.5405     0.09710    0.015810       0.790     0.10834  ...   \n",
       "GSM2947473      0.5796     0.10160    0.015760       0.765     0.12177  ...   \n",
       "GSM2947474      0.4622     0.10565    0.013145       0.779     0.12770  ...   \n",
       "\n",
       "            cg01040890  cg01040960  cg01041222  cg01041239  cg01041256  \\\n",
       "Unnamed: 0                                                               \n",
       "GSM2947470      0.7870      0.9263     0.05734      0.0823      0.9136   \n",
       "GSM2947471      0.8154      0.9194     0.06073      0.1356      0.9400   \n",
       "GSM2947472      0.7837      0.9250     0.07530      0.1345      0.9453   \n",
       "GSM2947473      0.7750      0.9346     0.07190      0.1761      0.9410   \n",
       "GSM2947474      0.7580      0.9270     0.04020      0.1342      0.9420   \n",
       "\n",
       "            cg01041284  cg01041405  cg01041455  cg01041681  cg01041703  \n",
       "Unnamed: 0                                                              \n",
       "GSM2947470      0.7950      0.2286      0.8345      0.3525     0.09000  \n",
       "GSM2947471      0.8203      0.2705      0.8447      0.3867     0.09800  \n",
       "GSM2947472      0.7930      0.2318      0.8490      0.3833     0.09296  \n",
       "GSM2947473      0.8190      0.2500      0.8633      0.4175     0.11320  \n",
       "GSM2947474      0.8120      0.3433      0.7730      0.3872     0.07740  \n",
       "\n",
       "[5 rows x 20000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_data_filtered = numerical_data_filtered.set_index(\"Unnamed: 0\")\n",
    "numerical_data_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea225d84-ba3b-4323-8eaf-0a5a2308d8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GSM2947470', 'GSM2947471', 'GSM2947472', 'GSM2947473', 'GSM2947474',\n",
       "       'GSM2947475', 'GSM2947476', 'GSM2947477', 'GSM2947478', 'GSM2947479',\n",
       "       ...\n",
       "       'GSM4565211', 'GSM4565212', 'GSM4565213', 'GSM4565214', 'GSM4565215',\n",
       "       'GSM4565216', 'GSM4565217', 'GSM4565218', 'GSM4565219', 'GSM4565220'],\n",
       "      dtype='object', name='geo_accession', length=37067)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "285847dd-5b4b-4988-94b8-61b7e0d8c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Argument parser to take split_data as input\n",
    "# parser = argparse.ArgumentParser(description=\"Process split data size.\")\n",
    "# parser.add_argument('--split_data', type=str, required=True, help='Size of the data to split, e.g., 5000, 10000, 20000, 37067, shuffled_10000')\n",
    "# args = parser.parse_args()\n",
    "# split_data_value = args.split_data\n",
    "split_data_value = '5000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6b0c2f0-cd67-40bd-ad72-5bc71d829f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main execution...\n",
      "Starting sample_data...\n",
      "Splitting data with size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/400650540.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return numerical_data_filtered.iloc[train_idx], [labels_encoded[i] for i in train_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data took 1.10 seconds to execute.\n",
      "Starting train_val_test_split...\n",
      "Splitting data into training, validation, and test sets...\n",
      "train_val_test_split took 0.84 seconds to execute.\n",
      "Starting scale_and_save_data...\n",
      "Scaling and saving data...\n",
      "Data saved successfully.\n",
      "scale_and_save_data took 3.08 seconds to execute.\n",
      "Training set size: (3390, 20000)\n",
      "Validation set size: (872, 20000)\n",
      "Test set size: (582, 20000)\n",
      "Main execution finished.\n",
      "Starting main execution...\n",
      "Starting sample_data...\n",
      "Splitting data with size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/400650540.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return numerical_data_filtered.iloc[train_idx], [labels_encoded[i] for i in train_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data took 1.83 seconds to execute.\n",
      "Starting train_val_test_split...\n",
      "Splitting data into training, validation, and test sets...\n",
      "train_val_test_split took 1.71 seconds to execute.\n",
      "Starting scale_and_save_data...\n",
      "Scaling and saving data...\n",
      "Data saved successfully.\n",
      "scale_and_save_data took 5.96 seconds to execute.\n",
      "Training set size: (6915, 20000)\n",
      "Validation set size: (1778, 20000)\n",
      "Test set size: (1186, 20000)\n",
      "Main execution finished.\n",
      "Starting main execution...\n",
      "Starting sample_data...\n",
      "Splitting data with size: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/400650540.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return numerical_data_filtered.iloc[train_idx], [labels_encoded[i] for i in train_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data took 2.67 seconds to execute.\n",
      "Starting train_val_test_split...\n",
      "Splitting data into training, validation, and test sets...\n",
      "train_val_test_split took 3.99 seconds to execute.\n",
      "Starting scale_and_save_data...\n",
      "Scaling and saving data...\n",
      "Data saved successfully.\n",
      "scale_and_save_data took 11.54 seconds to execute.\n",
      "Training set size: (13944, 20000)\n",
      "Validation set size: (3586, 20000)\n",
      "Test set size: (2391, 20000)\n",
      "Main execution finished.\n",
      "Starting main execution...\n",
      "Starting sample_data...\n",
      "sample_data took 0.00 seconds to execute.\n",
      "Starting train_val_test_split...\n",
      "Splitting data into training, validation, and test sets...\n",
      "train_val_test_split took 7.91 seconds to execute.\n",
      "Starting scale_and_save_data...\n",
      "Scaling and saving data...\n",
      "Data saved successfully.\n",
      "scale_and_save_data took 21.35 seconds to execute.\n",
      "Training set size: (25927, 20000)\n",
      "Validation set size: (6667, 20000)\n",
      "Test set size: (4445, 20000)\n",
      "Main execution finished.\n",
      "Starting main execution...\n",
      "Starting sample_data...\n",
      "Starting generate_shuffled_data...\n",
      "Generating shuffled data for null hypothesis...\n",
      "generate_shuffled_data took 41.01 seconds to execute.\n",
      "sample_data took 41.01 seconds to execute.\n",
      "Starting train_val_test_split...\n",
      "Splitting data into training, validation, and test sets...\n",
      "train_val_test_split took 1.80 seconds to execute.\n",
      "Starting scale_and_save_data...\n",
      "Scaling and saving data...\n",
      "Data saved successfully.\n",
      "scale_and_save_data took 6.18 seconds to execute.\n",
      "Training set size: (7000, 20000)\n",
      "Validation set size: (1800, 20000)\n",
      "Test set size: (1200, 20000)\n",
      "Main execution finished.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "for split_data_value in ['5000', '10000', '20000', '37067', 'shuffled_10000']:\n",
    "    print(\"Starting main execution...\")\n",
    "    labels_encoded = df_metadata['labels_encoded']\n",
    "    data, labels = sample_data(numerical_data_filtered, labels_encoded, split_data_value)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(data, labels)\n",
    "    scale_and_save_data((X_train, X_val, X_test, y_train, y_val, y_test), output_path='data/v2/m1')\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Validation set size: {X_val.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")\n",
    "    print(\"Main execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9977cb-c084-4569-9bb9-e9a982aaed06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c9c81-3cba-46a9-9c8f-67d4eccb7675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9665d9a-ffb3-47bd-b6e5-973309d09f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18c3ebe-6dab-4f0f-92d5-9496a58db72e",
   "metadata": {},
   "source": [
    "# Load Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09325c9d-4686-4cf0-be29-c012fa031b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = ['5000', '10000', '20000', '37067', 'shuffled_10000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a42ceb7a-386e-45e9-b4ac-97ed51a518be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable inside the script\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ad77-9006-41ec-9ff8-7b1a6d0753ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b485a-57b5-4d4f-af6b-7c5168286ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fbd7193-a7fe-4e11-92c0-dec7c94a9fc9",
   "metadata": {},
   "source": [
    "# Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1344de36-d29d-432d-b4cb-8f4e31882834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims=[2048,1024,512], dropout_rate=0.2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = self.build_layers(input_dim, hidden_dims, dropout_rate)\n",
    "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)  # for mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)  # for log variance\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_hidden_dims = hidden_dims[::-1]\n",
    "        self.decoder_layers =self.build_layers(latent_dim, decoder_hidden_dims, dropout_rate)\n",
    "        self.fc_output = nn.Linear(hidden_dims[0], input_dim)\n",
    "        # self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        # self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def build_layers(self, input_dim, hidden_dims, dropout_rate):\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = h_dim\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder_layers(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Check if logvar has NaN or Inf values\n",
    "        if torch.isnan(logvar).any() or torch.isinf(logvar).any():\n",
    "            print(f\"NaN or Inf detected in logvar: logvar={logvar}\")\n",
    "        \n",
    "        # Clamp logvar to prevent extreme values\n",
    "        logvar = torch.clamp(logvar, min=-5, max=5)\n",
    "        \n",
    "        # Calculate std from logvar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Check if std has NaN or Inf values\n",
    "        if torch.isnan(std).any() or torch.isinf(std).any():\n",
    "            print(f\"NaN or Inf detected in std computation: std={std}\")\n",
    "        \n",
    "        # Sample from the latent space\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Check if z has NaN or Inf values\n",
    "        if torch.isnan(z).any() or torch.isinf(z).any():\n",
    "            print(f\"NaN or Inf detected in z computation: z={z}\")\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_layers(z)\n",
    "        # h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc_output(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Method to get the latent embedding (the `z` vector) for an input.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)  # this is the embedding\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b62bd86a-8aff-492d-9745-fe7071e3446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Lightning(pl.LightningModule):\n",
    "    def __init__(self, input_dim=485577, latent_dim=128, hidden_dims=[2048, 1024, 512], dropout_rate=0.2, lr=1e-6):\n",
    "        super(VAE_Lightning, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()  # Save hyperparameters for checkpointing\n",
    "\n",
    "        self.model = VAE(input_dim, latent_dim, hidden_dims, dropout_rate)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.model.encode(x)\n",
    "        z = self.model.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        return self.model.get_latent_embedding(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)  # mask where values are not NaN\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Validation loss: {loss.item()}\")\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "  \n",
    "\n",
    "    def _vae_loss(self, original_x, x_hat, mu, logvar, mask):\n",
    "        # Apply mask to ignore NaN values in the loss calculation\n",
    "        recon_loss = F.mse_loss(x_hat[mask], original_x[mask], reduction='mean')\n",
    "    \n",
    "        # Scale the KL divergence to balance the losses\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss / original_x.shape[0]  # Normalize by batch size or apply weighting\n",
    "    \n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7d2c96b-8ddb-4e73-a3c2-7743bf5e4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last training epoch from the logs\n",
    "        train_loss = trainer.callback_metrics.get('train_loss')\n",
    "        if train_loss is not None:\n",
    "            self.train_losses.append(train_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last validation epoch from the logs\n",
    "        val_loss = trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "def replace_nan_with_mean(x):\n",
    "    # Calculate the column-wise mean, ignoring NaNs\n",
    "    col_mean = torch.nanmean(x, dim=0)\n",
    "    \n",
    "    # Find where NaN values are located\n",
    "    nan_mask = torch.isnan(x)\n",
    "    \n",
    "    # Replace NaNs with the corresponding column means\n",
    "    x[nan_mask] = torch.take(col_mean, nan_mask.nonzero()[:, 1])\n",
    "    \n",
    "    # Check if there are still NaN or Inf values\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in the input data after imputation!\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b9194-7cc9-4121-8b43-649794d892c3",
   "metadata": {},
   "source": [
    "## Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32624f45-a42b-40e1-a495-8eda0f11cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd2ceef-19cf-48e1-81d8-ee67dd3c2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c509808f-8e5e-4a7a-a9c4-acd96c9e87c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/2742524154.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_tensors = torch.load(\"/\".join([dataset_path, data_size, \"train_dataset_tensors.pt\"]))  # Loads (X_train, y_train)\n",
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/2742524154.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_tensors = torch.load(\"/\".join([dataset_path, data_size, \"val_dataset_tensors.pt\"]))      # Loads (X_val, y_val)\n",
      "/tmp/7391432.1.gpu.q/ipykernel_2441223/2742524154.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_tensors = torch.load(\"/\".join([dataset_path, data_size, \"test_dataset_tensors.pt\"]))    # Loads (X_test, y_test)\n"
     ]
    }
   ],
   "source": [
    "data_size = '5000'\n",
    "\n",
    "dataset_path = 'data/v2/m1/'\n",
    "\n",
    "\n",
    "# This loads both X (features) and y (labels) from the saved files\n",
    "train_tensors = torch.load(\"/\".join([dataset_path, data_size, \"train_dataset_tensors.pt\"]))  # Loads (X_train, y_train)\n",
    "val_tensors = torch.load(\"/\".join([dataset_path, data_size, \"val_dataset_tensors.pt\"]))      # Loads (X_val, y_val)\n",
    "test_tensors = torch.load(\"/\".join([dataset_path, data_size, \"test_dataset_tensors.pt\"]))    # Loads (X_test, y_test)\n",
    "# train_tensors = torch.load(f'{dataset_path}train_dataset_tensors.pt')  # Loads (X_train, y_train)\n",
    "# val_tensors = torch.load(f'{dataset_path}val_dataset_tensors.pt')      # Loads (X_val, y_val)\n",
    "# test_tensors = torch.load(f'{dataset_path}test_dataset_tensors.pt')    # Loads (X_test, y_test)\n",
    "\n",
    "# Recreate the TensorDataset\n",
    "train_dataset = TensorDataset(*train_tensors)\n",
    "val_dataset = TensorDataset(*val_tensors)\n",
    "test_dataset = TensorDataset(*test_tensors)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d648b65-faac-49ce-a8c2-ad4970ca0785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 87.6 M | train\n",
      "---------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.335   Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Validation loss: 3.0101165771484375\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.54it/s]Validation loss: 2.665041923522949\n",
      "Epoch 0:   0%|          | 0/212 [00:00<?, ?it/s]                           Training loss: 1.233135461807251\n",
      "Epoch 0:   0%|          | 1/212 [00:01<04:04,  0.86it/s, v_num=0]Training loss: 1.4413628578186035\n",
      "Epoch 0:   1%|          | 2/212 [00:01<02:03,  1.69it/s, v_num=0]Training loss: 1.4879541397094727\n",
      "Epoch 0:   1%|▏         | 3/212 [00:01<01:24,  2.48it/s, v_num=0]Training loss: 1.920178771018982\n",
      "Epoch 0:   2%|▏         | 4/212 [00:01<01:04,  3.24it/s, v_num=0]Training loss: 1.6109426021575928\n",
      "Epoch 0:   2%|▏         | 5/212 [00:01<00:52,  3.96it/s, v_num=0]Training loss: 1.510411262512207\n",
      "Epoch 0:   3%|▎         | 6/212 [00:01<00:44,  4.66it/s, v_num=0]Training loss: 2.2189767360687256\n",
      "Epoch 0:   3%|▎         | 7/212 [00:01<00:38,  5.32it/s, v_num=0]Training loss: 1.9329006671905518\n",
      "Epoch 0:   4%|▍         | 8/212 [00:01<00:34,  5.96it/s, v_num=0]Training loss: 1.4837136268615723\n",
      "Epoch 0:   4%|▍         | 9/212 [00:01<00:30,  6.58it/s, v_num=0]Training loss: 1.9231730699539185\n",
      "Epoch 0:   5%|▍         | 10/212 [00:01<00:28,  7.17it/s, v_num=0]Training loss: 1.4885838031768799\n",
      "Epoch 0:   5%|▌         | 11/212 [00:01<00:25,  7.74it/s, v_num=0]Training loss: 1.8000224828720093\n",
      "Epoch 0:   6%|▌         | 12/212 [00:01<00:24,  8.28it/s, v_num=0]Training loss: 1.6319172382354736\n",
      "Epoch 0:   6%|▌         | 13/212 [00:01<00:22,  8.81it/s, v_num=0]Training loss: 1.8944364786148071\n",
      "Epoch 0:   7%|▋         | 14/212 [00:01<00:21,  9.32it/s, v_num=0]Training loss: 2.248117208480835\n",
      "Epoch 0:   7%|▋         | 15/212 [00:01<00:20,  9.81it/s, v_num=0]Training loss: 1.4434034824371338\n",
      "Epoch 0:   8%|▊         | 16/212 [00:01<00:19, 10.29it/s, v_num=0]Training loss: 2.0001020431518555\n",
      "Epoch 0:   8%|▊         | 17/212 [00:01<00:18, 10.74it/s, v_num=0]Training loss: 1.573000431060791\n",
      "Epoch 0:   8%|▊         | 18/212 [00:01<00:17, 11.17it/s, v_num=0]Training loss: 1.3130881786346436\n",
      "Epoch 0:   9%|▉         | 19/212 [00:01<00:16, 11.59it/s, v_num=0]Training loss: 1.7632997035980225\n",
      "Epoch 0:   9%|▉         | 20/212 [00:01<00:15, 12.00it/s, v_num=0]Training loss: 1.8318188190460205\n",
      "Epoch 0:  10%|▉         | 21/212 [00:01<00:15, 12.40it/s, v_num=0]Training loss: 1.655663013458252\n",
      "Epoch 0:  10%|█         | 22/212 [00:01<00:14, 12.77it/s, v_num=0]Training loss: 1.6387028694152832\n",
      "Epoch 0:  11%|█         | 23/212 [00:01<00:14, 13.14it/s, v_num=0]Training loss: 1.6204683780670166\n",
      "Epoch 0:  11%|█▏        | 24/212 [00:01<00:13, 13.50it/s, v_num=0]Training loss: 1.1809206008911133\n",
      "Epoch 0:  12%|█▏        | 25/212 [00:01<00:13, 13.84it/s, v_num=0]Training loss: 5.484248161315918\n",
      "Epoch 0:  12%|█▏        | 26/212 [00:01<00:13, 14.18it/s, v_num=0]Training loss: 1.8580849170684814\n",
      "Epoch 0:  13%|█▎        | 27/212 [00:01<00:12, 14.52it/s, v_num=0]Training loss: 1.8254339694976807\n",
      "Epoch 0:  13%|█▎        | 28/212 [00:01<00:12, 14.84it/s, v_num=0]Training loss: 2.350409507751465\n",
      "Epoch 0:  14%|█▎        | 29/212 [00:01<00:12, 15.16it/s, v_num=0]Training loss: 1.2308608293533325\n",
      "Epoch 0:  14%|█▍        | 30/212 [00:01<00:11, 15.46it/s, v_num=0]Training loss: 1.116121768951416\n",
      "Epoch 0:  15%|█▍        | 31/212 [00:01<00:11, 15.76it/s, v_num=0]Training loss: 1.1997767686843872\n",
      "Epoch 0:  15%|█▌        | 32/212 [00:01<00:11, 16.05it/s, v_num=0]Training loss: 1.5447942018508911\n",
      "Epoch 0:  16%|█▌        | 33/212 [00:02<00:10, 16.33it/s, v_num=0]Training loss: 1.9494943618774414\n",
      "Epoch 0:  16%|█▌        | 34/212 [00:02<00:10, 16.61it/s, v_num=0]Training loss: 1.094761610031128\n",
      "Epoch 0:  17%|█▋        | 35/212 [00:02<00:10, 16.88it/s, v_num=0]Training loss: 1.2122430801391602\n",
      "Epoch 0:  17%|█▋        | 36/212 [00:02<00:10, 17.13it/s, v_num=0]Training loss: 1.2633743286132812\n",
      "Epoch 0:  17%|█▋        | 37/212 [00:02<00:10, 17.39it/s, v_num=0]Training loss: 1.4617772102355957\n",
      "Epoch 0:  18%|█▊        | 38/212 [00:02<00:09, 17.64it/s, v_num=0]Training loss: 2.1767351627349854\n",
      "Epoch 0:  18%|█▊        | 39/212 [00:02<00:09, 17.88it/s, v_num=0]Training loss: 1.3561278581619263\n",
      "Epoch 0:  19%|█▉        | 40/212 [00:02<00:09, 18.12it/s, v_num=0]Training loss: 1.067307949066162\n",
      "Epoch 0:  19%|█▉        | 41/212 [00:02<00:09, 18.35it/s, v_num=0]Training loss: 1.949723482131958\n",
      "Epoch 0:  20%|█▉        | 42/212 [00:02<00:09, 18.57it/s, v_num=0]Training loss: 1.1147574186325073\n",
      "Epoch 0:  20%|██        | 43/212 [00:02<00:08, 18.79it/s, v_num=0]Training loss: 1.3572927713394165\n",
      "Epoch 0:  21%|██        | 44/212 [00:02<00:08, 19.01it/s, v_num=0]Training loss: 1.71841561794281\n",
      "Epoch 0:  21%|██        | 45/212 [00:02<00:08, 19.22it/s, v_num=0]Training loss: 1.6716699600219727\n",
      "Epoch 0:  22%|██▏       | 46/212 [00:02<00:08, 19.42it/s, v_num=0]Training loss: 1.073227882385254\n",
      "Epoch 0:  22%|██▏       | 47/212 [00:02<00:08, 19.62it/s, v_num=0]Training loss: 1.0476999282836914\n",
      "Epoch 0:  23%|██▎       | 48/212 [00:02<00:08, 19.82it/s, v_num=0]Training loss: 1.3647828102111816\n",
      "Epoch 0:  23%|██▎       | 49/212 [00:02<00:08, 20.01it/s, v_num=0]Training loss: 1.166170597076416\n",
      "Epoch 0:  24%|██▎       | 50/212 [00:02<00:08, 20.20it/s, v_num=0]Training loss: 1.223700761795044\n",
      "Epoch 0:  24%|██▍       | 51/212 [00:02<00:07, 20.39it/s, v_num=0]Training loss: 1.075364351272583\n",
      "Epoch 0:  25%|██▍       | 52/212 [00:02<00:07, 20.56it/s, v_num=0]Training loss: 2.9956705570220947\n",
      "Epoch 0:  25%|██▌       | 53/212 [00:02<00:07, 20.74it/s, v_num=0]Training loss: 2.146827220916748\n",
      "Epoch 0:  25%|██▌       | 54/212 [00:02<00:07, 20.91it/s, v_num=0]Training loss: 2.0111918449401855\n",
      "Epoch 0:  26%|██▌       | 55/212 [00:02<00:07, 21.08it/s, v_num=0]Training loss: 1.6102714538574219\n",
      "Epoch 0:  26%|██▋       | 56/212 [00:02<00:07, 21.25it/s, v_num=0]Training loss: 1.8441897630691528\n",
      "Epoch 0:  27%|██▋       | 57/212 [00:02<00:07, 21.40it/s, v_num=0]Training loss: 1.6775251626968384\n",
      "Epoch 0:  27%|██▋       | 58/212 [00:02<00:07, 21.55it/s, v_num=0]Training loss: 1.8165346384048462\n",
      "Epoch 0:  28%|██▊       | 59/212 [00:02<00:07, 21.70it/s, v_num=0]Training loss: 2.083695888519287\n",
      "Epoch 0:  28%|██▊       | 60/212 [00:02<00:06, 21.83it/s, v_num=0]Training loss: 1.6158266067504883\n",
      "Epoch 0:  29%|██▉       | 61/212 [00:02<00:06, 21.97it/s, v_num=0]Training loss: 1.2786355018615723\n",
      "Epoch 0:  29%|██▉       | 62/212 [00:02<00:06, 22.11it/s, v_num=0]Training loss: 1.098069190979004\n",
      "Epoch 0:  30%|██▉       | 63/212 [00:02<00:06, 22.25it/s, v_num=0]Training loss: 1.2861988544464111\n",
      "Epoch 0:  30%|███       | 64/212 [00:02<00:06, 22.38it/s, v_num=0]Training loss: 1.519624948501587\n",
      "Epoch 0:  31%|███       | 65/212 [00:02<00:06, 22.51it/s, v_num=0]Training loss: 1.4893267154693604\n",
      "Epoch 0:  31%|███       | 66/212 [00:02<00:06, 22.64it/s, v_num=0]Training loss: 1.1200435161590576\n",
      "Epoch 0:  32%|███▏      | 67/212 [00:02<00:06, 22.78it/s, v_num=0]Training loss: 1.7062914371490479\n",
      "Epoch 0:  32%|███▏      | 68/212 [00:02<00:06, 22.90it/s, v_num=0]Training loss: 0.9867122173309326\n",
      "Epoch 0:  33%|███▎      | 69/212 [00:02<00:06, 23.04it/s, v_num=0]Training loss: 1.490204095840454\n",
      "Epoch 0:  33%|███▎      | 70/212 [00:03<00:06, 23.16it/s, v_num=0]Training loss: 0.8642137050628662\n",
      "Epoch 0:  33%|███▎      | 71/212 [00:03<00:06, 23.29it/s, v_num=0]Training loss: 1.6376862525939941\n",
      "Epoch 0:  34%|███▍      | 72/212 [00:03<00:05, 23.41it/s, v_num=0]Training loss: 1.1444432735443115\n",
      "Epoch 0:  34%|███▍      | 73/212 [00:03<00:05, 23.53it/s, v_num=0]Training loss: 1.3084635734558105\n",
      "Epoch 0:  35%|███▍      | 74/212 [00:03<00:05, 23.65it/s, v_num=0]Training loss: 1.4343326091766357\n",
      "Epoch 0:  35%|███▌      | 75/212 [00:03<00:05, 23.76it/s, v_num=0]Training loss: 1.715450644493103\n",
      "Epoch 0:  36%|███▌      | 76/212 [00:03<00:05, 23.88it/s, v_num=0]Training loss: 1.3908717632293701\n",
      "Epoch 0:  36%|███▋      | 77/212 [00:03<00:05, 23.99it/s, v_num=0]Training loss: 1.4878040552139282\n",
      "Epoch 0:  37%|███▋      | 78/212 [00:03<00:05, 24.10it/s, v_num=0]Training loss: 1.7242814302444458\n",
      "Epoch 0:  37%|███▋      | 79/212 [00:03<00:05, 24.21it/s, v_num=0]Training loss: 1.608537197113037\n",
      "Epoch 0:  38%|███▊      | 80/212 [00:03<00:05, 24.32it/s, v_num=0]Training loss: 2.2891488075256348\n",
      "Epoch 0:  38%|███▊      | 81/212 [00:03<00:05, 24.42it/s, v_num=0]Training loss: 1.7422999143600464\n",
      "Epoch 0:  39%|███▊      | 82/212 [00:03<00:05, 24.53it/s, v_num=0]Training loss: 14.108163833618164\n",
      "Epoch 0:  39%|███▉      | 83/212 [00:03<00:05, 24.63it/s, v_num=0]Training loss: 1.143557071685791\n",
      "Epoch 0:  40%|███▉      | 84/212 [00:03<00:05, 24.73it/s, v_num=0]Training loss: 1.1247341632843018\n",
      "Epoch 0:  40%|████      | 85/212 [00:03<00:05, 24.83it/s, v_num=0]Training loss: 1.9672691822052002\n",
      "Epoch 0:  41%|████      | 86/212 [00:03<00:05, 24.93it/s, v_num=0]Training loss: 1.8432214260101318\n",
      "Epoch 0:  41%|████      | 87/212 [00:03<00:04, 25.02it/s, v_num=0]Training loss: 1.181247591972351\n",
      "Epoch 0:  42%|████▏     | 88/212 [00:03<00:04, 25.12it/s, v_num=0]Training loss: 1.5506125688552856\n",
      "Epoch 0:  42%|████▏     | 89/212 [00:03<00:04, 25.21it/s, v_num=0]Training loss: 1.3233366012573242\n",
      "Epoch 0:  42%|████▏     | 90/212 [00:03<00:04, 25.30it/s, v_num=0]Training loss: 1.1824760437011719\n",
      "Epoch 0:  43%|████▎     | 91/212 [00:03<00:04, 25.39it/s, v_num=0]Training loss: 0.9945319890975952\n",
      "Epoch 0:  43%|████▎     | 92/212 [00:03<00:04, 25.48it/s, v_num=0]Training loss: 1.7867668867111206\n",
      "Epoch 0:  44%|████▍     | 93/212 [00:03<00:04, 25.57it/s, v_num=0]Training loss: 1.4664214849472046\n",
      "Epoch 0:  44%|████▍     | 94/212 [00:03<00:04, 25.66it/s, v_num=0]Training loss: 1.2995214462280273\n",
      "Epoch 0:  45%|████▍     | 95/212 [00:03<00:04, 25.74it/s, v_num=0]Training loss: 0.7741572856903076\n",
      "Epoch 0:  45%|████▌     | 96/212 [00:03<00:04, 25.82it/s, v_num=0]Training loss: 1.852468729019165\n",
      "Epoch 0:  46%|████▌     | 97/212 [00:03<00:04, 25.90it/s, v_num=0]Training loss: 1.1327903270721436\n",
      "Epoch 0:  46%|████▌     | 98/212 [00:03<00:04, 25.97it/s, v_num=0]Training loss: 2.6726772785186768\n",
      "Epoch 0:  47%|████▋     | 99/212 [00:03<00:04, 26.05it/s, v_num=0]Training loss: 1.781725287437439\n",
      "Epoch 0:  47%|████▋     | 100/212 [00:03<00:04, 26.12it/s, v_num=0]Training loss: 1.6956632137298584\n",
      "Epoch 0:  48%|████▊     | 101/212 [00:03<00:04, 26.16it/s, v_num=0]Training loss: 1.2441800832748413\n",
      "Epoch 0:  48%|████▊     | 102/212 [00:03<00:04, 26.22it/s, v_num=0]Training loss: 1.3871387243270874\n",
      "Epoch 0:  49%|████▊     | 103/212 [00:03<00:04, 26.29it/s, v_num=0]Training loss: 1.275971531867981\n",
      "Epoch 0:  49%|████▉     | 104/212 [00:03<00:04, 26.35it/s, v_num=0]Training loss: 1.4687927961349487\n",
      "Epoch 0:  50%|████▉     | 105/212 [00:03<00:04, 26.43it/s, v_num=0]Training loss: 1.6522667407989502\n",
      "Epoch 0:  50%|█████     | 106/212 [00:04<00:04, 26.50it/s, v_num=0]Training loss: 1.3327360153198242\n",
      "Epoch 0:  50%|█████     | 107/212 [00:04<00:03, 26.56it/s, v_num=0]Training loss: 2.845503568649292\n",
      "Epoch 0:  51%|█████     | 108/212 [00:04<00:03, 26.59it/s, v_num=0]Training loss: 1.3444836139678955\n",
      "Epoch 0:  51%|█████▏    | 109/212 [00:04<00:03, 26.66it/s, v_num=0]Training loss: 1.4416141510009766\n",
      "Epoch 0:  52%|█████▏    | 110/212 [00:04<00:03, 26.73it/s, v_num=0]Training loss: 1.6920485496520996\n",
      "Epoch 0:  52%|█████▏    | 111/212 [00:04<00:03, 26.79it/s, v_num=0]Training loss: 1.3872445821762085\n",
      "Epoch 0:  53%|█████▎    | 112/212 [00:04<00:03, 26.85it/s, v_num=0]Training loss: 2.6036524772644043\n",
      "Epoch 0:  53%|█████▎    | 113/212 [00:04<00:03, 26.91it/s, v_num=0]Training loss: 1.2687649726867676\n",
      "Epoch 0:  54%|█████▍    | 114/212 [00:04<00:03, 26.97it/s, v_num=0]Training loss: 1.7879830598831177\n",
      "Epoch 0:  54%|█████▍    | 115/212 [00:04<00:03, 27.03it/s, v_num=0]Training loss: 1.035282850265503\n",
      "Epoch 0:  55%|█████▍    | 116/212 [00:04<00:03, 27.10it/s, v_num=0]Training loss: 0.991928219795227\n",
      "Epoch 0:  55%|█████▌    | 117/212 [00:04<00:03, 27.16it/s, v_num=0]Training loss: 1.5793710947036743\n",
      "Epoch 0:  56%|█████▌    | 118/212 [00:04<00:03, 27.23it/s, v_num=0]Training loss: 1.87344491481781\n",
      "Epoch 0:  56%|█████▌    | 119/212 [00:04<00:03, 27.29it/s, v_num=0]Training loss: 1.2600101232528687\n",
      "Epoch 0:  57%|█████▋    | 120/212 [00:04<00:03, 27.35it/s, v_num=0]Training loss: 1.184523582458496\n",
      "Epoch 0:  57%|█████▋    | 121/212 [00:04<00:03, 27.41it/s, v_num=0]Training loss: 2.749540090560913\n",
      "Epoch 0:  58%|█████▊    | 122/212 [00:04<00:03, 27.47it/s, v_num=0]Training loss: 2.1975834369659424\n",
      "Epoch 0:  58%|█████▊    | 123/212 [00:04<00:03, 27.53it/s, v_num=0]Training loss: 2.4280428886413574\n",
      "Epoch 0:  58%|█████▊    | 124/212 [00:04<00:03, 27.59it/s, v_num=0]Training loss: 1.3697420358657837\n",
      "Epoch 0:  59%|█████▉    | 125/212 [00:04<00:03, 27.65it/s, v_num=0]Training loss: 1.4871561527252197\n",
      "Epoch 0:  59%|█████▉    | 126/212 [00:04<00:03, 27.71it/s, v_num=0]Training loss: 1.371415376663208\n",
      "Epoch 0:  60%|█████▉    | 127/212 [00:04<00:03, 27.76it/s, v_num=0]Training loss: 1.762845754623413\n",
      "Epoch 0:  60%|██████    | 128/212 [00:04<00:03, 27.82it/s, v_num=0]Training loss: 2.2056989669799805\n",
      "Epoch 0:  61%|██████    | 129/212 [00:04<00:02, 27.88it/s, v_num=0]Training loss: 4.272589206695557\n",
      "Epoch 0:  61%|██████▏   | 130/212 [00:04<00:02, 27.93it/s, v_num=0]Training loss: 2.1248254776000977\n",
      "Epoch 0:  62%|██████▏   | 131/212 [00:04<00:02, 27.98it/s, v_num=0]Training loss: 1.211951732635498\n",
      "Epoch 0:  62%|██████▏   | 132/212 [00:04<00:02, 28.04it/s, v_num=0]Training loss: 2.13785982131958\n",
      "Epoch 0:  63%|██████▎   | 133/212 [00:04<00:02, 28.09it/s, v_num=0]Training loss: 1.6335413455963135\n",
      "Epoch 0:  63%|██████▎   | 134/212 [00:04<00:02, 28.14it/s, v_num=0]Training loss: 1.2685014009475708\n",
      "Epoch 0:  64%|██████▎   | 135/212 [00:04<00:02, 28.19it/s, v_num=0]Training loss: 1.2193071842193604\n",
      "Epoch 0:  64%|██████▍   | 136/212 [00:04<00:02, 28.24it/s, v_num=0]Training loss: 1.0475233793258667\n",
      "Epoch 0:  65%|██████▍   | 137/212 [00:04<00:02, 28.28it/s, v_num=0]Training loss: 1.542586326599121\n",
      "Epoch 0:  65%|██████▌   | 138/212 [00:04<00:02, 28.33it/s, v_num=0]Training loss: 1.3283421993255615\n",
      "Epoch 0:  66%|██████▌   | 139/212 [00:04<00:02, 28.37it/s, v_num=0]Training loss: 1.7351341247558594\n",
      "Epoch 0:  66%|██████▌   | 140/212 [00:04<00:02, 28.41it/s, v_num=0]Training loss: 1.7032018899917603\n",
      "Epoch 0:  67%|██████▋   | 141/212 [00:04<00:02, 28.45it/s, v_num=0]Training loss: 1.0665168762207031\n",
      "Epoch 0:  67%|██████▋   | 142/212 [00:04<00:02, 28.49it/s, v_num=0]Training loss: 0.9010822772979736\n",
      "Epoch 0:  67%|██████▋   | 143/212 [00:05<00:02, 28.54it/s, v_num=0]Training loss: 1.623822808265686\n",
      "Epoch 0:  68%|██████▊   | 144/212 [00:05<00:02, 28.59it/s, v_num=0]Training loss: 1.3481323719024658\n",
      "Epoch 0:  68%|██████▊   | 145/212 [00:05<00:02, 28.64it/s, v_num=0]Training loss: 1.0616284608840942\n",
      "Epoch 0:  69%|██████▉   | 146/212 [00:05<00:02, 28.68it/s, v_num=0]Training loss: 2.082111120223999\n",
      "Epoch 0:  69%|██████▉   | 147/212 [00:05<00:02, 28.73it/s, v_num=0]Training loss: 1.2816293239593506\n",
      "Epoch 0:  70%|██████▉   | 148/212 [00:05<00:02, 28.77it/s, v_num=0]Training loss: 1.4417766332626343\n",
      "Epoch 0:  70%|███████   | 149/212 [00:05<00:02, 28.81it/s, v_num=0]Training loss: 1.5708128213882446\n",
      "Epoch 0:  71%|███████   | 150/212 [00:05<00:02, 28.86it/s, v_num=0]Training loss: 1.2547420263290405\n",
      "Epoch 0:  71%|███████   | 151/212 [00:05<00:02, 28.90it/s, v_num=0]Training loss: 1.7327632904052734\n",
      "Epoch 0:  72%|███████▏  | 152/212 [00:05<00:02, 28.95it/s, v_num=0]Training loss: 1.2069334983825684\n",
      "Epoch 0:  72%|███████▏  | 153/212 [00:05<00:02, 28.99it/s, v_num=0]Training loss: 2.245008945465088\n",
      "Epoch 0:  73%|███████▎  | 154/212 [00:05<00:01, 29.02it/s, v_num=0]Training loss: 1.8550901412963867\n",
      "Epoch 0:  73%|███████▎  | 155/212 [00:05<00:01, 29.06it/s, v_num=0]Training loss: 1.2107172012329102\n",
      "Epoch 0:  74%|███████▎  | 156/212 [00:05<00:01, 29.10it/s, v_num=0]Training loss: 1.609243631362915\n",
      "Epoch 0:  74%|███████▍  | 157/212 [00:05<00:01, 29.15it/s, v_num=0]Training loss: 1.6963218450546265\n",
      "Epoch 0:  75%|███████▍  | 158/212 [00:05<00:01, 29.19it/s, v_num=0]Training loss: 1.4669749736785889\n",
      "Epoch 0:  75%|███████▌  | 159/212 [00:05<00:01, 29.23it/s, v_num=0]Training loss: 1.1625380516052246\n",
      "Epoch 0:  75%|███████▌  | 160/212 [00:05<00:01, 29.27it/s, v_num=0]Training loss: 1.619093656539917\n",
      "Epoch 0:  76%|███████▌  | 161/212 [00:05<00:01, 29.31it/s, v_num=0]Training loss: 1.2564959526062012\n",
      "Epoch 0:  76%|███████▋  | 162/212 [00:05<00:01, 29.35it/s, v_num=0]Training loss: 0.9574665427207947\n",
      "Epoch 0:  77%|███████▋  | 163/212 [00:05<00:01, 29.39it/s, v_num=0]Training loss: 1.0588759183883667\n",
      "Epoch 0:  77%|███████▋  | 164/212 [00:05<00:01, 29.42it/s, v_num=0]Training loss: 1.0877461433410645\n",
      "Epoch 0:  78%|███████▊  | 165/212 [00:05<00:01, 29.46it/s, v_num=0]Training loss: 1.0193628072738647\n",
      "Epoch 0:  78%|███████▊  | 166/212 [00:05<00:01, 29.50it/s, v_num=0]Training loss: 1.2155646085739136\n",
      "Epoch 0:  79%|███████▉  | 167/212 [00:05<00:01, 29.54it/s, v_num=0]Training loss: 1.2778396606445312\n",
      "Epoch 0:  79%|███████▉  | 168/212 [00:05<00:01, 29.58it/s, v_num=0]Training loss: 0.7727267742156982\n",
      "Epoch 0:  80%|███████▉  | 169/212 [00:05<00:01, 29.61it/s, v_num=0]Training loss: 1.7732574939727783\n",
      "Epoch 0:  80%|████████  | 170/212 [00:05<00:01, 29.65it/s, v_num=0]Training loss: 11.741044998168945\n",
      "Epoch 0:  81%|████████  | 171/212 [00:05<00:01, 29.69it/s, v_num=0]Training loss: 1.5607502460479736\n",
      "Epoch 0:  81%|████████  | 172/212 [00:05<00:01, 29.72it/s, v_num=0]Training loss: 2.0035529136657715\n",
      "Epoch 0:  82%|████████▏ | 173/212 [00:05<00:01, 29.76it/s, v_num=0]Training loss: 0.8900190591812134\n",
      "Epoch 0:  82%|████████▏ | 174/212 [00:05<00:01, 29.79it/s, v_num=0]Training loss: 1.9647449254989624\n",
      "Epoch 0:  83%|████████▎ | 175/212 [00:05<00:01, 29.82it/s, v_num=0]Training loss: 1.5788092613220215\n",
      "Epoch 0:  83%|████████▎ | 176/212 [00:05<00:01, 29.85it/s, v_num=0]Training loss: 2.8043644428253174\n",
      "Epoch 0:  83%|████████▎ | 177/212 [00:05<00:01, 29.88it/s, v_num=0]Training loss: 1.5440032482147217\n",
      "Epoch 0:  84%|████████▍ | 178/212 [00:05<00:01, 29.91it/s, v_num=0]Training loss: 1.4943037033081055\n",
      "Epoch 0:  84%|████████▍ | 179/212 [00:05<00:01, 29.94it/s, v_num=0]Training loss: 1.4868115186691284\n",
      "Epoch 0:  85%|████████▍ | 180/212 [00:06<00:01, 29.96it/s, v_num=0]Training loss: 0.9739652276039124\n",
      "Epoch 0:  85%|████████▌ | 181/212 [00:06<00:01, 29.99it/s, v_num=0]Training loss: 0.8629321455955505\n",
      "Epoch 0:  86%|████████▌ | 182/212 [00:06<00:00, 30.02it/s, v_num=0]Training loss: 0.9457282423973083\n",
      "Epoch 0:  86%|████████▋ | 183/212 [00:06<00:00, 30.04it/s, v_num=0]Training loss: 1.4660108089447021\n",
      "Epoch 0:  87%|████████▋ | 184/212 [00:06<00:00, 30.07it/s, v_num=0]Training loss: 1.67000150680542\n",
      "Epoch 0:  87%|████████▋ | 185/212 [00:06<00:00, 30.10it/s, v_num=0]Training loss: 1.1638524532318115\n",
      "Epoch 0:  88%|████████▊ | 186/212 [00:06<00:00, 30.13it/s, v_num=0]Training loss: 1.8459519147872925\n",
      "Epoch 0:  88%|████████▊ | 187/212 [00:06<00:00, 30.17it/s, v_num=0]Training loss: 1.3846861124038696\n",
      "Epoch 0:  89%|████████▊ | 188/212 [00:06<00:00, 30.20it/s, v_num=0]Training loss: 2.241208791732788\n",
      "Epoch 0:  89%|████████▉ | 189/212 [00:06<00:00, 30.23it/s, v_num=0]Training loss: 1.6865501403808594\n",
      "Epoch 0:  90%|████████▉ | 190/212 [00:06<00:00, 30.26it/s, v_num=0]Training loss: 1.2105259895324707\n",
      "Epoch 0:  90%|█████████ | 191/212 [00:06<00:00, 30.29it/s, v_num=0]Training loss: 1.2335495948791504\n",
      "Epoch 0:  91%|█████████ | 192/212 [00:06<00:00, 30.32it/s, v_num=0]Training loss: 1.4084583520889282\n",
      "Epoch 0:  91%|█████████ | 193/212 [00:06<00:00, 30.35it/s, v_num=0]Training loss: 1.597691535949707\n",
      "Epoch 0:  92%|█████████▏| 194/212 [00:06<00:00, 30.37it/s, v_num=0]Training loss: 1.292980432510376\n",
      "Epoch 0:  92%|█████████▏| 195/212 [00:06<00:00, 30.40it/s, v_num=0]Training loss: 1.496685266494751\n",
      "Epoch 0:  92%|█████████▏| 196/212 [00:06<00:00, 30.43it/s, v_num=0]Training loss: 1.4055513143539429\n",
      "Epoch 0:  93%|█████████▎| 197/212 [00:06<00:00, 30.46it/s, v_num=0]Training loss: 2.022942066192627\n",
      "Epoch 0:  93%|█████████▎| 198/212 [00:06<00:00, 30.49it/s, v_num=0]Training loss: 1.0002179145812988\n",
      "Epoch 0:  94%|█████████▍| 199/212 [00:06<00:00, 30.52it/s, v_num=0]Training loss: 1.4870145320892334\n",
      "Epoch 0:  94%|█████████▍| 200/212 [00:06<00:00, 30.55it/s, v_num=0]Training loss: 1.9369323253631592\n",
      "Epoch 0:  95%|█████████▍| 201/212 [00:06<00:00, 30.57it/s, v_num=0]Training loss: 1.7313411235809326\n",
      "Epoch 0:  95%|█████████▌| 202/212 [00:06<00:00, 30.60it/s, v_num=0]Training loss: 1.1438912153244019\n",
      "Epoch 0:  96%|█████████▌| 203/212 [00:06<00:00, 30.63it/s, v_num=0]Training loss: 1.2862999439239502\n",
      "Epoch 0:  96%|█████████▌| 204/212 [00:06<00:00, 30.66it/s, v_num=0]Training loss: 1.4394837617874146\n",
      "Epoch 0:  97%|█████████▋| 205/212 [00:06<00:00, 30.68it/s, v_num=0]Training loss: 1.4316530227661133\n",
      "Epoch 0:  97%|█████████▋| 206/212 [00:06<00:00, 30.71it/s, v_num=0]Training loss: 1.1191895008087158\n",
      "Epoch 0:  98%|█████████▊| 207/212 [00:06<00:00, 30.74it/s, v_num=0]Training loss: 1.5374820232391357\n",
      "Epoch 0:  98%|█████████▊| 208/212 [00:06<00:00, 30.76it/s, v_num=0]Training loss: 1.0728703737258911\n",
      "Epoch 0:  99%|█████████▊| 209/212 [00:06<00:00, 30.79it/s, v_num=0]Training loss: 0.8923208713531494\n",
      "Epoch 0:  99%|█████████▉| 210/212 [00:06<00:00, 30.82it/s, v_num=0]Training loss: 1.5452463626861572\n",
      "Epoch 0: 100%|█████████▉| 211/212 [00:06<00:00, 30.84it/s, v_num=0]Training loss: 1.441107988357544\n",
      "Epoch 0: 100%|██████████| 212/212 [00:06<00:00, 30.86it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.4685044288635254\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 86.09it/s]\u001b[AValidation loss: 2.2059576511383057\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 89.88it/s]\u001b[AValidation loss: 1.2159359455108643\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 91.11it/s]\u001b[AValidation loss: 0.9168336391448975\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 91.34it/s]\u001b[AValidation loss: 0.931543231010437\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 90.93it/s]\u001b[AValidation loss: 1.1120555400848389\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 90.55it/s]\u001b[AValidation loss: 1.4699345827102661\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 90.44it/s]\u001b[AValidation loss: 2.517786741256714\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 90.20it/s]\u001b[AValidation loss: 0.8229758739471436\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 90.05it/s]\u001b[AValidation loss: 0.9426970481872559\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 88.67it/s]\u001b[AValidation loss: 0.9724435806274414\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 88.59it/s]\u001b[AValidation loss: 1.2979742288589478\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 88.62it/s]\u001b[AValidation loss: 1.266014814376831\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 88.59it/s]\u001b[AValidation loss: 1.2069151401519775\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 88.63it/s]\u001b[AValidation loss: 1.0021814107894897\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 88.62it/s]\u001b[AValidation loss: 1.3749943971633911\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 88.62it/s]\u001b[AValidation loss: 1.2682522535324097\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 88.66it/s]\u001b[AValidation loss: 1.8244004249572754\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 88.21it/s]\u001b[AValidation loss: 0.970653772354126\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 88.17it/s]\u001b[AValidation loss: 1.4733256101608276\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 88.13it/s]\u001b[AValidation loss: 1.330705165863037\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 88.16it/s]\u001b[AValidation loss: 1.3671411275863647\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 88.15it/s]\u001b[AValidation loss: 1.3482372760772705\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 88.06it/s]\u001b[AValidation loss: 1.202474594116211\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 88.14it/s]\u001b[AValidation loss: 1.663856863975525\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 88.31it/s]\u001b[AValidation loss: 0.9994215965270996\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 88.50it/s]\u001b[AValidation loss: 1.2283146381378174\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 88.69it/s]\u001b[AValidation loss: 1.2507957220077515\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 88.72it/s]\u001b[AValidation loss: 1.2523113489151\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 88.75it/s]\u001b[AValidation loss: 1.504989504814148\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 88.80it/s]\u001b[AValidation loss: 1.1125304698944092\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 88.86it/s]\u001b[AValidation loss: 1.2066389322280884\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 88.87it/s]\u001b[AValidation loss: 1.1836649179458618\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 88.87it/s]\u001b[AValidation loss: 1.0800909996032715\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 89.05it/s]\u001b[AValidation loss: 2.2060303688049316\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 89.08it/s]\u001b[AValidation loss: 0.8090701103210449\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 89.12it/s]\u001b[AValidation loss: 1.7896215915679932\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 89.26it/s]\u001b[AValidation loss: 0.8568350076675415\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 89.42it/s]\u001b[AValidation loss: 1.3092161417007446\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 89.53it/s]\u001b[AValidation loss: 1.203238606452942\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 89.68it/s]\u001b[AValidation loss: 1.072256326675415\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 89.82it/s]\u001b[AValidation loss: 0.9547459483146667\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 89.95it/s]\u001b[AValidation loss: 1.0615147352218628\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 90.04it/s]\u001b[AValidation loss: 1.5421801805496216\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 90.15it/s]\u001b[AValidation loss: 1.7135231494903564\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 90.25it/s]\u001b[AValidation loss: 0.9394508600234985\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 90.34it/s]\u001b[AValidation loss: 1.2163968086242676\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 90.43it/s]\u001b[AValidation loss: 1.206510066986084\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 90.49it/s]\u001b[AValidation loss: 1.3476839065551758\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 90.57it/s]\u001b[AValidation loss: 1.6621012687683105\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 90.59it/s]\u001b[AValidation loss: 1.503905177116394\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 90.58it/s]\u001b[AValidation loss: 1.3323787450790405\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 90.57it/s]\u001b[AValidation loss: 1.6743104457855225\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 90.67it/s]\u001b[AValidation loss: 1.3221713304519653\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 90.66it/s]\u001b[AValidation loss: 2.4288570880889893\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 90.79it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.1545199155807495\n",
      "Epoch 1:   0%|          | 1/212 [00:00<00:04, 43.08it/s, v_num=0]Training loss: 1.7533464431762695\n",
      "Epoch 1:   1%|          | 2/212 [00:00<00:05, 40.03it/s, v_num=0]Training loss: 1.3831565380096436\n",
      "Epoch 1:   1%|▏         | 3/212 [00:00<00:05, 39.12it/s, v_num=0]Training loss: 0.9112370610237122\n",
      "Epoch 1:   2%|▏         | 4/212 [00:00<00:05, 38.71it/s, v_num=0]Training loss: 1.2647672891616821\n",
      "Epoch 1:   2%|▏         | 5/212 [00:00<00:05, 38.42it/s, v_num=0]Training loss: 2.7055277824401855\n",
      "Epoch 1:   3%|▎         | 6/212 [00:00<00:05, 38.25it/s, v_num=0]Training loss: 1.9579702615737915\n",
      "Epoch 1:   3%|▎         | 7/212 [00:00<00:05, 38.14it/s, v_num=0]Training loss: 1.1573188304901123\n",
      "Epoch 1:   4%|▍         | 8/212 [00:00<00:05, 38.05it/s, v_num=0]Training loss: 3.8850767612457275\n",
      "Epoch 1:   4%|▍         | 9/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 0.9879122972488403\n",
      "Epoch 1:   5%|▍         | 10/212 [00:00<00:05, 37.90it/s, v_num=0]Training loss: 1.3245441913604736\n",
      "Epoch 1:   5%|▌         | 11/212 [00:00<00:05, 37.86it/s, v_num=0]Training loss: 1.2984435558319092\n",
      "Epoch 1:   6%|▌         | 12/212 [00:00<00:05, 37.82it/s, v_num=0]Training loss: 1.2401891946792603\n",
      "Epoch 1:   6%|▌         | 13/212 [00:00<00:05, 37.79it/s, v_num=0]Training loss: 0.9942031502723694\n",
      "Epoch 1:   7%|▋         | 14/212 [00:00<00:05, 37.76it/s, v_num=0]Training loss: 1.545994758605957\n",
      "Epoch 1:   7%|▋         | 15/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 1.2700508832931519\n",
      "Epoch 1:   8%|▊         | 16/212 [00:00<00:05, 37.71it/s, v_num=0]Training loss: 1.005725622177124\n",
      "Epoch 1:   8%|▊         | 17/212 [00:00<00:05, 37.70it/s, v_num=0]Training loss: 1.004297137260437\n",
      "Epoch 1:   8%|▊         | 18/212 [00:00<00:05, 37.65it/s, v_num=0]Training loss: 1.8523200750350952\n",
      "Epoch 1:   9%|▉         | 19/212 [00:00<00:05, 37.55it/s, v_num=0]Training loss: 1.7839146852493286\n",
      "Epoch 1:   9%|▉         | 20/212 [00:00<00:05, 37.48it/s, v_num=0]Training loss: 0.9772765636444092\n",
      "Epoch 1:  10%|▉         | 21/212 [00:00<00:05, 37.40it/s, v_num=0]Training loss: 1.795250654220581\n",
      "Epoch 1:  10%|█         | 22/212 [00:00<00:05, 37.34it/s, v_num=0]Training loss: 1.2776259183883667\n",
      "Epoch 1:  11%|█         | 23/212 [00:00<00:05, 37.29it/s, v_num=0]Training loss: 1.3371951580047607\n",
      "Epoch 1:  11%|█▏        | 24/212 [00:00<00:05, 37.18it/s, v_num=0]Training loss: 1.4035688638687134\n",
      "Epoch 1:  12%|█▏        | 25/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.1890727281570435\n",
      "Epoch 1:  12%|█▏        | 26/212 [00:00<00:05, 37.11it/s, v_num=0]Training loss: 1.4144951105117798\n",
      "Epoch 1:  13%|█▎        | 27/212 [00:00<00:04, 37.10it/s, v_num=0]Training loss: 0.6453545093536377\n",
      "Epoch 1:  13%|█▎        | 28/212 [00:00<00:04, 37.10it/s, v_num=0]Training loss: 1.5236531496047974\n",
      "Epoch 1:  14%|█▎        | 29/212 [00:00<00:04, 37.14it/s, v_num=0]Training loss: 1.5086982250213623\n",
      "Epoch 1:  14%|█▍        | 30/212 [00:00<00:04, 37.14it/s, v_num=0]Training loss: 1.6319520473480225\n",
      "Epoch 1:  15%|█▍        | 31/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 1.3798799514770508\n",
      "Epoch 1:  15%|█▌        | 32/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 1.0743749141693115\n",
      "Epoch 1:  16%|█▌        | 33/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 1.3861465454101562\n",
      "Epoch 1:  16%|█▌        | 34/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 1.0171369314193726\n",
      "Epoch 1:  17%|█▋        | 35/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 1.0401508808135986\n",
      "Epoch 1:  17%|█▋        | 36/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 2.0341036319732666\n",
      "Epoch 1:  17%|█▋        | 37/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 2.0865068435668945\n",
      "Epoch 1:  18%|█▊        | 38/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.9675858020782471\n",
      "Epoch 1:  18%|█▊        | 39/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.9069480299949646\n",
      "Epoch 1:  19%|█▉        | 40/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.4010437726974487\n",
      "Epoch 1:  19%|█▉        | 41/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.3688559532165527\n",
      "Epoch 1:  20%|█▉        | 42/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.2149074077606201\n",
      "Epoch 1:  20%|██        | 43/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.1498358249664307\n",
      "Epoch 1:  21%|██        | 44/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.2478997707366943\n",
      "Epoch 1:  21%|██        | 45/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.8469594717025757\n",
      "Epoch 1:  22%|██▏       | 46/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.4494743347167969\n",
      "Epoch 1:  22%|██▏       | 47/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.4035301208496094\n",
      "Epoch 1:  23%|██▎       | 48/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.704804539680481\n",
      "Epoch 1:  23%|██▎       | 49/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.9330053329467773\n",
      "Epoch 1:  24%|██▎       | 50/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.3942980766296387\n",
      "Epoch 1:  24%|██▍       | 51/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.120836853981018\n",
      "Epoch 1:  25%|██▍       | 52/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.5993016958236694\n",
      "Epoch 1:  25%|██▌       | 53/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.8497114181518555\n",
      "Epoch 1:  25%|██▌       | 54/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.684387445449829\n",
      "Epoch 1:  26%|██▌       | 55/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.773071050643921\n",
      "Epoch 1:  26%|██▋       | 56/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 3.9365315437316895\n",
      "Epoch 1:  27%|██▋       | 57/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.5384163856506348\n",
      "Epoch 1:  27%|██▋       | 58/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.2703489065170288\n",
      "Epoch 1:  28%|██▊       | 59/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.3305796384811401\n",
      "Epoch 1:  28%|██▊       | 60/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.7736465334892273\n",
      "Epoch 1:  29%|██▉       | 61/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 1.8608702421188354\n",
      "Epoch 1:  29%|██▉       | 62/212 [00:01<00:04, 37.13it/s, v_num=0]Training loss: 1.0439260005950928\n",
      "Epoch 1:  30%|██▉       | 63/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 1.5781939029693604\n",
      "Epoch 1:  30%|███       | 64/212 [00:01<00:03, 37.08it/s, v_num=0]Training loss: 1.5342793464660645\n",
      "Epoch 1:  31%|███       | 65/212 [00:01<00:03, 37.07it/s, v_num=0]Training loss: 0.9614989161491394\n",
      "Epoch 1:  31%|███       | 66/212 [00:01<00:03, 37.07it/s, v_num=0]Training loss: 1.4410783052444458\n",
      "Epoch 1:  32%|███▏      | 67/212 [00:01<00:03, 37.06it/s, v_num=0]Training loss: 1.2206499576568604\n",
      "Epoch 1:  32%|███▏      | 68/212 [00:01<00:03, 37.08it/s, v_num=0]Training loss: 0.8342791795730591\n",
      "Epoch 1:  33%|███▎      | 69/212 [00:01<00:03, 37.08it/s, v_num=0]Training loss: 1.469076156616211\n",
      "Epoch 1:  33%|███▎      | 70/212 [00:01<00:03, 37.09it/s, v_num=0]Training loss: 1.4289849996566772\n",
      "Epoch 1:  33%|███▎      | 71/212 [00:01<00:03, 37.09it/s, v_num=0]Training loss: 1.2283775806427002\n",
      "Epoch 1:  34%|███▍      | 72/212 [00:01<00:03, 37.09it/s, v_num=0]Training loss: 1.810802698135376\n",
      "Epoch 1:  34%|███▍      | 73/212 [00:01<00:03, 37.10it/s, v_num=0]Training loss: 1.2064515352249146\n",
      "Epoch 1:  35%|███▍      | 74/212 [00:01<00:03, 37.10it/s, v_num=0]Training loss: 1.3946595191955566\n",
      "Epoch 1:  35%|███▌      | 75/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 1.6096760034561157\n",
      "Epoch 1:  36%|███▌      | 76/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.9337698221206665\n",
      "Epoch 1:  36%|███▋      | 77/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.4744521379470825\n",
      "Epoch 1:  37%|███▋      | 78/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.109256625175476\n",
      "Epoch 1:  37%|███▋      | 79/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.3794242143630981\n",
      "Epoch 1:  38%|███▊      | 80/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.1205447912216187\n",
      "Epoch 1:  38%|███▊      | 81/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.8289846181869507\n",
      "Epoch 1:  39%|███▊      | 82/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 2.2727537155151367\n",
      "Epoch 1:  39%|███▉      | 83/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.2505004405975342\n",
      "Epoch 1:  40%|███▉      | 84/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.5685333013534546\n",
      "Epoch 1:  40%|████      | 85/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.9620494842529297\n",
      "Epoch 1:  41%|████      | 86/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.7819379568099976\n",
      "Epoch 1:  41%|████      | 87/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 0.971197247505188\n",
      "Epoch 1:  42%|████▏     | 88/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.3983559608459473\n",
      "Epoch 1:  42%|████▏     | 89/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.0555634498596191\n",
      "Epoch 1:  42%|████▏     | 90/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.5257333517074585\n",
      "Epoch 1:  43%|████▎     | 91/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.0127692222595215\n",
      "Epoch 1:  43%|████▎     | 92/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.1191010475158691\n",
      "Epoch 1:  44%|████▍     | 93/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.3190314769744873\n",
      "Epoch 1:  44%|████▍     | 94/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.5090668201446533\n",
      "Epoch 1:  45%|████▍     | 95/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.5651077032089233\n",
      "Epoch 1:  45%|████▌     | 96/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.1126270294189453\n",
      "Epoch 1:  46%|████▌     | 97/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.2719922065734863\n",
      "Epoch 1:  46%|████▌     | 98/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.7623894214630127\n",
      "Epoch 1:  47%|████▋     | 99/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 2.3139209747314453\n",
      "Epoch 1:  47%|████▋     | 100/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.120133876800537\n",
      "Epoch 1:  48%|████▊     | 101/212 [00:02<00:02, 37.11it/s, v_num=0]Training loss: 1.1516385078430176\n",
      "Epoch 1:  48%|████▊     | 102/212 [00:02<00:02, 37.10it/s, v_num=0]Training loss: 1.1617090702056885\n",
      "Epoch 1:  49%|████▊     | 103/212 [00:02<00:02, 37.08it/s, v_num=0]Training loss: 1.6877857446670532\n",
      "Epoch 1:  49%|████▉     | 104/212 [00:02<00:02, 37.07it/s, v_num=0]Training loss: 1.0808511972427368\n",
      "Epoch 1:  50%|████▉     | 105/212 [00:02<00:02, 37.07it/s, v_num=0]Training loss: 1.025264024734497\n",
      "Epoch 1:  50%|█████     | 106/212 [00:02<00:02, 37.07it/s, v_num=0]Training loss: 1.1209315061569214\n",
      "Epoch 1:  50%|█████     | 107/212 [00:02<00:02, 37.07it/s, v_num=0]Training loss: 1.650784969329834\n",
      "Epoch 1:  51%|█████     | 108/212 [00:02<00:02, 37.08it/s, v_num=0]Training loss: 2.3685224056243896\n",
      "Epoch 1:  51%|█████▏    | 109/212 [00:02<00:02, 37.08it/s, v_num=0]Training loss: 1.3825922012329102\n",
      "Epoch 1:  52%|█████▏    | 110/212 [00:02<00:02, 37.08it/s, v_num=0]Training loss: 10.947958946228027\n",
      "Epoch 1:  52%|█████▏    | 111/212 [00:02<00:02, 37.08it/s, v_num=0]Training loss: 1.7960846424102783\n",
      "Epoch 1:  53%|█████▎    | 112/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 0.8197438716888428\n",
      "Epoch 1:  53%|█████▎    | 113/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.5377986431121826\n",
      "Epoch 1:  54%|█████▍    | 114/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.504333257675171\n",
      "Epoch 1:  54%|█████▍    | 115/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.4801127910614014\n",
      "Epoch 1:  55%|█████▍    | 116/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.5185617208480835\n",
      "Epoch 1:  55%|█████▌    | 117/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.3712232112884521\n",
      "Epoch 1:  56%|█████▌    | 118/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.208399772644043\n",
      "Epoch 1:  56%|█████▌    | 119/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.0172038078308105\n",
      "Epoch 1:  57%|█████▋    | 120/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.4196031093597412\n",
      "Epoch 1:  57%|█████▋    | 121/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 2.0432703495025635\n",
      "Epoch 1:  58%|█████▊    | 122/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 2.1467108726501465\n",
      "Epoch 1:  58%|█████▊    | 123/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 2.1432762145996094\n",
      "Epoch 1:  58%|█████▊    | 124/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.8024113178253174\n",
      "Epoch 1:  59%|█████▉    | 125/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.8158848285675049\n",
      "Epoch 1:  59%|█████▉    | 126/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.4970351457595825\n",
      "Epoch 1:  60%|█████▉    | 127/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.4563149213790894\n",
      "Epoch 1:  60%|██████    | 128/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 0.9919178485870361\n",
      "Epoch 1:  61%|██████    | 129/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.6183100938796997\n",
      "Epoch 1:  61%|██████▏   | 130/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.2560245990753174\n",
      "Epoch 1:  62%|██████▏   | 131/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.3558526039123535\n",
      "Epoch 1:  62%|██████▏   | 132/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.0795114040374756\n",
      "Epoch 1:  63%|██████▎   | 133/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.057361125946045\n",
      "Epoch 1:  63%|██████▎   | 134/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.243822693824768\n",
      "Epoch 1:  64%|██████▎   | 135/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.9725974202156067\n",
      "Epoch 1:  64%|██████▍   | 136/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.0454124212265015\n",
      "Epoch 1:  65%|██████▍   | 137/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.7785192728042603\n",
      "Epoch 1:  65%|██████▌   | 138/212 [00:03<00:01, 37.08it/s, v_num=0]Training loss: 1.0019960403442383\n",
      "Epoch 1:  66%|██████▌   | 139/212 [00:03<00:01, 37.08it/s, v_num=0]Training loss: 0.8508005142211914\n",
      "Epoch 1:  66%|██████▌   | 140/212 [00:03<00:01, 37.07it/s, v_num=0]Training loss: 1.3195455074310303\n",
      "Epoch 1:  67%|██████▋   | 141/212 [00:03<00:01, 37.06it/s, v_num=0]Training loss: 9.947202682495117\n",
      "Epoch 1:  67%|██████▋   | 142/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 1.0824456214904785\n",
      "Epoch 1:  67%|██████▋   | 143/212 [00:03<00:01, 37.04it/s, v_num=0]Training loss: 0.8631409406661987\n",
      "Epoch 1:  68%|██████▊   | 144/212 [00:03<00:01, 37.04it/s, v_num=0]Training loss: 1.0677692890167236\n",
      "Epoch 1:  68%|██████▊   | 145/212 [00:03<00:01, 37.04it/s, v_num=0]Training loss: 1.596278429031372\n",
      "Epoch 1:  69%|██████▉   | 146/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 1.354150652885437\n",
      "Epoch 1:  69%|██████▉   | 147/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 1.1889210939407349\n",
      "Epoch 1:  70%|██████▉   | 148/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 0.9885622262954712\n",
      "Epoch 1:  70%|███████   | 149/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.3115222454071045\n",
      "Epoch 1:  71%|███████   | 150/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 1.0063059329986572\n",
      "Epoch 1:  71%|███████   | 151/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 1.0528455972671509\n",
      "Epoch 1:  72%|███████▏  | 152/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 0.9345411658287048\n",
      "Epoch 1:  72%|███████▏  | 153/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 1.520749807357788\n",
      "Epoch 1:  73%|███████▎  | 154/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.12509286403656\n",
      "Epoch 1:  73%|███████▎  | 155/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.2860491275787354\n",
      "Epoch 1:  74%|███████▎  | 156/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.9063570499420166\n",
      "Epoch 1:  74%|███████▍  | 157/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.8057142496109009\n",
      "Epoch 1:  75%|███████▍  | 158/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.1104092597961426\n",
      "Epoch 1:  75%|███████▌  | 159/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 1.104136347770691\n",
      "Epoch 1:  75%|███████▌  | 160/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 0.8445363640785217\n",
      "Epoch 1:  76%|███████▌  | 161/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 1.2754194736480713\n",
      "Epoch 1:  76%|███████▋  | 162/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 0.9694161415100098\n",
      "Epoch 1:  77%|███████▋  | 163/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 1.2134885787963867\n",
      "Epoch 1:  77%|███████▋  | 164/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 1.048475742340088\n",
      "Epoch 1:  78%|███████▊  | 165/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 0.8961797952651978\n",
      "Epoch 1:  78%|███████▊  | 166/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 1.225644826889038\n",
      "Epoch 1:  79%|███████▉  | 167/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 1.062246561050415\n",
      "Epoch 1:  79%|███████▉  | 168/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 0.7090433835983276\n",
      "Epoch 1:  80%|███████▉  | 169/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 1.1113481521606445\n",
      "Epoch 1:  80%|████████  | 170/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.2644169330596924\n",
      "Epoch 1:  81%|████████  | 171/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.4490195512771606\n",
      "Epoch 1:  81%|████████  | 172/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.0388370752334595\n",
      "Epoch 1:  82%|████████▏ | 173/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.508321762084961\n",
      "Epoch 1:  82%|████████▏ | 174/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.2654017210006714\n",
      "Epoch 1:  83%|████████▎ | 175/212 [00:04<00:00, 37.05it/s, v_num=0]Training loss: 1.1186938285827637\n",
      "Epoch 1:  83%|████████▎ | 176/212 [00:04<00:00, 37.04it/s, v_num=0]Training loss: 1.4515711069107056\n",
      "Epoch 1:  83%|████████▎ | 177/212 [00:04<00:00, 37.03it/s, v_num=0]Training loss: 2.3530964851379395\n",
      "Epoch 1:  84%|████████▍ | 178/212 [00:04<00:00, 37.03it/s, v_num=0]Training loss: 1.3121789693832397\n",
      "Epoch 1:  84%|████████▍ | 179/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.1240694522857666\n",
      "Epoch 1:  85%|████████▍ | 180/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.3023637533187866\n",
      "Epoch 1:  85%|████████▌ | 181/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 1.5197603702545166\n",
      "Epoch 1:  86%|████████▌ | 182/212 [00:04<00:00, 37.00it/s, v_num=0]Training loss: 1.2347133159637451\n",
      "Epoch 1:  86%|████████▋ | 183/212 [00:04<00:00, 37.00it/s, v_num=0]Training loss: 1.0271714925765991\n",
      "Epoch 1:  87%|████████▋ | 184/212 [00:04<00:00, 36.99it/s, v_num=0]Training loss: 3.2056331634521484\n",
      "Epoch 1:  87%|████████▋ | 185/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.4735863208770752\n",
      "Epoch 1:  88%|████████▊ | 186/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.1877975463867188\n",
      "Epoch 1:  88%|████████▊ | 187/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.3889787197113037\n",
      "Epoch 1:  89%|████████▊ | 188/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.5592097043991089\n",
      "Epoch 1:  89%|████████▉ | 189/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.9737346172332764\n",
      "Epoch 1:  90%|████████▉ | 190/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.8906614780426025\n",
      "Epoch 1:  90%|█████████ | 191/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.001811146736145\n",
      "Epoch 1:  91%|█████████ | 192/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.1584646701812744\n",
      "Epoch 1:  91%|█████████ | 193/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.1639610528945923\n",
      "Epoch 1:  92%|█████████▏| 194/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.0530939102172852\n",
      "Epoch 1:  92%|█████████▏| 195/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.5073055028915405\n",
      "Epoch 1:  92%|█████████▏| 196/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.330138087272644\n",
      "Epoch 1:  93%|█████████▎| 197/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.5894383192062378\n",
      "Epoch 1:  93%|█████████▎| 198/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.7420812845230103\n",
      "Epoch 1:  94%|█████████▍| 199/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.423370122909546\n",
      "Epoch 1:  94%|█████████▍| 200/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 2.7063443660736084\n",
      "Epoch 1:  95%|█████████▍| 201/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 1.3620017766952515\n",
      "Epoch 1:  95%|█████████▌| 202/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 1.0619741678237915\n",
      "Epoch 1:  96%|█████████▌| 203/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 1.0276437997817993\n",
      "Epoch 1:  96%|█████████▌| 204/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 2.1190266609191895\n",
      "Epoch 1:  97%|█████████▋| 205/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.8631141185760498\n",
      "Epoch 1:  97%|█████████▋| 206/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.8739111423492432\n",
      "Epoch 1:  98%|█████████▊| 207/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 1.9059712886810303\n",
      "Epoch 1:  98%|█████████▊| 208/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 1.051034927368164\n",
      "Epoch 1:  99%|█████████▊| 209/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.8749833106994629\n",
      "Epoch 1:  99%|█████████▉| 210/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 1.3316185474395752\n",
      "Epoch 1: 100%|█████████▉| 211/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 0.800148606300354\n",
      "Epoch 1: 100%|██████████| 212/212 [00:05<00:00, 37.05it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.3669278621673584\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 86.13it/s]\u001b[AValidation loss: 2.1432955265045166\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 83.50it/s]\u001b[AValidation loss: 1.1750714778900146\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 86.03it/s]\u001b[AValidation loss: 0.8775457143783569\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 88.47it/s]\u001b[AValidation loss: 0.8904790878295898\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 89.88it/s]\u001b[AValidation loss: 1.068639874458313\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 90.92it/s]\u001b[AValidation loss: 1.4247487783432007\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 91.46it/s]\u001b[AValidation loss: 2.4292075634002686\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 91.21it/s]\u001b[AValidation loss: 0.7842376232147217\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 90.86it/s]\u001b[AValidation loss: 0.9006727933883667\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 90.54it/s]\u001b[AValidation loss: 0.9289933443069458\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 90.36it/s]\u001b[AValidation loss: 1.2474737167358398\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 90.15it/s]\u001b[AValidation loss: 1.2116185426712036\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 90.05it/s]\u001b[AValidation loss: 1.1578912734985352\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 89.95it/s]\u001b[AValidation loss: 0.9588086009025574\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 89.85it/s]\u001b[AValidation loss: 1.3282335996627808\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 89.77it/s]\u001b[AValidation loss: 1.2204549312591553\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 89.64it/s]\u001b[AValidation loss: 1.7664761543273926\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 89.51it/s]\u001b[AValidation loss: 0.9273086190223694\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 89.45it/s]\u001b[AValidation loss: 1.4213733673095703\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 89.38it/s]\u001b[AValidation loss: 1.2823351621627808\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 88.98it/s]\u001b[AValidation loss: 1.3196114301681519\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 88.99it/s]\u001b[AValidation loss: 1.3010072708129883\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 88.91it/s]\u001b[AValidation loss: 1.157528281211853\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 88.80it/s]\u001b[AValidation loss: 1.6117985248565674\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 88.73it/s]\u001b[AValidation loss: 0.9567908644676208\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 88.66it/s]\u001b[AValidation loss: 1.1853129863739014\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 88.70it/s]\u001b[AValidation loss: 1.2119776010513306\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 88.83it/s]\u001b[AValidation loss: 1.2061080932617188\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 88.99it/s]\u001b[AValidation loss: 1.449784278869629\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 89.15it/s]\u001b[AValidation loss: 1.0715922117233276\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 89.16it/s]\u001b[AValidation loss: 1.1487482786178589\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 89.17it/s]\u001b[AValidation loss: 1.1363306045532227\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 89.21it/s]\u001b[AValidation loss: 1.0348701477050781\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 89.25it/s]\u001b[AValidation loss: 2.1333742141723633\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 89.20it/s]\u001b[AValidation loss: 0.7721307873725891\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 89.21it/s]\u001b[AValidation loss: 1.7222014665603638\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 89.22it/s]\u001b[AValidation loss: 0.8237683773040771\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 89.24it/s]\u001b[AValidation loss: 1.2631603479385376\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 89.38it/s]\u001b[AValidation loss: 1.1550874710083008\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 89.52it/s]\u001b[AValidation loss: 1.0288225412368774\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 89.67it/s]\u001b[AValidation loss: 0.9143980145454407\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 89.82it/s]\u001b[AValidation loss: 1.0185425281524658\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 89.95it/s]\u001b[AValidation loss: 1.4874446392059326\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 90.07it/s]\u001b[AValidation loss: 1.6498578786849976\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 90.17it/s]\u001b[AValidation loss: 0.8955371379852295\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 90.26it/s]\u001b[AValidation loss: 1.17276930809021\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 90.36it/s]\u001b[AValidation loss: 1.1608227491378784\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 90.45it/s]\u001b[AValidation loss: 1.297448992729187\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 90.52it/s]\u001b[AValidation loss: 1.5995066165924072\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 90.60it/s]\u001b[AValidation loss: 1.452290654182434\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 90.68it/s]\u001b[AValidation loss: 1.2780009508132935\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 90.75it/s]\u001b[AValidation loss: 1.6076123714447021\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 90.72it/s]\u001b[AValidation loss: 1.2720680236816406\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 90.69it/s]\u001b[AValidation loss: 2.3458447456359863\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 90.84it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 2.532712459564209\n",
      "Epoch 2:   0%|          | 1/212 [00:00<00:05, 40.24it/s, v_num=0]Training loss: 1.1813163757324219\n",
      "Epoch 2:   1%|          | 2/212 [00:00<00:05, 38.79it/s, v_num=0]Training loss: 1.069854736328125\n",
      "Epoch 2:   1%|▏         | 3/212 [00:00<00:05, 38.27it/s, v_num=0]Training loss: 0.9573714733123779\n",
      "Epoch 2:   2%|▏         | 4/212 [00:00<00:05, 38.02it/s, v_num=0]Training loss: 1.5521125793457031\n",
      "Epoch 2:   2%|▏         | 5/212 [00:00<00:05, 37.83it/s, v_num=0]Training loss: 10.471504211425781\n",
      "Epoch 2:   3%|▎         | 6/212 [00:00<00:05, 37.75it/s, v_num=0]Training loss: 1.343423843383789\n",
      "Epoch 2:   3%|▎         | 7/212 [00:00<00:05, 37.69it/s, v_num=0]Training loss: 1.2866153717041016\n",
      "Epoch 2:   4%|▍         | 8/212 [00:00<00:05, 37.65it/s, v_num=0]Training loss: 1.5227779150009155\n",
      "Epoch 2:   4%|▍         | 9/212 [00:00<00:05, 37.62it/s, v_num=0]Training loss: 0.9196474552154541\n",
      "Epoch 2:   5%|▍         | 10/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 1.0190088748931885\n",
      "Epoch 2:   5%|▌         | 11/212 [00:00<00:05, 37.59it/s, v_num=0]Training loss: 0.775590181350708\n",
      "Epoch 2:   6%|▌         | 12/212 [00:00<00:05, 37.59it/s, v_num=0]Training loss: 1.267570972442627\n",
      "Epoch 2:   6%|▌         | 13/212 [00:00<00:05, 37.57it/s, v_num=0]Training loss: 1.0358264446258545\n",
      "Epoch 2:   7%|▋         | 14/212 [00:00<00:05, 37.56it/s, v_num=0]Training loss: 1.3069934844970703\n",
      "Epoch 2:   7%|▋         | 15/212 [00:00<00:05, 37.55it/s, v_num=0]Training loss: 1.1910910606384277\n",
      "Epoch 2:   8%|▊         | 16/212 [00:00<00:05, 37.53it/s, v_num=0]Training loss: 1.4022464752197266\n",
      "Epoch 2:   8%|▊         | 17/212 [00:00<00:05, 37.53it/s, v_num=0]Training loss: 1.243951439857483\n",
      "Epoch 2:   8%|▊         | 18/212 [00:00<00:05, 37.53it/s, v_num=0]Training loss: 1.5404802560806274\n",
      "Epoch 2:   9%|▉         | 19/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 1.2152830362319946\n",
      "Epoch 2:   9%|▉         | 20/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 3.506024122238159\n",
      "Epoch 2:  10%|▉         | 21/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 1.3937642574310303\n",
      "Epoch 2:  10%|█         | 22/212 [00:00<00:05, 37.51it/s, v_num=0]Training loss: 1.390988826751709\n",
      "Epoch 2:  11%|█         | 23/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 1.8768391609191895\n",
      "Epoch 2:  11%|█▏        | 24/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 2.493424892425537\n",
      "Epoch 2:  12%|█▏        | 25/212 [00:00<00:04, 37.51it/s, v_num=0]Training loss: 0.930122971534729\n",
      "Epoch 2:  12%|█▏        | 26/212 [00:00<00:04, 37.50it/s, v_num=0]Training loss: 1.4622247219085693\n",
      "Epoch 2:  13%|█▎        | 27/212 [00:00<00:04, 37.42it/s, v_num=0]Training loss: 1.1244882345199585\n",
      "Epoch 2:  13%|█▎        | 28/212 [00:00<00:04, 37.37it/s, v_num=0]Training loss: 1.035164713859558\n",
      "Epoch 2:  14%|█▎        | 29/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.4757322072982788\n",
      "Epoch 2:  14%|█▍        | 30/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 1.1839239597320557\n",
      "Epoch 2:  15%|█▍        | 31/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 1.5637619495391846\n",
      "Epoch 2:  15%|█▌        | 32/212 [00:00<00:04, 37.18it/s, v_num=0]Training loss: 1.3307198286056519\n",
      "Epoch 2:  16%|█▌        | 33/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 0.9986559748649597\n",
      "Epoch 2:  16%|█▌        | 34/212 [00:00<00:04, 37.12it/s, v_num=0]Training loss: 0.97402024269104\n",
      "Epoch 2:  17%|█▋        | 35/212 [00:00<00:04, 37.13it/s, v_num=0]Training loss: 1.3296349048614502\n",
      "Epoch 2:  17%|█▋        | 36/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 0.9976638555526733\n",
      "Epoch 2:  17%|█▋        | 37/212 [00:00<00:04, 37.17it/s, v_num=0]Training loss: 1.0469504594802856\n",
      "Epoch 2:  18%|█▊        | 38/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.9912303686141968\n",
      "Epoch 2:  18%|█▊        | 39/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.8469821810722351\n",
      "Epoch 2:  19%|█▉        | 40/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.3341606855392456\n",
      "Epoch 2:  19%|█▉        | 41/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 2.3340353965759277\n",
      "Epoch 2:  20%|█▉        | 42/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.9613913297653198\n",
      "Epoch 2:  20%|██        | 43/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.1449344158172607\n",
      "Epoch 2:  21%|██        | 44/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.1704542636871338\n",
      "Epoch 2:  21%|██        | 45/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.314745306968689\n",
      "Epoch 2:  22%|██▏       | 46/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.8237088918685913\n",
      "Epoch 2:  22%|██▏       | 47/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.9416931867599487\n",
      "Epoch 2:  23%|██▎       | 48/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.4951238632202148\n",
      "Epoch 2:  23%|██▎       | 49/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.42181396484375\n",
      "Epoch 2:  24%|██▎       | 50/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.1608563661575317\n",
      "Epoch 2:  24%|██▍       | 51/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.0503106117248535\n",
      "Epoch 2:  25%|██▍       | 52/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.3529102802276611\n",
      "Epoch 2:  25%|██▌       | 53/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.0611212253570557\n",
      "Epoch 2:  25%|██▌       | 54/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.9754918217658997\n",
      "Epoch 2:  26%|██▌       | 55/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.542248249053955\n",
      "Epoch 2:  26%|██▋       | 56/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.228294849395752\n",
      "Epoch 2:  27%|██▋       | 57/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.2892504930496216\n",
      "Epoch 2:  27%|██▋       | 58/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 2.04066801071167\n",
      "Epoch 2:  28%|██▊       | 59/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.5252925157546997\n",
      "Epoch 2:  28%|██▊       | 60/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.9545207619667053\n",
      "Epoch 2:  29%|██▉       | 61/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.4503552913665771\n",
      "Epoch 2:  29%|██▉       | 62/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 4.380720138549805\n",
      "Epoch 2:  30%|██▉       | 63/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 1.8536317348480225\n",
      "Epoch 2:  30%|███       | 64/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.3917858600616455\n",
      "Epoch 2:  31%|███       | 65/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.4071077108383179\n",
      "Epoch 2:  31%|███       | 66/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.1048815250396729\n",
      "Epoch 2:  32%|███▏      | 67/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 1.1033837795257568\n",
      "Epoch 2:  32%|███▏      | 68/212 [00:01<00:03, 37.20it/s, v_num=0]Training loss: 1.5883320569992065\n",
      "Epoch 2:  33%|███▎      | 69/212 [00:01<00:03, 37.18it/s, v_num=0]Training loss: 1.0780630111694336\n",
      "Epoch 2:  33%|███▎      | 70/212 [00:01<00:03, 37.16it/s, v_num=0]Training loss: 1.4110356569290161\n",
      "Epoch 2:  33%|███▎      | 71/212 [00:01<00:03, 37.14it/s, v_num=0]Training loss: 0.9046319127082825\n",
      "Epoch 2:  34%|███▍      | 72/212 [00:01<00:03, 37.12it/s, v_num=0]Training loss: 1.8428822755813599\n",
      "Epoch 2:  34%|███▍      | 73/212 [00:01<00:03, 37.11it/s, v_num=0]Training loss: 0.9135535955429077\n",
      "Epoch 2:  35%|███▍      | 74/212 [00:01<00:03, 37.10it/s, v_num=0]Training loss: 1.1309151649475098\n",
      "Epoch 2:  35%|███▌      | 75/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 0.8092098236083984\n",
      "Epoch 2:  36%|███▌      | 76/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.6855974197387695\n",
      "Epoch 2:  36%|███▋      | 77/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.9748530983924866\n",
      "Epoch 2:  37%|███▋      | 78/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.1876351833343506\n",
      "Epoch 2:  37%|███▋      | 79/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.9349423050880432\n",
      "Epoch 2:  38%|███▊      | 80/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.2023879289627075\n",
      "Epoch 2:  38%|███▊      | 81/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.2950143814086914\n",
      "Epoch 2:  39%|███▊      | 82/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.2944421768188477\n",
      "Epoch 2:  39%|███▉      | 83/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.9764039516448975\n",
      "Epoch 2:  40%|███▉      | 84/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.7383791208267212\n",
      "Epoch 2:  40%|████      | 85/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.9461631774902344\n",
      "Epoch 2:  41%|████      | 86/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.497352123260498\n",
      "Epoch 2:  41%|████      | 87/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.0280790328979492\n",
      "Epoch 2:  42%|████▏     | 88/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.5136303901672363\n",
      "Epoch 2:  42%|████▏     | 89/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.1158349514007568\n",
      "Epoch 2:  42%|████▏     | 90/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 2.356637716293335\n",
      "Epoch 2:  43%|████▎     | 91/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.542773962020874\n",
      "Epoch 2:  43%|████▎     | 92/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 0.6885389089584351\n",
      "Epoch 2:  44%|████▍     | 93/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 0.7944769263267517\n",
      "Epoch 2:  44%|████▍     | 94/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.0034571886062622\n",
      "Epoch 2:  45%|████▍     | 95/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 0.7358649969100952\n",
      "Epoch 2:  45%|████▌     | 96/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.1341402530670166\n",
      "Epoch 2:  46%|████▌     | 97/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.480272889137268\n",
      "Epoch 2:  46%|████▌     | 98/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 0.9224223494529724\n",
      "Epoch 2:  47%|████▋     | 99/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.4278409481048584\n",
      "Epoch 2:  47%|████▋     | 100/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.111997365951538\n",
      "Epoch 2:  48%|████▊     | 101/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.1530592441558838\n",
      "Epoch 2:  48%|████▊     | 102/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.5400409698486328\n",
      "Epoch 2:  49%|████▊     | 103/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.3346763849258423\n",
      "Epoch 2:  49%|████▉     | 104/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 9.352526664733887\n",
      "Epoch 2:  50%|████▉     | 105/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 0.758138120174408\n",
      "Epoch 2:  50%|█████     | 106/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 0.8014412522315979\n",
      "Epoch 2:  50%|█████     | 107/212 [00:02<00:02, 37.16it/s, v_num=0]Training loss: 1.3901830911636353\n",
      "Epoch 2:  51%|█████     | 108/212 [00:02<00:02, 37.06it/s, v_num=0]Training loss: 1.0876725912094116\n",
      "Epoch 2:  51%|█████▏    | 109/212 [00:02<00:02, 37.04it/s, v_num=0]Training loss: 1.3386836051940918\n",
      "Epoch 2:  52%|█████▏    | 110/212 [00:02<00:02, 37.02it/s, v_num=0]Training loss: 1.585202693939209\n",
      "Epoch 2:  52%|█████▏    | 111/212 [00:02<00:02, 37.01it/s, v_num=0]Training loss: 1.4353400468826294\n",
      "Epoch 2:  53%|█████▎    | 112/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.546730875968933\n",
      "Epoch 2:  53%|█████▎    | 113/212 [00:03<00:02, 36.99it/s, v_num=0]Training loss: 1.0815556049346924\n",
      "Epoch 2:  54%|█████▍    | 114/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.2472466230392456\n",
      "Epoch 2:  54%|█████▍    | 115/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.072744369506836\n",
      "Epoch 2:  55%|█████▍    | 116/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.6326572895050049\n",
      "Epoch 2:  55%|█████▌    | 117/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.3272640705108643\n",
      "Epoch 2:  56%|█████▌    | 118/212 [00:03<00:02, 37.00it/s, v_num=0]Training loss: 1.1900206804275513\n",
      "Epoch 2:  56%|█████▌    | 119/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 1.1590228080749512\n",
      "Epoch 2:  57%|█████▋    | 120/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 1.5966657400131226\n",
      "Epoch 2:  57%|█████▋    | 121/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 1.6868586540222168\n",
      "Epoch 2:  58%|█████▊    | 122/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 1.153546929359436\n",
      "Epoch 2:  58%|█████▊    | 123/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 1.341933012008667\n",
      "Epoch 2:  58%|█████▊    | 124/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 1.6218671798706055\n",
      "Epoch 2:  59%|█████▉    | 125/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 2.130051612854004\n",
      "Epoch 2:  59%|█████▉    | 126/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 1.7963231801986694\n",
      "Epoch 2:  60%|█████▉    | 127/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 0.9117566347122192\n",
      "Epoch 2:  60%|██████    | 128/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 0.9257934093475342\n",
      "Epoch 2:  61%|██████    | 129/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 1.30781888961792\n",
      "Epoch 2:  61%|██████▏   | 130/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.0593758821487427\n",
      "Epoch 2:  62%|██████▏   | 131/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.0522022247314453\n",
      "Epoch 2:  62%|██████▏   | 132/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.5486949682235718\n",
      "Epoch 2:  63%|██████▎   | 133/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 0.8844362497329712\n",
      "Epoch 2:  63%|██████▎   | 134/212 [00:03<00:02, 37.05it/s, v_num=0]Training loss: 1.1834118366241455\n",
      "Epoch 2:  64%|██████▎   | 135/212 [00:03<00:02, 37.05it/s, v_num=0]Training loss: 1.0463132858276367\n",
      "Epoch 2:  64%|██████▍   | 136/212 [00:03<00:02, 36.99it/s, v_num=0]Training loss: 1.1344170570373535\n",
      "Epoch 2:  65%|██████▍   | 137/212 [00:03<00:02, 36.99it/s, v_num=0]Training loss: 0.9152127504348755\n",
      "Epoch 2:  65%|██████▌   | 138/212 [00:03<00:01, 37.00it/s, v_num=0]Training loss: 0.7718577980995178\n",
      "Epoch 2:  66%|██████▌   | 139/212 [00:03<00:01, 37.00it/s, v_num=0]Training loss: 1.9629428386688232\n",
      "Epoch 2:  66%|██████▌   | 140/212 [00:03<00:01, 37.01it/s, v_num=0]Training loss: 1.1289212703704834\n",
      "Epoch 2:  67%|██████▋   | 141/212 [00:03<00:01, 37.01it/s, v_num=0]Training loss: 0.8059021830558777\n",
      "Epoch 2:  67%|██████▋   | 142/212 [00:03<00:01, 37.01it/s, v_num=0]Training loss: 0.8670042753219604\n",
      "Epoch 2:  67%|██████▋   | 143/212 [00:03<00:01, 37.02it/s, v_num=0]Training loss: 0.8410772085189819\n",
      "Epoch 2:  68%|██████▊   | 144/212 [00:03<00:01, 37.02it/s, v_num=0]Training loss: 1.0915840864181519\n",
      "Epoch 2:  68%|██████▊   | 145/212 [00:03<00:01, 37.01it/s, v_num=0]Training loss: 2.997886896133423\n",
      "Epoch 2:  69%|██████▉   | 146/212 [00:03<00:01, 37.00it/s, v_num=0]Training loss: 1.1056941747665405\n",
      "Epoch 2:  69%|██████▉   | 147/212 [00:03<00:01, 36.99it/s, v_num=0]Training loss: 1.1543229818344116\n",
      "Epoch 2:  70%|██████▉   | 148/212 [00:04<00:01, 36.99it/s, v_num=0]Training loss: 1.253432035446167\n",
      "Epoch 2:  70%|███████   | 149/212 [00:04<00:01, 36.98it/s, v_num=0]Training loss: 1.1793614625930786\n",
      "Epoch 2:  71%|███████   | 150/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.0606411695480347\n",
      "Epoch 2:  71%|███████   | 151/212 [00:04<00:01, 36.96it/s, v_num=0]Training loss: 1.3501195907592773\n",
      "Epoch 2:  72%|███████▏  | 152/212 [00:04<00:01, 36.96it/s, v_num=0]Training loss: 1.0172086954116821\n",
      "Epoch 2:  72%|███████▏  | 153/212 [00:04<00:01, 36.95it/s, v_num=0]Training loss: 0.9465864300727844\n",
      "Epoch 2:  73%|███████▎  | 154/212 [00:04<00:01, 36.95it/s, v_num=0]Training loss: 1.0188229084014893\n",
      "Epoch 2:  73%|███████▎  | 155/212 [00:04<00:01, 36.95it/s, v_num=0]Training loss: 1.7118924856185913\n",
      "Epoch 2:  74%|███████▎  | 156/212 [00:04<00:01, 36.96it/s, v_num=0]Training loss: 1.1079195737838745\n",
      "Epoch 2:  74%|███████▍  | 157/212 [00:04<00:01, 36.96it/s, v_num=0]Training loss: 1.633533239364624\n",
      "Epoch 2:  75%|███████▍  | 158/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.023618459701538\n",
      "Epoch 2:  75%|███████▌  | 159/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 0.9604982137680054\n",
      "Epoch 2:  75%|███████▌  | 160/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.6003609895706177\n",
      "Epoch 2:  76%|███████▌  | 161/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.3763254880905151\n",
      "Epoch 2:  76%|███████▋  | 162/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.0098991394042969\n",
      "Epoch 2:  77%|███████▋  | 163/212 [00:04<00:01, 36.97it/s, v_num=0]Training loss: 1.8131165504455566\n",
      "Epoch 2:  77%|███████▋  | 164/212 [00:04<00:01, 36.98it/s, v_num=0]Training loss: 1.3174662590026855\n",
      "Epoch 2:  78%|███████▊  | 165/212 [00:04<00:01, 36.98it/s, v_num=0]Training loss: 0.8081801533699036\n",
      "Epoch 2:  78%|███████▊  | 166/212 [00:04<00:01, 36.98it/s, v_num=0]Training loss: 1.023425817489624\n",
      "Epoch 2:  79%|███████▉  | 167/212 [00:04<00:01, 36.98it/s, v_num=0]Training loss: 0.9870867729187012\n",
      "Epoch 2:  79%|███████▉  | 168/212 [00:04<00:01, 36.99it/s, v_num=0]Training loss: 0.8829063177108765\n",
      "Epoch 2:  80%|███████▉  | 169/212 [00:04<00:01, 36.99it/s, v_num=0]Training loss: 1.3901216983795166\n",
      "Epoch 2:  80%|████████  | 170/212 [00:04<00:01, 36.99it/s, v_num=0]Training loss: 1.6475343704223633\n",
      "Epoch 2:  81%|████████  | 171/212 [00:04<00:01, 36.99it/s, v_num=0]Training loss: 0.8338133692741394\n",
      "Epoch 2:  81%|████████  | 172/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 1.1091294288635254\n",
      "Epoch 2:  82%|████████▏ | 173/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 0.9254798889160156\n",
      "Epoch 2:  82%|████████▏ | 174/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 1.4329583644866943\n",
      "Epoch 2:  83%|████████▎ | 175/212 [00:04<00:00, 37.00it/s, v_num=0]Training loss: 1.0404890775680542\n",
      "Epoch 2:  83%|████████▎ | 176/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 1.0648736953735352\n",
      "Epoch 2:  83%|████████▎ | 177/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 1.8866969347000122\n",
      "Epoch 2:  84%|████████▍ | 178/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 1.2442169189453125\n",
      "Epoch 2:  84%|████████▍ | 179/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 2.3354909420013428\n",
      "Epoch 2:  85%|████████▍ | 180/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 0.9855588674545288\n",
      "Epoch 2:  85%|████████▌ | 181/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 0.9749745726585388\n",
      "Epoch 2:  86%|████████▌ | 182/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.777501106262207\n",
      "Epoch 2:  86%|████████▋ | 183/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.1291872262954712\n",
      "Epoch 2:  87%|████████▋ | 184/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.404055118560791\n",
      "Epoch 2:  87%|████████▋ | 185/212 [00:04<00:00, 37.01it/s, v_num=0]Training loss: 0.8753134608268738\n",
      "Epoch 2:  88%|████████▊ | 186/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.1684900522232056\n",
      "Epoch 2:  88%|████████▊ | 187/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.3037272691726685\n",
      "Epoch 2:  89%|████████▊ | 188/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 1.0251998901367188\n",
      "Epoch 2:  89%|████████▉ | 189/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 2.07832932472229\n",
      "Epoch 2:  90%|████████▉ | 190/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.4028677940368652\n",
      "Epoch 2:  90%|█████████ | 191/212 [00:05<00:00, 36.97it/s, v_num=0]Training loss: 1.0476438999176025\n",
      "Epoch 2:  91%|█████████ | 192/212 [00:05<00:00, 36.97it/s, v_num=0]Training loss: 1.4699523448944092\n",
      "Epoch 2:  91%|█████████ | 193/212 [00:05<00:00, 36.97it/s, v_num=0]Training loss: 0.9423801898956299\n",
      "Epoch 2:  92%|█████████▏| 194/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.137622594833374\n",
      "Epoch 2:  92%|█████████▏| 195/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.0855563879013062\n",
      "Epoch 2:  92%|█████████▏| 196/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.0938724279403687\n",
      "Epoch 2:  93%|█████████▎| 197/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 0.9574631452560425\n",
      "Epoch 2:  93%|█████████▎| 198/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.7986611127853394\n",
      "Epoch 2:  94%|█████████▍| 199/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.1051788330078125\n",
      "Epoch 2:  94%|█████████▍| 200/212 [00:05<00:00, 36.98it/s, v_num=0]Training loss: 1.5118412971496582\n",
      "Epoch 2:  95%|█████████▍| 201/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 0.88254314661026\n",
      "Epoch 2:  95%|█████████▌| 202/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 0.8431664109230042\n",
      "Epoch 2:  96%|█████████▌| 203/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 1.3333699703216553\n",
      "Epoch 2:  96%|█████████▌| 204/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 0.9354405403137207\n",
      "Epoch 2:  97%|█████████▋| 205/212 [00:05<00:00, 36.99it/s, v_num=0]Training loss: 1.8642231225967407\n",
      "Epoch 2:  97%|█████████▋| 206/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.413520097732544\n",
      "Epoch 2:  98%|█████████▊| 207/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.5964548587799072\n",
      "Epoch 2:  98%|█████████▊| 208/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 1.7999656200408936\n",
      "Epoch 2:  99%|█████████▊| 209/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 0.804454505443573\n",
      "Epoch 2:  99%|█████████▉| 210/212 [00:05<00:00, 37.00it/s, v_num=0]Training loss: 0.9505674839019775\n",
      "Epoch 2: 100%|█████████▉| 211/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.4470635652542114\n",
      "Epoch 2: 100%|██████████| 212/212 [00:05<00:00, 37.01it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.3121912479400635\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 85.97it/s]\u001b[AValidation loss: 2.0992960929870605\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 89.87it/s]\u001b[AValidation loss: 1.1364561319351196\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 91.27it/s]\u001b[AValidation loss: 0.8446049094200134\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 92.75it/s]\u001b[AValidation loss: 0.8564379215240479\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 93.53it/s]\u001b[AValidation loss: 1.0333683490753174\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 94.00it/s]\u001b[AValidation loss: 1.3885856866836548\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 94.08it/s]\u001b[AValidation loss: 2.377591133117676\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 94.11it/s]\u001b[AValidation loss: 0.7535597681999207\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 94.37it/s]\u001b[AValidation loss: 0.8691994547843933\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 94.05it/s]\u001b[AValidation loss: 0.8938342928886414\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 94.27it/s]\u001b[AValidation loss: 1.2104899883270264\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 94.48it/s]\u001b[AValidation loss: 1.1762067079544067\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 94.57it/s]\u001b[AValidation loss: 1.1197181940078735\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 94.77it/s]\u001b[AValidation loss: 0.9270011782646179\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 94.79it/s]\u001b[AValidation loss: 1.2908344268798828\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 94.90it/s]\u001b[AValidation loss: 1.185376763343811\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 95.05it/s]\u001b[AValidation loss: 1.7245159149169922\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 95.13it/s]\u001b[AValidation loss: 0.8945779204368591\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 95.13it/s]\u001b[AValidation loss: 1.3855924606323242\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 95.18it/s]\u001b[AValidation loss: 1.2465463876724243\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 95.14it/s]\u001b[AValidation loss: 1.2831610441207886\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.13it/s]\u001b[AValidation loss: 1.2640098333358765\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.21it/s]\u001b[AValidation loss: 1.121160864830017\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.29it/s]\u001b[AValidation loss: 1.5722254514694214\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.26it/s]\u001b[AValidation loss: 0.9216959476470947\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.33it/s]\u001b[AValidation loss: 1.1479511260986328\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 95.38it/s]\u001b[AValidation loss: 1.178372859954834\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 95.30it/s]\u001b[AValidation loss: 1.1704908609390259\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 95.07it/s]\u001b[AValidation loss: 1.414011001586914\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 94.81it/s]\u001b[AValidation loss: 1.0357297658920288\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 93.04it/s]\u001b[AValidation loss: 1.1138553619384766\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 92.77it/s]\u001b[AValidation loss: 1.1002925634384155\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 92.61it/s]\u001b[AValidation loss: 0.9979069232940674\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 92.48it/s]\u001b[AValidation loss: 2.085861921310425\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 92.16it/s]\u001b[AValidation loss: 0.7404505014419556\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 92.04it/s]\u001b[AValidation loss: 1.6822996139526367\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 91.64it/s]\u001b[AValidation loss: 0.7934777736663818\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 90.67it/s]\u001b[AValidation loss: 1.2247494459152222\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 90.58it/s]\u001b[AValidation loss: 1.1178109645843506\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 90.36it/s]\u001b[AValidation loss: 0.9932215809822083\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 90.32it/s]\u001b[AValidation loss: 0.8816579580307007\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 90.27it/s]\u001b[AValidation loss: 0.9825664758682251\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 90.21it/s]\u001b[AValidation loss: 1.4522929191589355\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 90.15it/s]\u001b[AValidation loss: 1.6090519428253174\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 90.08it/s]\u001b[AValidation loss: 0.863491415977478\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 90.03it/s]\u001b[AValidation loss: 1.1369998455047607\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 90.06it/s]\u001b[AValidation loss: 1.1255412101745605\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 90.13it/s]\u001b[AValidation loss: 1.2643486261367798\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 90.21it/s]\u001b[AValidation loss: 1.5585558414459229\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 90.30it/s]\u001b[AValidation loss: 1.413895845413208\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 90.40it/s]\u001b[AValidation loss: 1.2417293787002563\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 90.49it/s]\u001b[AValidation loss: 1.5636470317840576\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 90.57it/s]\u001b[AValidation loss: 1.2345449924468994\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 90.62it/s]\u001b[AValidation loss: 2.299783945083618\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 90.77it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.3064664602279663\n",
      "Epoch 3:   0%|          | 1/212 [00:00<00:08, 23.60it/s, v_num=0]Training loss: 0.8338660001754761\n",
      "Epoch 3:   1%|          | 2/212 [00:00<00:07, 28.86it/s, v_num=0]Training loss: 1.2745656967163086\n",
      "Epoch 3:   1%|▏         | 3/212 [00:00<00:06, 31.23it/s, v_num=0]Training loss: 1.7704200744628906\n",
      "Epoch 3:   2%|▏         | 4/212 [00:00<00:06, 32.56it/s, v_num=0]Training loss: 9.630342483520508\n",
      "Epoch 3:   2%|▏         | 5/212 [00:00<00:06, 33.43it/s, v_num=0]Training loss: 1.5923142433166504\n",
      "Epoch 3:   3%|▎         | 6/212 [00:00<00:06, 34.00it/s, v_num=0]Training loss: 1.1138886213302612\n",
      "Epoch 3:   3%|▎         | 7/212 [00:00<00:05, 34.35it/s, v_num=0]Training loss: 1.1204354763031006\n",
      "Epoch 3:   4%|▍         | 8/212 [00:00<00:05, 34.74it/s, v_num=0]Training loss: 1.0594309568405151\n",
      "Epoch 3:   4%|▍         | 9/212 [00:00<00:05, 34.99it/s, v_num=0]Training loss: 1.0494049787521362\n",
      "Epoch 3:   5%|▍         | 10/212 [00:00<00:05, 35.21it/s, v_num=0]Training loss: 1.235795259475708\n",
      "Epoch 3:   5%|▌         | 11/212 [00:00<00:05, 35.41it/s, v_num=0]Training loss: 1.3712551593780518\n",
      "Epoch 3:   6%|▌         | 12/212 [00:00<00:05, 35.57it/s, v_num=0]Training loss: 1.7653225660324097\n",
      "Epoch 3:   6%|▌         | 13/212 [00:00<00:05, 35.71it/s, v_num=0]Training loss: 1.7767229080200195\n",
      "Epoch 3:   7%|▋         | 14/212 [00:00<00:05, 35.83it/s, v_num=0]Training loss: 0.8753650188446045\n",
      "Epoch 3:   7%|▋         | 15/212 [00:00<00:05, 35.93it/s, v_num=0]Training loss: 1.205589771270752\n",
      "Epoch 3:   8%|▊         | 16/212 [00:00<00:05, 36.02it/s, v_num=0]Training loss: 0.7405109405517578\n",
      "Epoch 3:   8%|▊         | 17/212 [00:00<00:05, 36.10it/s, v_num=0]Training loss: 1.317884087562561\n",
      "Epoch 3:   8%|▊         | 18/212 [00:00<00:05, 36.16it/s, v_num=0]Training loss: 0.9827236533164978\n",
      "Epoch 3:   9%|▉         | 19/212 [00:00<00:05, 36.23it/s, v_num=0]Training loss: 1.0587090253829956\n",
      "Epoch 3:   9%|▉         | 20/212 [00:00<00:05, 36.29it/s, v_num=0]Training loss: 3.6815550327301025\n",
      "Epoch 3:  10%|▉         | 21/212 [00:00<00:05, 36.34it/s, v_num=0]Training loss: 1.0161515474319458\n",
      "Epoch 3:  10%|█         | 22/212 [00:00<00:05, 36.39it/s, v_num=0]Training loss: 0.9175838828086853\n",
      "Epoch 3:  11%|█         | 23/212 [00:00<00:05, 36.43it/s, v_num=0]Training loss: 1.074573278427124\n",
      "Epoch 3:  11%|█▏        | 24/212 [00:00<00:05, 35.13it/s, v_num=0]Training loss: 1.3506187200546265\n",
      "Epoch 3:  12%|█▏        | 25/212 [00:00<00:05, 35.21it/s, v_num=0]Training loss: 0.6975980997085571\n",
      "Epoch 3:  12%|█▏        | 26/212 [00:00<00:05, 35.30it/s, v_num=0]Training loss: 1.7313847541809082\n",
      "Epoch 3:  13%|█▎        | 27/212 [00:00<00:05, 35.37it/s, v_num=0]Training loss: 1.2950152158737183\n",
      "Epoch 3:  13%|█▎        | 28/212 [00:00<00:05, 35.43it/s, v_num=0]Training loss: 1.0115838050842285\n",
      "Epoch 3:  14%|█▎        | 29/212 [00:00<00:05, 35.43it/s, v_num=0]Training loss: 1.4832310676574707\n",
      "Epoch 3:  14%|█▍        | 30/212 [00:00<00:05, 35.45it/s, v_num=0]Training loss: 1.0166622400283813\n",
      "Epoch 3:  15%|█▍        | 31/212 [00:00<00:05, 35.47it/s, v_num=0]Training loss: 0.9478706121444702\n",
      "Epoch 3:  15%|█▌        | 32/212 [00:00<00:05, 35.49it/s, v_num=0]Training loss: 0.8846370577812195\n",
      "Epoch 3:  16%|█▌        | 33/212 [00:00<00:05, 35.51it/s, v_num=0]Training loss: 1.3098232746124268\n",
      "Epoch 3:  16%|█▌        | 34/212 [00:00<00:05, 35.50it/s, v_num=0]Training loss: 1.720270037651062\n",
      "Epoch 3:  17%|█▋        | 35/212 [00:00<00:04, 35.52it/s, v_num=0]Training loss: 1.2458219528198242\n",
      "Epoch 3:  17%|█▋        | 36/212 [00:01<00:04, 35.54it/s, v_num=0]Training loss: 0.9479108452796936\n",
      "Epoch 3:  17%|█▋        | 37/212 [00:01<00:04, 35.59it/s, v_num=0]Training loss: 0.9691458940505981\n",
      "Epoch 3:  18%|█▊        | 38/212 [00:01<00:04, 35.65it/s, v_num=0]Training loss: 0.8998821377754211\n",
      "Epoch 3:  18%|█▊        | 39/212 [00:01<00:04, 35.70it/s, v_num=0]Training loss: 1.1934576034545898\n",
      "Epoch 3:  19%|█▉        | 40/212 [00:01<00:04, 35.73it/s, v_num=0]Training loss: 1.160007119178772\n",
      "Epoch 3:  19%|█▉        | 41/212 [00:01<00:04, 35.76it/s, v_num=0]Training loss: 0.8981016278266907\n",
      "Epoch 3:  20%|█▉        | 42/212 [00:01<00:04, 35.80it/s, v_num=0]Training loss: 0.9341759085655212\n",
      "Epoch 3:  20%|██        | 43/212 [00:01<00:04, 35.83it/s, v_num=0]Training loss: 0.9190775752067566\n",
      "Epoch 3:  21%|██        | 44/212 [00:01<00:04, 35.87it/s, v_num=0]Training loss: 0.9695958495140076\n",
      "Epoch 3:  21%|██        | 45/212 [00:01<00:04, 35.90it/s, v_num=0]Training loss: 9.678387641906738\n",
      "Epoch 3:  22%|██▏       | 46/212 [00:01<00:04, 35.93it/s, v_num=0]Training loss: 0.7867140769958496\n",
      "Epoch 3:  22%|██▏       | 47/212 [00:01<00:04, 35.95it/s, v_num=0]Training loss: 1.9190841913223267\n",
      "Epoch 3:  23%|██▎       | 48/212 [00:01<00:04, 35.98it/s, v_num=0]Training loss: 1.2155331373214722\n",
      "Epoch 3:  23%|██▎       | 49/212 [00:01<00:04, 36.01it/s, v_num=0]Training loss: 1.1556421518325806\n",
      "Epoch 3:  24%|██▎       | 50/212 [00:01<00:04, 36.04it/s, v_num=0]Training loss: 0.9694231152534485\n",
      "Epoch 3:  24%|██▍       | 51/212 [00:01<00:04, 36.06it/s, v_num=0]Training loss: 1.064863681793213\n",
      "Epoch 3:  25%|██▍       | 52/212 [00:01<00:04, 36.09it/s, v_num=0]Training loss: 1.7434393167495728\n",
      "Epoch 3:  25%|██▌       | 53/212 [00:01<00:04, 36.11it/s, v_num=0]Training loss: 1.7344619035720825\n",
      "Epoch 3:  25%|██▌       | 54/212 [00:01<00:04, 36.14it/s, v_num=0]Training loss: 1.148653507232666\n",
      "Epoch 3:  26%|██▌       | 55/212 [00:01<00:04, 36.15it/s, v_num=0]Training loss: 1.074965238571167\n",
      "Epoch 3:  26%|██▋       | 56/212 [00:01<00:04, 36.18it/s, v_num=0]Training loss: 1.7238346338272095\n",
      "Epoch 3:  27%|██▋       | 57/212 [00:01<00:04, 36.20it/s, v_num=0]Training loss: 1.4736638069152832\n",
      "Epoch 3:  27%|██▋       | 58/212 [00:01<00:04, 36.22it/s, v_num=0]Training loss: 1.473851203918457\n",
      "Epoch 3:  28%|██▊       | 59/212 [00:01<00:04, 36.24it/s, v_num=0]Training loss: 1.309151291847229\n",
      "Epoch 3:  28%|██▊       | 60/212 [00:01<00:04, 36.26it/s, v_num=0]Training loss: 1.229193925857544\n",
      "Epoch 3:  29%|██▉       | 61/212 [00:01<00:04, 36.28it/s, v_num=0]Training loss: 1.3306142091751099\n",
      "Epoch 3:  29%|██▉       | 62/212 [00:01<00:04, 36.30it/s, v_num=0]Training loss: 1.205781102180481\n",
      "Epoch 3:  30%|██▉       | 63/212 [00:01<00:04, 36.31it/s, v_num=0]Training loss: 1.1164873838424683\n",
      "Epoch 3:  30%|███       | 64/212 [00:01<00:04, 36.33it/s, v_num=0]Training loss: 1.8480417728424072\n",
      "Epoch 3:  31%|███       | 65/212 [00:01<00:04, 36.35it/s, v_num=0]Training loss: 1.383558988571167\n",
      "Epoch 3:  31%|███       | 66/212 [00:01<00:04, 36.36it/s, v_num=0]Training loss: 1.0607240200042725\n",
      "Epoch 3:  32%|███▏      | 67/212 [00:01<00:03, 36.38it/s, v_num=0]Training loss: 1.0741467475891113\n",
      "Epoch 3:  32%|███▏      | 68/212 [00:01<00:03, 36.37it/s, v_num=0]Training loss: 1.0438511371612549\n",
      "Epoch 3:  33%|███▎      | 69/212 [00:01<00:03, 36.36it/s, v_num=0]Training loss: 1.1488769054412842\n",
      "Epoch 3:  33%|███▎      | 70/212 [00:01<00:03, 36.36it/s, v_num=0]Training loss: 1.6272536516189575\n",
      "Epoch 3:  33%|███▎      | 71/212 [00:01<00:03, 36.36it/s, v_num=0]Training loss: 1.147312879562378\n",
      "Epoch 3:  34%|███▍      | 72/212 [00:01<00:03, 36.35it/s, v_num=0]Training loss: 1.4190298318862915\n",
      "Epoch 3:  34%|███▍      | 73/212 [00:02<00:03, 36.33it/s, v_num=0]Training loss: 1.7453171014785767\n",
      "Epoch 3:  35%|███▍      | 74/212 [00:02<00:03, 36.33it/s, v_num=0]Training loss: 0.8680351972579956\n",
      "Epoch 3:  35%|███▌      | 75/212 [00:02<00:03, 36.32it/s, v_num=0]Training loss: 1.50661301612854\n",
      "Epoch 3:  36%|███▌      | 76/212 [00:02<00:03, 36.32it/s, v_num=0]Training loss: 1.285351037979126\n",
      "Epoch 3:  36%|███▋      | 77/212 [00:02<00:03, 36.33it/s, v_num=0]Training loss: 0.979743480682373\n",
      "Epoch 3:  37%|███▋      | 78/212 [00:02<00:03, 36.34it/s, v_num=0]Training loss: 0.6846615076065063\n",
      "Epoch 3:  37%|███▋      | 79/212 [00:02<00:03, 36.36it/s, v_num=0]Training loss: 1.035530686378479\n",
      "Epoch 3:  38%|███▊      | 80/212 [00:02<00:03, 36.37it/s, v_num=0]Training loss: 1.379621148109436\n",
      "Epoch 3:  38%|███▊      | 81/212 [00:02<00:03, 36.38it/s, v_num=0]Training loss: 1.3353151082992554\n",
      "Epoch 3:  39%|███▊      | 82/212 [00:02<00:03, 36.39it/s, v_num=0]Training loss: 0.8509437441825867\n",
      "Epoch 3:  39%|███▉      | 83/212 [00:02<00:03, 36.41it/s, v_num=0]Training loss: 1.2608951330184937\n",
      "Epoch 3:  40%|███▉      | 84/212 [00:02<00:03, 36.42it/s, v_num=0]Training loss: 1.1385265588760376\n",
      "Epoch 3:  40%|████      | 85/212 [00:02<00:03, 36.43it/s, v_num=0]Training loss: 1.6895999908447266\n",
      "Epoch 3:  41%|████      | 86/212 [00:02<00:03, 36.43it/s, v_num=0]Training loss: 0.6842750310897827\n",
      "Epoch 3:  41%|████      | 87/212 [00:02<00:03, 36.44it/s, v_num=0]Training loss: 1.2579610347747803\n",
      "Epoch 3:  42%|████▏     | 88/212 [00:02<00:03, 36.45it/s, v_num=0]Training loss: 0.9150530099868774\n",
      "Epoch 3:  42%|████▏     | 89/212 [00:02<00:03, 36.46it/s, v_num=0]Training loss: 1.564009666442871\n",
      "Epoch 3:  42%|████▏     | 90/212 [00:02<00:03, 36.47it/s, v_num=0]Training loss: 1.2072092294692993\n",
      "Epoch 3:  43%|████▎     | 91/212 [00:02<00:03, 36.48it/s, v_num=0]Training loss: 1.2412666082382202\n",
      "Epoch 3:  43%|████▎     | 92/212 [00:02<00:03, 36.42it/s, v_num=0]Training loss: 0.7286465167999268\n",
      "Epoch 3:  44%|████▍     | 93/212 [00:02<00:03, 36.42it/s, v_num=0]Training loss: 0.9328829050064087\n",
      "Epoch 3:  44%|████▍     | 94/212 [00:02<00:03, 36.44it/s, v_num=0]Training loss: 1.4391145706176758\n",
      "Epoch 3:  45%|████▍     | 95/212 [00:02<00:03, 36.45it/s, v_num=0]Training loss: 0.9214074611663818\n",
      "Epoch 3:  45%|████▌     | 96/212 [00:02<00:03, 36.46it/s, v_num=0]Training loss: 1.0718472003936768\n",
      "Epoch 3:  46%|████▌     | 97/212 [00:02<00:03, 36.47it/s, v_num=0]Training loss: 2.7829644680023193\n",
      "Epoch 3:  46%|████▌     | 98/212 [00:02<00:03, 36.48it/s, v_num=0]Training loss: 1.2999731302261353\n",
      "Epoch 3:  47%|████▋     | 99/212 [00:02<00:03, 36.49it/s, v_num=0]Training loss: 0.8771789073944092\n",
      "Epoch 3:  47%|████▋     | 100/212 [00:02<00:03, 36.50it/s, v_num=0]Training loss: 1.1477766036987305\n",
      "Epoch 3:  48%|████▊     | 101/212 [00:02<00:03, 36.50it/s, v_num=0]Training loss: 1.062362790107727\n",
      "Epoch 3:  48%|████▊     | 102/212 [00:02<00:03, 36.51it/s, v_num=0]Training loss: 1.62662672996521\n",
      "Epoch 3:  49%|████▊     | 103/212 [00:02<00:02, 36.52it/s, v_num=0]Training loss: 2.4785523414611816\n",
      "Epoch 3:  49%|████▉     | 104/212 [00:02<00:02, 36.53it/s, v_num=0]Training loss: 1.0775480270385742\n",
      "Epoch 3:  50%|████▉     | 105/212 [00:02<00:02, 36.54it/s, v_num=0]Training loss: 0.8995717763900757\n",
      "Epoch 3:  50%|█████     | 106/212 [00:02<00:02, 36.54it/s, v_num=0]Training loss: 1.1423205137252808\n",
      "Epoch 3:  50%|█████     | 107/212 [00:02<00:02, 36.55it/s, v_num=0]Training loss: 1.0560431480407715\n",
      "Epoch 3:  51%|█████     | 108/212 [00:02<00:02, 36.54it/s, v_num=0]Training loss: 1.1639454364776611\n",
      "Epoch 3:  51%|█████▏    | 109/212 [00:02<00:02, 36.54it/s, v_num=0]Training loss: 1.4410006999969482\n",
      "Epoch 3:  52%|█████▏    | 110/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 1.2343555688858032\n",
      "Epoch 3:  52%|█████▏    | 111/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 2.0195484161376953\n",
      "Epoch 3:  53%|█████▎    | 112/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 1.4110562801361084\n",
      "Epoch 3:  53%|█████▎    | 113/212 [00:03<00:02, 36.51it/s, v_num=0]Training loss: 1.161649465560913\n",
      "Epoch 3:  54%|█████▍    | 114/212 [00:03<00:02, 36.51it/s, v_num=0]Training loss: 1.553144931793213\n",
      "Epoch 3:  54%|█████▍    | 115/212 [00:03<00:02, 36.51it/s, v_num=0]Training loss: 1.0578185319900513\n",
      "Epoch 3:  55%|█████▍    | 116/212 [00:03<00:02, 36.51it/s, v_num=0]Training loss: 1.3145277500152588\n",
      "Epoch 3:  55%|█████▌    | 117/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 1.0105105638504028\n",
      "Epoch 3:  56%|█████▌    | 118/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 1.4375345706939697\n",
      "Epoch 3:  56%|█████▌    | 119/212 [00:03<00:02, 36.54it/s, v_num=0]Training loss: 1.1518198251724243\n",
      "Epoch 3:  57%|█████▋    | 120/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 0.8650367856025696\n",
      "Epoch 3:  57%|█████▋    | 121/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 1.8779346942901611\n",
      "Epoch 3:  58%|█████▊    | 122/212 [00:03<00:02, 36.56it/s, v_num=0]Training loss: 1.3271057605743408\n",
      "Epoch 3:  58%|█████▊    | 123/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 1.0361695289611816\n",
      "Epoch 3:  58%|█████▊    | 124/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 3.929811716079712\n",
      "Epoch 3:  59%|█████▉    | 125/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 1.974639892578125\n",
      "Epoch 3:  59%|█████▉    | 126/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 0.9802414774894714\n",
      "Epoch 3:  60%|█████▉    | 127/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 1.5354514122009277\n",
      "Epoch 3:  60%|██████    | 128/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 1.259821891784668\n",
      "Epoch 3:  61%|██████    | 129/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 1.3042839765548706\n",
      "Epoch 3:  61%|██████▏   | 130/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 0.9620488286018372\n",
      "Epoch 3:  62%|██████▏   | 131/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 1.2665035724639893\n",
      "Epoch 3:  62%|██████▏   | 132/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 1.149322271347046\n",
      "Epoch 3:  63%|██████▎   | 133/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 0.8665851950645447\n",
      "Epoch 3:  63%|██████▎   | 134/212 [00:03<00:02, 36.63it/s, v_num=0]Training loss: 1.1508746147155762\n",
      "Epoch 3:  64%|██████▎   | 135/212 [00:03<00:02, 36.63it/s, v_num=0]Training loss: 1.3467031717300415\n",
      "Epoch 3:  64%|██████▍   | 136/212 [00:03<00:02, 36.64it/s, v_num=0]Training loss: 1.298259973526001\n",
      "Epoch 3:  65%|██████▍   | 137/212 [00:03<00:02, 36.65it/s, v_num=0]Training loss: 1.0829861164093018\n",
      "Epoch 3:  65%|██████▌   | 138/212 [00:03<00:02, 36.65it/s, v_num=0]Training loss: 1.2695703506469727\n",
      "Epoch 3:  66%|██████▌   | 139/212 [00:03<00:01, 36.66it/s, v_num=0]Training loss: 1.8825246095657349\n",
      "Epoch 3:  66%|██████▌   | 140/212 [00:03<00:01, 36.66it/s, v_num=0]Training loss: 0.8178570866584778\n",
      "Epoch 3:  67%|██████▋   | 141/212 [00:03<00:01, 36.67it/s, v_num=0]Training loss: 1.3715864419937134\n",
      "Epoch 3:  67%|██████▋   | 142/212 [00:03<00:01, 36.67it/s, v_num=0]Training loss: 1.0636619329452515\n",
      "Epoch 3:  67%|██████▋   | 143/212 [00:03<00:01, 36.68it/s, v_num=0]Training loss: 1.188989520072937\n",
      "Epoch 3:  68%|██████▊   | 144/212 [00:03<00:01, 36.68it/s, v_num=0]Training loss: 0.9534844756126404\n",
      "Epoch 3:  68%|██████▊   | 145/212 [00:03<00:01, 36.69it/s, v_num=0]Training loss: 0.9499136805534363\n",
      "Epoch 3:  69%|██████▉   | 146/212 [00:03<00:01, 36.69it/s, v_num=0]Training loss: 0.7944809198379517\n",
      "Epoch 3:  69%|██████▉   | 147/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 1.2084221839904785\n",
      "Epoch 3:  70%|██████▉   | 148/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.9055656790733337\n",
      "Epoch 3:  70%|███████   | 149/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 1.236810326576233\n",
      "Epoch 3:  71%|███████   | 150/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.822337806224823\n",
      "Epoch 3:  71%|███████   | 151/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 2.579587697982788\n",
      "Epoch 3:  72%|███████▏  | 152/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 0.9188268184661865\n",
      "Epoch 3:  72%|███████▏  | 153/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.8817744255065918\n",
      "Epoch 3:  73%|███████▎  | 154/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 1.0711230039596558\n",
      "Epoch 3:  73%|███████▎  | 155/212 [00:04<00:01, 36.66it/s, v_num=0]Training loss: 0.9177451133728027\n",
      "Epoch 3:  74%|███████▎  | 156/212 [00:04<00:01, 36.66it/s, v_num=0]Training loss: 0.6940780282020569\n",
      "Epoch 3:  74%|███████▍  | 157/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 0.7528950572013855\n",
      "Epoch 3:  75%|███████▍  | 158/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 0.9574927091598511\n",
      "Epoch 3:  75%|███████▌  | 159/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.228480577468872\n",
      "Epoch 3:  75%|███████▌  | 160/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.0767269134521484\n",
      "Epoch 3:  76%|███████▌  | 161/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 1.6345096826553345\n",
      "Epoch 3:  76%|███████▋  | 162/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.9981034994125366\n",
      "Epoch 3:  77%|███████▋  | 163/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 1.0132136344909668\n",
      "Epoch 3:  77%|███████▋  | 164/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 1.2681607007980347\n",
      "Epoch 3:  78%|███████▊  | 165/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 2.3090643882751465\n",
      "Epoch 3:  78%|███████▊  | 166/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 1.4547199010849\n",
      "Epoch 3:  79%|███████▉  | 167/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.9796837568283081\n",
      "Epoch 3:  79%|███████▉  | 168/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.7406391501426697\n",
      "Epoch 3:  80%|███████▉  | 169/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.060397982597351\n",
      "Epoch 3:  80%|████████  | 170/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.1099395751953125\n",
      "Epoch 3:  81%|████████  | 171/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 0.9807825088500977\n",
      "Epoch 3:  81%|████████  | 172/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 1.3747388124465942\n",
      "Epoch 3:  82%|████████▏ | 173/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 0.961787223815918\n",
      "Epoch 3:  82%|████████▏ | 174/212 [00:04<00:01, 36.74it/s, v_num=0]Training loss: 0.9953517913818359\n",
      "Epoch 3:  83%|████████▎ | 175/212 [00:04<00:01, 36.74it/s, v_num=0]Training loss: 1.4629172086715698\n",
      "Epoch 3:  83%|████████▎ | 176/212 [00:04<00:00, 36.74it/s, v_num=0]Training loss: 0.9002822041511536\n",
      "Epoch 3:  83%|████████▎ | 177/212 [00:04<00:00, 36.75it/s, v_num=0]Training loss: 1.2102843523025513\n",
      "Epoch 3:  84%|████████▍ | 178/212 [00:04<00:00, 36.75it/s, v_num=0]Training loss: 0.9231308102607727\n",
      "Epoch 3:  84%|████████▍ | 179/212 [00:04<00:00, 36.76it/s, v_num=0]Training loss: 0.9528189897537231\n",
      "Epoch 3:  85%|████████▍ | 180/212 [00:04<00:00, 36.76it/s, v_num=0]Training loss: 1.7412556409835815\n",
      "Epoch 3:  85%|████████▌ | 181/212 [00:04<00:00, 36.76it/s, v_num=0]Training loss: 1.3782565593719482\n",
      "Epoch 3:  86%|████████▌ | 182/212 [00:04<00:00, 36.77it/s, v_num=0]Training loss: 0.9224296808242798\n",
      "Epoch 3:  86%|████████▋ | 183/212 [00:04<00:00, 36.77it/s, v_num=0]Training loss: 1.9223871231079102\n",
      "Epoch 3:  87%|████████▋ | 184/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.0860501527786255\n",
      "Epoch 3:  87%|████████▋ | 185/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.8216390013694763\n",
      "Epoch 3:  88%|████████▊ | 186/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.1201797723770142\n",
      "Epoch 3:  88%|████████▊ | 187/212 [00:05<00:00, 36.79it/s, v_num=0]Training loss: 0.9213426113128662\n",
      "Epoch 3:  89%|████████▊ | 188/212 [00:05<00:00, 36.79it/s, v_num=0]Training loss: 1.9774746894836426\n",
      "Epoch 3:  89%|████████▉ | 189/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.9493439197540283\n",
      "Epoch 3:  90%|████████▉ | 190/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.3285588026046753\n",
      "Epoch 3:  90%|█████████ | 191/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.9055638313293457\n",
      "Epoch 3:  91%|█████████ | 192/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.8963008522987366\n",
      "Epoch 3:  91%|█████████ | 193/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.8886884450912476\n",
      "Epoch 3:  92%|█████████▏| 194/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 0.8454175591468811\n",
      "Epoch 3:  92%|█████████▏| 195/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.2418557405471802\n",
      "Epoch 3:  92%|█████████▏| 196/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.1076164245605469\n",
      "Epoch 3:  93%|█████████▎| 197/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.3143402338027954\n",
      "Epoch 3:  93%|█████████▎| 198/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.3172006607055664\n",
      "Epoch 3:  94%|█████████▍| 199/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.151711106300354\n",
      "Epoch 3:  94%|█████████▍| 200/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.0389535427093506\n",
      "Epoch 3:  95%|█████████▍| 201/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.8712022304534912\n",
      "Epoch 3:  95%|█████████▌| 202/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.1748601198196411\n",
      "Epoch 3:  96%|█████████▌| 203/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.910161554813385\n",
      "Epoch 3:  96%|█████████▌| 204/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.4688405990600586\n",
      "Epoch 3:  97%|█████████▋| 205/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.1151593923568726\n",
      "Epoch 3:  97%|█████████▋| 206/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.4612531661987305\n",
      "Epoch 3:  98%|█████████▊| 207/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.7632682919502258\n",
      "Epoch 3:  98%|█████████▊| 208/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.5881708860397339\n",
      "Epoch 3:  99%|█████████▊| 209/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.938410758972168\n",
      "Epoch 3:  99%|█████████▉| 210/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.1359279155731201\n",
      "Epoch 3: 100%|█████████▉| 211/212 [00:05<00:00, 36.79it/s, v_num=0]Training loss: 0.9843603372573853\n",
      "Epoch 3: 100%|██████████| 212/212 [00:05<00:00, 36.79it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.255544662475586\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 94.93it/s]\u001b[AValidation loss: 2.039066791534424\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 95.14it/s]\u001b[AValidation loss: 1.0797851085662842\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 95.55it/s]\u001b[AValidation loss: 0.7968933582305908\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 95.80it/s]\u001b[AValidation loss: 0.8143318891525269\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 95.74it/s]\u001b[AValidation loss: 0.9867854118347168\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 95.89it/s]\u001b[AValidation loss: 1.3382536172866821\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 96.09it/s]\u001b[AValidation loss: 2.3185930252075195\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 96.15it/s]\u001b[AValidation loss: 0.7127094268798828\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 96.20it/s]\u001b[AValidation loss: 0.8267072439193726\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 96.30it/s]\u001b[AValidation loss: 0.8481061458587646\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 96.14it/s]\u001b[AValidation loss: 1.1597808599472046\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 96.20it/s]\u001b[AValidation loss: 1.1356518268585205\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 96.27it/s]\u001b[AValidation loss: 1.072356104850769\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 96.31it/s]\u001b[AValidation loss: 0.8829830288887024\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 96.36it/s]\u001b[AValidation loss: 1.2397518157958984\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 96.40it/s]\u001b[AValidation loss: 1.1404261589050293\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 96.43it/s]\u001b[AValidation loss: 1.6683034896850586\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 96.41it/s]\u001b[AValidation loss: 0.8509801030158997\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 95.77it/s]\u001b[AValidation loss: 1.3407468795776367\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 95.76it/s]\u001b[AValidation loss: 1.2014678716659546\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 95.80it/s]\u001b[AValidation loss: 1.2335387468338013\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.83it/s]\u001b[AValidation loss: 1.2157070636749268\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.87it/s]\u001b[AValidation loss: 1.0707733631134033\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.91it/s]\u001b[AValidation loss: 1.5165406465530396\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.94it/s]\u001b[AValidation loss: 0.8724638223648071\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.97it/s]\u001b[AValidation loss: 1.0933164358139038\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 95.96it/s]\u001b[AValidation loss: 1.1239619255065918\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 95.99it/s]\u001b[AValidation loss: 1.1184614896774292\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 96.05it/s]\u001b[AValidation loss: 1.3710076808929443\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 96.14it/s]\u001b[AValidation loss: 0.984851598739624\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 96.24it/s]\u001b[AValidation loss: 1.0729947090148926\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 96.33it/s]\u001b[AValidation loss: 1.0526307821273804\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 96.42it/s]\u001b[AValidation loss: 0.9477509260177612\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 96.50it/s]\u001b[AValidation loss: 2.018678665161133\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 96.56it/s]\u001b[AValidation loss: 0.6961157321929932\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 96.63it/s]\u001b[AValidation loss: 1.633715271949768\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 96.69it/s]\u001b[AValidation loss: 0.7497320175170898\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 96.76it/s]\u001b[AValidation loss: 1.1695622205734253\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 96.61it/s]\u001b[AValidation loss: 1.0700817108154297\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 96.46it/s]\u001b[AValidation loss: 0.942091166973114\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 96.30it/s]\u001b[AValidation loss: 0.8391784429550171\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 96.17it/s]\u001b[AValidation loss: 0.9290012121200562\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 96.01it/s]\u001b[AValidation loss: 1.4034807682037354\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 95.89it/s]\u001b[AValidation loss: 1.55219566822052\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 95.75it/s]\u001b[AValidation loss: 0.8189614415168762\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 95.58it/s]\u001b[AValidation loss: 1.0856289863586426\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 95.45it/s]\u001b[AValidation loss: 1.0717967748641968\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 95.14it/s]\u001b[AValidation loss: 1.2219303846359253\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 94.98it/s]\u001b[AValidation loss: 1.502977967262268\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 94.83it/s]\u001b[AValidation loss: 1.3587472438812256\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 94.71it/s]\u001b[AValidation loss: 1.1969633102416992\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 94.59it/s]\u001b[AValidation loss: 1.5095562934875488\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 94.45it/s]\u001b[AValidation loss: 1.1844499111175537\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 94.32it/s]\u001b[AValidation loss: 2.2499098777770996\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 94.42it/s]\u001b[A\n",
      "Epoch 4:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.200206995010376\n",
      "Epoch 4:   0%|          | 1/212 [00:00<00:05, 41.77it/s, v_num=0]Training loss: 1.0892921686172485\n",
      "Epoch 4:   1%|          | 2/212 [00:00<00:05, 38.73it/s, v_num=0]Training loss: 3.854736328125\n",
      "Epoch 4:   1%|▏         | 3/212 [00:00<00:05, 38.21it/s, v_num=0]Training loss: 1.2651147842407227\n",
      "Epoch 4:   2%|▏         | 4/212 [00:00<00:05, 38.01it/s, v_num=0]Training loss: 1.2169002294540405\n",
      "Epoch 4:   2%|▏         | 5/212 [00:00<00:05, 37.92it/s, v_num=0]Training loss: 1.4668192863464355\n",
      "Epoch 4:   3%|▎         | 6/212 [00:00<00:05, 37.73it/s, v_num=0]Training loss: 2.5833208560943604\n",
      "Epoch 4:   3%|▎         | 7/212 [00:00<00:05, 37.68it/s, v_num=0]Training loss: 1.014945149421692\n",
      "Epoch 4:   4%|▍         | 8/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 0.9068603515625\n",
      "Epoch 4:   4%|▍         | 9/212 [00:00<00:05, 37.58it/s, v_num=0]Training loss: 1.2585666179656982\n",
      "Epoch 4:   5%|▍         | 10/212 [00:00<00:05, 37.55it/s, v_num=0]Training loss: 1.7848849296569824\n",
      "Epoch 4:   5%|▌         | 11/212 [00:00<00:05, 37.53it/s, v_num=0]Training loss: 0.9105108976364136\n",
      "Epoch 4:   6%|▌         | 12/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 1.583975076675415\n",
      "Epoch 4:   6%|▌         | 13/212 [00:00<00:05, 37.48it/s, v_num=0]Training loss: 1.1159008741378784\n",
      "Epoch 4:   7%|▋         | 14/212 [00:00<00:05, 37.47it/s, v_num=0]Training loss: 1.4029566049575806\n",
      "Epoch 4:   7%|▋         | 15/212 [00:00<00:05, 37.47it/s, v_num=0]Training loss: 0.8457167148590088\n",
      "Epoch 4:   8%|▊         | 16/212 [00:00<00:05, 37.47it/s, v_num=0]Training loss: 0.9900256395339966\n",
      "Epoch 4:   8%|▊         | 17/212 [00:00<00:05, 37.46it/s, v_num=0]Training loss: 1.3629169464111328\n",
      "Epoch 4:   8%|▊         | 18/212 [00:00<00:05, 37.46it/s, v_num=0]Training loss: 0.9364790320396423\n",
      "Epoch 4:   9%|▉         | 19/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 0.8870655298233032\n",
      "Epoch 4:   9%|▉         | 20/212 [00:00<00:05, 37.46it/s, v_num=0]Training loss: 0.7930911779403687\n",
      "Epoch 4:  10%|▉         | 21/212 [00:00<00:05, 37.46it/s, v_num=0]Training loss: 1.248658299446106\n",
      "Epoch 4:  10%|█         | 22/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 1.2873104810714722\n",
      "Epoch 4:  11%|█         | 23/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 1.4944465160369873\n",
      "Epoch 4:  11%|█▏        | 24/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 1.0412734746932983\n",
      "Epoch 4:  12%|█▏        | 25/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.0554453134536743\n",
      "Epoch 4:  12%|█▏        | 26/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.1117697954177856\n",
      "Epoch 4:  13%|█▎        | 27/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 0.825049877166748\n",
      "Epoch 4:  13%|█▎        | 28/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.0081583261489868\n",
      "Epoch 4:  14%|█▎        | 29/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.068118691444397\n",
      "Epoch 4:  14%|█▍        | 30/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 2.0850155353546143\n",
      "Epoch 4:  15%|█▍        | 31/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.0187259912490845\n",
      "Epoch 4:  15%|█▌        | 32/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.0404353141784668\n",
      "Epoch 4:  16%|█▌        | 33/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 0.854741096496582\n",
      "Epoch 4:  16%|█▌        | 34/212 [00:00<00:04, 37.43it/s, v_num=0]Training loss: 1.0089805126190186\n",
      "Epoch 4:  17%|█▋        | 35/212 [00:00<00:04, 37.38it/s, v_num=0]Training loss: 0.7173865437507629\n",
      "Epoch 4:  17%|█▋        | 36/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 1.2827452421188354\n",
      "Epoch 4:  17%|█▋        | 37/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.5986496210098267\n",
      "Epoch 4:  18%|█▊        | 38/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.675755262374878\n",
      "Epoch 4:  18%|█▊        | 39/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.9943259358406067\n",
      "Epoch 4:  19%|█▉        | 40/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.1699621677398682\n",
      "Epoch 4:  19%|█▉        | 41/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 0.7422814965248108\n",
      "Epoch 4:  20%|█▉        | 42/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.242371678352356\n",
      "Epoch 4:  20%|██        | 43/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 1.4462674856185913\n",
      "Epoch 4:  21%|██        | 44/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.9121558666229248\n",
      "Epoch 4:  21%|██        | 45/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 1.0128610134124756\n",
      "Epoch 4:  22%|██▏       | 46/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 0.9811510443687439\n",
      "Epoch 4:  22%|██▏       | 47/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 1.3722187280654907\n",
      "Epoch 4:  23%|██▎       | 48/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.7870068550109863\n",
      "Epoch 4:  23%|██▎       | 49/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.228808045387268\n",
      "Epoch 4:  24%|██▎       | 50/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.8617820143699646\n",
      "Epoch 4:  24%|██▍       | 51/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.9589012861251831\n",
      "Epoch 4:  25%|██▍       | 52/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.1711671352386475\n",
      "Epoch 4:  25%|██▌       | 53/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.1558244228363037\n",
      "Epoch 4:  25%|██▌       | 54/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.1645082235336304\n",
      "Epoch 4:  26%|██▌       | 55/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.9879499077796936\n",
      "Epoch 4:  26%|██▋       | 56/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 2.5265004634857178\n",
      "Epoch 4:  27%|██▋       | 57/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.9926360249519348\n",
      "Epoch 4:  27%|██▋       | 58/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.9213405251502991\n",
      "Epoch 4:  28%|██▊       | 59/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.4779353141784668\n",
      "Epoch 4:  28%|██▊       | 60/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.9444738030433655\n",
      "Epoch 4:  29%|██▉       | 61/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.0871208906173706\n",
      "Epoch 4:  29%|██▉       | 62/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.0278887748718262\n",
      "Epoch 4:  30%|██▉       | 63/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.4195048809051514\n",
      "Epoch 4:  30%|███       | 64/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.9536964297294617\n",
      "Epoch 4:  31%|███       | 65/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.9560055136680603\n",
      "Epoch 4:  31%|███       | 66/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 1.5375711917877197\n",
      "Epoch 4:  32%|███▏      | 67/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.8612627983093262\n",
      "Epoch 4:  32%|███▏      | 68/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.2805808782577515\n",
      "Epoch 4:  33%|███▎      | 69/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.8553451895713806\n",
      "Epoch 4:  33%|███▎      | 70/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.858143925666809\n",
      "Epoch 4:  33%|███▎      | 71/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.2021187543869019\n",
      "Epoch 4:  34%|███▍      | 72/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.1750521659851074\n",
      "Epoch 4:  34%|███▍      | 73/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.2054190635681152\n",
      "Epoch 4:  35%|███▍      | 74/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.8236017227172852\n",
      "Epoch 4:  35%|███▌      | 75/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.4325847625732422\n",
      "Epoch 4:  36%|███▌      | 76/212 [00:02<00:03, 37.18it/s, v_num=0]Training loss: 0.7211787104606628\n",
      "Epoch 4:  36%|███▋      | 77/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.3114185333251953\n",
      "Epoch 4:  37%|███▋      | 78/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.0161417722702026\n",
      "Epoch 4:  37%|███▋      | 79/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.9201430082321167\n",
      "Epoch 4:  38%|███▊      | 80/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.1111819744110107\n",
      "Epoch 4:  38%|███▊      | 81/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 0.8471929430961609\n",
      "Epoch 4:  39%|███▊      | 82/212 [00:02<00:03, 37.09it/s, v_num=0]Training loss: 0.7767189145088196\n",
      "Epoch 4:  39%|███▉      | 83/212 [00:02<00:03, 37.09it/s, v_num=0]Training loss: 0.9955955743789673\n",
      "Epoch 4:  40%|███▉      | 84/212 [00:02<00:03, 37.09it/s, v_num=0]Training loss: 0.9570568203926086\n",
      "Epoch 4:  40%|████      | 85/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 1.1818585395812988\n",
      "Epoch 4:  41%|████      | 86/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.6573689579963684\n",
      "Epoch 4:  41%|████      | 87/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.098892331123352\n",
      "Epoch 4:  42%|████▏     | 88/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 2.231247901916504\n",
      "Epoch 4:  42%|████▏     | 89/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.8728267550468445\n",
      "Epoch 4:  42%|████▏     | 90/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.155831217765808\n",
      "Epoch 4:  43%|████▎     | 91/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.5465285778045654\n",
      "Epoch 4:  43%|████▎     | 92/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.9912721514701843\n",
      "Epoch 4:  44%|████▍     | 93/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.264277458190918\n",
      "Epoch 4:  44%|████▍     | 94/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.4423940181732178\n",
      "Epoch 4:  45%|████▍     | 95/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.7663134336471558\n",
      "Epoch 4:  45%|████▌     | 96/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.6901910305023193\n",
      "Epoch 4:  46%|████▌     | 97/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 1.3550727367401123\n",
      "Epoch 4:  46%|████▌     | 98/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.6618412733078003\n",
      "Epoch 4:  47%|████▋     | 99/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.1320780515670776\n",
      "Epoch 4:  47%|████▋     | 100/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.4515516757965088\n",
      "Epoch 4:  48%|████▊     | 101/212 [00:02<00:02, 37.14it/s, v_num=0]Training loss: 1.3296873569488525\n",
      "Epoch 4:  48%|████▊     | 102/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 0.7640647292137146\n",
      "Epoch 4:  49%|████▊     | 103/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 0.8346109390258789\n",
      "Epoch 4:  49%|████▉     | 104/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 4.169017791748047\n",
      "Epoch 4:  50%|████▉     | 105/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 0.704826831817627\n",
      "Epoch 4:  50%|█████     | 106/212 [00:02<00:02, 37.16it/s, v_num=0]Training loss: 1.52491295337677\n",
      "Epoch 4:  50%|█████     | 107/212 [00:02<00:02, 37.16it/s, v_num=0]Training loss: 0.9649664759635925\n",
      "Epoch 4:  51%|█████     | 108/212 [00:02<00:02, 37.16it/s, v_num=0]Training loss: 1.562551498413086\n",
      "Epoch 4:  51%|█████▏    | 109/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 1.2251827716827393\n",
      "Epoch 4:  52%|█████▏    | 110/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 0.9375380277633667\n",
      "Epoch 4:  52%|█████▏    | 111/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 1.5701359510421753\n",
      "Epoch 4:  53%|█████▎    | 112/212 [00:03<00:02, 37.17it/s, v_num=0]Training loss: 1.4414230585098267\n",
      "Epoch 4:  53%|█████▎    | 113/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 1.1900584697723389\n",
      "Epoch 4:  54%|█████▍    | 114/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 0.9440633058547974\n",
      "Epoch 4:  54%|█████▍    | 115/212 [00:03<00:02, 37.14it/s, v_num=0]Training loss: 1.3063464164733887\n",
      "Epoch 4:  55%|█████▍    | 116/212 [00:03<00:02, 37.13it/s, v_num=0]Training loss: 1.0256233215332031\n",
      "Epoch 4:  55%|█████▌    | 117/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.1996411085128784\n",
      "Epoch 4:  56%|█████▌    | 118/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.558530569076538\n",
      "Epoch 4:  56%|█████▌    | 119/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.9729540944099426\n",
      "Epoch 4:  57%|█████▋    | 120/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.1497379541397095\n",
      "Epoch 4:  57%|█████▋    | 121/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.4812428951263428\n",
      "Epoch 4:  58%|█████▊    | 122/212 [00:03<00:02, 37.07it/s, v_num=0]Training loss: 1.2981858253479004\n",
      "Epoch 4:  58%|█████▊    | 123/212 [00:03<00:02, 37.07it/s, v_num=0]Training loss: 0.9395155310630798\n",
      "Epoch 4:  58%|█████▊    | 124/212 [00:03<00:02, 37.07it/s, v_num=0]Training loss: 0.7462998628616333\n",
      "Epoch 4:  59%|█████▉    | 125/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 0.8370716571807861\n",
      "Epoch 4:  59%|█████▉    | 126/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.296613335609436\n",
      "Epoch 4:  60%|█████▉    | 127/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 18.363510131835938\n",
      "Epoch 4:  60%|██████    | 128/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.9329863786697388\n",
      "Epoch 4:  61%|██████    | 129/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.8305810689926147\n",
      "Epoch 4:  61%|██████▏   | 130/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.2410874366760254\n",
      "Epoch 4:  62%|██████▏   | 131/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.8852882385253906\n",
      "Epoch 4:  62%|██████▏   | 132/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.9791816473007202\n",
      "Epoch 4:  63%|██████▎   | 133/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.4847092628479004\n",
      "Epoch 4:  63%|██████▎   | 134/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.4949337244033813\n",
      "Epoch 4:  64%|██████▎   | 135/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.6632165908813477\n",
      "Epoch 4:  64%|██████▍   | 136/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.4278486967086792\n",
      "Epoch 4:  65%|██████▍   | 137/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.0019851922988892\n",
      "Epoch 4:  65%|██████▌   | 138/212 [00:03<00:01, 37.10it/s, v_num=0]Training loss: 1.0875792503356934\n",
      "Epoch 4:  66%|██████▌   | 139/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 0.9328606724739075\n",
      "Epoch 4:  66%|██████▌   | 140/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 1.2577046155929565\n",
      "Epoch 4:  67%|██████▋   | 141/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 1.2592644691467285\n",
      "Epoch 4:  67%|██████▋   | 142/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 0.7025424242019653\n",
      "Epoch 4:  67%|██████▋   | 143/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 0.8908783197402954\n",
      "Epoch 4:  68%|██████▊   | 144/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 1.0861589908599854\n",
      "Epoch 4:  68%|██████▊   | 145/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 1.0903129577636719\n",
      "Epoch 4:  69%|██████▉   | 146/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 1.2740110158920288\n",
      "Epoch 4:  69%|██████▉   | 147/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 1.000557541847229\n",
      "Epoch 4:  70%|██████▉   | 148/212 [00:03<00:01, 37.13it/s, v_num=0]Training loss: 1.1473749876022339\n",
      "Epoch 4:  70%|███████   | 149/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.9972578287124634\n",
      "Epoch 4:  71%|███████   | 150/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.0979195833206177\n",
      "Epoch 4:  71%|███████   | 151/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.6033188104629517\n",
      "Epoch 4:  72%|███████▏  | 152/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.2645211219787598\n",
      "Epoch 4:  72%|███████▏  | 153/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.5832430720329285\n",
      "Epoch 4:  73%|███████▎  | 154/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 0.8011579513549805\n",
      "Epoch 4:  73%|███████▎  | 155/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 1.0916662216186523\n",
      "Epoch 4:  74%|███████▎  | 156/212 [00:04<00:01, 37.10it/s, v_num=0]Training loss: 0.9029626250267029\n",
      "Epoch 4:  74%|███████▍  | 157/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 1.093482494354248\n",
      "Epoch 4:  75%|███████▍  | 158/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.1925300359725952\n",
      "Epoch 4:  75%|███████▌  | 159/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 3.533391237258911\n",
      "Epoch 4:  75%|███████▌  | 160/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 1.1525254249572754\n",
      "Epoch 4:  76%|███████▌  | 161/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 1.2953013181686401\n",
      "Epoch 4:  76%|███████▋  | 162/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 1.6830099821090698\n",
      "Epoch 4:  77%|███████▋  | 163/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 0.8684384226799011\n",
      "Epoch 4:  77%|███████▋  | 164/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 0.7192259430885315\n",
      "Epoch 4:  78%|███████▊  | 165/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 1.2326066493988037\n",
      "Epoch 4:  78%|███████▊  | 166/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.1603267192840576\n",
      "Epoch 4:  79%|███████▉  | 167/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 0.9832679033279419\n",
      "Epoch 4:  79%|███████▉  | 168/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.2506664991378784\n",
      "Epoch 4:  80%|███████▉  | 169/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.0394272804260254\n",
      "Epoch 4:  80%|████████  | 170/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 0.9103225469589233\n",
      "Epoch 4:  81%|████████  | 171/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 0.9571328163146973\n",
      "Epoch 4:  81%|████████  | 172/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.3863904476165771\n",
      "Epoch 4:  82%|████████▏ | 173/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 0.6168501377105713\n",
      "Epoch 4:  82%|████████▏ | 174/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.1270687580108643\n",
      "Epoch 4:  83%|████████▎ | 175/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.8770377039909363\n",
      "Epoch 4:  83%|████████▎ | 176/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.5848565697669983\n",
      "Epoch 4:  83%|████████▎ | 177/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 1.217839002609253\n",
      "Epoch 4:  84%|████████▍ | 178/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 1.0526303052902222\n",
      "Epoch 4:  84%|████████▍ | 179/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 1.3649799823760986\n",
      "Epoch 4:  85%|████████▍ | 180/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.7651623487472534\n",
      "Epoch 4:  85%|████████▌ | 181/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.9057964086532593\n",
      "Epoch 4:  86%|████████▌ | 182/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.9236563444137573\n",
      "Epoch 4:  86%|████████▋ | 183/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.7730021476745605\n",
      "Epoch 4:  87%|████████▋ | 184/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 1.3010443449020386\n",
      "Epoch 4:  87%|████████▋ | 185/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.63243168592453\n",
      "Epoch 4:  88%|████████▊ | 186/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.9060893058776855\n",
      "Epoch 4:  88%|████████▊ | 187/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 1.2548614740371704\n",
      "Epoch 4:  89%|████████▊ | 188/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.899175226688385\n",
      "Epoch 4:  89%|████████▉ | 189/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.8956884741783142\n",
      "Epoch 4:  90%|████████▉ | 190/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.944625735282898\n",
      "Epoch 4:  90%|█████████ | 191/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.9555914402008057\n",
      "Epoch 4:  91%|█████████ | 192/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 1.299553632736206\n",
      "Epoch 4:  91%|█████████ | 193/212 [00:05<00:00, 37.07it/s, v_num=0]Training loss: 0.8666344285011292\n",
      "Epoch 4:  92%|█████████▏| 194/212 [00:05<00:00, 37.06it/s, v_num=0]Training loss: 1.0871232748031616\n",
      "Epoch 4:  92%|█████████▏| 195/212 [00:05<00:00, 37.06it/s, v_num=0]Training loss: 1.5337207317352295\n",
      "Epoch 4:  92%|█████████▏| 196/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 0.7061938643455505\n",
      "Epoch 4:  93%|█████████▎| 197/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.8095278143882751\n",
      "Epoch 4:  93%|█████████▎| 198/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 0.883629560470581\n",
      "Epoch 4:  94%|█████████▍| 199/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.3228330612182617\n",
      "Epoch 4:  94%|█████████▍| 200/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.8765497207641602\n",
      "Epoch 4:  95%|█████████▍| 201/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.1273870468139648\n",
      "Epoch 4:  95%|█████████▌| 202/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.3037649393081665\n",
      "Epoch 4:  96%|█████████▌| 203/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.7690052390098572\n",
      "Epoch 4:  96%|█████████▌| 204/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.008345603942871\n",
      "Epoch 4:  97%|█████████▋| 205/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.993159830570221\n",
      "Epoch 4:  97%|█████████▋| 206/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.986091136932373\n",
      "Epoch 4:  98%|█████████▊| 207/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.6280875205993652\n",
      "Epoch 4:  98%|█████████▊| 208/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.9948287606239319\n",
      "Epoch 4:  99%|█████████▊| 209/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.2591252326965332\n",
      "Epoch 4:  99%|█████████▉| 210/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.8165348768234253\n",
      "Epoch 4: 100%|█████████▉| 211/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.8376275300979614\n",
      "Epoch 4: 100%|██████████| 212/212 [00:05<00:00, 37.02it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.191967725753784\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 86.95it/s]\u001b[AValidation loss: 1.9592798948287964\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 91.01it/s]\u001b[AValidation loss: 1.0008022785186768\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 92.66it/s]\u001b[AValidation loss: 0.7386205792427063\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 93.58it/s]\u001b[AValidation loss: 0.7574231624603271\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 93.97it/s]\u001b[AValidation loss: 0.9275925755500793\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 94.47it/s]\u001b[AValidation loss: 1.2667953968048096\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 94.74it/s]\u001b[AValidation loss: 2.2393109798431396\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 95.00it/s]\u001b[AValidation loss: 0.6595953106880188\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 95.19it/s]\u001b[AValidation loss: 0.77043616771698\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 94.81it/s]\u001b[AValidation loss: 0.7848753929138184\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 94.94it/s]\u001b[AValidation loss: 1.096805214881897\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 95.10it/s]\u001b[AValidation loss: 1.082804799079895\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 95.22it/s]\u001b[AValidation loss: 1.0078108310699463\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 95.34it/s]\u001b[AValidation loss: 0.8306821584701538\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 94.81it/s]\u001b[AValidation loss: 1.164719820022583\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 94.82it/s]\u001b[AValidation loss: 1.0827414989471436\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 94.82it/s]\u001b[AValidation loss: 1.5968875885009766\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 94.78it/s]\u001b[AValidation loss: 0.7938784956932068\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 94.80it/s]\u001b[AValidation loss: 1.282247543334961\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 94.86it/s]\u001b[AValidation loss: 1.1416523456573486\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 94.93it/s]\u001b[AValidation loss: 1.1740328073501587\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 94.99it/s]\u001b[AValidation loss: 1.1536616086959839\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 94.95it/s]\u001b[AValidation loss: 1.0070759057998657\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 94.95it/s]\u001b[AValidation loss: 1.447035312652588\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.02it/s]\u001b[AValidation loss: 0.8085000514984131\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.07it/s]\u001b[AValidation loss: 1.0188387632369995\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 95.12it/s]\u001b[AValidation loss: 1.0530028343200684\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 95.18it/s]\u001b[AValidation loss: 1.0495986938476562\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 95.23it/s]\u001b[AValidation loss: 1.311561942100525\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 95.27it/s]\u001b[AValidation loss: 0.9138704538345337\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 95.31it/s]\u001b[AValidation loss: 1.0236024856567383\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 95.36it/s]\u001b[AValidation loss: 0.9912688136100769\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 95.37it/s]\u001b[AValidation loss: 0.883337676525116\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 95.37it/s]\u001b[AValidation loss: 1.9383779764175415\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 95.39it/s]\u001b[AValidation loss: 0.6376304626464844\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 95.44it/s]\u001b[AValidation loss: 1.5779491662979126\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 95.47it/s]\u001b[AValidation loss: 0.692814290523529\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 95.52it/s]\u001b[AValidation loss: 1.0925110578536987\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 95.55it/s]\u001b[AValidation loss: 1.0099666118621826\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 95.58it/s]\u001b[AValidation loss: 0.8768267035484314\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 95.65it/s]\u001b[AValidation loss: 0.7814825177192688\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 95.73it/s]\u001b[AValidation loss: 0.8606611490249634\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 95.80it/s]\u001b[AValidation loss: 1.3467597961425781\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 95.79it/s]\u001b[AValidation loss: 1.4812595844268799\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 95.86it/s]\u001b[AValidation loss: 0.7694644331932068\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 95.93it/s]\u001b[AValidation loss: 1.0189366340637207\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 95.98it/s]\u001b[AValidation loss: 1.001775860786438\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 96.04it/s]\u001b[AValidation loss: 1.1706819534301758\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 96.03it/s]\u001b[AValidation loss: 1.4337570667266846\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 95.93it/s]\u001b[AValidation loss: 1.2909858226776123\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 95.80it/s]\u001b[AValidation loss: 1.1442943811416626\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 95.65it/s]\u001b[AValidation loss: 1.4353748559951782\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 95.54it/s]\u001b[AValidation loss: 1.115617275238037\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 95.45it/s]\u001b[AValidation loss: 2.191410541534424\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 95.53it/s]\u001b[A\n",
      "Epoch 5:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.1609975099563599\n",
      "Epoch 5:   0%|          | 1/212 [00:00<00:05, 40.00it/s, v_num=0]Training loss: 1.0701048374176025\n",
      "Epoch 5:   1%|          | 2/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 0.9888550639152527\n",
      "Epoch 5:   1%|▏         | 3/212 [00:00<00:05, 37.36it/s, v_num=0]Training loss: 0.7563501000404358\n",
      "Epoch 5:   2%|▏         | 4/212 [00:00<00:05, 37.05it/s, v_num=0]Training loss: 1.002662181854248\n",
      "Epoch 5:   2%|▏         | 5/212 [00:00<00:05, 36.84it/s, v_num=0]Training loss: 0.8381854295730591\n",
      "Epoch 5:   3%|▎         | 6/212 [00:00<00:05, 36.75it/s, v_num=0]Training loss: 1.3563439846038818\n",
      "Epoch 5:   3%|▎         | 7/212 [00:00<00:05, 36.66it/s, v_num=0]Training loss: 0.7909923791885376\n",
      "Epoch 5:   4%|▍         | 8/212 [00:00<00:05, 36.45it/s, v_num=0]Training loss: 1.0957963466644287\n",
      "Epoch 5:   4%|▍         | 9/212 [00:00<00:05, 36.40it/s, v_num=0]Training loss: 1.0360791683197021\n",
      "Epoch 5:   5%|▍         | 10/212 [00:00<00:05, 36.50it/s, v_num=0]Training loss: 1.0200815200805664\n",
      "Epoch 5:   5%|▌         | 11/212 [00:00<00:05, 36.66it/s, v_num=0]Training loss: 1.3583898544311523\n",
      "Epoch 5:   6%|▌         | 12/212 [00:00<00:05, 36.72it/s, v_num=0]Training loss: 0.9915987253189087\n",
      "Epoch 5:   6%|▌         | 13/212 [00:00<00:05, 36.75it/s, v_num=0]Training loss: 1.1024008989334106\n",
      "Epoch 5:   7%|▋         | 14/212 [00:00<00:05, 36.79it/s, v_num=0]Training loss: 0.877066969871521\n",
      "Epoch 5:   7%|▋         | 15/212 [00:00<00:05, 36.83it/s, v_num=0]Training loss: 1.0085461139678955\n",
      "Epoch 5:   8%|▊         | 16/212 [00:00<00:05, 36.86it/s, v_num=0]Training loss: 0.7458745241165161\n",
      "Epoch 5:   8%|▊         | 17/212 [00:00<00:05, 36.89it/s, v_num=0]Training loss: 10.332735061645508\n",
      "Epoch 5:   8%|▊         | 18/212 [00:00<00:05, 36.91it/s, v_num=0]Training loss: 0.8240143060684204\n",
      "Epoch 5:   9%|▉         | 19/212 [00:00<00:05, 36.93it/s, v_num=0]Training loss: 0.892577588558197\n",
      "Epoch 5:   9%|▉         | 20/212 [00:00<00:05, 36.94it/s, v_num=0]Training loss: 0.7020838856697083\n",
      "Epoch 5:  10%|▉         | 21/212 [00:00<00:05, 36.96it/s, v_num=0]Training loss: 1.898769736289978\n",
      "Epoch 5:  10%|█         | 22/212 [00:00<00:05, 36.98it/s, v_num=0]Training loss: 1.4291646480560303\n",
      "Epoch 5:  11%|█         | 23/212 [00:00<00:05, 37.00it/s, v_num=0]Training loss: 1.0525623559951782\n",
      "Epoch 5:  11%|█▏        | 24/212 [00:00<00:05, 37.02it/s, v_num=0]Training loss: 0.6567145586013794\n",
      "Epoch 5:  12%|█▏        | 25/212 [00:00<00:05, 37.03it/s, v_num=0]Training loss: 1.2110826969146729\n",
      "Epoch 5:  12%|█▏        | 26/212 [00:00<00:05, 37.04it/s, v_num=0]Training loss: 1.0111843347549438\n",
      "Epoch 5:  13%|█▎        | 27/212 [00:00<00:04, 37.06it/s, v_num=0]Training loss: 1.2212820053100586\n",
      "Epoch 5:  13%|█▎        | 28/212 [00:00<00:04, 37.07it/s, v_num=0]Training loss: 3.804014205932617\n",
      "Epoch 5:  14%|█▎        | 29/212 [00:00<00:04, 37.08it/s, v_num=0]Training loss: 1.3196977376937866\n",
      "Epoch 5:  14%|█▍        | 30/212 [00:00<00:04, 37.09it/s, v_num=0]Training loss: 0.9121321439743042\n",
      "Epoch 5:  15%|█▍        | 31/212 [00:00<00:04, 37.10it/s, v_num=0]Training loss: 1.498780369758606\n",
      "Epoch 5:  15%|█▌        | 32/212 [00:00<00:04, 37.11it/s, v_num=0]Training loss: 0.9609584212303162\n",
      "Epoch 5:  16%|█▌        | 33/212 [00:00<00:04, 37.12it/s, v_num=0]Training loss: 2.2697434425354004\n",
      "Epoch 5:  16%|█▌        | 34/212 [00:00<00:04, 37.13it/s, v_num=0]Training loss: 2.0169451236724854\n",
      "Epoch 5:  17%|█▋        | 35/212 [00:00<00:04, 37.14it/s, v_num=0]Training loss: 0.8402401804924011\n",
      "Epoch 5:  17%|█▋        | 36/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 1.171679139137268\n",
      "Epoch 5:  17%|█▋        | 37/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 2.3689746856689453\n",
      "Epoch 5:  18%|█▊        | 38/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.449419379234314\n",
      "Epoch 5:  18%|█▊        | 39/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.3364293575286865\n",
      "Epoch 5:  19%|█▉        | 40/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.940937340259552\n",
      "Epoch 5:  19%|█▉        | 41/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.3689556121826172\n",
      "Epoch 5:  20%|█▉        | 42/212 [00:01<00:04, 37.11it/s, v_num=0]Training loss: 1.753684401512146\n",
      "Epoch 5:  20%|██        | 43/212 [00:01<00:04, 37.09it/s, v_num=0]Training loss: 1.3959189653396606\n",
      "Epoch 5:  21%|██        | 44/212 [00:01<00:04, 37.07it/s, v_num=0]Training loss: 1.3320482969284058\n",
      "Epoch 5:  21%|██        | 45/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 1.179226040840149\n",
      "Epoch 5:  22%|██▏       | 46/212 [00:01<00:04, 37.01it/s, v_num=0]Training loss: 1.4024739265441895\n",
      "Epoch 5:  22%|██▏       | 47/212 [00:01<00:04, 36.99it/s, v_num=0]Training loss: 1.139987826347351\n",
      "Epoch 5:  23%|██▎       | 48/212 [00:01<00:04, 36.97it/s, v_num=0]Training loss: 1.13588285446167\n",
      "Epoch 5:  23%|██▎       | 49/212 [00:01<00:04, 36.96it/s, v_num=0]Training loss: 0.761151134967804\n",
      "Epoch 5:  24%|██▎       | 50/212 [00:01<00:04, 36.97it/s, v_num=0]Training loss: 0.8764702081680298\n",
      "Epoch 5:  24%|██▍       | 51/212 [00:01<00:04, 37.00it/s, v_num=0]Training loss: 0.7138558626174927\n",
      "Epoch 5:  25%|██▍       | 52/212 [00:01<00:04, 37.00it/s, v_num=0]Training loss: 0.9052053689956665\n",
      "Epoch 5:  25%|██▌       | 53/212 [00:01<00:04, 37.01it/s, v_num=0]Training loss: 1.3149532079696655\n",
      "Epoch 5:  25%|██▌       | 54/212 [00:01<00:04, 36.95it/s, v_num=0]Training loss: 2.2122976779937744\n",
      "Epoch 5:  26%|██▌       | 55/212 [00:01<00:04, 36.96it/s, v_num=0]Training loss: 0.9642210006713867\n",
      "Epoch 5:  26%|██▋       | 56/212 [00:01<00:04, 36.96it/s, v_num=0]Training loss: 0.845885694026947\n",
      "Epoch 5:  27%|██▋       | 57/212 [00:01<00:04, 36.97it/s, v_num=0]Training loss: 0.6487296223640442\n",
      "Epoch 5:  27%|██▋       | 58/212 [00:01<00:04, 36.97it/s, v_num=0]Training loss: 1.2409156560897827\n",
      "Epoch 5:  28%|██▊       | 59/212 [00:01<00:04, 36.98it/s, v_num=0]Training loss: 2.078857898712158\n",
      "Epoch 5:  28%|██▊       | 60/212 [00:01<00:04, 36.98it/s, v_num=0]Training loss: 1.5430867671966553\n",
      "Epoch 5:  29%|██▉       | 61/212 [00:01<00:04, 36.99it/s, v_num=0]Training loss: 0.8675949573516846\n",
      "Epoch 5:  29%|██▉       | 62/212 [00:01<00:04, 36.99it/s, v_num=0]Training loss: 1.1317205429077148\n",
      "Epoch 5:  30%|██▉       | 63/212 [00:01<00:04, 37.00it/s, v_num=0]Training loss: 1.2759298086166382\n",
      "Epoch 5:  30%|███       | 64/212 [00:01<00:03, 37.01it/s, v_num=0]Training loss: 1.0518122911453247\n",
      "Epoch 5:  31%|███       | 65/212 [00:01<00:03, 37.01it/s, v_num=0]Training loss: 0.8218629360198975\n",
      "Epoch 5:  31%|███       | 66/212 [00:01<00:03, 37.02it/s, v_num=0]Training loss: 0.8952271938323975\n",
      "Epoch 5:  32%|███▏      | 67/212 [00:01<00:03, 37.02it/s, v_num=0]Training loss: 1.134303331375122\n",
      "Epoch 5:  32%|███▏      | 68/212 [00:01<00:03, 37.03it/s, v_num=0]Training loss: 0.8626556992530823\n",
      "Epoch 5:  33%|███▎      | 69/212 [00:01<00:03, 37.03it/s, v_num=0]Training loss: 1.224115252494812\n",
      "Epoch 5:  33%|███▎      | 70/212 [00:01<00:03, 37.04it/s, v_num=0]Training loss: 1.014614224433899\n",
      "Epoch 5:  33%|███▎      | 71/212 [00:01<00:03, 37.05it/s, v_num=0]Training loss: 1.4361259937286377\n",
      "Epoch 5:  34%|███▍      | 72/212 [00:01<00:03, 37.05it/s, v_num=0]Training loss: 1.09977126121521\n",
      "Epoch 5:  34%|███▍      | 73/212 [00:01<00:03, 37.06it/s, v_num=0]Training loss: 0.9570461511611938\n",
      "Epoch 5:  35%|███▍      | 74/212 [00:01<00:03, 37.06it/s, v_num=0]Training loss: 1.2271240949630737\n",
      "Epoch 5:  35%|███▌      | 75/212 [00:02<00:03, 37.07it/s, v_num=0]Training loss: 1.2548696994781494\n",
      "Epoch 5:  36%|███▌      | 76/212 [00:02<00:03, 37.07it/s, v_num=0]Training loss: 0.9208247661590576\n",
      "Epoch 5:  36%|███▋      | 77/212 [00:02<00:03, 37.08it/s, v_num=0]Training loss: 1.3784106969833374\n",
      "Epoch 5:  37%|███▋      | 78/212 [00:02<00:03, 37.08it/s, v_num=0]Training loss: 1.0349596738815308\n",
      "Epoch 5:  37%|███▋      | 79/212 [00:02<00:03, 37.09it/s, v_num=0]Training loss: 1.8726757764816284\n",
      "Epoch 5:  38%|███▊      | 80/212 [00:02<00:03, 37.08it/s, v_num=0]Training loss: 0.9635398387908936\n",
      "Epoch 5:  38%|███▊      | 81/212 [00:02<00:03, 37.07it/s, v_num=0]Training loss: 0.7555423378944397\n",
      "Epoch 5:  39%|███▊      | 82/212 [00:02<00:03, 37.06it/s, v_num=0]Training loss: 0.774926483631134\n",
      "Epoch 5:  39%|███▉      | 83/212 [00:02<00:03, 37.04it/s, v_num=0]Training loss: 1.2321698665618896\n",
      "Epoch 5:  40%|███▉      | 84/212 [00:02<00:03, 37.03it/s, v_num=0]Training loss: 1.194462776184082\n",
      "Epoch 5:  40%|████      | 85/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 0.9176310896873474\n",
      "Epoch 5:  41%|████      | 86/212 [00:02<00:03, 36.99it/s, v_num=0]Training loss: 1.0117048025131226\n",
      "Epoch 5:  41%|████      | 87/212 [00:02<00:03, 36.99it/s, v_num=0]Training loss: 0.9108121991157532\n",
      "Epoch 5:  42%|████▏     | 88/212 [00:02<00:03, 36.98it/s, v_num=0]Training loss: 0.9288150072097778\n",
      "Epoch 5:  42%|████▏     | 89/212 [00:02<00:03, 36.98it/s, v_num=0]Training loss: 1.4364198446273804\n",
      "Epoch 5:  42%|████▏     | 90/212 [00:02<00:03, 37.00it/s, v_num=0]Training loss: 0.8568711280822754\n",
      "Epoch 5:  43%|████▎     | 91/212 [00:02<00:03, 37.00it/s, v_num=0]Training loss: 1.1043362617492676\n",
      "Epoch 5:  43%|████▎     | 92/212 [00:02<00:03, 37.00it/s, v_num=0]Training loss: 1.3552663326263428\n",
      "Epoch 5:  44%|████▍     | 93/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 1.7769365310668945\n",
      "Epoch 5:  44%|████▍     | 94/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 1.0127054452896118\n",
      "Epoch 5:  45%|████▍     | 95/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 0.8698395490646362\n",
      "Epoch 5:  45%|████▌     | 96/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 1.1844104528427124\n",
      "Epoch 5:  46%|████▌     | 97/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 1.016000747680664\n",
      "Epoch 5:  46%|████▌     | 98/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 0.7158363461494446\n",
      "Epoch 5:  47%|████▋     | 99/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 1.358682632446289\n",
      "Epoch 5:  47%|████▋     | 100/212 [00:02<00:03, 37.03it/s, v_num=0]Training loss: 0.8986051082611084\n",
      "Epoch 5:  48%|████▊     | 101/212 [00:02<00:02, 37.03it/s, v_num=0]Training loss: 1.9277681112289429\n",
      "Epoch 5:  48%|████▊     | 102/212 [00:02<00:02, 37.04it/s, v_num=0]Training loss: 0.9885347485542297\n",
      "Epoch 5:  49%|████▊     | 103/212 [00:02<00:02, 37.04it/s, v_num=0]Training loss: 1.0434073209762573\n",
      "Epoch 5:  49%|████▉     | 104/212 [00:02<00:02, 37.04it/s, v_num=0]Training loss: 1.1124801635742188\n",
      "Epoch 5:  50%|████▉     | 105/212 [00:02<00:02, 37.05it/s, v_num=0]Training loss: 0.706246018409729\n",
      "Epoch 5:  50%|█████     | 106/212 [00:02<00:02, 37.05it/s, v_num=0]Training loss: 1.1331825256347656\n",
      "Epoch 5:  50%|█████     | 107/212 [00:02<00:02, 37.05it/s, v_num=0]Training loss: 1.0909360647201538\n",
      "Epoch 5:  51%|█████     | 108/212 [00:02<00:02, 37.06it/s, v_num=0]Training loss: 0.8784949779510498\n",
      "Epoch 5:  51%|█████▏    | 109/212 [00:02<00:02, 37.06it/s, v_num=0]Training loss: 1.0700106620788574\n",
      "Epoch 5:  52%|█████▏    | 110/212 [00:02<00:02, 37.06it/s, v_num=0]Training loss: 0.9317472577095032\n",
      "Epoch 5:  52%|█████▏    | 111/212 [00:02<00:02, 37.07it/s, v_num=0]Training loss: 1.0348423719406128\n",
      "Epoch 5:  53%|█████▎    | 112/212 [00:03<00:02, 37.07it/s, v_num=0]Training loss: 1.1435691118240356\n",
      "Epoch 5:  53%|█████▎    | 113/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.1510229110717773\n",
      "Epoch 5:  54%|█████▍    | 114/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 0.6597995758056641\n",
      "Epoch 5:  54%|█████▍    | 115/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.4877595901489258\n",
      "Epoch 5:  55%|█████▍    | 116/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.2152173519134521\n",
      "Epoch 5:  55%|█████▌    | 117/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.0336281061172485\n",
      "Epoch 5:  56%|█████▌    | 118/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.2810070514678955\n",
      "Epoch 5:  56%|█████▌    | 119/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.7904520034790039\n",
      "Epoch 5:  57%|█████▋    | 120/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.0527571439743042\n",
      "Epoch 5:  57%|█████▋    | 121/212 [00:03<00:02, 37.07it/s, v_num=0]Training loss: 0.5783091187477112\n",
      "Epoch 5:  58%|█████▊    | 122/212 [00:03<00:02, 37.06it/s, v_num=0]Training loss: 0.7891317009925842\n",
      "Epoch 5:  58%|█████▊    | 123/212 [00:03<00:02, 37.06it/s, v_num=0]Training loss: 0.674328088760376\n",
      "Epoch 5:  58%|█████▊    | 124/212 [00:03<00:02, 37.05it/s, v_num=0]Training loss: 1.3413028717041016\n",
      "Epoch 5:  59%|█████▉    | 125/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 0.8126958012580872\n",
      "Epoch 5:  59%|█████▉    | 126/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 0.8236404061317444\n",
      "Epoch 5:  60%|█████▉    | 127/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 2.709214687347412\n",
      "Epoch 5:  60%|██████    | 128/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 1.0319410562515259\n",
      "Epoch 5:  61%|██████    | 129/212 [00:03<00:02, 37.02it/s, v_num=0]Training loss: 0.5776406526565552\n",
      "Epoch 5:  61%|██████▏   | 130/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 1.3175355195999146\n",
      "Epoch 5:  62%|██████▏   | 131/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 0.5369716286659241\n",
      "Epoch 5:  62%|██████▏   | 132/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 1.045112133026123\n",
      "Epoch 5:  63%|██████▎   | 133/212 [00:03<00:02, 37.03it/s, v_num=0]Training loss: 0.7684547901153564\n",
      "Epoch 5:  63%|██████▎   | 134/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.4911060333251953\n",
      "Epoch 5:  64%|██████▎   | 135/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 0.9320144057273865\n",
      "Epoch 5:  64%|██████▍   | 136/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.0476555824279785\n",
      "Epoch 5:  65%|██████▍   | 137/212 [00:03<00:02, 37.04it/s, v_num=0]Training loss: 1.1533247232437134\n",
      "Epoch 5:  65%|██████▌   | 138/212 [00:03<00:01, 37.04it/s, v_num=0]Training loss: 0.9687182903289795\n",
      "Epoch 5:  66%|██████▌   | 139/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 1.2220776081085205\n",
      "Epoch 5:  66%|██████▌   | 140/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 0.8272144794464111\n",
      "Epoch 5:  67%|██████▋   | 141/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 0.9163143634796143\n",
      "Epoch 5:  67%|██████▋   | 142/212 [00:03<00:01, 37.05it/s, v_num=0]Training loss: 1.1474323272705078\n",
      "Epoch 5:  67%|██████▋   | 143/212 [00:03<00:01, 37.06it/s, v_num=0]Training loss: 0.8371984958648682\n",
      "Epoch 5:  68%|██████▊   | 144/212 [00:03<00:01, 37.06it/s, v_num=0]Training loss: 1.1371320486068726\n",
      "Epoch 5:  68%|██████▊   | 145/212 [00:03<00:01, 37.06it/s, v_num=0]Training loss: 1.2243143320083618\n",
      "Epoch 5:  69%|██████▉   | 146/212 [00:03<00:01, 37.06it/s, v_num=0]Training loss: 1.1879147291183472\n",
      "Epoch 5:  69%|██████▉   | 147/212 [00:03<00:01, 37.07it/s, v_num=0]Training loss: 0.9254719614982605\n",
      "Epoch 5:  70%|██████▉   | 148/212 [00:03<00:01, 37.07it/s, v_num=0]Training loss: 0.8299610018730164\n",
      "Epoch 5:  70%|███████   | 149/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 0.6457757949829102\n",
      "Epoch 5:  71%|███████   | 150/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 1.4295406341552734\n",
      "Epoch 5:  71%|███████   | 151/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.2587494850158691\n",
      "Epoch 5:  72%|███████▏  | 152/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.2643545866012573\n",
      "Epoch 5:  72%|███████▏  | 153/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 0.7307901978492737\n",
      "Epoch 5:  73%|███████▎  | 154/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.194593071937561\n",
      "Epoch 5:  73%|███████▎  | 155/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 0.9795585870742798\n",
      "Epoch 5:  74%|███████▎  | 156/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 0.7937412858009338\n",
      "Epoch 5:  74%|███████▍  | 157/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.185445785522461\n",
      "Epoch 5:  75%|███████▍  | 158/212 [00:04<00:01, 37.06it/s, v_num=0]Training loss: 0.5195726156234741\n",
      "Epoch 5:  75%|███████▌  | 159/212 [00:04<00:01, 37.05it/s, v_num=0]Training loss: 1.0078599452972412\n",
      "Epoch 5:  75%|███████▌  | 160/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 1.0770225524902344\n",
      "Epoch 5:  76%|███████▌  | 161/212 [00:04<00:01, 37.04it/s, v_num=0]Training loss: 0.6721078157424927\n",
      "Epoch 5:  76%|███████▋  | 162/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 1.3442059755325317\n",
      "Epoch 5:  77%|███████▋  | 163/212 [00:04<00:01, 37.03it/s, v_num=0]Training loss: 0.931850016117096\n",
      "Epoch 5:  77%|███████▋  | 164/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 0.9670798778533936\n",
      "Epoch 5:  78%|███████▊  | 165/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 1.0895081758499146\n",
      "Epoch 5:  78%|███████▊  | 166/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 1.2686280012130737\n",
      "Epoch 5:  79%|███████▉  | 167/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 1.1298655271530151\n",
      "Epoch 5:  79%|███████▉  | 168/212 [00:04<00:01, 37.00it/s, v_num=0]Training loss: 1.234750747680664\n",
      "Epoch 5:  80%|███████▉  | 169/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 0.8421446681022644\n",
      "Epoch 5:  80%|████████  | 170/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 1.0166025161743164\n",
      "Epoch 5:  81%|████████  | 171/212 [00:04<00:01, 37.01it/s, v_num=0]Training loss: 0.7977455258369446\n",
      "Epoch 5:  81%|████████  | 172/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 1.3009690046310425\n",
      "Epoch 5:  82%|████████▏ | 173/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 0.885722279548645\n",
      "Epoch 5:  82%|████████▏ | 174/212 [00:04<00:01, 37.02it/s, v_num=0]Training loss: 0.4671786427497864\n",
      "Epoch 5:  83%|████████▎ | 175/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 0.922248363494873\n",
      "Epoch 5:  83%|████████▎ | 176/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 0.9337356686592102\n",
      "Epoch 5:  83%|████████▎ | 177/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.2967641353607178\n",
      "Epoch 5:  84%|████████▍ | 178/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.1394290924072266\n",
      "Epoch 5:  84%|████████▍ | 179/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.260019302368164\n",
      "Epoch 5:  85%|████████▍ | 180/212 [00:04<00:00, 37.03it/s, v_num=0]Training loss: 0.7852964401245117\n",
      "Epoch 5:  85%|████████▌ | 181/212 [00:04<00:00, 37.03it/s, v_num=0]Training loss: 0.8556246161460876\n",
      "Epoch 5:  86%|████████▌ | 182/212 [00:04<00:00, 37.03it/s, v_num=0]Training loss: 0.6758543848991394\n",
      "Epoch 5:  86%|████████▋ | 183/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 0.9656257629394531\n",
      "Epoch 5:  87%|████████▋ | 184/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 3.421903371810913\n",
      "Epoch 5:  87%|████████▋ | 185/212 [00:04<00:00, 37.02it/s, v_num=0]Training loss: 1.629047155380249\n",
      "Epoch 5:  88%|████████▊ | 186/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.9855719804763794\n",
      "Epoch 5:  88%|████████▊ | 187/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 2.6172497272491455\n",
      "Epoch 5:  89%|████████▊ | 188/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 9.844034194946289\n",
      "Epoch 5:  89%|████████▉ | 189/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 0.8127956390380859\n",
      "Epoch 5:  90%|████████▉ | 190/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.6216893792152405\n",
      "Epoch 5:  90%|█████████ | 191/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.734254777431488\n",
      "Epoch 5:  91%|█████████ | 192/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.8878348469734192\n",
      "Epoch 5:  91%|█████████ | 193/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.938088595867157\n",
      "Epoch 5:  92%|█████████▏| 194/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.698663055896759\n",
      "Epoch 5:  92%|█████████▏| 195/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 1.1250278949737549\n",
      "Epoch 5:  92%|█████████▏| 196/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 1.6049079895019531\n",
      "Epoch 5:  93%|█████████▎| 197/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 1.2938705682754517\n",
      "Epoch 5:  93%|█████████▎| 198/212 [00:05<00:00, 37.05it/s, v_num=0]Training loss: 1.0169011354446411\n",
      "Epoch 5:  94%|█████████▍| 199/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 0.6268703937530518\n",
      "Epoch 5:  94%|█████████▍| 200/212 [00:05<00:00, 37.04it/s, v_num=0]Training loss: 1.1457189321517944\n",
      "Epoch 5:  95%|█████████▍| 201/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 0.9318903684616089\n",
      "Epoch 5:  95%|█████████▌| 202/212 [00:05<00:00, 37.03it/s, v_num=0]Training loss: 0.7202453017234802\n",
      "Epoch 5:  96%|█████████▌| 203/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.7481741905212402\n",
      "Epoch 5:  96%|█████████▌| 204/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.039497971534729\n",
      "Epoch 5:  97%|█████████▋| 205/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.9092816710472107\n",
      "Epoch 5:  97%|█████████▋| 206/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.9062440991401672\n",
      "Epoch 5:  98%|█████████▊| 207/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.8911259174346924\n",
      "Epoch 5:  98%|█████████▊| 208/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 1.024289846420288\n",
      "Epoch 5:  99%|█████████▊| 209/212 [00:05<00:00, 37.01it/s, v_num=0]Training loss: 0.8402584791183472\n",
      "Epoch 5:  99%|█████████▉| 210/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 0.7827333211898804\n",
      "Epoch 5: 100%|█████████▉| 211/212 [00:05<00:00, 37.02it/s, v_num=0]Training loss: 1.1204094886779785\n",
      "Epoch 5: 100%|██████████| 212/212 [00:05<00:00, 37.02it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.1228339672088623\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 90.13it/s]\u001b[AValidation loss: 1.8775092363357544\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 91.36it/s]\u001b[AValidation loss: 0.9276627898216248\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 89.22it/s]\u001b[AValidation loss: 0.6822933554649353\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 85.62it/s]\u001b[AValidation loss: 0.7087057828903198\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 71.90it/s]\u001b[AValidation loss: 0.869620680809021\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 74.94it/s]\u001b[AValidation loss: 1.2063509225845337\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 77.29it/s]\u001b[AValidation loss: 2.16452693939209\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 79.17it/s]\u001b[AValidation loss: 0.6113815903663635\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 80.71it/s]\u001b[AValidation loss: 0.7229619026184082\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 82.00it/s]\u001b[AValidation loss: 0.7266588807106018\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 83.09it/s]\u001b[AValidation loss: 1.0354514122009277\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 83.99it/s]\u001b[AValidation loss: 1.0364145040512085\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 84.97it/s]\u001b[AValidation loss: 0.948754072189331\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 85.69it/s]\u001b[AValidation loss: 0.7831274271011353\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 86.34it/s]\u001b[AValidation loss: 1.0974301099777222\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 86.87it/s]\u001b[AValidation loss: 1.0334120988845825\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 87.41it/s]\u001b[AValidation loss: 1.5293446779251099\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 87.49it/s]\u001b[AValidation loss: 0.7381840944290161\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 87.81it/s]\u001b[AValidation loss: 1.2221719026565552\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 88.15it/s]\u001b[AValidation loss: 1.0855697393417358\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 88.48it/s]\u001b[AValidation loss: 1.11307954788208\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 88.81it/s]\u001b[AValidation loss: 1.0922152996063232\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 89.15it/s]\u001b[AValidation loss: 0.9370176792144775\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 89.48it/s]\u001b[AValidation loss: 1.3713481426239014\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 89.75it/s]\u001b[AValidation loss: 0.7492182850837708\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 90.00it/s]\u001b[AValidation loss: 0.9451703429222107\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 90.23it/s]\u001b[AValidation loss: 0.9817785620689392\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 90.45it/s]\u001b[AValidation loss: 0.9872969388961792\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 90.64it/s]\u001b[AValidation loss: 1.2572300434112549\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 90.84it/s]\u001b[AValidation loss: 0.8503063917160034\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 91.01it/s]\u001b[AValidation loss: 0.981311559677124\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 91.16it/s]\u001b[AValidation loss: 0.9313079714775085\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 91.33it/s]\u001b[AValidation loss: 0.8172933459281921\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 91.50it/s]\u001b[AValidation loss: 1.8463863134384155\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 90.75it/s]\u001b[AValidation loss: 0.5829959511756897\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 89.74it/s]\u001b[AValidation loss: 1.5292418003082275\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 88.78it/s]\u001b[AValidation loss: 0.6436491012573242\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 88.04it/s]\u001b[AValidation loss: 1.0211769342422485\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 87.35it/s]\u001b[AValidation loss: 0.9521801471710205\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 86.72it/s]\u001b[AValidation loss: 0.8123841285705566\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 86.12it/s]\u001b[AValidation loss: 0.7256625890731812\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 85.50it/s]\u001b[AValidation loss: 0.7936251759529114\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 84.98it/s]\u001b[AValidation loss: 1.2831003665924072\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 84.43it/s]\u001b[AValidation loss: 1.4081264734268188\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 83.98it/s]\u001b[AValidation loss: 0.7151954174041748\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 83.53it/s]\u001b[AValidation loss: 0.9583979249000549\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 83.16it/s]\u001b[AValidation loss: 0.9380099773406982\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 82.76it/s]\u001b[AValidation loss: 1.119064211845398\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 82.41it/s]\u001b[AValidation loss: 1.3676023483276367\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 82.07it/s]\u001b[AValidation loss: 1.225233793258667\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 81.75it/s]\u001b[AValidation loss: 1.0907657146453857\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 81.43it/s]\u001b[AValidation loss: 1.373207449913025\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 81.14it/s]\u001b[AValidation loss: 1.0540355443954468\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 80.85it/s]\u001b[AValidation loss: 2.142655611038208\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 80.70it/s]\u001b[A\n",
      "Epoch 6:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.9534025192260742\n",
      "Epoch 6:   0%|          | 1/212 [00:00<00:15, 13.29it/s, v_num=0]Training loss: 1.1540391445159912\n",
      "Epoch 6:   1%|          | 2/212 [00:00<00:10, 19.64it/s, v_num=0]Training loss: 0.8492335081100464\n",
      "Epoch 6:   1%|▏         | 3/212 [00:00<00:08, 23.32it/s, v_num=0]Training loss: 0.8529399633407593\n",
      "Epoch 6:   2%|▏         | 4/212 [00:00<00:08, 25.73it/s, v_num=0]Training loss: 1.431397795677185\n",
      "Epoch 6:   2%|▏         | 5/212 [00:00<00:07, 27.43it/s, v_num=0]Training loss: 1.0528062582015991\n",
      "Epoch 6:   3%|▎         | 6/212 [00:00<00:07, 28.61it/s, v_num=0]Training loss: 0.9186140894889832\n",
      "Epoch 6:   3%|▎         | 7/212 [00:00<00:06, 29.60it/s, v_num=0]Training loss: 1.3344414234161377\n",
      "Epoch 6:   4%|▍         | 8/212 [00:00<00:06, 30.39it/s, v_num=0]Training loss: 0.6746624708175659\n",
      "Epoch 6:   4%|▍         | 9/212 [00:00<00:06, 31.02it/s, v_num=0]Training loss: 1.077816128730774\n",
      "Epoch 6:   5%|▍         | 10/212 [00:00<00:06, 31.53it/s, v_num=0]Training loss: 1.1975970268249512\n",
      "Epoch 6:   5%|▌         | 11/212 [00:00<00:06, 31.97it/s, v_num=0]Training loss: 1.3895071744918823\n",
      "Epoch 6:   6%|▌         | 12/212 [00:00<00:06, 32.36it/s, v_num=0]Training loss: 1.1339513063430786\n",
      "Epoch 6:   6%|▌         | 13/212 [00:00<00:06, 32.70it/s, v_num=0]Training loss: 1.3187378644943237\n",
      "Epoch 6:   7%|▋         | 14/212 [00:00<00:05, 33.00it/s, v_num=0]Training loss: 0.7304919958114624\n",
      "Epoch 6:   7%|▋         | 15/212 [00:00<00:05, 33.27it/s, v_num=0]Training loss: 1.1689890623092651\n",
      "Epoch 6:   8%|▊         | 16/212 [00:00<00:05, 33.50it/s, v_num=0]Training loss: 0.7893835306167603\n",
      "Epoch 6:   8%|▊         | 17/212 [00:00<00:05, 33.72it/s, v_num=0]Training loss: 3.9977030754089355\n",
      "Epoch 6:   8%|▊         | 18/212 [00:00<00:05, 33.90it/s, v_num=0]Training loss: 0.5241624116897583\n",
      "Epoch 6:   9%|▉         | 19/212 [00:00<00:05, 34.07it/s, v_num=0]Training loss: 0.9928627014160156\n",
      "Epoch 6:   9%|▉         | 20/212 [00:00<00:05, 34.22it/s, v_num=0]Training loss: 0.8557921648025513\n",
      "Epoch 6:  10%|▉         | 21/212 [00:00<00:05, 34.36it/s, v_num=0]Training loss: 0.9175824522972107\n",
      "Epoch 6:  10%|█         | 22/212 [00:00<00:05, 34.49it/s, v_num=0]Training loss: 1.2749412059783936\n",
      "Epoch 6:  11%|█         | 23/212 [00:00<00:05, 34.61it/s, v_num=0]Training loss: 1.1764824390411377\n",
      "Epoch 6:  11%|█▏        | 24/212 [00:00<00:05, 34.71it/s, v_num=0]Training loss: 0.9299543499946594\n",
      "Epoch 6:  12%|█▏        | 25/212 [00:00<00:05, 34.82it/s, v_num=0]Training loss: 1.173326015472412\n",
      "Epoch 6:  12%|█▏        | 26/212 [00:00<00:05, 34.91it/s, v_num=0]Training loss: 0.7993980050086975\n",
      "Epoch 6:  13%|█▎        | 27/212 [00:00<00:05, 35.00it/s, v_num=0]Training loss: 2.2443370819091797\n",
      "Epoch 6:  13%|█▎        | 28/212 [00:00<00:05, 35.08it/s, v_num=0]Training loss: 1.0152581930160522\n",
      "Epoch 6:  14%|█▎        | 29/212 [00:00<00:05, 35.16it/s, v_num=0]Training loss: 0.793236494064331\n",
      "Epoch 6:  14%|█▍        | 30/212 [00:00<00:05, 35.23it/s, v_num=0]Training loss: 1.370671272277832\n",
      "Epoch 6:  15%|█▍        | 31/212 [00:00<00:05, 35.29it/s, v_num=0]Training loss: 1.0380945205688477\n",
      "Epoch 6:  15%|█▌        | 32/212 [00:00<00:05, 35.30it/s, v_num=0]Training loss: 0.9025927782058716\n",
      "Epoch 6:  16%|█▌        | 33/212 [00:00<00:05, 35.32it/s, v_num=0]Training loss: 0.8963961601257324\n",
      "Epoch 6:  16%|█▌        | 34/212 [00:00<00:05, 35.35it/s, v_num=0]Training loss: 0.801939845085144\n",
      "Epoch 6:  17%|█▋        | 35/212 [00:00<00:05, 35.37it/s, v_num=0]Training loss: 1.405224323272705\n",
      "Epoch 6:  17%|█▋        | 36/212 [00:01<00:04, 35.39it/s, v_num=0]Training loss: 1.2578270435333252\n",
      "Epoch 6:  17%|█▋        | 37/212 [00:01<00:04, 35.39it/s, v_num=0]Training loss: 0.6168341040611267\n",
      "Epoch 6:  18%|█▊        | 38/212 [00:01<00:04, 35.41it/s, v_num=0]Training loss: 1.2819775342941284\n",
      "Epoch 6:  18%|█▊        | 39/212 [00:01<00:04, 35.42it/s, v_num=0]Training loss: 0.6899515986442566\n",
      "Epoch 6:  19%|█▉        | 40/212 [00:01<00:04, 35.46it/s, v_num=0]Training loss: 0.8555155992507935\n",
      "Epoch 6:  19%|█▉        | 41/212 [00:01<00:04, 35.53it/s, v_num=0]Training loss: 0.799201250076294\n",
      "Epoch 6:  20%|█▉        | 42/212 [00:01<00:04, 35.57it/s, v_num=0]Training loss: 1.4184904098510742\n",
      "Epoch 6:  20%|██        | 43/212 [00:01<00:04, 35.61it/s, v_num=0]Training loss: 0.7852007746696472\n",
      "Epoch 6:  21%|██        | 44/212 [00:01<00:04, 35.65it/s, v_num=0]Training loss: 0.8393261432647705\n",
      "Epoch 6:  21%|██        | 45/212 [00:01<00:04, 35.68it/s, v_num=0]Training loss: 0.7635137438774109\n",
      "Epoch 6:  22%|██▏       | 46/212 [00:01<00:04, 35.72it/s, v_num=0]Training loss: 0.7228107452392578\n",
      "Epoch 6:  22%|██▏       | 47/212 [00:01<00:04, 35.75it/s, v_num=0]Training loss: 0.8659892678260803\n",
      "Epoch 6:  23%|██▎       | 48/212 [00:01<00:04, 35.78it/s, v_num=0]Training loss: 1.428926706314087\n",
      "Epoch 6:  23%|██▎       | 49/212 [00:01<00:04, 35.82it/s, v_num=0]Training loss: 0.8369579315185547\n",
      "Epoch 6:  24%|██▎       | 50/212 [00:01<00:04, 35.84it/s, v_num=0]Training loss: 0.7224538922309875\n",
      "Epoch 6:  24%|██▍       | 51/212 [00:01<00:04, 35.86it/s, v_num=0]Training loss: 1.0926153659820557\n",
      "Epoch 6:  25%|██▍       | 52/212 [00:01<00:04, 35.89it/s, v_num=0]Training loss: 0.8318504095077515\n",
      "Epoch 6:  25%|██▌       | 53/212 [00:01<00:04, 35.92it/s, v_num=0]Training loss: 0.9402477741241455\n",
      "Epoch 6:  25%|██▌       | 54/212 [00:01<00:04, 35.94it/s, v_num=0]Training loss: 1.1555137634277344\n",
      "Epoch 6:  26%|██▌       | 55/212 [00:01<00:04, 35.97it/s, v_num=0]Training loss: 0.6041392087936401\n",
      "Epoch 6:  26%|██▋       | 56/212 [00:01<00:04, 36.00it/s, v_num=0]Training loss: 0.6781433820724487\n",
      "Epoch 6:  27%|██▋       | 57/212 [00:01<00:04, 36.02it/s, v_num=0]Training loss: 0.6394379734992981\n",
      "Epoch 6:  27%|██▋       | 58/212 [00:01<00:04, 36.04it/s, v_num=0]Training loss: 0.6838251948356628\n",
      "Epoch 6:  28%|██▊       | 59/212 [00:01<00:04, 36.07it/s, v_num=0]Training loss: 0.8693689703941345\n",
      "Epoch 6:  28%|██▊       | 60/212 [00:01<00:04, 36.09it/s, v_num=0]Training loss: 1.38284170627594\n",
      "Epoch 6:  29%|██▉       | 61/212 [00:01<00:04, 36.11it/s, v_num=0]Training loss: 1.4637186527252197\n",
      "Epoch 6:  29%|██▉       | 62/212 [00:01<00:04, 36.13it/s, v_num=0]Training loss: 0.8604570627212524\n",
      "Epoch 6:  30%|██▉       | 63/212 [00:01<00:04, 36.15it/s, v_num=0]Training loss: 0.9497804045677185\n",
      "Epoch 6:  30%|███       | 64/212 [00:01<00:04, 36.17it/s, v_num=0]Training loss: 0.917679488658905\n",
      "Epoch 6:  31%|███       | 65/212 [00:01<00:04, 36.19it/s, v_num=0]Training loss: 1.423958659172058\n",
      "Epoch 6:  31%|███       | 66/212 [00:01<00:04, 36.21it/s, v_num=0]Training loss: 0.8536301851272583\n",
      "Epoch 6:  32%|███▏      | 67/212 [00:01<00:04, 36.23it/s, v_num=0]Training loss: 1.6003323793411255\n",
      "Epoch 6:  32%|███▏      | 68/212 [00:01<00:03, 36.24it/s, v_num=0]Training loss: 0.800422191619873\n",
      "Epoch 6:  33%|███▎      | 69/212 [00:01<00:03, 36.26it/s, v_num=0]Training loss: 0.9717317819595337\n",
      "Epoch 6:  33%|███▎      | 70/212 [00:01<00:03, 36.28it/s, v_num=0]Training loss: 0.892545223236084\n",
      "Epoch 6:  33%|███▎      | 71/212 [00:01<00:03, 36.29it/s, v_num=0]Training loss: 0.8999327421188354\n",
      "Epoch 6:  34%|███▍      | 72/212 [00:01<00:03, 36.28it/s, v_num=0]Training loss: 0.9282330274581909\n",
      "Epoch 6:  34%|███▍      | 73/212 [00:02<00:03, 36.28it/s, v_num=0]Training loss: 1.1175552606582642\n",
      "Epoch 6:  35%|███▍      | 74/212 [00:02<00:03, 36.28it/s, v_num=0]Training loss: 0.9120526313781738\n",
      "Epoch 6:  35%|███▌      | 75/212 [00:02<00:03, 36.27it/s, v_num=0]Training loss: 1.3067874908447266\n",
      "Epoch 6:  36%|███▌      | 76/212 [00:02<00:03, 36.27it/s, v_num=0]Training loss: 0.7072416543960571\n",
      "Epoch 6:  36%|███▋      | 77/212 [00:02<00:03, 36.26it/s, v_num=0]Training loss: 1.2215903997421265\n",
      "Epoch 6:  37%|███▋      | 78/212 [00:02<00:03, 36.26it/s, v_num=0]Training loss: 1.8249001502990723\n",
      "Epoch 6:  37%|███▋      | 79/212 [00:02<00:03, 36.25it/s, v_num=0]Training loss: 0.8440158367156982\n",
      "Epoch 6:  38%|███▊      | 80/212 [00:02<00:03, 36.25it/s, v_num=0]Training loss: 1.420225977897644\n",
      "Epoch 6:  38%|███▊      | 81/212 [00:02<00:03, 36.26it/s, v_num=0]Training loss: 0.7849909663200378\n",
      "Epoch 6:  39%|███▊      | 82/212 [00:02<00:03, 36.27it/s, v_num=0]Training loss: 0.6573063135147095\n",
      "Epoch 6:  39%|███▉      | 83/212 [00:02<00:03, 36.29it/s, v_num=0]Training loss: 9.758673667907715\n",
      "Epoch 6:  40%|███▉      | 84/212 [00:02<00:03, 36.30it/s, v_num=0]Training loss: 1.0834909677505493\n",
      "Epoch 6:  40%|████      | 85/212 [00:02<00:03, 36.32it/s, v_num=0]Training loss: 1.0499718189239502\n",
      "Epoch 6:  41%|████      | 86/212 [00:02<00:03, 36.33it/s, v_num=0]Training loss: 1.1867239475250244\n",
      "Epoch 6:  41%|████      | 87/212 [00:02<00:03, 36.34it/s, v_num=0]Training loss: 0.9509956240653992\n",
      "Epoch 6:  42%|████▏     | 88/212 [00:02<00:03, 36.35it/s, v_num=0]Training loss: 0.9644593596458435\n",
      "Epoch 6:  42%|████▏     | 89/212 [00:02<00:03, 36.36it/s, v_num=0]Training loss: 0.8081542253494263\n",
      "Epoch 6:  42%|████▏     | 90/212 [00:02<00:03, 36.37it/s, v_num=0]Training loss: 1.2134796380996704\n",
      "Epoch 6:  43%|████▎     | 91/212 [00:02<00:03, 36.38it/s, v_num=0]Training loss: 0.9002857804298401\n",
      "Epoch 6:  43%|████▎     | 92/212 [00:02<00:03, 36.39it/s, v_num=0]Training loss: 0.5890278816223145\n",
      "Epoch 6:  44%|████▍     | 93/212 [00:02<00:03, 36.40it/s, v_num=0]Training loss: 1.0770490169525146\n",
      "Epoch 6:  44%|████▍     | 94/212 [00:02<00:03, 36.41it/s, v_num=0]Training loss: 0.8970245122909546\n",
      "Epoch 6:  45%|████▍     | 95/212 [00:02<00:03, 36.42it/s, v_num=0]Training loss: 0.8108576536178589\n",
      "Epoch 6:  45%|████▌     | 96/212 [00:02<00:03, 36.43it/s, v_num=0]Training loss: 1.0156044960021973\n",
      "Epoch 6:  46%|████▌     | 97/212 [00:02<00:03, 36.44it/s, v_num=0]Training loss: 1.1816776990890503\n",
      "Epoch 6:  46%|████▌     | 98/212 [00:02<00:03, 36.45it/s, v_num=0]Training loss: 1.2118358612060547\n",
      "Epoch 6:  47%|████▋     | 99/212 [00:02<00:03, 36.46it/s, v_num=0]Training loss: 1.221845030784607\n",
      "Epoch 6:  47%|████▋     | 100/212 [00:02<00:03, 36.47it/s, v_num=0]Training loss: 1.1109334230422974\n",
      "Epoch 6:  48%|████▊     | 101/212 [00:02<00:03, 36.48it/s, v_num=0]Training loss: 0.8055219054222107\n",
      "Epoch 6:  48%|████▊     | 102/212 [00:02<00:03, 36.49it/s, v_num=0]Training loss: 1.4558621644973755\n",
      "Epoch 6:  49%|████▊     | 103/212 [00:02<00:02, 36.50it/s, v_num=0]Training loss: 1.0347685813903809\n",
      "Epoch 6:  49%|████▉     | 104/212 [00:02<00:02, 36.51it/s, v_num=0]Training loss: 1.0951869487762451\n",
      "Epoch 6:  50%|████▉     | 105/212 [00:02<00:02, 36.52it/s, v_num=0]Training loss: 0.8632015585899353\n",
      "Epoch 6:  50%|█████     | 106/212 [00:02<00:02, 36.53it/s, v_num=0]Training loss: 0.7707720398902893\n",
      "Epoch 6:  50%|█████     | 107/212 [00:02<00:02, 36.53it/s, v_num=0]Training loss: 0.9769925475120544\n",
      "Epoch 6:  51%|█████     | 108/212 [00:02<00:02, 36.54it/s, v_num=0]Training loss: 0.7572185397148132\n",
      "Epoch 6:  51%|█████▏    | 109/212 [00:02<00:02, 36.55it/s, v_num=0]Training loss: 1.2336924076080322\n",
      "Epoch 6:  52%|█████▏    | 110/212 [00:03<00:02, 36.56it/s, v_num=0]Training loss: 1.0928648710250854\n",
      "Epoch 6:  52%|█████▏    | 111/212 [00:03<00:02, 36.56it/s, v_num=0]Training loss: 0.8932188153266907\n",
      "Epoch 6:  53%|█████▎    | 112/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 0.8275423049926758\n",
      "Epoch 6:  53%|█████▎    | 113/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 0.841816782951355\n",
      "Epoch 6:  54%|█████▍    | 114/212 [00:03<00:02, 36.54it/s, v_num=0]Training loss: 0.7196885943412781\n",
      "Epoch 6:  54%|█████▍    | 115/212 [00:03<00:02, 36.54it/s, v_num=0]Training loss: 1.09368896484375\n",
      "Epoch 6:  55%|█████▍    | 116/212 [00:03<00:02, 36.54it/s, v_num=0]Training loss: 2.1671290397644043\n",
      "Epoch 6:  55%|█████▌    | 117/212 [00:03<00:02, 36.52it/s, v_num=0]Training loss: 0.7553074955940247\n",
      "Epoch 6:  56%|█████▌    | 118/212 [00:03<00:02, 36.52it/s, v_num=0]Training loss: 0.766471803188324\n",
      "Epoch 6:  56%|█████▌    | 119/212 [00:03<00:02, 36.52it/s, v_num=0]Training loss: 1.0916268825531006\n",
      "Epoch 6:  57%|█████▋    | 120/212 [00:03<00:02, 36.53it/s, v_num=0]Training loss: 3.3629231452941895\n",
      "Epoch 6:  57%|█████▋    | 121/212 [00:03<00:02, 36.54it/s, v_num=0]Training loss: 1.097044587135315\n",
      "Epoch 6:  58%|█████▊    | 122/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 0.8672517538070679\n",
      "Epoch 6:  58%|█████▊    | 123/212 [00:03<00:02, 36.55it/s, v_num=0]Training loss: 0.616234540939331\n",
      "Epoch 6:  58%|█████▊    | 124/212 [00:03<00:02, 36.56it/s, v_num=0]Training loss: 0.8942456841468811\n",
      "Epoch 6:  59%|█████▉    | 125/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 1.0169638395309448\n",
      "Epoch 6:  59%|█████▉    | 126/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 1.1491565704345703\n",
      "Epoch 6:  60%|█████▉    | 127/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 2.510035991668701\n",
      "Epoch 6:  60%|██████    | 128/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 1.278849482536316\n",
      "Epoch 6:  61%|██████    | 129/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 0.7523180842399597\n",
      "Epoch 6:  61%|██████▏   | 130/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 0.8916005492210388\n",
      "Epoch 6:  62%|██████▏   | 131/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 1.1646133661270142\n",
      "Epoch 6:  62%|██████▏   | 132/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.7337967753410339\n",
      "Epoch 6:  63%|██████▎   | 133/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 1.0364046096801758\n",
      "Epoch 6:  63%|██████▎   | 134/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 1.0983799695968628\n",
      "Epoch 6:  64%|██████▎   | 135/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 0.9166088700294495\n",
      "Epoch 6:  64%|██████▍   | 136/212 [00:03<00:02, 36.63it/s, v_num=0]Training loss: 0.8339344263076782\n",
      "Epoch 6:  65%|██████▍   | 137/212 [00:03<00:02, 36.63it/s, v_num=0]Training loss: 0.7051712274551392\n",
      "Epoch 6:  65%|██████▌   | 138/212 [00:03<00:02, 36.64it/s, v_num=0]Training loss: 0.9331783056259155\n",
      "Epoch 6:  66%|██████▌   | 139/212 [00:03<00:01, 36.64it/s, v_num=0]Training loss: 1.4468588829040527\n",
      "Epoch 6:  66%|██████▌   | 140/212 [00:03<00:01, 36.65it/s, v_num=0]Training loss: 1.126098394393921\n",
      "Epoch 6:  67%|██████▋   | 141/212 [00:03<00:01, 36.66it/s, v_num=0]Training loss: 0.8314861059188843\n",
      "Epoch 6:  67%|██████▋   | 142/212 [00:03<00:01, 36.66it/s, v_num=0]Training loss: 0.8937162160873413\n",
      "Epoch 6:  67%|██████▋   | 143/212 [00:03<00:01, 36.67it/s, v_num=0]Training loss: 0.7312930822372437\n",
      "Epoch 6:  68%|██████▊   | 144/212 [00:03<00:01, 36.67it/s, v_num=0]Training loss: 0.6415565609931946\n",
      "Epoch 6:  68%|██████▊   | 145/212 [00:03<00:01, 36.68it/s, v_num=0]Training loss: 1.1333250999450684\n",
      "Epoch 6:  69%|██████▉   | 146/212 [00:03<00:01, 36.68it/s, v_num=0]Training loss: 0.9053592681884766\n",
      "Epoch 6:  69%|██████▉   | 147/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.8858060836791992\n",
      "Epoch 6:  70%|██████▉   | 148/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.8250653147697449\n",
      "Epoch 6:  70%|███████   | 149/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.9630333781242371\n",
      "Epoch 6:  71%|███████   | 150/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.9496724009513855\n",
      "Epoch 6:  71%|███████   | 151/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.6701010465621948\n",
      "Epoch 6:  72%|███████▏  | 152/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.8039571046829224\n",
      "Epoch 6:  72%|███████▏  | 153/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 1.2714686393737793\n",
      "Epoch 6:  73%|███████▎  | 154/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.6902307271957397\n",
      "Epoch 6:  73%|███████▎  | 155/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.1743242740631104\n",
      "Epoch 6:  74%|███████▎  | 156/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.376913070678711\n",
      "Epoch 6:  74%|███████▍  | 157/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 0.9444475173950195\n",
      "Epoch 6:  75%|███████▍  | 158/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 0.8243597745895386\n",
      "Epoch 6:  75%|███████▌  | 159/212 [00:04<00:01, 36.66it/s, v_num=0]Training loss: 1.076160192489624\n",
      "Epoch 6:  75%|███████▌  | 160/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 0.7794381380081177\n",
      "Epoch 6:  76%|███████▌  | 161/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 0.7923198938369751\n",
      "Epoch 6:  76%|███████▋  | 162/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.6774507761001587\n",
      "Epoch 6:  77%|███████▋  | 163/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 1.6229045391082764\n",
      "Epoch 6:  77%|███████▋  | 164/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.6509495973587036\n",
      "Epoch 6:  78%|███████▊  | 165/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 2.411325216293335\n",
      "Epoch 6:  78%|███████▊  | 166/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.8839545249938965\n",
      "Epoch 6:  79%|███████▉  | 167/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 1.0346906185150146\n",
      "Epoch 6:  79%|███████▉  | 168/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.9092473983764648\n",
      "Epoch 6:  80%|███████▉  | 169/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.6016405820846558\n",
      "Epoch 6:  80%|████████  | 170/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 1.1429213285446167\n",
      "Epoch 6:  81%|████████  | 171/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.9948086738586426\n",
      "Epoch 6:  81%|████████  | 172/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 1.3815877437591553\n",
      "Epoch 6:  82%|████████▏ | 173/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.273439645767212\n",
      "Epoch 6:  82%|████████▏ | 174/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.3139433860778809\n",
      "Epoch 6:  83%|████████▎ | 175/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 0.7609526515007019\n",
      "Epoch 6:  83%|████████▎ | 176/212 [00:04<00:00, 36.73it/s, v_num=0]Training loss: 1.4764034748077393\n",
      "Epoch 6:  83%|████████▎ | 177/212 [00:04<00:00, 36.73it/s, v_num=0]Training loss: 1.392842173576355\n",
      "Epoch 6:  84%|████████▍ | 178/212 [00:04<00:00, 36.74it/s, v_num=0]Training loss: 9.768209457397461\n",
      "Epoch 6:  84%|████████▍ | 179/212 [00:04<00:00, 36.74it/s, v_num=0]Training loss: 1.0383833646774292\n",
      "Epoch 6:  85%|████████▍ | 180/212 [00:04<00:00, 36.75it/s, v_num=0]Training loss: 1.5266828536987305\n",
      "Epoch 6:  85%|████████▌ | 181/212 [00:04<00:00, 36.75it/s, v_num=0]Training loss: 1.1582562923431396\n",
      "Epoch 6:  86%|████████▌ | 182/212 [00:04<00:00, 36.75it/s, v_num=0]Training loss: 1.294066071510315\n",
      "Epoch 6:  86%|████████▋ | 183/212 [00:04<00:00, 36.76it/s, v_num=0]Training loss: 1.171515941619873\n",
      "Epoch 6:  87%|████████▋ | 184/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.7250356078147888\n",
      "Epoch 6:  87%|████████▋ | 185/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.8663803339004517\n",
      "Epoch 6:  88%|████████▊ | 186/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.8262802958488464\n",
      "Epoch 6:  88%|████████▊ | 187/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.8824610114097595\n",
      "Epoch 6:  89%|████████▊ | 188/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.5140937566757202\n",
      "Epoch 6:  89%|████████▉ | 189/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.8421680927276611\n",
      "Epoch 6:  90%|████████▉ | 190/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.7313846349716187\n",
      "Epoch 6:  90%|█████████ | 191/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.8148981332778931\n",
      "Epoch 6:  91%|█████████ | 192/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.6864421367645264\n",
      "Epoch 6:  91%|█████████ | 193/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.810684323310852\n",
      "Epoch 6:  92%|█████████▏| 194/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.6123133301734924\n",
      "Epoch 6:  92%|█████████▏| 195/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.1183446645736694\n",
      "Epoch 6:  92%|█████████▏| 196/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.3071134090423584\n",
      "Epoch 6:  93%|█████████▎| 197/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.7640472650527954\n",
      "Epoch 6:  93%|█████████▎| 198/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.7444392442703247\n",
      "Epoch 6:  94%|█████████▍| 199/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.1302372217178345\n",
      "Epoch 6:  94%|█████████▍| 200/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 2.2571961879730225\n",
      "Epoch 6:  95%|█████████▍| 201/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.7538017630577087\n",
      "Epoch 6:  95%|█████████▌| 202/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.1656404733657837\n",
      "Epoch 6:  96%|█████████▌| 203/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.255582332611084\n",
      "Epoch 6:  96%|█████████▌| 204/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 2.071119785308838\n",
      "Epoch 6:  97%|█████████▋| 205/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.439024806022644\n",
      "Epoch 6:  97%|█████████▋| 206/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 0.7155069708824158\n",
      "Epoch 6:  98%|█████████▊| 207/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.4912303686141968\n",
      "Epoch 6:  98%|█████████▊| 208/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 1.1516724824905396\n",
      "Epoch 6:  99%|█████████▊| 209/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.623253345489502\n",
      "Epoch 6:  99%|█████████▉| 210/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.8524615168571472\n",
      "Epoch 6: 100%|█████████▉| 211/212 [00:05<00:00, 36.79it/s, v_num=0]Training loss: 0.9415211081504822\n",
      "Epoch 6: 100%|██████████| 212/212 [00:05<00:00, 36.79it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.080850601196289\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 87.10it/s]\u001b[AValidation loss: 1.8043625354766846\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 91.45it/s]\u001b[AValidation loss: 0.8681179285049438\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 92.81it/s]\u001b[AValidation loss: 0.6374775767326355\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 93.63it/s]\u001b[AValidation loss: 0.6749736666679382\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 94.08it/s]\u001b[AValidation loss: 0.8330567479133606\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 94.49it/s]\u001b[AValidation loss: 1.1477336883544922\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 94.77it/s]\u001b[AValidation loss: 2.107808828353882\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 94.91it/s]\u001b[AValidation loss: 0.576519787311554\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 95.11it/s]\u001b[AValidation loss: 0.6853390336036682\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 94.69it/s]\u001b[AValidation loss: 0.6828314661979675\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 94.75it/s]\u001b[AValidation loss: 0.9825605750083923\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 94.89it/s]\u001b[AValidation loss: 1.0020402669906616\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 95.02it/s]\u001b[AValidation loss: 0.9077538847923279\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 95.15it/s]\u001b[AValidation loss: 0.7497427463531494\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 95.26it/s]\u001b[AValidation loss: 1.044560194015503\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 95.35it/s]\u001b[AValidation loss: 0.9917856454849243\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 95.42it/s]\u001b[AValidation loss: 1.4728251695632935\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 95.48it/s]\u001b[AValidation loss: 0.7044972777366638\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 95.50it/s]\u001b[AValidation loss: 1.181790828704834\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 95.53it/s]\u001b[AValidation loss: 1.0481035709381104\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 95.59it/s]\u001b[AValidation loss: 1.0704734325408936\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.59it/s]\u001b[AValidation loss: 1.0389362573623657\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.64it/s]\u001b[AValidation loss: 0.8850399255752563\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.68it/s]\u001b[AValidation loss: 1.3074417114257812\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.71it/s]\u001b[AValidation loss: 0.7079996466636658\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.76it/s]\u001b[AValidation loss: 0.8807192444801331\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 95.80it/s]\u001b[AValidation loss: 0.9335175156593323\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 95.84it/s]\u001b[AValidation loss: 0.9349240064620972\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 95.88it/s]\u001b[AValidation loss: 1.2199136018753052\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 95.94it/s]\u001b[AValidation loss: 0.7960279583930969\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 96.02it/s]\u001b[AValidation loss: 0.9546383619308472\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 96.07it/s]\u001b[AValidation loss: 0.8928802013397217\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 96.16it/s]\u001b[AValidation loss: 0.769925594329834\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 96.24it/s]\u001b[AValidation loss: 1.7892745733261108\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 96.32it/s]\u001b[AValidation loss: 0.5407640933990479\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 96.37it/s]\u001b[AValidation loss: 1.4882677793502808\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 96.43it/s]\u001b[AValidation loss: 0.605722963809967\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 96.48it/s]\u001b[AValidation loss: 0.9702156782150269\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 96.50it/s]\u001b[AValidation loss: 0.9049270153045654\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 96.54it/s]\u001b[AValidation loss: 0.7612486481666565\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 96.58it/s]\u001b[AValidation loss: 0.6872785091400146\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 96.62it/s]\u001b[AValidation loss: 0.7418767213821411\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 96.65it/s]\u001b[AValidation loss: 1.238165020942688\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 96.70it/s]\u001b[AValidation loss: 1.3546909093856812\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 96.75it/s]\u001b[AValidation loss: 0.6815890669822693\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 96.77it/s]\u001b[AValidation loss: 0.9125646948814392\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 96.80it/s]\u001b[AValidation loss: 0.8876275420188904\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 96.82it/s]\u001b[AValidation loss: 1.0876230001449585\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 96.70it/s]\u001b[AValidation loss: 1.3124834299087524\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 96.57it/s]\u001b[AValidation loss: 1.1738066673278809\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 96.45it/s]\u001b[AValidation loss: 1.0568310022354126\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 96.33it/s]\u001b[AValidation loss: 1.3193548917770386\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 96.23it/s]\u001b[AValidation loss: 1.0054019689559937\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 96.11it/s]\u001b[AValidation loss: 2.108283042907715\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 96.15it/s]\u001b[A\n",
      "Epoch 7:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.6614723205566406\n",
      "Epoch 7:   0%|          | 1/212 [00:00<00:04, 42.53it/s, v_num=0]Training loss: 1.0378714799880981\n",
      "Epoch 7:   1%|          | 2/212 [00:00<00:05, 39.10it/s, v_num=0]Training loss: 0.5264509916305542\n",
      "Epoch 7:   1%|▏         | 3/212 [00:00<00:05, 38.06it/s, v_num=0]Training loss: 1.2366183996200562\n",
      "Epoch 7:   2%|▏         | 4/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 1.2270739078521729\n",
      "Epoch 7:   2%|▏         | 5/212 [00:00<00:05, 37.30it/s, v_num=0]Training loss: 1.522098183631897\n",
      "Epoch 7:   3%|▎         | 6/212 [00:00<00:05, 37.12it/s, v_num=0]Training loss: 1.014642596244812\n",
      "Epoch 7:   3%|▎         | 7/212 [00:00<00:05, 36.98it/s, v_num=0]Training loss: 0.878895103931427\n",
      "Epoch 7:   4%|▍         | 8/212 [00:00<00:05, 36.87it/s, v_num=0]Training loss: 1.1787015199661255\n",
      "Epoch 7:   4%|▍         | 9/212 [00:00<00:05, 37.04it/s, v_num=0]Training loss: 0.8192929029464722\n",
      "Epoch 7:   5%|▍         | 10/212 [00:00<00:05, 37.07it/s, v_num=0]Training loss: 1.0108375549316406\n",
      "Epoch 7:   5%|▌         | 11/212 [00:00<00:05, 37.06it/s, v_num=0]Training loss: 1.4883416891098022\n",
      "Epoch 7:   6%|▌         | 12/212 [00:00<00:05, 37.09it/s, v_num=0]Training loss: 0.9935353994369507\n",
      "Epoch 7:   6%|▌         | 13/212 [00:00<00:05, 37.10it/s, v_num=0]Training loss: 0.9446232318878174\n",
      "Epoch 7:   7%|▋         | 14/212 [00:00<00:05, 37.12it/s, v_num=0]Training loss: 0.9166914820671082\n",
      "Epoch 7:   7%|▋         | 15/212 [00:00<00:05, 37.11it/s, v_num=0]Training loss: 0.7604199647903442\n",
      "Epoch 7:   8%|▊         | 16/212 [00:00<00:05, 37.13it/s, v_num=0]Training loss: 0.6340405941009521\n",
      "Epoch 7:   8%|▊         | 17/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 0.8500758409500122\n",
      "Epoch 7:   8%|▊         | 18/212 [00:00<00:05, 37.13it/s, v_num=0]Training loss: 1.241175889968872\n",
      "Epoch 7:   9%|▉         | 19/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.1626449823379517\n",
      "Epoch 7:   9%|▉         | 20/212 [00:00<00:05, 37.16it/s, v_num=0]Training loss: 1.1840133666992188\n",
      "Epoch 7:  10%|▉         | 21/212 [00:00<00:05, 37.16it/s, v_num=0]Training loss: 1.4866633415222168\n",
      "Epoch 7:  10%|█         | 22/212 [00:00<00:05, 37.18it/s, v_num=0]Training loss: 0.9208611249923706\n",
      "Epoch 7:  11%|█         | 23/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 0.6513485312461853\n",
      "Epoch 7:  11%|█▏        | 24/212 [00:00<00:05, 37.20it/s, v_num=0]Training loss: 0.7916789054870605\n",
      "Epoch 7:  12%|█▏        | 25/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 0.7621924877166748\n",
      "Epoch 7:  12%|█▏        | 26/212 [00:00<00:04, 37.22it/s, v_num=0]Training loss: 0.9348704218864441\n",
      "Epoch 7:  13%|█▎        | 27/212 [00:00<00:04, 37.23it/s, v_num=0]Training loss: 0.7779628038406372\n",
      "Epoch 7:  13%|█▎        | 28/212 [00:00<00:04, 37.24it/s, v_num=0]Training loss: 1.0649378299713135\n",
      "Epoch 7:  14%|█▎        | 29/212 [00:00<00:04, 37.24it/s, v_num=0]Training loss: 1.389788031578064\n",
      "Epoch 7:  14%|█▍        | 30/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 0.7584784030914307\n",
      "Epoch 7:  15%|█▍        | 31/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 0.9937059879302979\n",
      "Epoch 7:  15%|█▌        | 32/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 0.9285866618156433\n",
      "Epoch 7:  16%|█▌        | 33/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.070300579071045\n",
      "Epoch 7:  16%|█▌        | 34/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 3.320305585861206\n",
      "Epoch 7:  17%|█▋        | 35/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 0.6432353258132935\n",
      "Epoch 7:  17%|█▋        | 36/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 1.0692576169967651\n",
      "Epoch 7:  17%|█▋        | 37/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 0.8683434724807739\n",
      "Epoch 7:  18%|█▊        | 38/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 0.7723298668861389\n",
      "Epoch 7:  18%|█▊        | 39/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.1410914659500122\n",
      "Epoch 7:  19%|█▉        | 40/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.7928453087806702\n",
      "Epoch 7:  19%|█▉        | 41/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.9404697418212891\n",
      "Epoch 7:  20%|█▉        | 42/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.0399965047836304\n",
      "Epoch 7:  20%|██        | 43/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.2019518613815308\n",
      "Epoch 7:  21%|██        | 44/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.3524019718170166\n",
      "Epoch 7:  21%|██        | 45/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 0.9870579838752747\n",
      "Epoch 7:  22%|██▏       | 46/212 [00:01<00:04, 37.07it/s, v_num=0]Training loss: 0.566733181476593\n",
      "Epoch 7:  22%|██▏       | 47/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 1.7703369855880737\n",
      "Epoch 7:  23%|██▎       | 48/212 [00:01<00:04, 37.07it/s, v_num=0]Training loss: 0.8060564994812012\n",
      "Epoch 7:  23%|██▎       | 49/212 [00:01<00:04, 37.09it/s, v_num=0]Training loss: 1.1796351671218872\n",
      "Epoch 7:  24%|██▎       | 50/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 0.6375321745872498\n",
      "Epoch 7:  24%|██▍       | 51/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 2.3438310623168945\n",
      "Epoch 7:  25%|██▍       | 52/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 0.7988603711128235\n",
      "Epoch 7:  25%|██▌       | 53/212 [00:01<00:04, 37.11it/s, v_num=0]Training loss: 0.6225488185882568\n",
      "Epoch 7:  25%|██▌       | 54/212 [00:01<00:04, 37.11it/s, v_num=0]Training loss: 0.8169454336166382\n",
      "Epoch 7:  26%|██▌       | 55/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 0.7443044185638428\n",
      "Epoch 7:  26%|██▋       | 56/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 0.9457927942276001\n",
      "Epoch 7:  27%|██▋       | 57/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 1.0524342060089111\n",
      "Epoch 7:  27%|██▋       | 58/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 1.0628968477249146\n",
      "Epoch 7:  28%|██▊       | 59/212 [00:01<00:04, 37.13it/s, v_num=0]Training loss: 0.7327557802200317\n",
      "Epoch 7:  28%|██▊       | 60/212 [00:01<00:04, 37.13it/s, v_num=0]Training loss: 1.699082851409912\n",
      "Epoch 7:  29%|██▉       | 61/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 0.9208208918571472\n",
      "Epoch 7:  29%|██▉       | 62/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 0.9805992245674133\n",
      "Epoch 7:  30%|██▉       | 63/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 0.9867108464241028\n",
      "Epoch 7:  30%|███       | 64/212 [00:01<00:03, 37.15it/s, v_num=0]Training loss: 1.3869614601135254\n",
      "Epoch 7:  31%|███       | 65/212 [00:01<00:03, 37.15it/s, v_num=0]Training loss: 0.9664239883422852\n",
      "Epoch 7:  31%|███       | 66/212 [00:01<00:03, 37.16it/s, v_num=0]Training loss: 0.8562078475952148\n",
      "Epoch 7:  32%|███▏      | 67/212 [00:01<00:03, 37.16it/s, v_num=0]Training loss: 1.1423708200454712\n",
      "Epoch 7:  32%|███▏      | 68/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 0.6460915803909302\n",
      "Epoch 7:  33%|███▎      | 69/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 0.7569571733474731\n",
      "Epoch 7:  33%|███▎      | 70/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 10.111923217773438\n",
      "Epoch 7:  33%|███▎      | 71/212 [00:01<00:03, 37.18it/s, v_num=0]Training loss: 0.7657798528671265\n",
      "Epoch 7:  34%|███▍      | 72/212 [00:01<00:03, 37.18it/s, v_num=0]Training loss: 0.9581146836280823\n",
      "Epoch 7:  34%|███▍      | 73/212 [00:01<00:03, 37.19it/s, v_num=0]Training loss: 0.532517671585083\n",
      "Epoch 7:  35%|███▍      | 74/212 [00:01<00:03, 37.19it/s, v_num=0]Training loss: 1.3728961944580078\n",
      "Epoch 7:  35%|███▌      | 75/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 0.9464943408966064\n",
      "Epoch 7:  36%|███▌      | 76/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 0.8510937690734863\n",
      "Epoch 7:  36%|███▋      | 77/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.1596407890319824\n",
      "Epoch 7:  37%|███▋      | 78/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 0.6765462756156921\n",
      "Epoch 7:  37%|███▋      | 79/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 0.7003811001777649\n",
      "Epoch 7:  38%|███▊      | 80/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.8938047885894775\n",
      "Epoch 7:  38%|███▊      | 81/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.3602454662322998\n",
      "Epoch 7:  39%|███▊      | 82/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 0.924405038356781\n",
      "Epoch 7:  39%|███▉      | 83/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 0.489503413438797\n",
      "Epoch 7:  40%|███▉      | 84/212 [00:02<00:03, 37.13it/s, v_num=0]Training loss: 0.9367853403091431\n",
      "Epoch 7:  40%|████      | 85/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 0.8062934875488281\n",
      "Epoch 7:  41%|████      | 86/212 [00:02<00:03, 37.09it/s, v_num=0]Training loss: 0.9188280701637268\n",
      "Epoch 7:  41%|████      | 87/212 [00:02<00:03, 37.08it/s, v_num=0]Training loss: 0.8916250467300415\n",
      "Epoch 7:  42%|████▏     | 88/212 [00:02<00:03, 37.08it/s, v_num=0]Training loss: 0.6630428433418274\n",
      "Epoch 7:  42%|████▏     | 89/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 0.5925962328910828\n",
      "Epoch 7:  42%|████▏     | 90/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 1.4069499969482422\n",
      "Epoch 7:  43%|████▎     | 91/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 1.7183359861373901\n",
      "Epoch 7:  43%|████▎     | 92/212 [00:02<00:03, 37.10it/s, v_num=0]Training loss: 1.0201005935668945\n",
      "Epoch 7:  44%|████▍     | 93/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.5172972083091736\n",
      "Epoch 7:  44%|████▍     | 94/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.7953557968139648\n",
      "Epoch 7:  45%|████▍     | 95/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.0603526830673218\n",
      "Epoch 7:  45%|████▌     | 96/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 1.1378037929534912\n",
      "Epoch 7:  46%|████▌     | 97/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 1.1940556764602661\n",
      "Epoch 7:  46%|████▌     | 98/212 [00:02<00:03, 37.11it/s, v_num=0]Training loss: 0.9330848455429077\n",
      "Epoch 7:  47%|████▋     | 99/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.7403620481491089\n",
      "Epoch 7:  47%|████▋     | 100/212 [00:02<00:03, 37.12it/s, v_num=0]Training loss: 0.7465691566467285\n",
      "Epoch 7:  48%|████▊     | 101/212 [00:02<00:02, 37.12it/s, v_num=0]Training loss: 1.5539788007736206\n",
      "Epoch 7:  48%|████▊     | 102/212 [00:02<00:02, 37.13it/s, v_num=0]Training loss: 0.9468809962272644\n",
      "Epoch 7:  49%|████▊     | 103/212 [00:02<00:02, 37.13it/s, v_num=0]Training loss: 0.5783706307411194\n",
      "Epoch 7:  49%|████▉     | 104/212 [00:02<00:02, 37.13it/s, v_num=0]Training loss: 0.693683922290802\n",
      "Epoch 7:  50%|████▉     | 105/212 [00:02<00:02, 37.14it/s, v_num=0]Training loss: 0.553056001663208\n",
      "Epoch 7:  50%|█████     | 106/212 [00:02<00:02, 37.14it/s, v_num=0]Training loss: 1.1404025554656982\n",
      "Epoch 7:  50%|█████     | 107/212 [00:02<00:02, 37.14it/s, v_num=0]Training loss: 0.669987142086029\n",
      "Epoch 7:  51%|█████     | 108/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 1.365737795829773\n",
      "Epoch 7:  51%|█████▏    | 109/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 1.407485008239746\n",
      "Epoch 7:  52%|█████▏    | 110/212 [00:02<00:02, 37.15it/s, v_num=0]Training loss: 0.9709312319755554\n",
      "Epoch 7:  52%|█████▏    | 111/212 [00:02<00:02, 37.16it/s, v_num=0]Training loss: 1.0436993837356567\n",
      "Epoch 7:  53%|█████▎    | 112/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 1.8640087842941284\n",
      "Epoch 7:  53%|█████▎    | 113/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 0.8709044456481934\n",
      "Epoch 7:  54%|█████▍    | 114/212 [00:03<00:02, 37.17it/s, v_num=0]Training loss: 1.2112795114517212\n",
      "Epoch 7:  54%|█████▍    | 115/212 [00:03<00:02, 37.17it/s, v_num=0]Training loss: 1.3870265483856201\n",
      "Epoch 7:  55%|█████▍    | 116/212 [00:03<00:02, 37.17it/s, v_num=0]Training loss: 1.824192762374878\n",
      "Epoch 7:  55%|█████▌    | 117/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 1.3089017868041992\n",
      "Epoch 7:  56%|█████▌    | 118/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 1.2699339389801025\n",
      "Epoch 7:  56%|█████▌    | 119/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 0.6063316464424133\n",
      "Epoch 7:  57%|█████▋    | 120/212 [00:03<00:02, 37.14it/s, v_num=0]Training loss: 0.8245895504951477\n",
      "Epoch 7:  57%|█████▋    | 121/212 [00:03<00:02, 37.13it/s, v_num=0]Training loss: 0.8056729435920715\n",
      "Epoch 7:  58%|█████▊    | 122/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.0784345865249634\n",
      "Epoch 7:  58%|█████▊    | 123/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.4041314125061035\n",
      "Epoch 7:  58%|█████▊    | 124/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 9.949039459228516\n",
      "Epoch 7:  59%|█████▉    | 125/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.6328065395355225\n",
      "Epoch 7:  59%|█████▉    | 126/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 0.7535949349403381\n",
      "Epoch 7:  60%|█████▉    | 127/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 1.2770707607269287\n",
      "Epoch 7:  60%|██████    | 128/212 [00:03<00:02, 37.08it/s, v_num=0]Training loss: 0.9533880352973938\n",
      "Epoch 7:  61%|██████    | 129/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.9915255308151245\n",
      "Epoch 7:  61%|██████▏   | 130/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 2.4075183868408203\n",
      "Epoch 7:  62%|██████▏   | 131/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.7916222214698792\n",
      "Epoch 7:  62%|██████▏   | 132/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.0090423822402954\n",
      "Epoch 7:  63%|██████▎   | 133/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.9749189615249634\n",
      "Epoch 7:  63%|██████▎   | 134/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.7469753623008728\n",
      "Epoch 7:  64%|██████▎   | 135/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.1978784799575806\n",
      "Epoch 7:  64%|██████▍   | 136/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.1299781799316406\n",
      "Epoch 7:  65%|██████▍   | 137/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 0.8600847125053406\n",
      "Epoch 7:  65%|██████▌   | 138/212 [00:03<00:01, 37.10it/s, v_num=0]Training loss: 0.8670709729194641\n",
      "Epoch 7:  66%|██████▌   | 139/212 [00:03<00:01, 37.10it/s, v_num=0]Training loss: 1.1159741878509521\n",
      "Epoch 7:  66%|██████▌   | 140/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 0.6084533929824829\n",
      "Epoch 7:  67%|██████▋   | 141/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 1.2999961376190186\n",
      "Epoch 7:  67%|██████▋   | 142/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 2.021880865097046\n",
      "Epoch 7:  67%|██████▋   | 143/212 [00:03<00:01, 37.11it/s, v_num=0]Training loss: 2.2008979320526123\n",
      "Epoch 7:  68%|██████▊   | 144/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 1.0131803750991821\n",
      "Epoch 7:  68%|██████▊   | 145/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 0.9041836857795715\n",
      "Epoch 7:  69%|██████▉   | 146/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 0.8746688961982727\n",
      "Epoch 7:  69%|██████▉   | 147/212 [00:03<00:01, 37.12it/s, v_num=0]Training loss: 0.7126103043556213\n",
      "Epoch 7:  70%|██████▉   | 148/212 [00:03<00:01, 37.13it/s, v_num=0]Training loss: 1.2111775875091553\n",
      "Epoch 7:  70%|███████   | 149/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 0.9056262373924255\n",
      "Epoch 7:  71%|███████   | 150/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.7373501062393188\n",
      "Epoch 7:  71%|███████   | 151/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.6125897169113159\n",
      "Epoch 7:  72%|███████▏  | 152/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.8724995851516724\n",
      "Epoch 7:  72%|███████▏  | 153/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.768204927444458\n",
      "Epoch 7:  73%|███████▎  | 154/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.9072482585906982\n",
      "Epoch 7:  73%|███████▎  | 155/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.8325074911117554\n",
      "Epoch 7:  74%|███████▎  | 156/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.0273138284683228\n",
      "Epoch 7:  74%|███████▍  | 157/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.7214418053627014\n",
      "Epoch 7:  75%|███████▍  | 158/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 1.2316781282424927\n",
      "Epoch 7:  75%|███████▌  | 159/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.8860687017440796\n",
      "Epoch 7:  75%|███████▌  | 160/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.9830467700958252\n",
      "Epoch 7:  76%|███████▌  | 161/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 1.2252355813980103\n",
      "Epoch 7:  76%|███████▋  | 162/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 0.6884033679962158\n",
      "Epoch 7:  77%|███████▋  | 163/212 [00:04<00:01, 37.10it/s, v_num=0]Training loss: 1.9932153224945068\n",
      "Epoch 7:  77%|███████▋  | 164/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.8490400910377502\n",
      "Epoch 7:  78%|███████▊  | 165/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.123603343963623\n",
      "Epoch 7:  78%|███████▊  | 166/212 [00:04<00:01, 37.07it/s, v_num=0]Training loss: 1.629195213317871\n",
      "Epoch 7:  79%|███████▉  | 167/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.1727285385131836\n",
      "Epoch 7:  79%|███████▉  | 168/212 [00:04<00:01, 37.08it/s, v_num=0]Training loss: 1.138045310974121\n",
      "Epoch 7:  80%|███████▉  | 169/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.8289826512336731\n",
      "Epoch 7:  80%|████████  | 170/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.9126261472702026\n",
      "Epoch 7:  81%|████████  | 171/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.7511964440345764\n",
      "Epoch 7:  81%|████████  | 172/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.5757016539573669\n",
      "Epoch 7:  82%|████████▏ | 173/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.8280380368232727\n",
      "Epoch 7:  82%|████████▏ | 174/212 [00:04<00:01, 37.09it/s, v_num=0]Training loss: 0.6159258484840393\n",
      "Epoch 7:  83%|████████▎ | 175/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.701397716999054\n",
      "Epoch 7:  83%|████████▎ | 176/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.780678927898407\n",
      "Epoch 7:  83%|████████▎ | 177/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 1.1448395252227783\n",
      "Epoch 7:  84%|████████▍ | 178/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.9351018667221069\n",
      "Epoch 7:  84%|████████▍ | 179/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 1.0128146409988403\n",
      "Epoch 7:  85%|████████▍ | 180/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.7902489900588989\n",
      "Epoch 7:  85%|████████▌ | 181/212 [00:04<00:00, 37.09it/s, v_num=0]Training loss: 0.7246537208557129\n",
      "Epoch 7:  86%|████████▌ | 182/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 1.0340461730957031\n",
      "Epoch 7:  86%|████████▋ | 183/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.6104238629341125\n",
      "Epoch 7:  87%|████████▋ | 184/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.9084200859069824\n",
      "Epoch 7:  87%|████████▋ | 185/212 [00:04<00:00, 37.10it/s, v_num=0]Training loss: 0.7187181115150452\n",
      "Epoch 7:  88%|████████▊ | 186/212 [00:05<00:00, 37.10it/s, v_num=0]Training loss: 0.6795324087142944\n",
      "Epoch 7:  88%|████████▊ | 187/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.5783190727233887\n",
      "Epoch 7:  89%|████████▊ | 188/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.9268300533294678\n",
      "Epoch 7:  89%|████████▉ | 189/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.6624792814254761\n",
      "Epoch 7:  90%|████████▉ | 190/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.7207183837890625\n",
      "Epoch 7:  90%|█████████ | 191/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.6976476907730103\n",
      "Epoch 7:  91%|█████████ | 192/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 1.211987018585205\n",
      "Epoch 7:  91%|█████████ | 193/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 0.7270488739013672\n",
      "Epoch 7:  92%|█████████▏| 194/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 1.6890887022018433\n",
      "Epoch 7:  92%|█████████▏| 195/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 1.0104875564575195\n",
      "Epoch 7:  92%|█████████▏| 196/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 0.9377618432044983\n",
      "Epoch 7:  93%|█████████▎| 197/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 1.1030765771865845\n",
      "Epoch 7:  93%|█████████▎| 198/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 0.6955488324165344\n",
      "Epoch 7:  94%|█████████▍| 199/212 [00:05<00:00, 37.12it/s, v_num=0]Training loss: 1.0481863021850586\n",
      "Epoch 7:  94%|█████████▍| 200/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 1.4554389715194702\n",
      "Epoch 7:  95%|█████████▍| 201/212 [00:05<00:00, 37.11it/s, v_num=0]Training loss: 0.9019858837127686\n",
      "Epoch 7:  95%|█████████▌| 202/212 [00:05<00:00, 37.10it/s, v_num=0]Training loss: 0.5411190986633301\n",
      "Epoch 7:  96%|█████████▌| 203/212 [00:05<00:00, 37.10it/s, v_num=0]Training loss: 3.904804229736328\n",
      "Epoch 7:  96%|█████████▌| 204/212 [00:05<00:00, 37.09it/s, v_num=0]Training loss: 0.7060686945915222\n",
      "Epoch 7:  97%|█████████▋| 205/212 [00:05<00:00, 37.08it/s, v_num=0]Training loss: 1.1685186624526978\n",
      "Epoch 7:  97%|█████████▋| 206/212 [00:05<00:00, 37.08it/s, v_num=0]Training loss: 0.780004620552063\n",
      "Epoch 7:  98%|█████████▊| 207/212 [00:05<00:00, 37.07it/s, v_num=0]Training loss: 0.8315613865852356\n",
      "Epoch 7:  98%|█████████▊| 208/212 [00:05<00:00, 37.08it/s, v_num=0]Training loss: 1.307397723197937\n",
      "Epoch 7:  99%|█████████▊| 209/212 [00:05<00:00, 37.09it/s, v_num=0]Training loss: 0.9695153832435608\n",
      "Epoch 7:  99%|█████████▉| 210/212 [00:05<00:00, 37.09it/s, v_num=0]Training loss: 0.7591685056686401\n",
      "Epoch 7: 100%|█████████▉| 211/212 [00:05<00:00, 37.09it/s, v_num=0]Training loss: 0.9509531855583191\n",
      "Epoch 7: 100%|██████████| 212/212 [00:05<00:00, 37.09it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.0486104488372803\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 88.38it/s]\u001b[AValidation loss: 1.7650747299194336\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.50it/s]\u001b[AValidation loss: 0.824488639831543\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 91.32it/s]\u001b[AValidation loss: 0.6063187718391418\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 92.44it/s]\u001b[AValidation loss: 0.6523374319076538\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 92.96it/s]\u001b[AValidation loss: 0.8062233328819275\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 93.56it/s]\u001b[AValidation loss: 1.1188452243804932\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 93.56it/s]\u001b[AValidation loss: 2.0728213787078857\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 93.86it/s]\u001b[AValidation loss: 0.5587802529335022\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 94.10it/s]\u001b[AValidation loss: 0.6658134460449219\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 94.28it/s]\u001b[AValidation loss: 0.6609818935394287\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 94.38it/s]\u001b[AValidation loss: 0.9549574255943298\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 94.51it/s]\u001b[AValidation loss: 0.9854727983474731\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 94.60it/s]\u001b[AValidation loss: 0.8794115781784058\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 94.54it/s]\u001b[AValidation loss: 0.724666953086853\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 94.29it/s]\u001b[AValidation loss: 1.0186889171600342\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 94.38it/s]\u001b[AValidation loss: 0.9656798243522644\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 94.41it/s]\u001b[AValidation loss: 1.4350581169128418\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 94.53it/s]\u001b[AValidation loss: 0.6852995753288269\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 94.61it/s]\u001b[AValidation loss: 1.1612024307250977\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 94.70it/s]\u001b[AValidation loss: 1.0199462175369263\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 94.79it/s]\u001b[AValidation loss: 1.0416724681854248\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 94.88it/s]\u001b[AValidation loss: 1.0145225524902344\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 94.92it/s]\u001b[AValidation loss: 0.8593030571937561\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.00it/s]\u001b[AValidation loss: 1.2815912961959839\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.07it/s]\u001b[AValidation loss: 0.678138792514801\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.12it/s]\u001b[AValidation loss: 0.850369393825531\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 95.17it/s]\u001b[AValidation loss: 0.8989222049713135\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 95.22it/s]\u001b[AValidation loss: 0.8993049263954163\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 95.25it/s]\u001b[AValidation loss: 1.19502854347229\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 95.30it/s]\u001b[AValidation loss: 0.766903817653656\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 95.30it/s]\u001b[AValidation loss: 0.9419147968292236\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 95.42it/s]\u001b[AValidation loss: 0.8611140847206116\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 95.52it/s]\u001b[AValidation loss: 0.7389345765113831\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 95.62it/s]\u001b[AValidation loss: 1.7358533143997192\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 95.72it/s]\u001b[AValidation loss: 0.5191291570663452\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 95.81it/s]\u001b[AValidation loss: 1.4694218635559082\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 95.87it/s]\u001b[AValidation loss: 0.582342803478241\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 95.94it/s]\u001b[AValidation loss: 0.9363008141517639\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 96.00it/s]\u001b[AValidation loss: 0.8792243599891663\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 96.05it/s]\u001b[AValidation loss: 0.7334040999412537\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 96.06it/s]\u001b[AValidation loss: 0.661490261554718\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 96.10it/s]\u001b[AValidation loss: 0.7108278274536133\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 96.14it/s]\u001b[AValidation loss: 1.2150894403457642\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 96.17it/s]\u001b[AValidation loss: 1.324397325515747\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 96.21it/s]\u001b[AValidation loss: 0.6586182117462158\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 96.25it/s]\u001b[AValidation loss: 0.8778954148292542\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 96.28it/s]\u001b[AValidation loss: 0.8588939309120178\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 96.32it/s]\u001b[AValidation loss: 1.069736361503601\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 96.37it/s]\u001b[AValidation loss: 1.2874011993408203\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 96.40it/s]\u001b[AValidation loss: 1.1432504653930664\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 96.44it/s]\u001b[AValidation loss: 1.0409133434295654\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 96.47it/s]\u001b[AValidation loss: 1.2873613834381104\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 96.50it/s]\u001b[AValidation loss: 0.9759736657142639\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 96.53it/s]\u001b[AValidation loss: 2.082946538925171\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 96.62it/s]\u001b[A\n",
      "Epoch 8:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.9639942646026611\n",
      "Epoch 8:   0%|          | 1/212 [00:00<00:04, 43.58it/s, v_num=0]Training loss: 1.0334241390228271\n",
      "Epoch 8:   1%|          | 2/212 [00:00<00:05, 40.30it/s, v_num=0]Training loss: 2.176281452178955\n",
      "Epoch 8:   1%|▏         | 3/212 [00:00<00:05, 39.32it/s, v_num=0]Training loss: 0.6191583275794983\n",
      "Epoch 8:   2%|▏         | 4/212 [00:00<00:05, 38.85it/s, v_num=0]Training loss: 0.9327870607376099\n",
      "Epoch 8:   2%|▏         | 5/212 [00:00<00:05, 38.56it/s, v_num=0]Training loss: 1.1806622743606567\n",
      "Epoch 8:   3%|▎         | 6/212 [00:00<00:05, 38.37it/s, v_num=0]Training loss: 0.9584643840789795\n",
      "Epoch 8:   3%|▎         | 7/212 [00:00<00:05, 37.85it/s, v_num=0]Training loss: 0.7139039635658264\n",
      "Epoch 8:   4%|▍         | 8/212 [00:00<00:05, 37.77it/s, v_num=0]Training loss: 0.6947789788246155\n",
      "Epoch 8:   4%|▍         | 9/212 [00:00<00:05, 37.63it/s, v_num=0]Training loss: 0.9184677004814148\n",
      "Epoch 8:   5%|▍         | 10/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 0.8724516034126282\n",
      "Epoch 8:   5%|▌         | 11/212 [00:00<00:05, 37.35it/s, v_num=0]Training loss: 0.950008749961853\n",
      "Epoch 8:   6%|▌         | 12/212 [00:00<00:05, 37.23it/s, v_num=0]Training loss: 0.6830275058746338\n",
      "Epoch 8:   6%|▌         | 13/212 [00:00<00:05, 37.05it/s, v_num=0]Training loss: 0.7362467646598816\n",
      "Epoch 8:   7%|▋         | 14/212 [00:00<00:05, 37.08it/s, v_num=0]Training loss: 0.8865729570388794\n",
      "Epoch 8:   7%|▋         | 15/212 [00:00<00:05, 36.94it/s, v_num=0]Training loss: 0.8327933549880981\n",
      "Epoch 8:   8%|▊         | 16/212 [00:00<00:05, 36.88it/s, v_num=0]Training loss: 2.2994604110717773\n",
      "Epoch 8:   8%|▊         | 17/212 [00:00<00:05, 36.84it/s, v_num=0]Training loss: 1.212247610092163\n",
      "Epoch 8:   8%|▊         | 18/212 [00:00<00:05, 36.90it/s, v_num=0]Training loss: 0.6529850959777832\n",
      "Epoch 8:   9%|▉         | 19/212 [00:00<00:05, 36.94it/s, v_num=0]Training loss: 1.0800414085388184\n",
      "Epoch 8:   9%|▉         | 20/212 [00:00<00:05, 36.97it/s, v_num=0]Training loss: 1.304871916770935\n",
      "Epoch 8:  10%|▉         | 21/212 [00:00<00:05, 36.98it/s, v_num=0]Training loss: 1.183162808418274\n",
      "Epoch 8:  10%|█         | 22/212 [00:00<00:05, 36.99it/s, v_num=0]Training loss: 0.5383663177490234\n",
      "Epoch 8:  11%|█         | 23/212 [00:00<00:05, 37.01it/s, v_num=0]Training loss: 1.196908950805664\n",
      "Epoch 8:  11%|█▏        | 24/212 [00:00<00:05, 36.83it/s, v_num=0]Training loss: 1.1848198175430298\n",
      "Epoch 8:  12%|█▏        | 25/212 [00:00<00:05, 36.84it/s, v_num=0]Training loss: 1.1383798122406006\n",
      "Epoch 8:  12%|█▏        | 26/212 [00:00<00:05, 36.87it/s, v_num=0]Training loss: 1.2676314115524292\n",
      "Epoch 8:  13%|█▎        | 27/212 [00:00<00:05, 36.88it/s, v_num=0]Training loss: 1.4409350156784058\n",
      "Epoch 8:  13%|█▎        | 28/212 [00:00<00:04, 36.88it/s, v_num=0]Training loss: 1.2454133033752441\n",
      "Epoch 8:  14%|█▎        | 29/212 [00:00<00:04, 36.90it/s, v_num=0]Training loss: 0.9875045418739319\n",
      "Epoch 8:  14%|█▍        | 30/212 [00:00<00:04, 36.92it/s, v_num=0]Training loss: 1.5752558708190918\n",
      "Epoch 8:  15%|█▍        | 31/212 [00:00<00:04, 36.94it/s, v_num=0]Training loss: 1.0613867044448853\n",
      "Epoch 8:  15%|█▌        | 32/212 [00:00<00:04, 36.95it/s, v_num=0]Training loss: 1.0455207824707031\n",
      "Epoch 8:  16%|█▌        | 33/212 [00:00<00:04, 36.97it/s, v_num=0]Training loss: 0.6185501217842102\n",
      "Epoch 8:  16%|█▌        | 34/212 [00:00<00:04, 36.98it/s, v_num=0]Training loss: 1.0404373407363892\n",
      "Epoch 8:  17%|█▋        | 35/212 [00:00<00:04, 37.00it/s, v_num=0]Training loss: 0.7827877998352051\n",
      "Epoch 8:  17%|█▋        | 36/212 [00:00<00:04, 37.01it/s, v_num=0]Training loss: 0.5281837582588196\n",
      "Epoch 8:  17%|█▋        | 37/212 [00:00<00:04, 37.02it/s, v_num=0]Training loss: 0.618905782699585\n",
      "Epoch 8:  18%|█▊        | 38/212 [00:01<00:04, 37.03it/s, v_num=0]Training loss: 1.4763754606246948\n",
      "Epoch 8:  18%|█▊        | 39/212 [00:01<00:04, 37.04it/s, v_num=0]Training loss: 1.4440323114395142\n",
      "Epoch 8:  19%|█▉        | 40/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 0.7293261885643005\n",
      "Epoch 8:  19%|█▉        | 41/212 [00:01<00:04, 37.06it/s, v_num=0]Training loss: 0.7866584062576294\n",
      "Epoch 8:  20%|█▉        | 42/212 [00:01<00:04, 37.07it/s, v_num=0]Training loss: 0.9752020239830017\n",
      "Epoch 8:  20%|██        | 43/212 [00:01<00:04, 37.08it/s, v_num=0]Training loss: 0.5882418751716614\n",
      "Epoch 8:  21%|██        | 44/212 [00:01<00:04, 37.08it/s, v_num=0]Training loss: 1.2665939331054688\n",
      "Epoch 8:  21%|██        | 45/212 [00:01<00:04, 36.99it/s, v_num=0]Training loss: 1.1358444690704346\n",
      "Epoch 8:  22%|██▏       | 46/212 [00:01<00:04, 37.00it/s, v_num=0]Training loss: 0.9896310567855835\n",
      "Epoch 8:  22%|██▏       | 47/212 [00:01<00:04, 37.01it/s, v_num=0]Training loss: 1.1267231702804565\n",
      "Epoch 8:  23%|██▎       | 48/212 [00:01<00:04, 37.02it/s, v_num=0]Training loss: 1.0494637489318848\n",
      "Epoch 8:  23%|██▎       | 49/212 [00:01<00:04, 36.98it/s, v_num=0]Training loss: 0.8834406137466431\n",
      "Epoch 8:  24%|██▎       | 50/212 [00:01<00:04, 36.97it/s, v_num=0]Training loss: 0.6782516241073608\n",
      "Epoch 8:  24%|██▍       | 51/212 [00:01<00:04, 36.93it/s, v_num=0]Training loss: 1.1104578971862793\n",
      "Epoch 8:  25%|██▍       | 52/212 [00:01<00:04, 36.93it/s, v_num=0]Training loss: 1.0005446672439575\n",
      "Epoch 8:  25%|██▌       | 53/212 [00:01<00:04, 36.92it/s, v_num=0]Training loss: 0.7365540862083435\n",
      "Epoch 8:  25%|██▌       | 54/212 [00:01<00:04, 36.89it/s, v_num=0]Training loss: 0.8649876713752747\n",
      "Epoch 8:  26%|██▌       | 55/212 [00:01<00:04, 36.87it/s, v_num=0]Training loss: 0.9623054265975952\n",
      "Epoch 8:  26%|██▋       | 56/212 [00:01<00:04, 36.86it/s, v_num=0]Training loss: 0.9633413553237915\n",
      "Epoch 8:  27%|██▋       | 57/212 [00:01<00:04, 36.86it/s, v_num=0]Training loss: 1.45099675655365\n",
      "Epoch 8:  27%|██▋       | 58/212 [00:01<00:04, 36.89it/s, v_num=0]Training loss: 0.8339875936508179\n",
      "Epoch 8:  28%|██▊       | 59/212 [00:01<00:04, 36.90it/s, v_num=0]Training loss: 1.2888305187225342\n",
      "Epoch 8:  28%|██▊       | 60/212 [00:01<00:04, 36.90it/s, v_num=0]Training loss: 1.1955175399780273\n",
      "Epoch 8:  29%|██▉       | 61/212 [00:01<00:04, 36.91it/s, v_num=0]Training loss: 0.7511601448059082\n",
      "Epoch 8:  29%|██▉       | 62/212 [00:01<00:04, 36.91it/s, v_num=0]Training loss: 0.9869897365570068\n",
      "Epoch 8:  30%|██▉       | 63/212 [00:01<00:04, 36.92it/s, v_num=0]Training loss: 0.9230958819389343\n",
      "Epoch 8:  30%|███       | 64/212 [00:01<00:04, 36.93it/s, v_num=0]Training loss: 0.9759353995323181\n",
      "Epoch 8:  31%|███       | 65/212 [00:01<00:03, 36.93it/s, v_num=0]Training loss: 1.5152560472488403\n",
      "Epoch 8:  31%|███       | 66/212 [00:01<00:03, 36.94it/s, v_num=0]Training loss: 0.9413465857505798\n",
      "Epoch 8:  32%|███▏      | 67/212 [00:01<00:03, 36.94it/s, v_num=0]Training loss: 0.6106159090995789\n",
      "Epoch 8:  32%|███▏      | 68/212 [00:01<00:03, 36.94it/s, v_num=0]Training loss: 1.030992865562439\n",
      "Epoch 8:  33%|███▎      | 69/212 [00:01<00:03, 36.95it/s, v_num=0]Training loss: 1.8041177988052368\n",
      "Epoch 8:  33%|███▎      | 70/212 [00:01<00:03, 36.96it/s, v_num=0]Training loss: 0.5552703142166138\n",
      "Epoch 8:  33%|███▎      | 71/212 [00:01<00:03, 36.96it/s, v_num=0]Training loss: 0.6598195433616638\n",
      "Epoch 8:  34%|███▍      | 72/212 [00:01<00:03, 36.97it/s, v_num=0]Training loss: 0.8643050193786621\n",
      "Epoch 8:  34%|███▍      | 73/212 [00:01<00:03, 36.98it/s, v_num=0]Training loss: 0.861466109752655\n",
      "Epoch 8:  35%|███▍      | 74/212 [00:02<00:03, 36.98it/s, v_num=0]Training loss: 3.408299684524536\n",
      "Epoch 8:  35%|███▌      | 75/212 [00:02<00:03, 36.99it/s, v_num=0]Training loss: 0.9096144437789917\n",
      "Epoch 8:  36%|███▌      | 76/212 [00:02<00:03, 36.99it/s, v_num=0]Training loss: 1.1012974977493286\n",
      "Epoch 8:  36%|███▋      | 77/212 [00:02<00:03, 37.00it/s, v_num=0]Training loss: 0.5494135618209839\n",
      "Epoch 8:  37%|███▋      | 78/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 1.4476438760757446\n",
      "Epoch 8:  37%|███▋      | 79/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 0.9901925921440125\n",
      "Epoch 8:  38%|███▊      | 80/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 0.4327731132507324\n",
      "Epoch 8:  38%|███▊      | 81/212 [00:02<00:03, 37.02it/s, v_num=0]Training loss: 0.6880549192428589\n",
      "Epoch 8:  39%|███▊      | 82/212 [00:02<00:03, 37.03it/s, v_num=0]Training loss: 1.2658687829971313\n",
      "Epoch 8:  39%|███▉      | 83/212 [00:02<00:03, 37.03it/s, v_num=0]Training loss: 1.947284460067749\n",
      "Epoch 8:  40%|███▉      | 84/212 [00:02<00:03, 37.04it/s, v_num=0]Training loss: 1.102954387664795\n",
      "Epoch 8:  40%|████      | 85/212 [00:02<00:03, 37.04it/s, v_num=0]Training loss: 0.8796691298484802\n",
      "Epoch 8:  41%|████      | 86/212 [00:02<00:03, 37.05it/s, v_num=0]Training loss: 0.5809077024459839\n",
      "Epoch 8:  41%|████      | 87/212 [00:02<00:03, 37.05it/s, v_num=0]Training loss: 1.022001028060913\n",
      "Epoch 8:  42%|████▏     | 88/212 [00:02<00:03, 37.05it/s, v_num=0]Training loss: 1.0014008283615112\n",
      "Epoch 8:  42%|████▏     | 89/212 [00:02<00:03, 37.04it/s, v_num=0]Training loss: 0.7079384922981262\n",
      "Epoch 8:  42%|████▏     | 90/212 [00:02<00:03, 37.03it/s, v_num=0]Training loss: 0.705786943435669\n",
      "Epoch 8:  43%|████▎     | 91/212 [00:02<00:03, 37.01it/s, v_num=0]Training loss: 0.5164654850959778\n",
      "Epoch 8:  43%|████▎     | 92/212 [00:02<00:03, 37.00it/s, v_num=0]Training loss: 0.5455091595649719\n",
      "Epoch 8:  44%|████▍     | 93/212 [00:02<00:03, 36.99it/s, v_num=0]Training loss: 0.8656491637229919\n",
      "Epoch 8:  44%|████▍     | 94/212 [00:02<00:03, 36.97it/s, v_num=0]Training loss: 0.8776208758354187\n",
      "Epoch 8:  45%|████▍     | 95/212 [00:02<00:03, 36.96it/s, v_num=0]Training loss: 0.9673711061477661\n",
      "Epoch 8:  45%|████▌     | 96/212 [00:02<00:03, 36.96it/s, v_num=0]Training loss: 1.018546462059021\n",
      "Epoch 8:  46%|████▌     | 97/212 [00:02<00:03, 36.95it/s, v_num=0]Training loss: 1.0927419662475586\n",
      "Epoch 8:  46%|████▌     | 98/212 [00:02<00:03, 36.95it/s, v_num=0]Training loss: 0.999252200126648\n",
      "Epoch 8:  47%|████▋     | 99/212 [00:02<00:03, 36.97it/s, v_num=0]Training loss: 0.744554877281189\n",
      "Epoch 8:  47%|████▋     | 100/212 [00:02<00:03, 36.97it/s, v_num=0]Training loss: 0.7576190829277039\n",
      "Epoch 8:  48%|████▊     | 101/212 [00:02<00:03, 36.97it/s, v_num=0]Training loss: 0.8719555139541626\n",
      "Epoch 8:  48%|████▊     | 102/212 [00:02<00:02, 36.98it/s, v_num=0]Training loss: 0.760211169719696\n",
      "Epoch 8:  49%|████▊     | 103/212 [00:02<00:02, 36.98it/s, v_num=0]Training loss: 0.7521578073501587\n",
      "Epoch 8:  49%|████▉     | 104/212 [00:02<00:02, 36.98it/s, v_num=0]Training loss: 1.4842435121536255\n",
      "Epoch 8:  50%|████▉     | 105/212 [00:02<00:02, 36.99it/s, v_num=0]Training loss: 0.9885326027870178\n",
      "Epoch 8:  50%|█████     | 106/212 [00:02<00:02, 36.99it/s, v_num=0]Training loss: 9.37646198272705\n",
      "Epoch 8:  50%|█████     | 107/212 [00:02<00:02, 36.99it/s, v_num=0]Training loss: 0.6872329115867615\n",
      "Epoch 8:  51%|█████     | 108/212 [00:02<00:02, 36.99it/s, v_num=0]Training loss: 0.9927369356155396\n",
      "Epoch 8:  51%|█████▏    | 109/212 [00:02<00:02, 37.00it/s, v_num=0]Training loss: 2.115866184234619\n",
      "Epoch 8:  52%|█████▏    | 110/212 [00:02<00:02, 37.00it/s, v_num=0]Training loss: 0.6807947158813477\n",
      "Epoch 8:  52%|█████▏    | 111/212 [00:02<00:02, 37.00it/s, v_num=0]Training loss: 0.5453263521194458\n",
      "Epoch 8:  53%|█████▎    | 112/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 0.7715017795562744\n",
      "Epoch 8:  53%|█████▎    | 113/212 [00:03<00:02, 37.01it/s, v_num=0]Training loss: 0.8974855542182922\n",
      "Epoch 8:  54%|█████▍    | 114/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 0.8402791619300842\n",
      "Epoch 8:  54%|█████▍    | 115/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 0.94026118516922\n",
      "Epoch 8:  55%|█████▍    | 116/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 0.7269959449768066\n",
      "Epoch 8:  55%|█████▌    | 117/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 1.145425796508789\n",
      "Epoch 8:  56%|█████▌    | 118/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 0.6142881512641907\n",
      "Epoch 8:  56%|█████▌    | 119/212 [00:03<00:02, 36.57it/s, v_num=0]Training loss: 1.1231518983840942\n",
      "Epoch 8:  57%|█████▋    | 120/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 1.0029935836791992\n",
      "Epoch 8:  57%|█████▋    | 121/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 0.8753967881202698\n",
      "Epoch 8:  58%|█████▊    | 122/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 0.7048759460449219\n",
      "Epoch 8:  58%|█████▊    | 123/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 0.7849416732788086\n",
      "Epoch 8:  58%|█████▊    | 124/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.7688694596290588\n",
      "Epoch 8:  59%|█████▉    | 125/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 1.0556236505508423\n",
      "Epoch 8:  59%|█████▉    | 126/212 [00:03<00:02, 36.62it/s, v_num=0]Training loss: 0.6678909063339233\n",
      "Epoch 8:  60%|█████▉    | 127/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.742137610912323\n",
      "Epoch 8:  60%|██████    | 128/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.6885064244270325\n",
      "Epoch 8:  61%|██████    | 129/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.8847818374633789\n",
      "Epoch 8:  61%|██████▏   | 130/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 0.4690508246421814\n",
      "Epoch 8:  62%|██████▏   | 131/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 0.937630295753479\n",
      "Epoch 8:  62%|██████▏   | 132/212 [00:03<00:02, 36.59it/s, v_num=0]Training loss: 1.0565091371536255\n",
      "Epoch 8:  63%|██████▎   | 133/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 0.946456253528595\n",
      "Epoch 8:  63%|██████▎   | 134/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 0.5553415417671204\n",
      "Epoch 8:  64%|██████▎   | 135/212 [00:03<00:02, 36.58it/s, v_num=0]Training loss: 0.7923561334609985\n",
      "Epoch 8:  64%|██████▍   | 136/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 1.0081291198730469\n",
      "Epoch 8:  65%|██████▍   | 137/212 [00:03<00:02, 36.60it/s, v_num=0]Training loss: 0.9173155426979065\n",
      "Epoch 8:  65%|██████▌   | 138/212 [00:03<00:02, 36.61it/s, v_num=0]Training loss: 0.7455645203590393\n",
      "Epoch 8:  66%|██████▌   | 139/212 [00:03<00:01, 36.61it/s, v_num=0]Training loss: 1.7864046096801758\n",
      "Epoch 8:  66%|██████▌   | 140/212 [00:03<00:01, 36.62it/s, v_num=0]Training loss: 0.8799393773078918\n",
      "Epoch 8:  67%|██████▋   | 141/212 [00:03<00:01, 36.62it/s, v_num=0]Training loss: 2.6148033142089844\n",
      "Epoch 8:  67%|██████▋   | 142/212 [00:03<00:01, 36.63it/s, v_num=0]Training loss: 0.7496001124382019\n",
      "Epoch 8:  67%|██████▋   | 143/212 [00:03<00:01, 36.63it/s, v_num=0]Training loss: 0.9851354956626892\n",
      "Epoch 8:  68%|██████▊   | 144/212 [00:03<00:01, 36.63it/s, v_num=0]Training loss: 0.7335261702537537\n",
      "Epoch 8:  68%|██████▊   | 145/212 [00:03<00:01, 36.64it/s, v_num=0]Training loss: 0.7743706703186035\n",
      "Epoch 8:  69%|██████▉   | 146/212 [00:03<00:01, 36.64it/s, v_num=0]Training loss: 0.5961979627609253\n",
      "Epoch 8:  69%|██████▉   | 147/212 [00:04<00:01, 36.65it/s, v_num=0]Training loss: 1.7291157245635986\n",
      "Epoch 8:  70%|██████▉   | 148/212 [00:04<00:01, 36.65it/s, v_num=0]Training loss: 0.703387975692749\n",
      "Epoch 8:  70%|███████   | 149/212 [00:04<00:01, 36.65it/s, v_num=0]Training loss: 0.8297194242477417\n",
      "Epoch 8:  71%|███████   | 150/212 [00:04<00:01, 36.66it/s, v_num=0]Training loss: 0.9017367959022522\n",
      "Epoch 8:  71%|███████   | 151/212 [00:04<00:01, 36.66it/s, v_num=0]Training loss: 1.0475108623504639\n",
      "Epoch 8:  72%|███████▏  | 152/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 0.8437126278877258\n",
      "Epoch 8:  72%|███████▏  | 153/212 [00:04<00:01, 36.67it/s, v_num=0]Training loss: 1.3608269691467285\n",
      "Epoch 8:  73%|███████▎  | 154/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 0.5438896417617798\n",
      "Epoch 8:  73%|███████▎  | 155/212 [00:04<00:01, 36.68it/s, v_num=0]Training loss: 0.8460150957107544\n",
      "Epoch 8:  74%|███████▎  | 156/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.7763888835906982\n",
      "Epoch 8:  74%|███████▍  | 157/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 0.7073292136192322\n",
      "Epoch 8:  75%|███████▍  | 158/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.8609559535980225\n",
      "Epoch 8:  75%|███████▌  | 159/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.9250715970993042\n",
      "Epoch 8:  75%|███████▌  | 160/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 1.0134646892547607\n",
      "Epoch 8:  76%|███████▌  | 161/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.7007003426551819\n",
      "Epoch 8:  76%|███████▋  | 162/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 1.301820993423462\n",
      "Epoch 8:  77%|███████▋  | 163/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 0.725587010383606\n",
      "Epoch 8:  77%|███████▋  | 164/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 0.9477712512016296\n",
      "Epoch 8:  78%|███████▊  | 165/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 0.9566949605941772\n",
      "Epoch 8:  78%|███████▊  | 166/212 [00:04<00:01, 36.73it/s, v_num=0]Training loss: 0.8208391070365906\n",
      "Epoch 8:  79%|███████▉  | 167/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.2054722309112549\n",
      "Epoch 8:  79%|███████▉  | 168/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 1.2519030570983887\n",
      "Epoch 8:  80%|███████▉  | 169/212 [00:04<00:01, 36.72it/s, v_num=0]Training loss: 0.8626801371574402\n",
      "Epoch 8:  80%|████████  | 170/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.4944351017475128\n",
      "Epoch 8:  81%|████████  | 171/212 [00:04<00:01, 36.71it/s, v_num=0]Training loss: 0.7043681740760803\n",
      "Epoch 8:  81%|████████  | 172/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.827475905418396\n",
      "Epoch 8:  82%|████████▏ | 173/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 1.0255296230316162\n",
      "Epoch 8:  82%|████████▏ | 174/212 [00:04<00:01, 36.69it/s, v_num=0]Training loss: 1.315519094467163\n",
      "Epoch 8:  83%|████████▎ | 175/212 [00:04<00:01, 36.70it/s, v_num=0]Training loss: 0.5881186723709106\n",
      "Epoch 8:  83%|████████▎ | 176/212 [00:04<00:00, 36.69it/s, v_num=0]Training loss: 0.9683467149734497\n",
      "Epoch 8:  83%|████████▎ | 177/212 [00:04<00:00, 36.69it/s, v_num=0]Training loss: 0.8910526037216187\n",
      "Epoch 8:  84%|████████▍ | 178/212 [00:04<00:00, 36.69it/s, v_num=0]Training loss: 0.570045530796051\n",
      "Epoch 8:  84%|████████▍ | 179/212 [00:04<00:00, 36.70it/s, v_num=0]Training loss: 0.8932462334632874\n",
      "Epoch 8:  85%|████████▍ | 180/212 [00:04<00:00, 36.70it/s, v_num=0]Training loss: 1.0050498247146606\n",
      "Epoch 8:  85%|████████▌ | 181/212 [00:04<00:00, 36.70it/s, v_num=0]Training loss: 3.355163097381592\n",
      "Epoch 8:  86%|████████▌ | 182/212 [00:04<00:00, 36.71it/s, v_num=0]Training loss: 1.3089512586593628\n",
      "Epoch 8:  86%|████████▋ | 183/212 [00:04<00:00, 36.71it/s, v_num=0]Training loss: 0.6544787883758545\n",
      "Epoch 8:  87%|████████▋ | 184/212 [00:05<00:00, 36.71it/s, v_num=0]Training loss: 2.207265853881836\n",
      "Epoch 8:  87%|████████▋ | 185/212 [00:05<00:00, 36.72it/s, v_num=0]Training loss: 1.1918294429779053\n",
      "Epoch 8:  88%|████████▊ | 186/212 [00:05<00:00, 36.72it/s, v_num=0]Training loss: 0.5902702212333679\n",
      "Epoch 8:  88%|████████▊ | 187/212 [00:05<00:00, 36.72it/s, v_num=0]Training loss: 1.1451141834259033\n",
      "Epoch 8:  89%|████████▊ | 188/212 [00:05<00:00, 36.73it/s, v_num=0]Training loss: 0.7474844455718994\n",
      "Epoch 8:  89%|████████▉ | 189/212 [00:05<00:00, 36.73it/s, v_num=0]Training loss: 0.7763243317604065\n",
      "Epoch 8:  90%|████████▉ | 190/212 [00:05<00:00, 36.73it/s, v_num=0]Training loss: 0.5884458422660828\n",
      "Epoch 8:  90%|█████████ | 191/212 [00:05<00:00, 36.74it/s, v_num=0]Training loss: 1.747675895690918\n",
      "Epoch 8:  91%|█████████ | 192/212 [00:05<00:00, 36.74it/s, v_num=0]Training loss: 0.6355321407318115\n",
      "Epoch 8:  91%|█████████ | 193/212 [00:05<00:00, 36.74it/s, v_num=0]Training loss: 1.3680834770202637\n",
      "Epoch 8:  92%|█████████▏| 194/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 0.7737729549407959\n",
      "Epoch 8:  92%|█████████▏| 195/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 10.207498550415039\n",
      "Epoch 8:  92%|█████████▏| 196/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.2990087270736694\n",
      "Epoch 8:  93%|█████████▎| 197/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.3203755617141724\n",
      "Epoch 8:  93%|█████████▎| 198/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.003170371055603\n",
      "Epoch 8:  94%|█████████▍| 199/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.8266457319259644\n",
      "Epoch 8:  94%|█████████▍| 200/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.2072163820266724\n",
      "Epoch 8:  95%|█████████▍| 201/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.4012675285339355\n",
      "Epoch 8:  95%|█████████▌| 202/212 [00:05<00:00, 36.77it/s, v_num=0]Training loss: 1.2419407367706299\n",
      "Epoch 8:  96%|█████████▌| 203/212 [00:05<00:00, 36.78it/s, v_num=0]Training loss: 0.8674876093864441\n",
      "Epoch 8:  96%|█████████▌| 204/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 1.1912376880645752\n",
      "Epoch 8:  97%|█████████▋| 205/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.8476473093032837\n",
      "Epoch 8:  97%|█████████▋| 206/212 [00:05<00:00, 36.76it/s, v_num=0]Training loss: 0.8925755023956299\n",
      "Epoch 8:  98%|█████████▊| 207/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 1.1129703521728516\n",
      "Epoch 8:  98%|█████████▊| 208/212 [00:05<00:00, 36.75it/s, v_num=0]Training loss: 0.9687408208847046\n",
      "Epoch 8:  99%|█████████▊| 209/212 [00:05<00:00, 36.74it/s, v_num=0]Training loss: 0.7088729739189148\n",
      "Epoch 8:  99%|█████████▉| 210/212 [00:05<00:00, 36.74it/s, v_num=0]Training loss: 1.6405874490737915\n",
      "Epoch 8: 100%|█████████▉| 211/212 [00:05<00:00, 36.73it/s, v_num=0]Training loss: 0.8824717402458191\n",
      "Epoch 8: 100%|██████████| 212/212 [00:05<00:00, 36.73it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.034207820892334\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 86.30it/s]\u001b[AValidation loss: 1.7395950555801392\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 87.47it/s]\u001b[AValidation loss: 0.7988555431365967\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 89.18it/s]\u001b[AValidation loss: 0.5935274362564087\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 90.34it/s]\u001b[AValidation loss: 0.6401011347770691\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 91.17it/s]\u001b[AValidation loss: 0.7880793809890747\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 91.64it/s]\u001b[AValidation loss: 1.0974820852279663\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 92.23it/s]\u001b[AValidation loss: 2.0438079833984375\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 92.07it/s]\u001b[AValidation loss: 0.5450279712677002\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 92.54it/s]\u001b[AValidation loss: 0.6527600884437561\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 92.91it/s]\u001b[AValidation loss: 0.6454986929893494\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 93.09it/s]\u001b[AValidation loss: 0.9376521706581116\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 93.23it/s]\u001b[AValidation loss: 0.9721791744232178\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 93.41it/s]\u001b[AValidation loss: 0.8578394651412964\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 93.59it/s]\u001b[AValidation loss: 0.7143355011940002\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 93.70it/s]\u001b[AValidation loss: 0.9912706613540649\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 93.50it/s]\u001b[AValidation loss: 0.9555205702781677\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 93.65it/s]\u001b[AValidation loss: 1.4121524095535278\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 93.75it/s]\u001b[AValidation loss: 0.6682131290435791\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 93.42it/s]\u001b[AValidation loss: 1.1487823724746704\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 93.23it/s]\u001b[AValidation loss: 1.0090080499649048\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 93.10it/s]\u001b[AValidation loss: 1.0252279043197632\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 92.93it/s]\u001b[AValidation loss: 0.990761935710907\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 93.07it/s]\u001b[AValidation loss: 0.833416759967804\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 92.94it/s]\u001b[AValidation loss: 1.2538551092147827\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 93.02it/s]\u001b[AValidation loss: 0.6596207618713379\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 93.09it/s]\u001b[AValidation loss: 0.8318156003952026\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 93.13it/s]\u001b[AValidation loss: 0.8738399744033813\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 93.21it/s]\u001b[AValidation loss: 0.8777141571044922\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 93.24it/s]\u001b[AValidation loss: 1.1819723844528198\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 93.29it/s]\u001b[AValidation loss: 0.7434963583946228\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 93.34it/s]\u001b[AValidation loss: 0.9318435788154602\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 93.41it/s]\u001b[AValidation loss: 0.8442965149879456\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 93.47it/s]\u001b[AValidation loss: 0.7190983891487122\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 93.52it/s]\u001b[AValidation loss: 1.7070493698120117\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 93.58it/s]\u001b[AValidation loss: 0.502594530582428\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 93.66it/s]\u001b[AValidation loss: 1.4610553979873657\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 93.57it/s]\u001b[AValidation loss: 0.5661720633506775\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 93.47it/s]\u001b[AValidation loss: 0.9126836657524109\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 93.40it/s]\u001b[AValidation loss: 0.8663043975830078\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 93.31it/s]\u001b[AValidation loss: 0.7150784730911255\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 93.26it/s]\u001b[AValidation loss: 0.6471061706542969\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 93.34it/s]\u001b[AValidation loss: 0.6895728707313538\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 93.29it/s]\u001b[AValidation loss: 1.1975600719451904\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 93.36it/s]\u001b[AValidation loss: 1.3115956783294678\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 93.31it/s]\u001b[AValidation loss: 0.6470907926559448\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 93.36it/s]\u001b[AValidation loss: 0.8642238974571228\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 93.43it/s]\u001b[AValidation loss: 0.8380572199821472\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 93.37it/s]\u001b[AValidation loss: 1.06014084815979\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 93.43it/s]\u001b[AValidation loss: 1.2633585929870605\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 93.48it/s]\u001b[AValidation loss: 1.1200000047683716\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 93.54it/s]\u001b[AValidation loss: 1.0203979015350342\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 93.50it/s]\u001b[AValidation loss: 1.2640608549118042\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 93.54it/s]\u001b[AValidation loss: 0.9559429883956909\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 93.58it/s]\u001b[AValidation loss: 2.068953275680542\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 93.72it/s]\u001b[A\n",
      "Epoch 9:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.7160565257072449\n",
      "Epoch 9:   0%|          | 1/212 [00:00<00:04, 43.89it/s, v_num=0]Training loss: 0.7816319465637207\n",
      "Epoch 9:   1%|          | 2/212 [00:00<00:05, 40.38it/s, v_num=0]Training loss: 1.0887025594711304\n",
      "Epoch 9:   1%|▏         | 3/212 [00:00<00:05, 39.34it/s, v_num=0]Training loss: 0.7590650916099548\n",
      "Epoch 9:   2%|▏         | 4/212 [00:00<00:05, 38.83it/s, v_num=0]Training loss: 0.9394786357879639\n",
      "Epoch 9:   2%|▏         | 5/212 [00:00<00:05, 38.55it/s, v_num=0]Training loss: 0.949622392654419\n",
      "Epoch 9:   3%|▎         | 6/212 [00:00<00:05, 38.37it/s, v_num=0]Training loss: 0.5609993934631348\n",
      "Epoch 9:   3%|▎         | 7/212 [00:00<00:05, 38.23it/s, v_num=0]Training loss: 0.6787063479423523\n",
      "Epoch 9:   4%|▍         | 8/212 [00:00<00:05, 37.93it/s, v_num=0]Training loss: 1.063115119934082\n",
      "Epoch 9:   4%|▍         | 9/212 [00:00<00:05, 37.69it/s, v_num=0]Training loss: 0.7365497350692749\n",
      "Epoch 9:   5%|▍         | 10/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 1.074173092842102\n",
      "Epoch 9:   5%|▌         | 11/212 [00:00<00:05, 37.39it/s, v_num=0]Training loss: 0.693416178226471\n",
      "Epoch 9:   6%|▌         | 12/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 0.8330677151679993\n",
      "Epoch 9:   6%|▌         | 13/212 [00:00<00:05, 37.12it/s, v_num=0]Training loss: 0.8281433582305908\n",
      "Epoch 9:   7%|▋         | 14/212 [00:00<00:05, 37.03it/s, v_num=0]Training loss: 0.7682608962059021\n",
      "Epoch 9:   7%|▋         | 15/212 [00:00<00:05, 36.99it/s, v_num=0]Training loss: 0.9487239122390747\n",
      "Epoch 9:   8%|▊         | 16/212 [00:00<00:05, 36.91it/s, v_num=0]Training loss: 1.2753918170928955\n",
      "Epoch 9:   8%|▊         | 17/212 [00:00<00:05, 36.92it/s, v_num=0]Training loss: 0.7593350410461426\n",
      "Epoch 9:   8%|▊         | 18/212 [00:00<00:05, 36.93it/s, v_num=0]Training loss: 1.5420550107955933\n",
      "Epoch 9:   9%|▉         | 19/212 [00:00<00:05, 37.01it/s, v_num=0]Training loss: 0.5638633966445923\n",
      "Epoch 9:   9%|▉         | 20/212 [00:00<00:05, 37.00it/s, v_num=0]Training loss: 1.494483232498169\n",
      "Epoch 9:  10%|▉         | 21/212 [00:00<00:05, 37.01it/s, v_num=0]Training loss: 1.0235158205032349\n",
      "Epoch 9:  10%|█         | 22/212 [00:00<00:05, 37.03it/s, v_num=0]Training loss: 0.8730626106262207\n",
      "Epoch 9:  11%|█         | 23/212 [00:00<00:05, 37.04it/s, v_num=0]Training loss: 0.650473952293396\n",
      "Epoch 9:  11%|█▏        | 24/212 [00:00<00:05, 37.05it/s, v_num=0]Training loss: 0.9701290726661682\n",
      "Epoch 9:  12%|█▏        | 25/212 [00:00<00:05, 37.06it/s, v_num=0]Training loss: 1.0367785692214966\n",
      "Epoch 9:  12%|█▏        | 26/212 [00:00<00:05, 37.07it/s, v_num=0]Training loss: 0.9979127049446106\n",
      "Epoch 9:  13%|█▎        | 27/212 [00:00<00:04, 37.07it/s, v_num=0]Training loss: 1.0113579034805298\n",
      "Epoch 9:  13%|█▎        | 28/212 [00:00<00:04, 37.07it/s, v_num=0]Training loss: 0.6062906980514526\n",
      "Epoch 9:  14%|█▎        | 29/212 [00:00<00:04, 37.09it/s, v_num=0]Training loss: 0.8411436080932617\n",
      "Epoch 9:  14%|█▍        | 30/212 [00:00<00:04, 37.10it/s, v_num=0]Training loss: 0.9586659669876099\n",
      "Epoch 9:  15%|█▍        | 31/212 [00:00<00:04, 37.11it/s, v_num=0]Training loss: 0.8421779870986938\n",
      "Epoch 9:  15%|█▌        | 32/212 [00:00<00:04, 37.12it/s, v_num=0]Training loss: 1.250369668006897\n",
      "Epoch 9:  16%|█▌        | 33/212 [00:00<00:04, 37.14it/s, v_num=0]Training loss: 0.817876398563385\n",
      "Epoch 9:  16%|█▌        | 34/212 [00:00<00:04, 37.14it/s, v_num=0]Training loss: 0.813510000705719\n",
      "Epoch 9:  17%|█▋        | 35/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 1.6475839614868164\n",
      "Epoch 9:  17%|█▋        | 36/212 [00:00<00:04, 37.15it/s, v_num=0]Training loss: 1.1271196603775024\n",
      "Epoch 9:  17%|█▋        | 37/212 [00:00<00:04, 37.16it/s, v_num=0]Training loss: 1.4942741394042969\n",
      "Epoch 9:  18%|█▊        | 38/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.5137163400650024\n",
      "Epoch 9:  18%|█▊        | 39/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.4318332374095917\n",
      "Epoch 9:  19%|█▉        | 40/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.0283925533294678\n",
      "Epoch 9:  19%|█▉        | 41/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.6727099418640137\n",
      "Epoch 9:  20%|█▉        | 42/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.5724602937698364\n",
      "Epoch 9:  20%|██        | 43/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.347288727760315\n",
      "Epoch 9:  21%|██        | 44/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.3932437896728516\n",
      "Epoch 9:  21%|██        | 45/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 0.6577491760253906\n",
      "Epoch 9:  22%|██▏       | 46/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.0148361921310425\n",
      "Epoch 9:  22%|██▏       | 47/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.7880935668945312\n",
      "Epoch 9:  23%|██▎       | 48/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.1486515998840332\n",
      "Epoch 9:  23%|██▎       | 49/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.6513484716415405\n",
      "Epoch 9:  24%|██▎       | 50/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.5924217700958252\n",
      "Epoch 9:  24%|██▍       | 51/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 0.6133882403373718\n",
      "Epoch 9:  25%|██▍       | 52/212 [00:01<00:04, 37.11it/s, v_num=0]Training loss: 0.6477988958358765\n",
      "Epoch 9:  25%|██▌       | 53/212 [00:01<00:04, 37.10it/s, v_num=0]Training loss: 1.074337363243103\n",
      "Epoch 9:  25%|██▌       | 54/212 [00:01<00:04, 37.06it/s, v_num=0]Training loss: 0.4548734128475189\n",
      "Epoch 9:  26%|██▌       | 55/212 [00:01<00:04, 37.04it/s, v_num=0]Training loss: 0.7042809724807739\n",
      "Epoch 9:  26%|██▋       | 56/212 [00:01<00:04, 37.03it/s, v_num=0]Training loss: 1.2350952625274658\n",
      "Epoch 9:  27%|██▋       | 57/212 [00:01<00:04, 37.02it/s, v_num=0]Training loss: 0.7449244856834412\n",
      "Epoch 9:  27%|██▋       | 58/212 [00:01<00:04, 37.03it/s, v_num=0]Training loss: 0.899912416934967\n",
      "Epoch 9:  28%|██▊       | 59/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 0.7628390192985535\n",
      "Epoch 9:  28%|██▊       | 60/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 2.1618776321411133\n",
      "Epoch 9:  29%|██▉       | 61/212 [00:01<00:04, 37.05it/s, v_num=0]Training loss: 2.170701503753662\n",
      "Epoch 9:  29%|██▉       | 62/212 [00:01<00:04, 37.06it/s, v_num=0]Training loss: 0.6553528904914856\n",
      "Epoch 9:  30%|██▉       | 63/212 [00:01<00:04, 37.07it/s, v_num=0]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "\n",
    "# Create a logger\n",
    "logger = loggers.CSVLogger('lightning_logs/', name=f'v2_m1_vae_{data_size}')\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Initialize the VAE Lightning model\n",
    "input_dim = train_tensors[0].shape[1]  # The number of input features\n",
    "# input_dim = X_train_tensor.shape[1]  # The number of input features\n",
    "latent_dim = 256  # Latent dimension size, can be tuned\n",
    "hidden_dims = [2048, 1024, 512]\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-6\n",
    "\n",
    "model = VAE_Lightning(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    dropout_rate=dropout_rate,\n",
    "    lr=lr)\n",
    "\n",
    "# Training\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    dirpath=f'{logger.save_dir}/{logger.name}/version_{logger.version}/checkpoints/',\n",
    "    filename='m1-vae-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    gradient_clip_val=0.1,  # Clip gradients to avoid explosion\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, loss_history_callback],\n",
    "    precision=32,\n",
    "    accelerator='gpu',          # Use 'gpu' or 'cpu'\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',  # Use 1 GPU or CPU ('auto' will pick the appropriate one)\n",
    "    deterministic=True,  # Ensure reproducibility\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41d126-b304-44d2-a04e-2acb99ad917b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3d0f77-c8b8-4caa-a189-56b80f990dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'df_logs' is your DataFrame with the logs\n",
    "# Drop the rows with NaN values for plotting each loss curve separately\n",
    "df_logs = pd.read_csv(\"lightning_logs/m1_vae/version_0/metrics.csv\")\n",
    "\n",
    "# Extract relevant columns\n",
    "epochs = df_logs['epoch'].unique()  # Get unique epoch values\n",
    "train_loss = df_logs['train_loss'].dropna()  # Drop NaN values for train loss\n",
    "val_loss = df_logs['val_loss'].dropna()      # Drop NaN values for validation loss\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs[:len(val_loss)], val_loss, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c431f599-80a3-41d2-94f0-d91882af4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': 0.2, 'hidden_dims': [2048, 1024, 512], 'input_dim': 451747, 'latent_dim': 256, 'lr': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the hyperparameters from the hparams.yaml file\n",
    "hparams_path = 'lightning_logs/m1_vae/version_0/hparams.yaml'  # Replace with the correct path\n",
    "with open(hparams_path) as file:\n",
    "    hparams = yaml.safe_load(file)\n",
    "\n",
    "print(hparams)  # To inspect the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b179030-528c-486f-b322-ac02c9078e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb373265-2411-49c0-ab8e-b739615c35c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_Lightning(\n",
       "  (model): VAE(\n",
       "    (encoder_layers): Sequential(\n",
       "      (0): Linear(in_features=451747, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_logvar): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_output): Linear(in_features=2048, out_features=451747, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "checkpoint_path = \"lightning_logs/m1_vae/version_0/checkpoints/m1-vae-epoch=21-val_loss=0.96.ckpt\"\n",
    "vae_model = VAE_Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    **hparams\n",
    "    )\n",
    "\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7151a777-f314-4cc2-8456-aa01e7b220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_embeddings(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x,y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Replace NaNs with zero or another neutral value for forward pass\n",
    "            x_filled = replace_nan_with_mean(x)\n",
    "            # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "            \n",
    "            z, _, _ = model.forward(x_filled)\n",
    "            embeddings.append(z)\n",
    "            labels.append(y)\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = get_latent_embeddings(vae_model, train_loader)\n",
    "val_embeddings, val_labels = get_latent_embeddings(vae_model, val_loader)\n",
    "test_embeddings, test_labels = get_latent_embeddings(vae_model, test_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbab96-e2f0-44ee-bf0c-63e74b8659dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "595a6bf4-15d5-4bf3-a6c9-2f113fc5db7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Combine train and validation embeddings\n",
    "# combined_embeddings = torch.cat([train_embeddings, val_embeddings], dim=0).cpu().numpy()\n",
    "# combined_labels = torch.cat([train_labels, val_labels], dim=0).cpu().numpy()\n",
    "# # Convert one-hot encoded labels to class indices using argmax\n",
    "# combined_labels_1d = combined_labels.argmax(axis=1)  # Find the index of the maximum (1) in each one-hot vector\n",
    "\n",
    "# # Fit UMAP on the combined embeddings\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "# combined_umap = umap_model.fit_transform(combined_embeddings)\n",
    "\n",
    "# # Plot UMAP embeddings (train + validation)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(combined_umap[:, 0], combined_umap[:, 1], c=combined_labels_1d, cmap='Spectral', s=10, alpha=0.8)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title('UMAP Projection of Train + Validation Embeddings')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3529e-6a6a-4ab5-9103-6c50c6b83cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc9933dc-318e-4ba3-838f-87836ef4e237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Convert test one-hot encoded labels to class indices\n",
    "# test_labels_1d = test_labels.cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# # Project test embeddings onto the UMAP space\n",
    "# test_umap = umap_model.transform(test_embeddings.cpu().numpy())\n",
    "\n",
    "# # Plot the test UMAP embeddings\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(test_umap[:, 0], test_umap[:, 1], c=test_labels_1d, cmap='Spectral', s=10, alpha=0.8)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title('UMAP Projection of Test Embeddings')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bc319-9669-41cd-927e-778ddc8ff8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d939d-d920-4f44-9ba6-6a3dfde75331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ba6f-2fe3-4b94-a544-41c527fb8a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f916b22f-7888-485b-a7d3-517aaba4f247",
   "metadata": {},
   "source": [
    "# MLP building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "147f8621-94ab-4914-b610-5de0314770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_dim, layer_dims, num_classes, dropout_rate=0.2, lr=1e-3, class_weights=None):\n",
    "        \"\"\"\n",
    "        :param input_dim: Number of input features (e.g., from VAE embeddings)\n",
    "        :param layer_dims: List of integers defining the number of units in each layer\n",
    "        :param num_classes: Number of output classes for classification (should match the number of columns in one-hot labels)\n",
    "        :param dropout_rate: Dropout rate to be applied after each layer\n",
    "        :param lr: Learning rate\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()  # Save hyperparameters for checkpointing\n",
    "\n",
    "        # Store learning rate\n",
    "        self.lr = lr\n",
    "        # self.class_weights = class_weights  # Add class weights\n",
    "        \n",
    "        # Store the class weights and move them to the right device in the forward pass\n",
    "        self.register_buffer('class_weights', class_weights)\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        for dim in layer_dims:\n",
    "            layers.append(nn.Linear(current_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_dim = dim\n",
    "\n",
    "        # Output layer for one-hot encoded labels (num_classes)\n",
    "        layers.append(nn.Linear(current_dim, num_classes))  # Output layer with num_classes\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y is class index (not one-hot encoded)\n",
    "        y_hat = self(x)\n",
    "    \n",
    "        # Use CrossEntropyLoss for single-label classification with class weighting\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights.to(self.device))\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "    \n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y is class index (not one-hot encoded)\n",
    "        y_hat = self(x)\n",
    "    \n",
    "        val_loss = F.cross_entropy(y_hat, y, weight=self.class_weights.to(self.device))\n",
    "        # val_loss = F.cross_entropy(y_hat, y)\n",
    "    \n",
    "        print(f\"Validation loss: {val_loss.item()}\")\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n",
    "        return val_loss\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6bfee4ed-e03a-482e-bf6e-431552b1254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 843,   80,  124,    6,  671,   35, 3766,   15,  100,   21,   78,\n",
       "         85,   50,   73,   40,   28,   78,  168, 1030,   21,  185,   50,\n",
       "          3,  503,   26,   45,   61])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels_class = np.argmax(train_tensors[1], axis=1)  # Convert one-hot to class labels\n",
    "labels_class = labels_class.numpy()\n",
    "\n",
    "unique, counts = np.unique(labels_class, return_counts=True)\n",
    "# unique, counts = unique.tolist(), counts.tolist()\n",
    "# Convert back to tensor if needed, or just print the results\n",
    "value_counts = dict(zip(unique, counts))\n",
    "class_counts = counts\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b0405c73-b4b2-4df8-ab20-ce28e7dc8107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2265e-03, 1.2924e-02, 8.3380e-03, 1.7232e-01, 1.5409e-03, 2.9540e-02,\n",
       "        2.7454e-04, 6.8928e-02, 1.0339e-02, 4.9234e-02, 1.3255e-02, 1.2164e-02,\n",
       "        2.0678e-02, 1.4163e-02, 2.5848e-02, 3.6926e-02, 1.3255e-02, 6.1543e-03,\n",
       "        1.0038e-03, 4.9234e-02, 5.5887e-03, 2.0678e-02, 3.4464e-01, 2.0555e-03,\n",
       "        3.9766e-02, 2.2976e-02, 1.6949e-02])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Assume you have a tensor of class frequencies\n",
    "# class_counts = torch.tensor([181, 17, 27, 1, 144, 7, 807, 3, 22, 4, 17, 19, 11, 16, 8, 6, 17, 36, 221, 4, 40, 10, 1, 108, 5, 10, 13])  # example\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize weights\n",
    "class_weights = torch.tensor(class_weights).to(torch.float32)\n",
    "\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cd9ea-7148-4851-ae21-7e3980937df6",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "00486e88-b408-4169-8ac0-170ff25c0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "targets = torch.argmax(train_labels, dim=1)  # 'targets' contains class indices\n",
    "\n",
    "# Count how many samples belong to each class\n",
    "class_sample_count = torch.bincount(targets)  # Class counts for each class in targets\n",
    "\n",
    "# Compute weights for each class\n",
    "weights = torch.sqrt(1.0 / class_sample_count.float())  # Inverse of class frequency\n",
    "\n",
    "# Assign weights to each sample based on its class\n",
    "samples_weight = torch.tensor([weights[t] for t in targets])\n",
    "\n",
    "# Define a WeightedRandomSampler with the computed sample weights\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "# # Create DataLoader with the sampler\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57173715-2c36-4c88-af9a-020b5309c038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1d1d5733-062b-4850-b83b-090edbaea82b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 19.4 K | train\n",
      "---------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Validation loss: 3.2722280025482178\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 246.19it/s]Validation loss: 3.2505276203155518\n",
      "Epoch 0:   0%|          | 0/128 [00:00<?, ?it/s]                            Training loss: 3.3476412296295166\n",
      "Epoch 0:   1%|          | 1/128 [00:00<00:01, 104.29it/s, v_num=20]Training loss: 3.3425498008728027\n",
      "Epoch 0:   2%|▏         | 2/128 [00:00<00:00, 127.71it/s, v_num=20]Training loss: 3.3466525077819824\n",
      "Epoch 0:   2%|▏         | 3/128 [00:00<00:00, 138.47it/s, v_num=20]Training loss: 3.3278656005859375\n",
      "Epoch 0:   3%|▎         | 4/128 [00:00<00:00, 144.57it/s, v_num=20]Training loss: 3.2603001594543457\n",
      "Epoch 0:   4%|▍         | 5/128 [00:00<00:00, 148.48it/s, v_num=20]Training loss: 3.336038827896118\n",
      "Epoch 0:   5%|▍         | 6/128 [00:00<00:00, 151.29it/s, v_num=20]Training loss: 3.164912223815918\n",
      "Epoch 0:   5%|▌         | 7/128 [00:00<00:00, 153.26it/s, v_num=20]Training loss: 3.2855544090270996\n",
      "Epoch 0:   6%|▋         | 8/128 [00:00<00:00, 154.80it/s, v_num=20]Training loss: 3.3410189151763916\n",
      "Epoch 0:   7%|▋         | 9/128 [00:00<00:00, 156.04it/s, v_num=20]Training loss: 3.2871932983398438\n",
      "Epoch 0:   8%|▊         | 10/128 [00:00<00:00, 157.03it/s, v_num=20]Training loss: 3.349045991897583\n",
      "Epoch 0:   9%|▊         | 11/128 [00:00<00:00, 153.75it/s, v_num=20]Training loss: 3.3002846240997314\n",
      "Epoch 0:   9%|▉         | 12/128 [00:00<00:00, 154.72it/s, v_num=20]Training loss: 3.251664876937866\n",
      "Epoch 0:  10%|█         | 13/128 [00:00<00:00, 155.58it/s, v_num=20]Training loss: 3.4286069869995117\n",
      "Epoch 0:  11%|█         | 14/128 [00:00<00:00, 156.33it/s, v_num=20]Training loss: 3.3598122596740723\n",
      "Epoch 0:  12%|█▏        | 15/128 [00:00<00:00, 157.01it/s, v_num=20]Training loss: 3.328124761581421\n",
      "Epoch 0:  12%|█▎        | 16/128 [00:00<00:00, 157.52it/s, v_num=20]Training loss: 3.259289026260376\n",
      "Epoch 0:  13%|█▎        | 17/128 [00:00<00:00, 158.09it/s, v_num=20]Training loss: 3.271850347518921\n",
      "Epoch 0:  14%|█▍        | 18/128 [00:00<00:00, 158.55it/s, v_num=20]Training loss: 3.3809974193573\n",
      "Epoch 0:  15%|█▍        | 19/128 [00:00<00:00, 158.92it/s, v_num=20]Training loss: 3.1899633407592773\n",
      "Epoch 0:  16%|█▌        | 20/128 [00:00<00:00, 159.38it/s, v_num=20]Training loss: 3.412853717803955\n",
      "Epoch 0:  16%|█▋        | 21/128 [00:00<00:00, 159.74it/s, v_num=20]Training loss: 3.4405791759490967\n",
      "Epoch 0:  17%|█▋        | 22/128 [00:00<00:00, 160.05it/s, v_num=20]Training loss: 3.2655656337738037\n",
      "Epoch 0:  18%|█▊        | 23/128 [00:00<00:00, 160.36it/s, v_num=20]Training loss: 3.230043888092041\n",
      "Epoch 0:  19%|█▉        | 24/128 [00:00<00:00, 160.45it/s, v_num=20]Training loss: 3.4102556705474854\n",
      "Epoch 0:  20%|█▉        | 25/128 [00:00<00:00, 160.85it/s, v_num=20]Training loss: 3.359835147857666\n",
      "Epoch 0:  20%|██        | 26/128 [00:00<00:00, 161.07it/s, v_num=20]Training loss: 3.295130491256714\n",
      "Epoch 0:  21%|██        | 27/128 [00:00<00:00, 161.29it/s, v_num=20]Training loss: 3.315497636795044\n",
      "Epoch 0:  22%|██▏       | 28/128 [00:00<00:00, 161.43it/s, v_num=20]Training loss: 3.355248212814331\n",
      "Epoch 0:  23%|██▎       | 29/128 [00:00<00:00, 161.59it/s, v_num=20]Training loss: 3.3648409843444824\n",
      "Epoch 0:  23%|██▎       | 30/128 [00:00<00:00, 161.74it/s, v_num=20]Training loss: 3.429039716720581\n",
      "Epoch 0:  24%|██▍       | 31/128 [00:00<00:00, 161.35it/s, v_num=20]Training loss: 3.317934036254883\n",
      "Epoch 0:  25%|██▌       | 32/128 [00:00<00:00, 160.97it/s, v_num=20]Training loss: 3.342780351638794\n",
      "Epoch 0:  26%|██▌       | 33/128 [00:00<00:00, 160.64it/s, v_num=20]Training loss: 3.311408281326294\n",
      "Epoch 0:  27%|██▋       | 34/128 [00:00<00:00, 160.34it/s, v_num=20]Training loss: 3.3340065479278564\n",
      "Epoch 0:  27%|██▋       | 35/128 [00:00<00:00, 160.07it/s, v_num=20]Training loss: 3.3362221717834473\n",
      "Epoch 0:  28%|██▊       | 36/128 [00:00<00:00, 159.79it/s, v_num=20]Training loss: 3.2817723751068115\n",
      "Epoch 0:  29%|██▉       | 37/128 [00:00<00:00, 159.94it/s, v_num=20]Training loss: 3.2846999168395996\n",
      "Epoch 0:  30%|██▉       | 38/128 [00:00<00:00, 160.10it/s, v_num=20]Training loss: 3.5954153537750244\n",
      "Epoch 0:  30%|███       | 39/128 [00:00<00:00, 160.26it/s, v_num=20]Training loss: 3.2033298015594482\n",
      "Epoch 0:  31%|███▏      | 40/128 [00:00<00:00, 160.42it/s, v_num=20]Training loss: 3.4166500568389893\n",
      "Epoch 0:  32%|███▏      | 41/128 [00:00<00:00, 159.81it/s, v_num=20]Training loss: 3.371731758117676\n",
      "Epoch 0:  33%|███▎      | 42/128 [00:00<00:00, 159.67it/s, v_num=20]Training loss: 3.3693103790283203\n",
      "Epoch 0:  34%|███▎      | 43/128 [00:00<00:00, 159.80it/s, v_num=20]Training loss: 3.3588085174560547\n",
      "Epoch 0:  34%|███▍      | 44/128 [00:00<00:00, 159.95it/s, v_num=20]Training loss: 3.0407114028930664\n",
      "Epoch 0:  35%|███▌      | 45/128 [00:00<00:00, 160.08it/s, v_num=20]Training loss: 3.22109055519104\n",
      "Epoch 0:  36%|███▌      | 46/128 [00:00<00:00, 160.21it/s, v_num=20]Training loss: 3.251830577850342\n",
      "Epoch 0:  37%|███▋      | 47/128 [00:00<00:00, 160.33it/s, v_num=20]Training loss: 3.3884644508361816\n",
      "Epoch 0:  38%|███▊      | 48/128 [00:00<00:00, 160.45it/s, v_num=20]Training loss: 3.3516314029693604\n",
      "Epoch 0:  38%|███▊      | 49/128 [00:00<00:00, 160.54it/s, v_num=20]Training loss: 3.3633124828338623\n",
      "Epoch 0:  39%|███▉      | 50/128 [00:00<00:00, 160.61it/s, v_num=20]Training loss: 3.3677825927734375\n",
      "Epoch 0:  40%|███▉      | 51/128 [00:00<00:00, 160.09it/s, v_num=20]Training loss: 3.2962260246276855\n",
      "Epoch 0:  41%|████      | 52/128 [00:00<00:00, 159.91it/s, v_num=20]Training loss: 3.058321237564087\n",
      "Epoch 0:  41%|████▏     | 53/128 [00:00<00:00, 159.72it/s, v_num=20]Training loss: 3.465508222579956\n",
      "Epoch 0:  42%|████▏     | 54/128 [00:00<00:00, 159.81it/s, v_num=20]Training loss: 3.4674324989318848\n",
      "Epoch 0:  43%|████▎     | 55/128 [00:00<00:00, 159.93it/s, v_num=20]Training loss: 3.347191095352173\n",
      "Epoch 0:  44%|████▍     | 56/128 [00:00<00:00, 160.02it/s, v_num=20]Training loss: 3.29168701171875\n",
      "Epoch 0:  45%|████▍     | 57/128 [00:00<00:00, 160.13it/s, v_num=20]Training loss: 3.243715763092041\n",
      "Epoch 0:  45%|████▌     | 58/128 [00:00<00:00, 160.21it/s, v_num=20]Training loss: 3.3211889266967773\n",
      "Epoch 0:  46%|████▌     | 59/128 [00:00<00:00, 160.31it/s, v_num=20]Training loss: 3.4232189655303955\n",
      "Epoch 0:  47%|████▋     | 60/128 [00:00<00:00, 160.40it/s, v_num=20]Training loss: 3.204622268676758\n",
      "Epoch 0:  48%|████▊     | 61/128 [00:00<00:00, 160.47it/s, v_num=20]Training loss: 3.362734794616699\n",
      "Epoch 0:  48%|████▊     | 62/128 [00:00<00:00, 160.56it/s, v_num=20]Training loss: 3.270725965499878\n",
      "Epoch 0:  49%|████▉     | 63/128 [00:00<00:00, 160.64it/s, v_num=20]Training loss: 3.3433005809783936\n",
      "Epoch 0:  50%|█████     | 64/128 [00:00<00:00, 160.47it/s, v_num=20]Training loss: 3.2982890605926514\n",
      "Epoch 0:  51%|█████     | 65/128 [00:00<00:00, 160.32it/s, v_num=20]Training loss: 3.3160626888275146\n",
      "Epoch 0:  52%|█████▏    | 66/128 [00:00<00:00, 160.17it/s, v_num=20]Training loss: 3.335383176803589\n",
      "Epoch 0:  52%|█████▏    | 67/128 [00:00<00:00, 160.01it/s, v_num=20]Training loss: 3.426095962524414\n",
      "Epoch 0:  53%|█████▎    | 68/128 [00:00<00:00, 159.87it/s, v_num=20]Training loss: 3.382065534591675\n",
      "Epoch 0:  54%|█████▍    | 69/128 [00:00<00:00, 159.73it/s, v_num=20]Training loss: 3.357741117477417\n",
      "Epoch 0:  55%|█████▍    | 70/128 [00:00<00:00, 159.67it/s, v_num=20]Training loss: 3.28067946434021\n",
      "Epoch 0:  55%|█████▌    | 71/128 [00:00<00:00, 159.76it/s, v_num=20]Training loss: 3.1789140701293945\n",
      "Epoch 0:  56%|█████▋    | 72/128 [00:00<00:00, 159.83it/s, v_num=20]Training loss: 3.2015750408172607\n",
      "Epoch 0:  57%|█████▋    | 73/128 [00:00<00:00, 159.72it/s, v_num=20]Training loss: 3.458195686340332\n",
      "Epoch 0:  58%|█████▊    | 74/128 [00:00<00:00, 159.79it/s, v_num=20]Training loss: 3.2486472129821777\n",
      "Epoch 0:  59%|█████▊    | 75/128 [00:00<00:00, 159.68it/s, v_num=20]Training loss: 3.383329153060913\n",
      "Epoch 0:  59%|█████▉    | 76/128 [00:00<00:00, 159.75it/s, v_num=20]Training loss: 3.3295998573303223\n",
      "Epoch 0:  60%|██████    | 77/128 [00:00<00:00, 159.82it/s, v_num=20]Training loss: 3.3055026531219482\n",
      "Epoch 0:  61%|██████    | 78/128 [00:00<00:00, 159.90it/s, v_num=20]Training loss: 3.2895820140838623\n",
      "Epoch 0:  62%|██████▏   | 79/128 [00:00<00:00, 159.98it/s, v_num=20]Training loss: 3.369098424911499\n",
      "Epoch 0:  62%|██████▎   | 80/128 [00:00<00:00, 160.05it/s, v_num=20]Training loss: 3.4013943672180176\n",
      "Epoch 0:  63%|██████▎   | 81/128 [00:00<00:00, 160.13it/s, v_num=20]Training loss: 3.323259115219116\n",
      "Epoch 0:  64%|██████▍   | 82/128 [00:00<00:00, 160.19it/s, v_num=20]Training loss: 3.2759525775909424\n",
      "Epoch 0:  65%|██████▍   | 83/128 [00:00<00:00, 160.25it/s, v_num=20]Training loss: 3.3114380836486816\n",
      "Epoch 0:  66%|██████▌   | 84/128 [00:00<00:00, 160.12it/s, v_num=20]Training loss: 3.2828996181488037\n",
      "Epoch 0:  66%|██████▋   | 85/128 [00:00<00:00, 160.01it/s, v_num=20]Training loss: 3.224815607070923\n",
      "Epoch 0:  67%|██████▋   | 86/128 [00:00<00:00, 159.88it/s, v_num=20]Training loss: 3.263303756713867\n",
      "Epoch 0:  68%|██████▊   | 87/128 [00:00<00:00, 159.77it/s, v_num=20]Training loss: 3.3166699409484863\n",
      "Epoch 0:  69%|██████▉   | 88/128 [00:00<00:00, 159.84it/s, v_num=20]Training loss: 3.3478477001190186\n",
      "Epoch 0:  70%|██████▉   | 89/128 [00:00<00:00, 159.91it/s, v_num=20]Training loss: 3.283064842224121\n",
      "Epoch 0:  70%|███████   | 90/128 [00:00<00:00, 159.99it/s, v_num=20]Training loss: 3.341244697570801\n",
      "Epoch 0:  71%|███████   | 91/128 [00:00<00:00, 160.06it/s, v_num=20]Training loss: 3.3728373050689697\n",
      "Epoch 0:  72%|███████▏  | 92/128 [00:00<00:00, 160.13it/s, v_num=20]Training loss: 3.1177079677581787\n",
      "Epoch 0:  73%|███████▎  | 93/128 [00:00<00:00, 160.19it/s, v_num=20]Training loss: 3.315835475921631\n",
      "Epoch 0:  73%|███████▎  | 94/128 [00:00<00:00, 160.25it/s, v_num=20]Training loss: 3.31217885017395\n",
      "Epoch 0:  74%|███████▍  | 95/128 [00:00<00:00, 160.32it/s, v_num=20]Training loss: 3.313877582550049\n",
      "Epoch 0:  75%|███████▌  | 96/128 [00:00<00:00, 160.38it/s, v_num=20]Training loss: 3.2782888412475586\n",
      "Epoch 0:  76%|███████▌  | 97/128 [00:00<00:00, 160.28it/s, v_num=20]Training loss: 3.362583875656128\n",
      "Epoch 0:  77%|███████▋  | 98/128 [00:00<00:00, 160.17it/s, v_num=20]Training loss: 3.33793044090271\n",
      "Epoch 0:  77%|███████▋  | 99/128 [00:00<00:00, 160.07it/s, v_num=20]Training loss: 3.2793033123016357\n",
      "Epoch 0:  78%|███████▊  | 100/128 [00:00<00:00, 159.96it/s, v_num=20]Training loss: 3.317559242248535\n",
      "Epoch 0:  79%|███████▉  | 101/128 [00:00<00:00, 159.87it/s, v_num=20]Training loss: 3.2811992168426514\n",
      "Epoch 0:  80%|███████▉  | 102/128 [00:00<00:00, 159.77it/s, v_num=20]Training loss: 3.2794525623321533\n",
      "Epoch 0:  80%|████████  | 103/128 [00:00<00:00, 159.67it/s, v_num=20]Training loss: 3.082979202270508\n",
      "Epoch 0:  81%|████████▏ | 104/128 [00:00<00:00, 159.72it/s, v_num=20]Training loss: 3.2789759635925293\n",
      "Epoch 0:  82%|████████▏ | 105/128 [00:00<00:00, 159.76it/s, v_num=20]Training loss: 3.0886125564575195\n",
      "Epoch 0:  83%|████████▎ | 106/128 [00:00<00:00, 159.67it/s, v_num=20]Training loss: 3.289719820022583\n",
      "Epoch 0:  84%|████████▎ | 107/128 [00:00<00:00, 159.72it/s, v_num=20]Training loss: 3.230252265930176\n",
      "Epoch 0:  84%|████████▍ | 108/128 [00:00<00:00, 159.58it/s, v_num=20]Training loss: 3.2338099479675293\n",
      "Epoch 0:  85%|████████▌ | 109/128 [00:00<00:00, 159.53it/s, v_num=20]Training loss: 3.128756523132324\n",
      "Epoch 0:  86%|████████▌ | 110/128 [00:00<00:00, 159.42it/s, v_num=20]Training loss: 3.402207374572754\n",
      "Epoch 0:  87%|████████▋ | 111/128 [00:00<00:00, 159.35it/s, v_num=20]Training loss: 3.277531623840332\n",
      "Epoch 0:  88%|████████▊ | 112/128 [00:00<00:00, 159.27it/s, v_num=20]Training loss: 3.3370752334594727\n",
      "Epoch 0:  88%|████████▊ | 113/128 [00:00<00:00, 159.19it/s, v_num=20]Training loss: 3.279832601547241\n",
      "Epoch 0:  89%|████████▉ | 114/128 [00:00<00:00, 159.24it/s, v_num=20]Training loss: 3.3604507446289062\n",
      "Epoch 0:  90%|████████▉ | 115/128 [00:00<00:00, 159.30it/s, v_num=20]Training loss: 3.2833690643310547\n",
      "Epoch 0:  91%|█████████ | 116/128 [00:00<00:00, 159.23it/s, v_num=20]Training loss: 3.288461685180664\n",
      "Epoch 0:  91%|█████████▏| 117/128 [00:00<00:00, 159.16it/s, v_num=20]Training loss: 3.397357702255249\n",
      "Epoch 0:  92%|█████████▏| 118/128 [00:00<00:00, 159.03it/s, v_num=20]Training loss: 3.2243921756744385\n",
      "Epoch 0:  93%|█████████▎| 119/128 [00:00<00:00, 158.99it/s, v_num=20]Training loss: 3.3193109035491943\n",
      "Epoch 0:  94%|█████████▍| 120/128 [00:00<00:00, 158.91it/s, v_num=20]Training loss: 3.150832414627075\n",
      "Epoch 0:  95%|█████████▍| 121/128 [00:00<00:00, 158.80it/s, v_num=20]Training loss: 3.3092551231384277\n",
      "Epoch 0:  95%|█████████▌| 122/128 [00:00<00:00, 158.76it/s, v_num=20]Training loss: 3.26678466796875\n",
      "Epoch 0:  96%|█████████▌| 123/128 [00:00<00:00, 158.68it/s, v_num=20]Training loss: 3.3240089416503906\n",
      "Epoch 0:  97%|█████████▋| 124/128 [00:00<00:00, 158.57it/s, v_num=20]Training loss: 3.2700042724609375\n",
      "Epoch 0:  98%|█████████▊| 125/128 [00:00<00:00, 158.53it/s, v_num=20]Training loss: 3.2567567825317383\n",
      "Epoch 0:  98%|█████████▊| 126/128 [00:00<00:00, 158.48it/s, v_num=20]Training loss: 3.35469651222229\n",
      "Epoch 0:  99%|█████████▉| 127/128 [00:00<00:00, 158.41it/s, v_num=20]Training loss: 3.3281571865081787\n",
      "Epoch 0: 100%|██████████| 128/128 [00:00<00:00, 158.33it/s, v_num=20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[AValidation loss: 3.270726203918457\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 143.11it/s]\u001b[AValidation loss: 3.2547619342803955\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 174.09it/s]\u001b[AValidation loss: 3.389413833618164\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 181.16it/s]\u001b[AValidation loss: 3.359450340270996\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 189.71it/s]\u001b[AValidation loss: 3.2584776878356934\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 191.30it/s]\u001b[AValidation loss: 3.347144603729248\n",
      "\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 195.79it/s]\u001b[AValidation loss: 3.266162633895874\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 199.16it/s]\u001b[AValidation loss: 3.303565740585327\n",
      "\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 198.68it/s]\u001b[AValidation loss: 3.363039493560791\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 201.18it/s]\u001b[AValidation loss: 3.300957679748535\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 202.69it/s]\u001b[AValidation loss: 3.2828097343444824\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 204.02it/s]\u001b[AValidation loss: 3.2594106197357178\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 203.16it/s]\u001b[AValidation loss: 3.259680986404419\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 200.77it/s]\u001b[AValidation loss: 3.351411819458008\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 202.63it/s]\u001b[AValidation loss: 3.3367435932159424\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 201.27it/s]\u001b[AValidation loss: 3.3296241760253906\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 202.31it/s]\u001b[AValidation loss: 3.236259937286377\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 200.71it/s]\u001b[AValidation loss: 3.27605938911438\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 202.37it/s]\u001b[AValidation loss: 3.2928965091705322\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 203.18it/s]\u001b[AValidation loss: 3.2581191062927246\n",
      "\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 202.91it/s]\u001b[AValidation loss: 3.3092381954193115\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 203.58it/s]\u001b[AValidation loss: 3.348024606704712\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 203.67it/s]\u001b[AValidation loss: 3.2178709506988525\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 202.67it/s]\u001b[AValidation loss: 3.298950672149658\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 203.44it/s]\u001b[AValidation loss: 3.273516893386841\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 202.63it/s]\u001b[AValidation loss: 3.3615643978118896\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 203.82it/s]\u001b[AValidation loss: 3.311911106109619\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 204.39it/s]\u001b[AValidation loss: 3.289728879928589\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 204.02it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s, v_num=20]               \u001b[ATraining loss: 3.401444435119629\n",
      "Epoch 1:   1%|          | 1/128 [00:00<00:01, 121.43it/s, v_num=20]Training loss: 3.3295512199401855\n",
      "Epoch 1:   2%|▏         | 2/128 [00:00<00:00, 133.79it/s, v_num=20]Training loss: 3.3385322093963623\n",
      "Epoch 1:   2%|▏         | 3/128 [00:00<00:00, 136.58it/s, v_num=20]Training loss: 3.2958381175994873\n",
      "Epoch 1:   3%|▎         | 4/128 [00:00<00:00, 138.43it/s, v_num=20]Training loss: 3.318481206893921\n",
      "Epoch 1:   4%|▍         | 5/128 [00:00<00:00, 139.42it/s, v_num=20]Training loss: 3.322601079940796\n",
      "Epoch 1:   5%|▍         | 6/128 [00:00<00:00, 141.51it/s, v_num=20]Training loss: 3.3148069381713867\n",
      "Epoch 1:   5%|▌         | 7/128 [00:00<00:00, 142.41it/s, v_num=20]Training loss: 3.262951374053955\n",
      "Epoch 1:   6%|▋         | 8/128 [00:00<00:00, 143.03it/s, v_num=20]Training loss: 3.2488410472869873\n",
      "Epoch 1:   7%|▋         | 9/128 [00:00<00:00, 142.76it/s, v_num=20]Training loss: 3.1875863075256348\n",
      "Epoch 1:   8%|▊         | 10/128 [00:00<00:00, 142.04it/s, v_num=20]Training loss: 3.3122951984405518\n",
      "Epoch 1:   9%|▊         | 11/128 [00:00<00:00, 142.64it/s, v_num=20]Training loss: 3.325472593307495\n",
      "Epoch 1:   9%|▉         | 12/128 [00:00<00:00, 142.70it/s, v_num=20]Training loss: 3.3190882205963135\n",
      "Epoch 1:  10%|█         | 13/128 [00:00<00:00, 143.55it/s, v_num=20]Training loss: 3.2225558757781982\n",
      "Epoch 1:  11%|█         | 14/128 [00:00<00:00, 143.52it/s, v_num=20]Training loss: 3.2096543312072754\n",
      "Epoch 1:  12%|█▏        | 15/128 [00:00<00:00, 144.19it/s, v_num=20]Training loss: 3.2486894130706787\n",
      "Epoch 1:  12%|█▎        | 16/128 [00:00<00:00, 144.07it/s, v_num=20]Training loss: 3.3153162002563477\n",
      "Epoch 1:  13%|█▎        | 17/128 [00:00<00:00, 144.59it/s, v_num=20]Training loss: 3.386291265487671\n",
      "Epoch 1:  14%|█▍        | 18/128 [00:00<00:00, 144.88it/s, v_num=20]Training loss: 3.2719712257385254\n",
      "Epoch 1:  15%|█▍        | 19/128 [00:00<00:00, 145.11it/s, v_num=20]Training loss: 3.3920059204101562\n",
      "Epoch 1:  16%|█▌        | 20/128 [00:00<00:00, 145.05it/s, v_num=20]Training loss: 3.3345203399658203\n",
      "Epoch 1:  16%|█▋        | 21/128 [00:00<00:00, 145.41it/s, v_num=20]Training loss: 3.352445602416992\n",
      "Epoch 1:  17%|█▋        | 22/128 [00:00<00:00, 145.57it/s, v_num=20]Training loss: 3.2883641719818115\n",
      "Epoch 1:  18%|█▊        | 23/128 [00:00<00:00, 145.73it/s, v_num=20]Training loss: 3.2310853004455566\n",
      "Epoch 1:  19%|█▉        | 24/128 [00:00<00:00, 145.78it/s, v_num=20]Training loss: 3.2830374240875244\n",
      "Epoch 1:  20%|█▉        | 25/128 [00:00<00:00, 145.40it/s, v_num=20]Training loss: 3.2091503143310547\n",
      "Epoch 1:  20%|██        | 26/128 [00:00<00:00, 145.29it/s, v_num=20]Training loss: 3.2786190509796143\n",
      "Epoch 1:  21%|██        | 27/128 [00:00<00:00, 145.24it/s, v_num=20]Training loss: 3.3430707454681396\n",
      "Epoch 1:  22%|██▏       | 28/128 [00:00<00:00, 145.39it/s, v_num=20]Training loss: 3.2930908203125\n",
      "Epoch 1:  23%|██▎       | 29/128 [00:00<00:00, 146.03it/s, v_num=20]Training loss: 3.3632121086120605\n",
      "Epoch 1:  23%|██▎       | 30/128 [00:00<00:00, 146.60it/s, v_num=20]Training loss: 3.39784836769104\n",
      "Epoch 1:  24%|██▍       | 31/128 [00:00<00:00, 146.74it/s, v_num=20]Training loss: 3.3290812969207764\n",
      "Epoch 1:  25%|██▌       | 32/128 [00:00<00:00, 146.84it/s, v_num=20]Training loss: 3.3817672729492188\n",
      "Epoch 1:  26%|██▌       | 33/128 [00:00<00:00, 146.74it/s, v_num=20]Training loss: 3.224762201309204\n",
      "Epoch 1:  27%|██▋       | 34/128 [00:00<00:00, 146.90it/s, v_num=20]Training loss: 3.19478440284729\n",
      "Epoch 1:  27%|██▋       | 35/128 [00:00<00:00, 146.94it/s, v_num=20]Training loss: 3.3335089683532715\n",
      "Epoch 1:  28%|██▊       | 36/128 [00:00<00:00, 146.99it/s, v_num=20]Training loss: 3.227858304977417\n",
      "Epoch 1:  29%|██▉       | 37/128 [00:00<00:00, 146.93it/s, v_num=20]Training loss: 3.366349697113037\n",
      "Epoch 1:  30%|██▉       | 38/128 [00:00<00:00, 147.02it/s, v_num=20]Training loss: 3.3106019496917725\n",
      "Epoch 1:  30%|███       | 39/128 [00:00<00:00, 147.06it/s, v_num=20]Training loss: 3.2981419563293457\n",
      "Epoch 1:  31%|███▏      | 40/128 [00:00<00:00, 147.11it/s, v_num=20]Training loss: 3.331016778945923\n",
      "Epoch 1:  32%|███▏      | 41/128 [00:00<00:00, 147.04it/s, v_num=20]Training loss: 3.2450475692749023\n",
      "Epoch 1:  33%|███▎      | 42/128 [00:00<00:00, 147.15it/s, v_num=20]Training loss: 3.1595373153686523\n",
      "Epoch 1:  34%|███▎      | 43/128 [00:00<00:00, 147.27it/s, v_num=20]Training loss: 3.2381253242492676\n",
      "Epoch 1:  34%|███▍      | 44/128 [00:00<00:00, 147.67it/s, v_num=20]Training loss: 3.2891759872436523\n",
      "Epoch 1:  35%|███▌      | 45/128 [00:00<00:00, 147.90it/s, v_num=20]Training loss: 3.0753333568573\n",
      "Epoch 1:  36%|███▌      | 46/128 [00:00<00:00, 148.17it/s, v_num=20]Training loss: 3.3442928791046143\n",
      "Epoch 1:  37%|███▋      | 47/128 [00:00<00:00, 148.21it/s, v_num=20]Training loss: 3.239999532699585\n",
      "Epoch 1:  38%|███▊      | 48/128 [00:00<00:00, 148.25it/s, v_num=20]Training loss: 3.326850414276123\n",
      "Epoch 1:  38%|███▊      | 49/128 [00:00<00:00, 148.29it/s, v_num=20]Training loss: 3.3195974826812744\n",
      "Epoch 1:  39%|███▉      | 50/128 [00:00<00:00, 148.32it/s, v_num=20]Training loss: 3.32452392578125\n",
      "Epoch 1:  40%|███▉      | 51/128 [00:00<00:00, 148.31it/s, v_num=20]Training loss: 3.2974352836608887\n",
      "Epoch 1:  41%|████      | 52/128 [00:00<00:00, 148.35it/s, v_num=20]Training loss: 3.3974668979644775\n",
      "Epoch 1:  41%|████▏     | 53/128 [00:00<00:00, 148.37it/s, v_num=20]Training loss: 3.432607412338257\n",
      "Epoch 1:  42%|████▏     | 54/128 [00:00<00:00, 148.39it/s, v_num=20]Training loss: 3.3311944007873535\n",
      "Epoch 1:  43%|████▎     | 55/128 [00:00<00:00, 148.39it/s, v_num=20]Training loss: 3.3371353149414062\n",
      "Epoch 1:  44%|████▍     | 56/128 [00:00<00:00, 148.39it/s, v_num=20]Training loss: 3.4299376010894775\n",
      "Epoch 1:  45%|████▍     | 57/128 [00:00<00:00, 148.46it/s, v_num=20]Training loss: 3.285229206085205\n",
      "Epoch 1:  45%|████▌     | 58/128 [00:00<00:00, 148.49it/s, v_num=20]Training loss: 3.267918825149536\n",
      "Epoch 1:  46%|████▌     | 59/128 [00:00<00:00, 148.50it/s, v_num=20]Training loss: 3.4768054485321045\n",
      "Epoch 1:  47%|████▋     | 60/128 [00:00<00:00, 148.56it/s, v_num=20]Training loss: 3.340834140777588\n",
      "Epoch 1:  48%|████▊     | 61/128 [00:00<00:00, 148.59it/s, v_num=20]Training loss: 3.4333081245422363\n",
      "Epoch 1:  48%|████▊     | 62/128 [00:00<00:00, 148.61it/s, v_num=20]Training loss: 3.2067134380340576\n",
      "Epoch 1:  49%|████▉     | 63/128 [00:00<00:00, 148.63it/s, v_num=20]Training loss: 3.315155267715454\n",
      "Epoch 1:  50%|█████     | 64/128 [00:00<00:00, 148.63it/s, v_num=20]Training loss: 3.234375238418579\n",
      "Epoch 1:  51%|█████     | 65/128 [00:00<00:00, 148.64it/s, v_num=20]Training loss: 3.3395862579345703\n",
      "Epoch 1:  52%|█████▏    | 66/128 [00:00<00:00, 148.65it/s, v_num=20]Training loss: 3.1826581954956055\n",
      "Epoch 1:  52%|█████▏    | 67/128 [00:00<00:00, 148.59it/s, v_num=20]Training loss: 3.311326503753662\n",
      "Epoch 1:  53%|█████▎    | 68/128 [00:00<00:00, 148.59it/s, v_num=20]Training loss: 3.180640459060669\n",
      "Epoch 1:  54%|█████▍    | 69/128 [00:00<00:00, 148.60it/s, v_num=20]Training loss: 3.346907138824463\n",
      "Epoch 1:  55%|█████▍    | 70/128 [00:00<00:00, 148.62it/s, v_num=20]Training loss: 3.396314859390259\n",
      "Epoch 1:  55%|█████▌    | 71/128 [00:00<00:00, 148.63it/s, v_num=20]Training loss: 3.2883026599884033\n",
      "Epoch 1:  56%|█████▋    | 72/128 [00:00<00:00, 148.67it/s, v_num=20]Training loss: 3.188873767852783\n",
      "Epoch 1:  57%|█████▋    | 73/128 [00:00<00:00, 148.70it/s, v_num=20]Training loss: 3.224151134490967\n",
      "Epoch 1:  58%|█████▊    | 74/128 [00:00<00:00, 148.74it/s, v_num=20]Training loss: 3.3467202186584473\n",
      "Epoch 1:  59%|█████▊    | 75/128 [00:00<00:00, 148.78it/s, v_num=20]Training loss: 3.191638708114624\n",
      "Epoch 1:  59%|█████▉    | 76/128 [00:00<00:00, 148.80it/s, v_num=20]Training loss: 3.192840576171875\n",
      "Epoch 1:  60%|██████    | 77/128 [00:00<00:00, 148.73it/s, v_num=20]Training loss: 3.238534927368164\n",
      "Epoch 1:  61%|██████    | 78/128 [00:00<00:00, 148.98it/s, v_num=20]Training loss: 3.364570140838623\n",
      "Epoch 1:  62%|██████▏   | 79/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.259162187576294\n",
      "Epoch 1:  62%|██████▎   | 80/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.19917368888855\n",
      "Epoch 1:  63%|██████▎   | 81/128 [00:00<00:00, 148.96it/s, v_num=20]Training loss: 3.4323151111602783\n",
      "Epoch 1:  64%|██████▍   | 82/128 [00:00<00:00, 148.97it/s, v_num=20]Training loss: 3.3459291458129883\n",
      "Epoch 1:  65%|██████▍   | 83/128 [00:00<00:00, 148.95it/s, v_num=20]Training loss: 2.9128665924072266\n",
      "Epoch 1:  66%|██████▌   | 84/128 [00:00<00:00, 148.96it/s, v_num=20]Training loss: 3.0309293270111084\n",
      "Epoch 1:  66%|██████▋   | 85/128 [00:00<00:00, 148.90it/s, v_num=20]Training loss: 3.3295745849609375\n",
      "Epoch 1:  67%|██████▋   | 86/128 [00:00<00:00, 148.74it/s, v_num=20]Training loss: 3.336810350418091\n",
      "Epoch 1:  68%|██████▊   | 87/128 [00:00<00:00, 148.99it/s, v_num=20]Training loss: 3.365992546081543\n",
      "Epoch 1:  69%|██████▉   | 88/128 [00:00<00:00, 148.88it/s, v_num=20]Training loss: 3.3864409923553467\n",
      "Epoch 1:  70%|██████▉   | 89/128 [00:00<00:00, 148.78it/s, v_num=20]Training loss: 3.282451868057251\n",
      "Epoch 1:  70%|███████   | 90/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.292461633682251\n",
      "Epoch 1:  71%|███████   | 91/128 [00:00<00:00, 149.03it/s, v_num=20]Training loss: 3.3096659183502197\n",
      "Epoch 1:  72%|███████▏  | 92/128 [00:00<00:00, 149.04it/s, v_num=20]Training loss: 3.2124311923980713\n",
      "Epoch 1:  73%|███████▎  | 93/128 [00:00<00:00, 149.04it/s, v_num=20]Training loss: 3.3124325275421143\n",
      "Epoch 1:  73%|███████▎  | 94/128 [00:00<00:00, 149.04it/s, v_num=20]Training loss: 3.285743474960327\n",
      "Epoch 1:  74%|███████▍  | 95/128 [00:00<00:00, 149.05it/s, v_num=20]Training loss: 3.3655014038085938\n",
      "Epoch 1:  75%|███████▌  | 96/128 [00:00<00:00, 149.06it/s, v_num=20]Training loss: 3.3482766151428223\n",
      "Epoch 1:  76%|███████▌  | 97/128 [00:00<00:00, 149.00it/s, v_num=20]Training loss: 3.314807176589966\n",
      "Epoch 1:  77%|███████▋  | 98/128 [00:00<00:00, 149.00it/s, v_num=20]Training loss: 3.1044859886169434\n",
      "Epoch 1:  77%|███████▋  | 99/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.2463219165802\n",
      "Epoch 1:  78%|███████▊  | 100/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.2357726097106934\n",
      "Epoch 1:  79%|███████▉  | 101/128 [00:00<00:00, 148.92it/s, v_num=20]Training loss: 3.1399753093719482\n",
      "Epoch 1:  80%|███████▉  | 102/128 [00:00<00:00, 149.11it/s, v_num=20]Training loss: 3.2698445320129395\n",
      "Epoch 1:  80%|████████  | 103/128 [00:00<00:00, 149.08it/s, v_num=20]Training loss: 3.349889039993286\n",
      "Epoch 1:  81%|████████▏ | 104/128 [00:00<00:00, 149.08it/s, v_num=20]Training loss: 3.3173980712890625\n",
      "Epoch 1:  82%|████████▏ | 105/128 [00:00<00:00, 149.10it/s, v_num=20]Training loss: 3.1184804439544678\n",
      "Epoch 1:  83%|████████▎ | 106/128 [00:00<00:00, 149.05it/s, v_num=20]Training loss: 3.2747347354888916\n",
      "Epoch 1:  84%|████████▎ | 107/128 [00:00<00:00, 149.01it/s, v_num=20]Training loss: 3.3507840633392334\n",
      "Epoch 1:  84%|████████▍ | 108/128 [00:00<00:00, 149.18it/s, v_num=20]Training loss: 3.3695905208587646\n",
      "Epoch 1:  85%|████████▌ | 109/128 [00:00<00:00, 149.26it/s, v_num=20]Training loss: 3.2711119651794434\n",
      "Epoch 1:  86%|████████▌ | 110/128 [00:00<00:00, 149.26it/s, v_num=20]Training loss: 3.077228546142578\n",
      "Epoch 1:  87%|████████▋ | 111/128 [00:00<00:00, 149.21it/s, v_num=20]Training loss: 3.355698585510254\n",
      "Epoch 1:  88%|████████▊ | 112/128 [00:00<00:00, 149.17it/s, v_num=20]Training loss: 3.288545846939087\n",
      "Epoch 1:  88%|████████▊ | 113/128 [00:00<00:00, 149.24it/s, v_num=20]Training loss: 3.103754997253418\n",
      "Epoch 1:  89%|████████▉ | 114/128 [00:00<00:00, 149.23it/s, v_num=20]Training loss: 3.2634437084198\n",
      "Epoch 1:  90%|████████▉ | 115/128 [00:00<00:00, 149.25it/s, v_num=20]Training loss: 3.3527536392211914\n",
      "Epoch 1:  91%|█████████ | 116/128 [00:00<00:00, 149.26it/s, v_num=20]Training loss: 3.3401012420654297\n",
      "Epoch 1:  91%|█████████▏| 117/128 [00:00<00:00, 149.25it/s, v_num=20]Training loss: 3.3077027797698975\n",
      "Epoch 1:  92%|█████████▏| 118/128 [00:00<00:00, 149.25it/s, v_num=20]Training loss: 3.320603847503662\n",
      "Epoch 1:  93%|█████████▎| 119/128 [00:00<00:00, 149.26it/s, v_num=20]Training loss: 3.343427896499634\n",
      "Epoch 1:  94%|█████████▍| 120/128 [00:00<00:00, 149.26it/s, v_num=20]Training loss: 3.2495667934417725\n",
      "Epoch 1:  95%|█████████▍| 121/128 [00:00<00:00, 149.25it/s, v_num=20]Training loss: 3.124757766723633\n",
      "Epoch 1:  95%|█████████▌| 122/128 [00:00<00:00, 149.27it/s, v_num=20]Training loss: 3.329136610031128\n",
      "Epoch 1:  96%|█████████▌| 123/128 [00:00<00:00, 149.27it/s, v_num=20]Training loss: 3.1628894805908203\n",
      "Epoch 1:  97%|█████████▋| 124/128 [00:00<00:00, 149.23it/s, v_num=20]Training loss: 3.2713708877563477\n",
      "Epoch 1:  98%|█████████▊| 125/128 [00:00<00:00, 149.30it/s, v_num=20]Training loss: 3.2907071113586426\n",
      "Epoch 1:  98%|█████████▊| 126/128 [00:00<00:00, 149.33it/s, v_num=20]Training loss: 3.2445125579833984\n",
      "Epoch 1:  99%|█████████▉| 127/128 [00:00<00:00, 149.32it/s, v_num=20]Training loss: 3.231398582458496\n",
      "Epoch 1: 100%|██████████| 128/128 [00:00<00:00, 149.32it/s, v_num=20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[AValidation loss: 3.2697975635528564\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 172.72it/s]\u001b[AValidation loss: 3.256234645843506\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 174.18it/s]\u001b[AValidation loss: 3.3842861652374268\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 187.10it/s]\u001b[AValidation loss: 3.358358383178711\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 194.08it/s]\u001b[AValidation loss: 3.26061749458313\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 196.34it/s]\u001b[AValidation loss: 3.3412833213806152\n",
      "\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 193.88it/s]\u001b[AValidation loss: 3.2691361904144287\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 197.34it/s]\u001b[AValidation loss: 3.291271209716797\n",
      "\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 199.76it/s]\u001b[AValidation loss: 3.3749046325683594\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 202.51it/s]\u001b[AValidation loss: 3.309445858001709\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 202.83it/s]\u001b[AValidation loss: 3.3015851974487305\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 204.16it/s]\u001b[AValidation loss: 3.2657713890075684\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 205.57it/s]\u001b[AValidation loss: 3.2640340328216553\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 204.45it/s]\u001b[AValidation loss: 3.3493268489837646\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 205.51it/s]\u001b[AValidation loss: 3.3353564739227295\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 205.34it/s]\u001b[AValidation loss: 3.3338520526885986\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 206.81it/s]\u001b[AValidation loss: 3.2427358627319336\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 205.21it/s]\u001b[AValidation loss: 3.2788708209991455\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 205.36it/s]\u001b[AValidation loss: 3.295175075531006\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 205.35it/s]\u001b[AValidation loss: 3.257497549057007\n",
      "\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 205.35it/s]\u001b[AValidation loss: 3.3111073970794678\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 202.94it/s]\u001b[AValidation loss: 3.340895891189575\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 204.04it/s]\u001b[AValidation loss: 3.2191295623779297\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 203.72it/s]\u001b[AValidation loss: 3.293020248413086\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 204.35it/s]\u001b[AValidation loss: 3.27247953414917\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 204.91it/s]\u001b[AValidation loss: 3.366058111190796\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 204.54it/s]\u001b[AValidation loss: 3.310606002807617\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 205.10it/s]\u001b[AValidation loss: 3.28528094291687\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 205.14it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s, v_num=20]               \u001b[ATraining loss: 3.2730627059936523\n",
      "Epoch 2:   1%|          | 1/128 [00:00<00:01, 122.19it/s, v_num=20]Training loss: 3.2668678760528564\n",
      "Epoch 2:   2%|▏         | 2/128 [00:00<00:00, 134.29it/s, v_num=20]Training loss: 3.2934718132019043\n",
      "Epoch 2:   2%|▏         | 3/128 [00:00<00:00, 139.86it/s, v_num=20]Training loss: 3.252019166946411\n",
      "Epoch 2:   3%|▎         | 4/128 [00:00<00:00, 142.33it/s, v_num=20]Training loss: 3.3236005306243896\n",
      "Epoch 2:   4%|▍         | 5/128 [00:00<00:00, 143.87it/s, v_num=20]Training loss: 3.2610890865325928\n",
      "Epoch 2:   5%|▍         | 6/128 [00:00<00:00, 143.82it/s, v_num=20]Training loss: 3.0512545108795166\n",
      "Epoch 2:   5%|▌         | 7/128 [00:00<00:00, 144.69it/s, v_num=20]Training loss: 3.2886526584625244\n",
      "Epoch 2:   6%|▋         | 8/128 [00:00<00:00, 145.01it/s, v_num=20]Training loss: 3.305985450744629\n",
      "Epoch 2:   7%|▋         | 9/128 [00:00<00:00, 144.75it/s, v_num=20]Training loss: 3.253720760345459\n",
      "Epoch 2:   8%|▊         | 10/128 [00:00<00:00, 145.50it/s, v_num=20]Training loss: 3.342399835586548\n",
      "Epoch 2:   9%|▊         | 11/128 [00:00<00:00, 145.23it/s, v_num=20]Training loss: 3.234330892562866\n",
      "Epoch 2:   9%|▉         | 12/128 [00:00<00:00, 145.44it/s, v_num=20]Training loss: 3.2467517852783203\n",
      "Epoch 2:  10%|█         | 13/128 [00:00<00:00, 145.69it/s, v_num=20]Training loss: 3.1753668785095215\n",
      "Epoch 2:  11%|█         | 14/128 [00:00<00:00, 145.90it/s, v_num=20]Training loss: 2.733030319213867\n",
      "Epoch 2:  12%|█▏        | 15/128 [00:00<00:00, 146.19it/s, v_num=20]Training loss: 3.321704387664795\n",
      "Epoch 2:  12%|█▎        | 16/128 [00:00<00:00, 145.98it/s, v_num=20]Training loss: 3.336719274520874\n",
      "Epoch 2:  13%|█▎        | 17/128 [00:00<00:00, 146.40it/s, v_num=20]Training loss: 3.25810170173645\n",
      "Epoch 2:  14%|█▍        | 18/128 [00:00<00:00, 146.21it/s, v_num=20]Training loss: 3.1016476154327393\n",
      "Epoch 2:  15%|█▍        | 19/128 [00:00<00:00, 146.56it/s, v_num=20]Training loss: 2.928839921951294\n",
      "Epoch 2:  16%|█▌        | 20/128 [00:00<00:00, 146.73it/s, v_num=20]Training loss: 3.3244991302490234\n",
      "Epoch 2:  16%|█▋        | 21/128 [00:00<00:00, 146.74it/s, v_num=20]Training loss: 3.3168368339538574\n",
      "Epoch 2:  17%|█▋        | 22/128 [00:00<00:00, 146.90it/s, v_num=20]Training loss: 3.317469358444214\n",
      "Epoch 2:  18%|█▊        | 23/128 [00:00<00:00, 146.95it/s, v_num=20]Training loss: 3.3048388957977295\n",
      "Epoch 2:  19%|█▉        | 24/128 [00:00<00:00, 146.96it/s, v_num=20]Training loss: 3.139404773712158\n",
      "Epoch 2:  20%|█▉        | 25/128 [00:00<00:00, 146.96it/s, v_num=20]Training loss: 3.205669641494751\n",
      "Epoch 2:  20%|██        | 26/128 [00:00<00:00, 146.82it/s, v_num=20]Training loss: 3.1729910373687744\n",
      "Epoch 2:  21%|██        | 27/128 [00:00<00:00, 146.98it/s, v_num=20]Training loss: 3.284912109375\n",
      "Epoch 2:  22%|██▏       | 28/128 [00:00<00:00, 147.07it/s, v_num=20]Training loss: 3.2987070083618164\n",
      "Epoch 2:  23%|██▎       | 29/128 [00:00<00:00, 147.08it/s, v_num=20]Training loss: 3.3008272647857666\n",
      "Epoch 2:  23%|██▎       | 30/128 [00:00<00:00, 147.03it/s, v_num=20]Training loss: 3.3013417720794678\n",
      "Epoch 2:  24%|██▍       | 31/128 [00:00<00:00, 147.12it/s, v_num=20]Training loss: 3.142928123474121\n",
      "Epoch 2:  25%|██▌       | 32/128 [00:00<00:00, 147.02it/s, v_num=20]Training loss: 3.252619981765747\n",
      "Epoch 2:  26%|██▌       | 33/128 [00:00<00:00, 147.16it/s, v_num=20]Training loss: 2.9176526069641113\n",
      "Epoch 2:  27%|██▋       | 34/128 [00:00<00:00, 147.24it/s, v_num=20]Training loss: 3.3441684246063232\n",
      "Epoch 2:  27%|██▋       | 35/128 [00:00<00:00, 147.31it/s, v_num=20]Training loss: 3.25726056098938\n",
      "Epoch 2:  28%|██▊       | 36/128 [00:00<00:00, 147.38it/s, v_num=20]Training loss: 3.380594253540039\n",
      "Epoch 2:  29%|██▉       | 37/128 [00:00<00:00, 147.43it/s, v_num=20]Training loss: 2.9944937229156494\n",
      "Epoch 2:  30%|██▉       | 38/128 [00:00<00:00, 147.31it/s, v_num=20]Training loss: 3.252373456954956\n",
      "Epoch 2:  30%|███       | 39/128 [00:00<00:00, 147.37it/s, v_num=20]Training loss: 3.269258499145508\n",
      "Epoch 2:  31%|███▏      | 40/128 [00:00<00:00, 147.30it/s, v_num=20]Training loss: 3.3042263984680176\n",
      "Epoch 2:  32%|███▏      | 41/128 [00:00<00:00, 147.19it/s, v_num=20]Training loss: 3.182324171066284\n",
      "Epoch 2:  33%|███▎      | 42/128 [00:00<00:00, 147.29it/s, v_num=20]Training loss: 2.8871402740478516\n",
      "Epoch 2:  34%|███▎      | 43/128 [00:00<00:00, 147.33it/s, v_num=20]Training loss: 3.2335453033447266\n",
      "Epoch 2:  34%|███▍      | 44/128 [00:00<00:00, 147.37it/s, v_num=20]Training loss: 3.0990092754364014\n",
      "Epoch 2:  35%|███▌      | 45/128 [00:00<00:00, 147.27it/s, v_num=20]Training loss: 3.3566150665283203\n",
      "Epoch 2:  36%|███▌      | 46/128 [00:00<00:00, 147.35it/s, v_num=20]Training loss: 3.2287685871124268\n",
      "Epoch 2:  37%|███▋      | 47/128 [00:00<00:00, 147.39it/s, v_num=20]Training loss: 3.3719983100891113\n",
      "Epoch 2:  38%|███▊      | 48/128 [00:00<00:00, 147.43it/s, v_num=20]Training loss: 3.0846962928771973\n",
      "Epoch 2:  38%|███▊      | 49/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 3.0705251693725586\n",
      "Epoch 2:  39%|███▉      | 50/128 [00:00<00:00, 147.52it/s, v_num=20]Training loss: 3.2797462940216064\n",
      "Epoch 2:  40%|███▉      | 51/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.239168643951416\n",
      "Epoch 2:  41%|████      | 52/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 3.390268087387085\n",
      "Epoch 2:  41%|████▏     | 53/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.138864755630493\n",
      "Epoch 2:  42%|████▏     | 54/128 [00:00<00:00, 147.61it/s, v_num=20]Training loss: 3.038257360458374\n",
      "Epoch 2:  43%|████▎     | 55/128 [00:00<00:00, 147.55it/s, v_num=20]Training loss: 2.9380269050598145\n",
      "Epoch 2:  44%|████▍     | 56/128 [00:00<00:00, 147.46it/s, v_num=20]Training loss: 3.2809486389160156\n",
      "Epoch 2:  45%|████▍     | 57/128 [00:00<00:00, 147.54it/s, v_num=20]Training loss: 3.2563583850860596\n",
      "Epoch 2:  45%|████▌     | 58/128 [00:00<00:00, 147.59it/s, v_num=20]Training loss: 3.1344547271728516\n",
      "Epoch 2:  46%|████▌     | 59/128 [00:00<00:00, 147.63it/s, v_num=20]Training loss: 3.3361616134643555\n",
      "Epoch 2:  47%|████▋     | 60/128 [00:00<00:00, 147.65it/s, v_num=20]Training loss: 3.311197519302368\n",
      "Epoch 2:  48%|████▊     | 61/128 [00:00<00:00, 147.69it/s, v_num=20]Training loss: 3.282451629638672\n",
      "Epoch 2:  48%|████▊     | 62/128 [00:00<00:00, 147.69it/s, v_num=20]Training loss: 2.9637765884399414\n",
      "Epoch 2:  49%|████▉     | 63/128 [00:00<00:00, 147.71it/s, v_num=20]Training loss: 3.16433048248291\n",
      "Epoch 2:  50%|█████     | 64/128 [00:00<00:00, 147.72it/s, v_num=20]Training loss: 2.9769961833953857\n",
      "Epoch 2:  51%|█████     | 65/128 [00:00<00:00, 147.82it/s, v_num=20]Training loss: 3.3059451580047607\n",
      "Epoch 2:  52%|█████▏    | 66/128 [00:00<00:00, 147.73it/s, v_num=20]Training loss: 3.1020572185516357\n",
      "Epoch 2:  52%|█████▏    | 67/128 [00:00<00:00, 147.78it/s, v_num=20]Training loss: 3.354326009750366\n",
      "Epoch 2:  53%|█████▎    | 68/128 [00:00<00:00, 147.78it/s, v_num=20]Training loss: 3.329833507537842\n",
      "Epoch 2:  54%|█████▍    | 69/128 [00:00<00:00, 147.88it/s, v_num=20]Training loss: 3.322571039199829\n",
      "Epoch 2:  55%|█████▍    | 70/128 [00:00<00:00, 147.82it/s, v_num=20]Training loss: 3.086183786392212\n",
      "Epoch 2:  55%|█████▌    | 71/128 [00:00<00:00, 147.79it/s, v_num=20]Training loss: 3.3493778705596924\n",
      "Epoch 2:  56%|█████▋    | 72/128 [00:00<00:00, 147.74it/s, v_num=20]Training loss: 3.3127260208129883\n",
      "Epoch 2:  57%|█████▋    | 73/128 [00:00<00:00, 147.79it/s, v_num=20]Training loss: 3.260456085205078\n",
      "Epoch 2:  58%|█████▊    | 74/128 [00:00<00:00, 147.83it/s, v_num=20]Training loss: 3.3353030681610107\n",
      "Epoch 2:  59%|█████▊    | 75/128 [00:00<00:00, 147.89it/s, v_num=20]Training loss: 3.324740171432495\n",
      "Epoch 2:  59%|█████▉    | 76/128 [00:00<00:00, 147.92it/s, v_num=20]Training loss: 3.3903005123138428\n",
      "Epoch 2:  60%|██████    | 77/128 [00:00<00:00, 147.95it/s, v_num=20]Training loss: 3.3137691020965576\n",
      "Epoch 2:  61%|██████    | 78/128 [00:00<00:00, 147.98it/s, v_num=20]Training loss: 3.370776414871216\n",
      "Epoch 2:  62%|██████▏   | 79/128 [00:00<00:00, 148.03it/s, v_num=20]Training loss: 3.307661771774292\n",
      "Epoch 2:  62%|██████▎   | 80/128 [00:00<00:00, 148.06it/s, v_num=20]Training loss: 2.6402828693389893\n",
      "Epoch 2:  63%|██████▎   | 81/128 [00:00<00:00, 148.02it/s, v_num=20]Training loss: 3.122361660003662\n",
      "Epoch 2:  64%|██████▍   | 82/128 [00:00<00:00, 148.08it/s, v_num=20]Training loss: 3.3784971237182617\n",
      "Epoch 2:  65%|██████▍   | 83/128 [00:00<00:00, 148.28it/s, v_num=20]Training loss: 3.1693801879882812\n",
      "Epoch 2:  66%|██████▌   | 84/128 [00:00<00:00, 148.24it/s, v_num=20]Training loss: 3.211902379989624\n",
      "Epoch 2:  66%|██████▋   | 85/128 [00:00<00:00, 148.26it/s, v_num=20]Training loss: 3.3427109718322754\n",
      "Epoch 2:  67%|██████▋   | 86/128 [00:00<00:00, 148.32it/s, v_num=20]Training loss: 3.278550863265991\n",
      "Epoch 2:  68%|██████▊   | 87/128 [00:00<00:00, 148.35it/s, v_num=20]Training loss: 3.317868947982788\n",
      "Epoch 2:  69%|██████▉   | 88/128 [00:00<00:00, 148.30it/s, v_num=20]Training loss: 3.306370496749878\n",
      "Epoch 2:  70%|██████▉   | 89/128 [00:00<00:00, 148.39it/s, v_num=20]Training loss: 3.260697841644287\n",
      "Epoch 2:  70%|███████   | 90/128 [00:00<00:00, 148.35it/s, v_num=20]Training loss: 3.219115972518921\n",
      "Epoch 2:  71%|███████   | 91/128 [00:00<00:00, 148.38it/s, v_num=20]Training loss: 2.862879991531372\n",
      "Epoch 2:  72%|███████▏  | 92/128 [00:00<00:00, 148.47it/s, v_num=20]Training loss: 3.31079363822937\n",
      "Epoch 2:  73%|███████▎  | 93/128 [00:00<00:00, 148.49it/s, v_num=20]Training loss: 3.1181721687316895\n",
      "Epoch 2:  73%|███████▎  | 94/128 [00:00<00:00, 148.50it/s, v_num=20]Training loss: 3.318593978881836\n",
      "Epoch 2:  74%|███████▍  | 95/128 [00:00<00:00, 148.51it/s, v_num=20]Training loss: 3.255495309829712\n",
      "Epoch 2:  75%|███████▌  | 96/128 [00:00<00:00, 148.52it/s, v_num=20]Training loss: 3.3137147426605225\n",
      "Epoch 2:  76%|███████▌  | 97/128 [00:00<00:00, 148.53it/s, v_num=20]Training loss: 3.297955274581909\n",
      "Epoch 2:  77%|███████▋  | 98/128 [00:00<00:00, 148.54it/s, v_num=20]Training loss: 3.315053701400757\n",
      "Epoch 2:  77%|███████▋  | 99/128 [00:00<00:00, 148.55it/s, v_num=20]Training loss: 3.275428533554077\n",
      "Epoch 2:  78%|███████▊  | 100/128 [00:00<00:00, 148.56it/s, v_num=20]Training loss: 3.2401444911956787\n",
      "Epoch 2:  79%|███████▉  | 101/128 [00:00<00:00, 148.57it/s, v_num=20]Training loss: 3.155827283859253\n",
      "Epoch 2:  80%|███████▉  | 102/128 [00:00<00:00, 148.57it/s, v_num=20]Training loss: 3.246919631958008\n",
      "Epoch 2:  80%|████████  | 103/128 [00:00<00:00, 148.57it/s, v_num=20]Training loss: 3.1794707775115967\n",
      "Epoch 2:  81%|████████▏ | 104/128 [00:00<00:00, 148.57it/s, v_num=20]Training loss: 3.363009214401245\n",
      "Epoch 2:  82%|████████▏ | 105/128 [00:00<00:00, 148.58it/s, v_num=20]Training loss: 2.735546827316284\n",
      "Epoch 2:  83%|████████▎ | 106/128 [00:00<00:00, 148.55it/s, v_num=20]Training loss: 3.286259412765503\n",
      "Epoch 2:  84%|████████▎ | 107/128 [00:00<00:00, 148.49it/s, v_num=20]Training loss: 3.295428514480591\n",
      "Epoch 2:  84%|████████▍ | 108/128 [00:00<00:00, 148.59it/s, v_num=20]Training loss: 3.1527135372161865\n",
      "Epoch 2:  85%|████████▌ | 109/128 [00:00<00:00, 148.61it/s, v_num=20]Training loss: 2.985574245452881\n",
      "Epoch 2:  86%|████████▌ | 110/128 [00:00<00:00, 148.65it/s, v_num=20]Training loss: 3.287771463394165\n",
      "Epoch 2:  87%|████████▋ | 111/128 [00:00<00:00, 148.66it/s, v_num=20]Training loss: 3.104715347290039\n",
      "Epoch 2:  88%|████████▊ | 112/128 [00:00<00:00, 148.62it/s, v_num=20]Training loss: 3.066999673843384\n",
      "Epoch 2:  88%|████████▊ | 113/128 [00:00<00:00, 148.62it/s, v_num=20]Training loss: 3.3193328380584717\n",
      "Epoch 2:  89%|████████▉ | 114/128 [00:00<00:00, 148.58it/s, v_num=20]Training loss: 3.3413665294647217\n",
      "Epoch 2:  90%|████████▉ | 115/128 [00:00<00:00, 148.58it/s, v_num=20]Training loss: 3.2800819873809814\n",
      "Epoch 2:  91%|█████████ | 116/128 [00:00<00:00, 148.59it/s, v_num=20]Training loss: 3.329981803894043\n",
      "Epoch 2:  91%|█████████▏| 117/128 [00:00<00:00, 148.67it/s, v_num=20]Training loss: 3.2987189292907715\n",
      "Epoch 2:  92%|█████████▏| 118/128 [00:00<00:00, 148.68it/s, v_num=20]Training loss: 3.227511405944824\n",
      "Epoch 2:  93%|█████████▎| 119/128 [00:00<00:00, 148.70it/s, v_num=20]Training loss: 3.1946840286254883\n",
      "Epoch 2:  94%|█████████▍| 120/128 [00:00<00:00, 148.66it/s, v_num=20]Training loss: 3.260551691055298\n",
      "Epoch 2:  95%|█████████▍| 121/128 [00:00<00:00, 148.69it/s, v_num=20]Training loss: 2.943903923034668\n",
      "Epoch 2:  95%|█████████▌| 122/128 [00:00<00:00, 148.72it/s, v_num=20]Training loss: 3.3117692470550537\n",
      "Epoch 2:  96%|█████████▌| 123/128 [00:00<00:00, 148.69it/s, v_num=20]Training loss: 3.05145525932312\n",
      "Epoch 2:  97%|█████████▋| 124/128 [00:00<00:00, 148.84it/s, v_num=20]Training loss: 3.3193459510803223\n",
      "Epoch 2:  98%|█████████▊| 125/128 [00:00<00:00, 148.86it/s, v_num=20]Training loss: 3.306351661682129\n",
      "Epoch 2:  98%|█████████▊| 126/128 [00:00<00:00, 148.83it/s, v_num=20]Training loss: 3.291991949081421\n",
      "Epoch 2:  99%|█████████▉| 127/128 [00:00<00:00, 148.90it/s, v_num=20]Training loss: 3.2702043056488037\n",
      "Epoch 2: 100%|██████████| 128/128 [00:00<00:00, 148.91it/s, v_num=20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[AValidation loss: 3.2691855430603027\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 162.21it/s]\u001b[AValidation loss: 3.2606027126312256\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 181.65it/s]\u001b[AValidation loss: 3.3810293674468994\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 192.66it/s]\u001b[AValidation loss: 3.356933355331421\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 199.57it/s]\u001b[AValidation loss: 3.2657735347747803\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 203.77it/s]\u001b[AValidation loss: 3.336899995803833\n",
      "\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 208.18it/s]\u001b[AValidation loss: 3.2765297889709473\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 207.85it/s]\u001b[AValidation loss: 3.2807059288024902\n",
      "\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 209.49it/s]\u001b[AValidation loss: 3.3886332511901855\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 206.62it/s]\u001b[AValidation loss: 3.3172316551208496\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 209.19it/s]\u001b[AValidation loss: 3.320082902908325\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 210.02it/s]\u001b[AValidation loss: 3.2699358463287354\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 206.90it/s]\u001b[AValidation loss: 3.2764370441436768\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 208.85it/s]\u001b[AValidation loss: 3.345381021499634\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 209.63it/s]\u001b[AValidation loss: 3.3373758792877197\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 210.25it/s]\u001b[AValidation loss: 3.3371803760528564\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 210.96it/s]\u001b[AValidation loss: 3.248359441757202\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 211.46it/s]\u001b[AValidation loss: 3.283388137817383\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 212.08it/s]\u001b[AValidation loss: 3.296093463897705\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 212.55it/s]\u001b[AValidation loss: 3.2569642066955566\n",
      "\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 212.76it/s]\u001b[AValidation loss: 3.315119981765747\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 212.40it/s]\u001b[AValidation loss: 3.332369327545166\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 211.18it/s]\u001b[AValidation loss: 3.22477126121521\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 212.10it/s]\u001b[AValidation loss: 3.2843017578125\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 212.44it/s]\u001b[AValidation loss: 3.274383544921875\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 212.60it/s]\u001b[AValidation loss: 3.3702030181884766\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 211.85it/s]\u001b[AValidation loss: 3.3118672370910645\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 211.52it/s]\u001b[AValidation loss: 3.2792460918426514\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 212.23it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s, v_num=20]               \u001b[ATraining loss: 3.313084125518799\n",
      "Epoch 3:   1%|          | 1/128 [00:00<00:01, 122.78it/s, v_num=20]Training loss: 3.1987814903259277\n",
      "Epoch 3:   2%|▏         | 2/128 [00:00<00:00, 129.13it/s, v_num=20]Training loss: 3.3213207721710205\n",
      "Epoch 3:   2%|▏         | 3/128 [00:00<00:00, 135.12it/s, v_num=20]Training loss: 3.165534496307373\n",
      "Epoch 3:   3%|▎         | 4/128 [00:00<00:00, 138.44it/s, v_num=20]Training loss: 3.2823448181152344\n",
      "Epoch 3:   4%|▍         | 5/128 [00:00<00:00, 139.57it/s, v_num=20]Training loss: 3.2697770595550537\n",
      "Epoch 3:   5%|▍         | 6/128 [00:00<00:00, 141.00it/s, v_num=20]Training loss: 3.3143253326416016\n",
      "Epoch 3:   5%|▌         | 7/128 [00:00<00:00, 142.46it/s, v_num=20]Training loss: 2.826530933380127\n",
      "Epoch 3:   6%|▋         | 8/128 [00:00<00:00, 143.26it/s, v_num=20]Training loss: 3.3059895038604736\n",
      "Epoch 3:   7%|▋         | 9/128 [00:00<00:00, 143.32it/s, v_num=20]Training loss: 3.332461357116699\n",
      "Epoch 3:   8%|▊         | 10/128 [00:00<00:00, 144.07it/s, v_num=20]Training loss: 3.302422523498535\n",
      "Epoch 3:   9%|▊         | 11/128 [00:00<00:00, 144.43it/s, v_num=20]Training loss: 3.333120584487915\n",
      "Epoch 3:   9%|▉         | 12/128 [00:00<00:00, 145.09it/s, v_num=20]Training loss: 3.2511634826660156\n",
      "Epoch 3:  10%|█         | 13/128 [00:00<00:00, 144.85it/s, v_num=20]Training loss: 3.2224369049072266\n",
      "Epoch 3:  11%|█         | 14/128 [00:00<00:00, 145.20it/s, v_num=20]Training loss: 3.2975401878356934\n",
      "Epoch 3:  12%|█▏        | 15/128 [00:00<00:00, 145.37it/s, v_num=20]Training loss: 3.1523478031158447\n",
      "Epoch 3:  12%|█▎        | 16/128 [00:00<00:00, 145.55it/s, v_num=20]Training loss: 3.2154314517974854\n",
      "Epoch 3:  13%|█▎        | 17/128 [00:00<00:00, 144.95it/s, v_num=20]Training loss: 3.317600965499878\n",
      "Epoch 3:  14%|█▍        | 18/128 [00:00<00:00, 144.74it/s, v_num=20]Training loss: 3.400437116622925\n",
      "Epoch 3:  15%|█▍        | 19/128 [00:00<00:00, 145.28it/s, v_num=20]Training loss: 2.974294662475586\n",
      "Epoch 3:  16%|█▌        | 20/128 [00:00<00:00, 145.03it/s, v_num=20]Training loss: 3.269587278366089\n",
      "Epoch 3:  16%|█▋        | 21/128 [00:00<00:00, 145.39it/s, v_num=20]Training loss: 3.0444865226745605\n",
      "Epoch 3:  17%|█▋        | 22/128 [00:00<00:00, 145.50it/s, v_num=20]Training loss: 3.4633073806762695\n",
      "Epoch 3:  18%|█▊        | 23/128 [00:00<00:00, 145.37it/s, v_num=20]Training loss: 2.8042216300964355\n",
      "Epoch 3:  19%|█▉        | 24/128 [00:00<00:00, 145.67it/s, v_num=20]Training loss: 3.355252265930176\n",
      "Epoch 3:  20%|█▉        | 25/128 [00:00<00:00, 145.86it/s, v_num=20]Training loss: 3.0758602619171143\n",
      "Epoch 3:  20%|██        | 26/128 [00:00<00:00, 145.91it/s, v_num=20]Training loss: 3.2867136001586914\n",
      "Epoch 3:  21%|██        | 27/128 [00:00<00:00, 146.04it/s, v_num=20]Training loss: 3.3236262798309326\n",
      "Epoch 3:  22%|██▏       | 28/128 [00:00<00:00, 146.40it/s, v_num=20]Training loss: 3.234199047088623\n",
      "Epoch 3:  23%|██▎       | 29/128 [00:00<00:00, 146.60it/s, v_num=20]Training loss: 3.3303840160369873\n",
      "Epoch 3:  23%|██▎       | 30/128 [00:00<00:00, 146.78it/s, v_num=20]Training loss: 3.033045530319214\n",
      "Epoch 3:  24%|██▍       | 31/128 [00:00<00:00, 146.85it/s, v_num=20]Training loss: 2.953925848007202\n",
      "Epoch 3:  25%|██▌       | 32/128 [00:00<00:00, 146.87it/s, v_num=20]Training loss: 3.3211171627044678\n",
      "Epoch 3:  26%|██▌       | 33/128 [00:00<00:00, 146.91it/s, v_num=20]Training loss: 3.2073543071746826\n",
      "Epoch 3:  27%|██▋       | 34/128 [00:00<00:00, 146.96it/s, v_num=20]Training loss: 3.2439684867858887\n",
      "Epoch 3:  27%|██▋       | 35/128 [00:00<00:00, 146.91it/s, v_num=20]Training loss: 3.313847541809082\n",
      "Epoch 3:  28%|██▊       | 36/128 [00:00<00:00, 147.02it/s, v_num=20]Training loss: 3.3597753047943115\n",
      "Epoch 3:  29%|██▉       | 37/128 [00:00<00:00, 147.06it/s, v_num=20]Training loss: 3.242047071456909\n",
      "Epoch 3:  30%|██▉       | 38/128 [00:00<00:00, 147.10it/s, v_num=20]Training loss: 3.239229440689087\n",
      "Epoch 3:  30%|███       | 39/128 [00:00<00:00, 147.12it/s, v_num=20]Training loss: 3.32008695602417\n",
      "Epoch 3:  31%|███▏      | 40/128 [00:00<00:00, 147.11it/s, v_num=20]Training loss: 3.35056209564209\n",
      "Epoch 3:  32%|███▏      | 41/128 [00:00<00:00, 146.99it/s, v_num=20]Training loss: 3.244337320327759\n",
      "Epoch 3:  33%|███▎      | 42/128 [00:00<00:00, 147.06it/s, v_num=20]Training loss: 3.2739391326904297\n",
      "Epoch 3:  34%|███▎      | 43/128 [00:00<00:00, 147.11it/s, v_num=20]Training loss: 3.216174840927124\n",
      "Epoch 3:  34%|███▍      | 44/128 [00:00<00:00, 147.17it/s, v_num=20]Training loss: 3.280083417892456\n",
      "Epoch 3:  35%|███▌      | 45/128 [00:00<00:00, 147.21it/s, v_num=20]Training loss: 3.3079938888549805\n",
      "Epoch 3:  36%|███▌      | 46/128 [00:00<00:00, 147.10it/s, v_num=20]Training loss: 3.250439167022705\n",
      "Epoch 3:  37%|███▋      | 47/128 [00:00<00:00, 147.13it/s, v_num=20]Training loss: 3.2924864292144775\n",
      "Epoch 3:  38%|███▊      | 48/128 [00:00<00:00, 147.18it/s, v_num=20]Training loss: 3.103133201599121\n",
      "Epoch 3:  38%|███▊      | 49/128 [00:00<00:00, 147.18it/s, v_num=20]Training loss: 3.286039113998413\n",
      "Epoch 3:  39%|███▉      | 50/128 [00:00<00:00, 147.18it/s, v_num=20]Training loss: 3.2307586669921875\n",
      "Epoch 3:  40%|███▉      | 51/128 [00:00<00:00, 147.08it/s, v_num=20]Training loss: 3.232093095779419\n",
      "Epoch 3:  41%|████      | 52/128 [00:00<00:00, 147.11it/s, v_num=20]Training loss: 3.2167744636535645\n",
      "Epoch 3:  41%|████▏     | 53/128 [00:00<00:00, 147.16it/s, v_num=20]Training loss: 3.206967353820801\n",
      "Epoch 3:  42%|████▏     | 54/128 [00:00<00:00, 147.28it/s, v_num=20]Training loss: 3.275139093399048\n",
      "Epoch 3:  43%|████▎     | 55/128 [00:00<00:00, 147.23it/s, v_num=20]Training loss: 3.2520246505737305\n",
      "Epoch 3:  44%|████▍     | 56/128 [00:00<00:00, 147.28it/s, v_num=20]Training loss: 3.3496572971343994\n",
      "Epoch 3:  45%|████▍     | 57/128 [00:00<00:00, 147.34it/s, v_num=20]Training loss: 3.3601109981536865\n",
      "Epoch 3:  45%|████▌     | 58/128 [00:00<00:00, 147.39it/s, v_num=20]Training loss: 3.2109994888305664\n",
      "Epoch 3:  46%|████▌     | 59/128 [00:00<00:00, 147.35it/s, v_num=20]Training loss: 3.3541009426116943\n",
      "Epoch 3:  47%|████▋     | 60/128 [00:00<00:00, 147.37it/s, v_num=20]Training loss: 3.2276713848114014\n",
      "Epoch 3:  48%|████▊     | 61/128 [00:00<00:00, 147.40it/s, v_num=20]Training loss: 2.7933621406555176\n",
      "Epoch 3:  48%|████▊     | 62/128 [00:00<00:00, 147.32it/s, v_num=20]Training loss: 3.1653337478637695\n",
      "Epoch 3:  49%|████▉     | 63/128 [00:00<00:00, 147.39it/s, v_num=20]Training loss: 3.284846305847168\n",
      "Epoch 3:  50%|█████     | 64/128 [00:00<00:00, 147.42it/s, v_num=20]Training loss: 3.066638708114624\n",
      "Epoch 3:  51%|█████     | 65/128 [00:00<00:00, 147.45it/s, v_num=20]Training loss: 3.298321485519409\n",
      "Epoch 3:  52%|█████▏    | 66/128 [00:00<00:00, 147.39it/s, v_num=20]Training loss: 3.1236155033111572\n",
      "Epoch 3:  52%|█████▏    | 67/128 [00:00<00:00, 147.32it/s, v_num=20]Training loss: 3.2716825008392334\n",
      "Epoch 3:  53%|█████▎    | 68/128 [00:00<00:00, 147.31it/s, v_num=20]Training loss: 3.271717071533203\n",
      "Epoch 3:  54%|█████▍    | 69/128 [00:00<00:00, 147.34it/s, v_num=20]Training loss: 2.7360761165618896\n",
      "Epoch 3:  55%|█████▍    | 70/128 [00:00<00:00, 147.37it/s, v_num=20]Training loss: 2.8292293548583984\n",
      "Epoch 3:  55%|█████▌    | 71/128 [00:00<00:00, 147.44it/s, v_num=20]Training loss: 3.1640141010284424\n",
      "Epoch 3:  56%|█████▋    | 72/128 [00:00<00:00, 147.47it/s, v_num=20]Training loss: 3.328233480453491\n",
      "Epoch 3:  57%|█████▋    | 73/128 [00:00<00:00, 147.51it/s, v_num=20]Training loss: 3.2913806438446045\n",
      "Epoch 3:  58%|█████▊    | 74/128 [00:00<00:00, 147.46it/s, v_num=20]Training loss: 2.9981234073638916\n",
      "Epoch 3:  59%|█████▊    | 75/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 3.2428858280181885\n",
      "Epoch 3:  59%|█████▉    | 76/128 [00:00<00:00, 147.51it/s, v_num=20]Training loss: 3.3275599479675293\n",
      "Epoch 3:  60%|██████    | 77/128 [00:00<00:00, 147.54it/s, v_num=20]Training loss: 3.247593402862549\n",
      "Epoch 3:  61%|██████    | 78/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 3.2146072387695312\n",
      "Epoch 3:  62%|██████▏   | 79/128 [00:00<00:00, 147.52it/s, v_num=20]Training loss: 3.3287265300750732\n",
      "Epoch 3:  62%|██████▎   | 80/128 [00:00<00:00, 147.53it/s, v_num=20]Training loss: 2.916877269744873\n",
      "Epoch 3:  63%|██████▎   | 81/128 [00:00<00:00, 147.53it/s, v_num=20]Training loss: 3.1212291717529297\n",
      "Epoch 3:  64%|██████▍   | 82/128 [00:00<00:00, 147.53it/s, v_num=20]Training loss: 3.209712505340576\n",
      "Epoch 3:  65%|██████▍   | 83/128 [00:00<00:00, 147.54it/s, v_num=20]Training loss: 3.255540370941162\n",
      "Epoch 3:  66%|██████▌   | 84/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.1938533782958984\n",
      "Epoch 3:  66%|██████▋   | 85/128 [00:00<00:00, 147.57it/s, v_num=20]Training loss: 3.2911734580993652\n",
      "Epoch 3:  67%|██████▋   | 86/128 [00:00<00:00, 147.58it/s, v_num=20]Training loss: 2.9501800537109375\n",
      "Epoch 3:  68%|██████▊   | 87/128 [00:00<00:00, 147.60it/s, v_num=20]Training loss: 3.2291438579559326\n",
      "Epoch 3:  69%|██████▉   | 88/128 [00:00<00:00, 147.61it/s, v_num=20]Training loss: 3.3017826080322266\n",
      "Epoch 3:  70%|██████▉   | 89/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 3.279940605163574\n",
      "Epoch 3:  70%|███████   | 90/128 [00:00<00:00, 147.57it/s, v_num=20]Training loss: 3.132939100265503\n",
      "Epoch 3:  71%|███████   | 91/128 [00:00<00:00, 147.53it/s, v_num=20]Training loss: 3.209108591079712\n",
      "Epoch 3:  72%|███████▏  | 92/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.044995069503784\n",
      "Epoch 3:  73%|███████▎  | 93/128 [00:00<00:00, 147.58it/s, v_num=20]Training loss: 3.1863913536071777\n",
      "Epoch 3:  73%|███████▎  | 94/128 [00:00<00:00, 147.60it/s, v_num=20]Training loss: 2.9369232654571533\n",
      "Epoch 3:  74%|███████▍  | 95/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 3.2066431045532227\n",
      "Epoch 3:  75%|███████▌  | 96/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 3.26961612701416\n",
      "Epoch 3:  76%|███████▌  | 97/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 3.2524521350860596\n",
      "Epoch 3:  77%|███████▋  | 98/128 [00:00<00:00, 147.63it/s, v_num=20]Training loss: 3.1323368549346924\n",
      "Epoch 3:  77%|███████▋  | 99/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 2.8164217472076416\n",
      "Epoch 3:  78%|███████▊  | 100/128 [00:00<00:00, 147.64it/s, v_num=20]Training loss: 3.2802412509918213\n",
      "Epoch 3:  79%|███████▉  | 101/128 [00:00<00:00, 147.66it/s, v_num=20]Training loss: 3.230022668838501\n",
      "Epoch 3:  80%|███████▉  | 102/128 [00:00<00:00, 147.67it/s, v_num=20]Training loss: 3.0415496826171875\n",
      "Epoch 3:  80%|████████  | 103/128 [00:00<00:00, 147.62it/s, v_num=20]Training loss: 3.163753032684326\n",
      "Epoch 3:  81%|████████▏ | 104/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 2.771916627883911\n",
      "Epoch 3:  82%|████████▏ | 105/128 [00:00<00:00, 147.57it/s, v_num=20]Training loss: 3.3092763423919678\n",
      "Epoch 3:  83%|████████▎ | 106/128 [00:00<00:00, 147.52it/s, v_num=20]Training loss: 3.2927584648132324\n",
      "Epoch 3:  84%|████████▎ | 107/128 [00:00<00:00, 147.55it/s, v_num=20]Training loss: 3.0658702850341797\n",
      "Epoch 3:  84%|████████▍ | 108/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.235670566558838\n",
      "Epoch 3:  85%|████████▌ | 109/128 [00:00<00:00, 147.57it/s, v_num=20]Training loss: 3.279329776763916\n",
      "Epoch 3:  86%|████████▌ | 110/128 [00:00<00:00, 147.59it/s, v_num=20]Training loss: 3.18269944190979\n",
      "Epoch 3:  87%|████████▋ | 111/128 [00:00<00:00, 147.61it/s, v_num=20]Training loss: 3.303068161010742\n",
      "Epoch 3:  88%|████████▊ | 112/128 [00:00<00:00, 147.56it/s, v_num=20]Training loss: 3.2819008827209473\n",
      "Epoch 3:  88%|████████▊ | 113/128 [00:00<00:00, 147.60it/s, v_num=20]Training loss: 3.204580307006836\n",
      "Epoch 3:  89%|████████▉ | 114/128 [00:00<00:00, 147.61it/s, v_num=20]Training loss: 2.8925259113311768\n",
      "Epoch 3:  90%|████████▉ | 115/128 [00:00<00:00, 147.64it/s, v_num=20]Training loss: 2.344365358352661\n",
      "Epoch 3:  91%|█████████ | 116/128 [00:00<00:00, 147.59it/s, v_num=20]Training loss: 3.1217477321624756\n",
      "Epoch 3:  91%|█████████▏| 117/128 [00:00<00:00, 147.55it/s, v_num=20]Training loss: 2.6271722316741943\n",
      "Epoch 3:  92%|█████████▏| 118/128 [00:00<00:00, 147.52it/s, v_num=20]Training loss: 2.9405879974365234\n",
      "Epoch 3:  93%|█████████▎| 119/128 [00:00<00:00, 147.43it/s, v_num=20]Training loss: 3.3495371341705322\n",
      "Epoch 3:  94%|█████████▍| 120/128 [00:00<00:00, 147.44it/s, v_num=20]Training loss: 3.28609561920166\n",
      "Epoch 3:  95%|█████████▍| 121/128 [00:00<00:00, 147.48it/s, v_num=20]Training loss: 3.307480573654175\n",
      "Epoch 3:  95%|█████████▌| 122/128 [00:00<00:00, 147.49it/s, v_num=20]Training loss: 3.2628016471862793\n",
      "Epoch 3:  96%|█████████▌| 123/128 [00:00<00:00, 147.39it/s, v_num=20]Training loss: 3.1882309913635254\n",
      "Epoch 3:  97%|█████████▋| 124/128 [00:00<00:00, 147.41it/s, v_num=20]Training loss: 3.2985470294952393\n",
      "Epoch 3:  98%|█████████▊| 125/128 [00:00<00:00, 147.38it/s, v_num=20]Training loss: 3.256293535232544\n",
      "Epoch 3:  98%|█████████▊| 126/128 [00:00<00:00, 147.44it/s, v_num=20]Training loss: 3.3716347217559814\n",
      "Epoch 3:  99%|█████████▉| 127/128 [00:00<00:00, 147.50it/s, v_num=20]Training loss: 3.050889492034912\n",
      "Epoch 3: 100%|██████████| 128/128 [00:00<00:00, 147.51it/s, v_num=20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[AValidation loss: 3.268477439880371\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 148.90it/s]\u001b[AValidation loss: 3.2628066539764404\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 172.64it/s]\u001b[AValidation loss: 3.380213975906372\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 186.23it/s]\u001b[AValidation loss: 3.3544209003448486\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 191.05it/s]\u001b[AValidation loss: 3.265857219696045\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 190.12it/s]\u001b[AValidation loss: 3.3321166038513184\n",
      "\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 196.15it/s]\u001b[AValidation loss: 3.2826101779937744\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 198.70it/s]\u001b[AValidation loss: 3.2712583541870117\n",
      "\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 198.10it/s]\u001b[AValidation loss: 3.3999879360198975\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 200.64it/s]\u001b[AValidation loss: 3.324007272720337\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 202.29it/s]\u001b[AValidation loss: 3.339587450027466\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 203.86it/s]\u001b[AValidation loss: 3.2770519256591797\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 205.29it/s]\u001b[AValidation loss: 3.2803916931152344\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 206.44it/s]\u001b[AValidation loss: 3.3452701568603516\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 204.58it/s]\u001b[AValidation loss: 3.337965965270996\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 205.62it/s]\u001b[AValidation loss: 3.345629930496216\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 205.05it/s]\u001b[AValidation loss: 3.2531630992889404\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 204.98it/s]\u001b[AValidation loss: 3.2884974479675293\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 204.36it/s]\u001b[AValidation loss: 3.3014676570892334\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 204.30it/s]\u001b[AValidation loss: 3.258812189102173\n",
      "\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 204.27it/s]\u001b[AValidation loss: 3.3196001052856445\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 204.96it/s]\u001b[AValidation loss: 3.3278045654296875\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 204.60it/s]\u001b[AValidation loss: 3.2221152782440186\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 205.24it/s]\u001b[AValidation loss: 3.2791614532470703\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 205.21it/s]\u001b[AValidation loss: 3.2764735221862793\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 205.72it/s]\u001b[AValidation loss: 3.3744566440582275\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 206.28it/s]\u001b[AValidation loss: 3.316453695297241\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 206.71it/s]\u001b[AValidation loss: 3.278080463409424\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 206.62it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 128/128 [00:01<00:00, 125.26it/s, v_num=20]    \u001b[A\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Assuming train_embeddings, val_embeddings, test_embeddings are the VAE embeddings\n",
    "# and train_labels, val_labels, test_labels are one-hot encoded (not converting them back to class indices)\n",
    "\n",
    "# # Create TensorDatasets\n",
    "# train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "# val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "# test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices using argmax\n",
    "train_labels_indices = torch.argmax(train_labels, dim=1)\n",
    "val_labels_indices = torch.argmax(val_labels, dim=1)\n",
    "test_labels_indices = torch.argmax(test_labels, dim=1)\n",
    "\n",
    "# Now, use the class indices labels instead of one-hot encoded labels in your dataset\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels_indices)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels_indices)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, worker_init_fn=lambda _: np.random.seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Create a logger\n",
    "logger = loggers.CSVLogger('lightning_logs/', name='m1_mlp')\n",
    "\n",
    "\n",
    "# Define MLP layer dimensions\n",
    "layer_dims = [64, 32]  # Example of multiple hidden layers\n",
    "dropout_rate=0.6\n",
    "lr=1e-4\n",
    "\n",
    "# Initialize the MLP model with dropout\n",
    "mlp_model = MLP(input_dim=train_embeddings.shape[1],  # Input dimension should match VAE embeddings size\n",
    "                layer_dims=layer_dims, \n",
    "                num_classes=train_labels.shape[1],  # Number of output classes (same as number of columns in one-hot encoded labels)\n",
    "                dropout_rate=dropout_rate, \n",
    "                lr=lr,\n",
    "                class_weights=class_weights)\n",
    "\n",
    "# Create the loss history callback\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    dirpath=f'{logger.save_dir}/{logger.name}/version_{logger.version}/checkpoints/',\n",
    "    filename='m1-mlp-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Initialize PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, loss_history_callback],\n",
    "    accelerator='gpu',\n",
    "    precision=32,\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    deterministic=True,  # Ensure reproducibility\n",
    "    logger=logger,\n",
    "    )\n",
    "\n",
    "# Train the MLP model\n",
    "trainer.fit(mlp_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a4398-7b5c-4d72-9bdd-8d42201460fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'df_logs' is your DataFrame with the logs\n",
    "# Drop the rows with NaN values for plotting each loss curve separately\n",
    "df_logs = pd.read_csv(\"lightning_logs/v2_m1_vae_5000/\")\n",
    "\n",
    "# Extract relevant columns\n",
    "epochs = df_logs['epoch'].unique()  # Get unique epoch values\n",
    "train_loss = df_logs['train_loss'].dropna()  # Drop NaN values for train loss\n",
    "val_loss = df_logs['val_loss'].dropna()      # Drop NaN values for validation loss\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs[:len(val_loss)], val_loss, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdf501-6b93-4c07-9291-430548cd896b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b946d-b166-4ca9-b449-107a5c71a67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5cb5d362-cecf-476f-874d-cf0c01c9043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get predictions from the MLP model\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)  # Move model to the correct device (e.g., GPU)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, labels = batch\n",
    "            \n",
    "            # Move data to the same device as the model\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get logits from the model\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Get predicted classes\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Append predictions and labels\n",
    "            all_preds.append(preds.cpu())  # Move predictions back to CPU\n",
    "            all_labels.append(labels.cpu())  # Move labels back to CPU\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Define the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get predictions and labels on the test set\n",
    "test_preds, labels_test = get_predictions(mlp_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "63f97e47-f2bd-4449-bb65-3a0c582bdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "\n",
    "# # Load the hyperparameters from the hparams.yaml file\n",
    "# hparams_path = 'lightning_logs/m1_mlp/'  # Replace with the correct path\n",
    "# with open(hparams_path) as file:\n",
    "#     hparams = yaml.safe_load(file)\n",
    "\n",
    "# print(hparams)  # To inspect the hyperparameters\n",
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# checkpoint_path = \"lightning_logs/m1_vae/version_0/checkpoints/m1-vae-epoch=21-val_loss=0.96.ckpt\"\n",
    "# vae_model = VAE_Lightning.load_from_checkpoint(\n",
    "#     checkpoint_path,\n",
    "#     map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "#     **hparams\n",
    "#     )\n",
    "\n",
    "# vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f9f173a5-3673-4b1d-b531-5950ef4f0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00       181\n",
      "     Class 1       0.01      0.76      0.02        17\n",
      "     Class 2       0.00      0.00      0.00        27\n",
      "     Class 3       0.00      0.00      0.00         1\n",
      "     Class 4       0.00      0.00      0.00       144\n",
      "     Class 5       0.00      0.00      0.00         7\n",
      "     Class 6       0.00      0.00      0.00       807\n",
      "     Class 7       0.00      0.00      0.00         3\n",
      "     Class 8       0.00      0.00      0.00        22\n",
      "     Class 9       0.00      0.00      0.00         4\n",
      "    Class 10       0.00      0.00      0.00        17\n",
      "    Class 11       0.00      0.00      0.00        19\n",
      "    Class 12       0.00      0.00      0.00        11\n",
      "    Class 13       0.00      0.00      0.00        16\n",
      "    Class 14       0.00      0.00      0.00         8\n",
      "    Class 15       0.00      0.00      0.00         6\n",
      "    Class 16       0.00      0.00      0.00        17\n",
      "    Class 17       0.00      0.00      0.00        36\n",
      "    Class 18       0.00      0.00      0.00       221\n",
      "    Class 19       0.00      0.00      0.00         4\n",
      "    Class 20       0.00      0.00      0.00        40\n",
      "    Class 21       0.00      0.00      0.00        10\n",
      "    Class 22       0.00      0.00      0.00         1\n",
      "    Class 23       0.06      0.14      0.08       108\n",
      "    Class 24       0.00      0.00      0.00         5\n",
      "    Class 25       0.00      0.00      0.00        10\n",
      "    Class 26       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.02      1755\n",
      "   macro avg       0.00      0.03      0.00      1755\n",
      "weighted avg       0.00      0.02      0.01      1755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert one-hot encoded labels back to class indices\n",
    "# test_labels_argmax = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Generate class names for 27 classes\n",
    "target_names = [f\"Class {i}\" for i in range(27)]\n",
    "\n",
    "# Generate the classification report using class labels\n",
    "report = classification_report(labels_test, test_preds, target_names=target_names, zero_division=0)\n",
    "\n",
    "\n",
    "# Print the report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7ca9d-0f27-4a05-8aae-448d05623a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85318797-2e74-42ef-995c-6e337790a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x147e47cd22f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "670ab7d6-7600-4478-931a-654a639648a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x147e47cd27d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1e040c0-3970-429a-82ba-167d0beea418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe5bf8f5-9414-4856-98b8-6f0a3b039512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23532eb6-036b-4412-9e94-89ffd900b60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50d5be-06bf-425d-8aaf-f28efd07a449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meth]",
   "language": "python",
   "name": "conda-env-meth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
