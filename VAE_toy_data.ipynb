{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f18039-d090-4590-8147-7015aea29fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285847dd-5b4b-4988-94b8-61b7e0d8c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df1 = pd.read_pickle('data/methyl_scores_v1_HM450k_1.pkl', compression=\"bz2\")\n",
    "df2 = pd.read_pickle('data/methyl_scores_v1_HM450k_2.pkl', compression=\"bz2\")\n",
    "df3 = pd.read_pickle('data/methyl_scores_v1_HM450k_3.pkl', compression=\"bz2\")\n",
    "df4 = pd.read_pickle('data/methyl_scores_v1_HM450k_4.pkl', compression=\"bz2\")\n",
    "df5 = pd.read_pickle('data/methyl_scores_v1_HM450k_5.pkl', compression=\"bz2\")\n",
    "df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0c2f0-cd67-40bd-ad72-5bc71d829f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9665d9a-ffb3-47bd-b6e5-973309d09f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'geo_accession', 'title', 'sex', 'age', 'race', 'disease',\n",
    "       'tissue', 'geo_platform', 'inferred_sex', 'inferred_age_Hannum',\n",
    "       'inferred_age_SkinBlood', 'inferred_age_Horvath353']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461eab8-829d-4fcb-bc5f-427a39e2d3ba",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cca18a-9d43-48be-9926-2b34f38c9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "metadata_columns = ['id', 'geo_accession', 'title', 'sex', 'age', 'race',\n",
    "                    'tissue', 'geo_platform', 'inferred_age_Hannum',\n",
    "                    'inferred_age_SkinBlood', 'inferred_age_Horvath353']  # list of metadata columns\n",
    "\n",
    "label_column = 'disease'  # column with target values for classification/regression\n",
    "condition_column = 'inferred_sex'\n",
    "numerical_data = df.drop(metadata_columns + [label_column] + [condition_column], axis=1)  # features for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a32489-463c-4167-9b5f-b118e722a85a",
   "metadata": {},
   "source": [
    "Fill in the NA values in the `label_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7488b8-4ffa-48b9-b016-aad9f5390627",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = 'no_label'\n",
    "df[label_column].fillna(default_value, inplace=True)\n",
    "\n",
    "labels = df[label_column]  # target/label for model training\n",
    "conditions = df[condition_column]  # target/label for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3d066-d303-4a4f-be53-fef6f8ced205",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a6365-ab37-4a7c-9fd3-25e0fb273dd2",
   "metadata": {},
   "source": [
    "## Detect and drop unreliable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c0d03-0791-46a2-98ff-ab2b4fe8e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d4837-9f4d-45c5-bba1-4bb1d4c2a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # Calculate the percentage of NaN values in each column\n",
    "nan_percentage = numerical_data.isna().sum(axis=0) / numerical_data.shape[0] * 100\n",
    "\n",
    "# Plot the histogram of the percentage of NaN values per column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(nan_percentage, bins=50, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Histogram of Percentage of NaN Values Per Column\")\n",
    "plt.xlabel(\"Percentage of NaN values\")  \n",
    "plt.ylabel(\"Number of Columns\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfa51f-7009-4628-96e0-56379bc43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"more than 1% NaN: {(nan_percentage>1).sum()}\")\n",
    "print(f\"more than 5% NaN: {(nan_percentage>5).sum()}\")\n",
    "print(f\"more than 10% NaN: {(nan_percentage>10).sum()}\")\n",
    "print(f\"more than 15% NaN: {(nan_percentage>15).sum()}\")\n",
    "print(f\"more than 20% NaN: {(nan_percentage>20).sum()}\")\n",
    "print(f\"more than 30% NaN: {(nan_percentage>30).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520711a-e424-4ecd-ba2b-0771a4460fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of columns where NaN percentage is less than 10%\n",
    "selected_columns = nan_percentage[nan_percentage < 10].index.tolist()\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "numerical_data_filtered = numerical_data[selected_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d32e8-a353-4fee-9cb6-71bc0f962dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5862f95-701b-4ccd-98fd-525ebbf2e2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40df9a1-49e4-41a4-83aa-8aa67c0cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Convert categorical labels to one-hot vectors\n",
    "labels_onehot = onehot_encoder.fit_transform(labels.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af488e12-0c6e-4b75-9bb2-028948dd4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4520a-deb7-4889-b16b-57be32332afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SCALE = True\n",
    "## With SCALE = False, the loss would explode! therefore, we scale the data to control the range of the loss and the gradients\n",
    "if SCALE:\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(numerical_data_filtered)\n",
    "else:\n",
    "    features_scaled = numerical_data_filtered.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a140e-879d-4afd-9a81-a86d2fa41a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "732dd697-3d2e-472c-bbbe-58336cb18dd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552d6e6-2d73-4c95-8ef8-8f1b2d94d781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61bfb9-80c2-4a39-b733-e064574ed050",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "X = features_scaled\n",
    "y = labels_onehot\n",
    "\n",
    "# Stratified shuffle split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_temp = X[train_index], X[test_index]\n",
    "    y_train, y_temp = y[train_index], y[test_index]\n",
    "\n",
    "# Split the temp set into validation and test sets (15% val, 15% test)\n",
    "splitter_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in splitter_val_test.split(X_temp, y_temp):\n",
    "    X_val, X_test = X_temp[val_index], X_temp[test_index]\n",
    "    y_val, y_test = y_temp[val_index], y_temp[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47349ad-ddbe-4803-abec-6114f2fd865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check the stratified split\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ad77-9006-41ec-9ff8-7b1a6d0753ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b485a-57b5-4d4f-af6b-7c5168286ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fbd7193-a7fe-4e11-92c0-dec7c94a9fc9",
   "metadata": {},
   "source": [
    "# Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ceb7a-386e-45e9-b4ac-97ed51a518be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable inside the script\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344de36-d29d-432d-b4cb-8f4e31882834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim=256):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mu = nn.Linear(hidden_dim, latent_dim)  # for mean\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)  # for log variance\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Check if logvar has NaN or Inf values\n",
    "        if torch.isnan(logvar).any() or torch.isinf(logvar).any():\n",
    "            print(f\"NaN or Inf detected in logvar: logvar={logvar}\")\n",
    "        \n",
    "        # Clamp logvar to prevent extreme values\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        \n",
    "        # Calculate std from logvar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Check if std has NaN or Inf values\n",
    "        if torch.isnan(std).any() or torch.isinf(std).any():\n",
    "            print(f\"NaN or Inf detected in std computation: std={std}\")\n",
    "        \n",
    "        # Sample from the latent space\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Check if z has NaN or Inf values\n",
    "        if torch.isnan(z).any() or torch.isinf(z).any():\n",
    "            print(f\"NaN or Inf detected in z computation: z={z}\")\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Method to get the latent embedding (the `z` vector) for an input.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)  # this is the embedding\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bd86a-8aff-492d-9745-fe7071e3446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Lightning(pl.LightningModule):\n",
    "    def __init__(self, input_dim=485577, latent_dim=100, hidden_dim=512, lr=1e-3):\n",
    "        super(VAE_Lightning, self).__init__()\n",
    "        self.model = VAE(input_dim, latent_dim, hidden_dim)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.model.encode(x)\n",
    "        z = self.model.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        return self.model.get_latent_embedding(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)  # mask where values are not NaN\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Validation loss: {loss.item()}\")\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "  \n",
    "\n",
    "    def _vae_loss(self, original_x, x_hat, mu, logvar, mask):\n",
    "        # Apply mask to ignore NaN values in the loss calculation\n",
    "        recon_loss = F.mse_loss(x_hat[mask], original_x[mask], reduction='mean')\n",
    "    \n",
    "        # Scale the KL divergence to balance the losses\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss / original_x.shape[0]  # Normalize by batch size or apply weighting\n",
    "    \n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2c96b-8ddb-4e73-a3c2-7743bf5e4b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185b9194-7cc9-4121-8b43-649794d892c3",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce2614-0091-472f-9c1e-19367ea40675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee508d82-0352-4299-9fc6-ff43d53fbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Extract embeddings for the entire dataset\n",
    "# def extract_embeddings(model, dataloader):\n",
    "#     model.eval()\n",
    "#     embeddings = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             x, _ = batch\n",
    "#             z, _, _ = model.forward(x)  # Get the latent embedding z\n",
    "#             embeddings.append(z)\n",
    "#     return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# # Train the VAE model first\n",
    "# trainer = Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# # Extract latent embeddings for training and validation sets\n",
    "# train_embeddings = extract_embeddings(model, train_loader)\n",
    "# val_embeddings = extract_embeddings(model, val_loader)\n",
    "\n",
    "# # Convert embeddings to numpy for later use\n",
    "# train_embeddings_np = train_embeddings.cpu().numpy()\n",
    "# val_embeddings_np = val_embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48303f9a-623a-4261-b1e0-3002b82723f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333623b6-b9bb-48da-9fee-a357b3d1952a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc26a1-70ff-43fd-9197-a11ae8e90abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32624f45-a42b-40e1-a495-8eda0f11cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2ceef-19cf-48e1-81d8-ee67dd3c2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d648b65-faac-49ce-a8c2-ad4970ca0785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 904 M  | train\n",
      "---------------------------------------\n",
      "904 M     Trainable params\n",
      "0         Non-trainable params\n",
      "904 M     Total params\n",
      "3,616.992 Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Validation loss: 9.681659698486328\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 51.87it/s]Validation loss: 3.884653091430664\n",
      "Epoch 0:   0%|          | 0/512 [00:00<?, ?it/s]                           Training loss: 3.478109121322632\n",
      "Epoch 0:   0%|          | 1/512 [00:00<01:28,  5.80it/s, v_num=11]Training loss: 2.715825080871582\n",
      "Epoch 0:   0%|          | 2/512 [00:00<01:10,  7.26it/s, v_num=11]Training loss: 6.115746974945068\n",
      "Epoch 0:   1%|          | 3/512 [00:00<01:19,  6.41it/s, v_num=11]Training loss: 6.810033798217773\n",
      "Epoch 0:   1%|          | 4/512 [00:00<01:23,  6.06it/s, v_num=11]Training loss: 3.564426898956299\n",
      "Epoch 0:   1%|          | 5/512 [00:00<01:26,  5.87it/s, v_num=11]Training loss: 3.145612955093384\n",
      "Epoch 0:   1%|          | 6/512 [00:01<01:28,  5.74it/s, v_num=11]Training loss: 3.9566006660461426\n",
      "Epoch 0:   1%|▏         | 7/512 [00:01<01:29,  5.66it/s, v_num=11]Training loss: 3.710942268371582\n",
      "Epoch 0:   2%|▏         | 8/512 [00:01<01:30,  5.60it/s, v_num=11]Training loss: 3.85373592376709\n",
      "Epoch 0:   2%|▏         | 9/512 [00:01<01:30,  5.55it/s, v_num=11]Training loss: 3.3702597618103027\n",
      "Epoch 0:   2%|▏         | 10/512 [00:01<01:31,  5.51it/s, v_num=11]Training loss: 3.171100616455078\n",
      "Epoch 0:   2%|▏         | 11/512 [00:02<01:31,  5.48it/s, v_num=11]Training loss: 2.697352886199951\n",
      "Epoch 0:   2%|▏         | 12/512 [00:02<01:31,  5.46it/s, v_num=11]Training loss: 4.121243953704834\n",
      "Epoch 0:   3%|▎         | 13/512 [00:02<01:31,  5.44it/s, v_num=11]Training loss: 2.259552240371704\n",
      "Epoch 0:   3%|▎         | 14/512 [00:02<01:31,  5.42it/s, v_num=11]Training loss: 3.1335887908935547\n",
      "Epoch 0:   3%|▎         | 15/512 [00:02<01:32,  5.40it/s, v_num=11]Training loss: 2.465501308441162\n",
      "Epoch 0:   3%|▎         | 16/512 [00:02<01:32,  5.39it/s, v_num=11]Training loss: 7.702536106109619\n",
      "Epoch 0:   3%|▎         | 17/512 [00:03<01:32,  5.38it/s, v_num=11]Training loss: 2.5786709785461426\n",
      "Epoch 0:   4%|▎         | 18/512 [00:03<01:32,  5.37it/s, v_num=11]Training loss: 6.907134056091309\n",
      "Epoch 0:   4%|▎         | 19/512 [00:03<01:32,  5.36it/s, v_num=11]Training loss: 2.6182429790496826\n",
      "Epoch 0:   4%|▍         | 20/512 [00:03<01:31,  5.35it/s, v_num=11]Training loss: 1.978676199913025\n",
      "Epoch 0:   4%|▍         | 21/512 [00:03<01:31,  5.34it/s, v_num=11]Training loss: 2.7550861835479736\n",
      "Epoch 0:   4%|▍         | 22/512 [00:04<01:31,  5.34it/s, v_num=11]Training loss: 2.664520502090454\n",
      "Epoch 0:   4%|▍         | 23/512 [00:04<01:31,  5.33it/s, v_num=11]Training loss: 2.807961940765381\n",
      "Epoch 0:   5%|▍         | 24/512 [00:04<01:31,  5.32it/s, v_num=11]Training loss: 2.6694681644439697\n",
      "Epoch 0:   5%|▍         | 25/512 [00:04<01:31,  5.32it/s, v_num=11]Training loss: 3.0244007110595703\n",
      "Epoch 0:   5%|▌         | 26/512 [00:04<01:31,  5.31it/s, v_num=11]Training loss: 2.018188953399658\n",
      "Epoch 0:   5%|▌         | 27/512 [00:05<01:31,  5.31it/s, v_num=11]Training loss: 4.2016096115112305\n",
      "Epoch 0:   5%|▌         | 28/512 [00:05<01:31,  5.31it/s, v_num=11]Training loss: 96.66520690917969\n",
      "Epoch 0:   6%|▌         | 29/512 [00:05<01:31,  5.30it/s, v_num=11]Training loss: 2.304908275604248\n",
      "Epoch 0:   6%|▌         | 30/512 [00:05<01:30,  5.30it/s, v_num=11]Training loss: 2.0183298587799072\n",
      "Epoch 0:   6%|▌         | 31/512 [00:05<01:30,  5.30it/s, v_num=11]Training loss: 5.8205790519714355\n",
      "Epoch 0:   6%|▋         | 32/512 [00:06<01:30,  5.29it/s, v_num=11]Training loss: 2.217249631881714\n",
      "Epoch 0:   6%|▋         | 33/512 [00:06<01:30,  5.29it/s, v_num=11]Training loss: 1.9408217668533325\n",
      "Epoch 0:   7%|▋         | 34/512 [00:06<01:30,  5.29it/s, v_num=11]Training loss: 2.5284276008605957\n",
      "Epoch 0:   7%|▋         | 35/512 [00:06<01:30,  5.28it/s, v_num=11]Training loss: 1.7956018447875977\n",
      "Epoch 0:   7%|▋         | 36/512 [00:06<01:30,  5.28it/s, v_num=11]Training loss: 2.143766403198242\n",
      "Epoch 0:   7%|▋         | 37/512 [00:07<01:29,  5.28it/s, v_num=11]Training loss: 1.5924152135849\n",
      "Epoch 0:   7%|▋         | 38/512 [00:07<01:29,  5.28it/s, v_num=11]Training loss: 1.735511064529419\n",
      "Epoch 0:   8%|▊         | 39/512 [00:07<01:29,  5.28it/s, v_num=11]Training loss: 5.8151140213012695\n",
      "Epoch 0:   8%|▊         | 40/512 [00:07<01:29,  5.27it/s, v_num=11]Training loss: 4.481963634490967\n",
      "Epoch 0:   8%|▊         | 41/512 [00:07<01:29,  5.27it/s, v_num=11]Training loss: 2.0252318382263184\n",
      "Epoch 0:   8%|▊         | 42/512 [00:07<01:29,  5.27it/s, v_num=11]Training loss: 2.3086161613464355\n",
      "Epoch 0:   8%|▊         | 43/512 [00:08<01:29,  5.27it/s, v_num=11]Training loss: 2.483950138092041\n",
      "Epoch 0:   9%|▊         | 44/512 [00:08<01:28,  5.27it/s, v_num=11]Training loss: 1.9514124393463135\n",
      "Epoch 0:   9%|▉         | 45/512 [00:08<01:28,  5.27it/s, v_num=11]Training loss: 1.8879406452178955\n",
      "Epoch 0:   9%|▉         | 46/512 [00:08<01:28,  5.26it/s, v_num=11]Training loss: 2.6988778114318848\n",
      "Epoch 0:   9%|▉         | 47/512 [00:08<01:28,  5.26it/s, v_num=11]Training loss: 1.2937381267547607\n",
      "Epoch 0:   9%|▉         | 48/512 [00:09<01:28,  5.26it/s, v_num=11]Training loss: 1.6205182075500488\n",
      "Epoch 0:  10%|▉         | 49/512 [00:09<01:28,  5.26it/s, v_num=11]Training loss: 2.8614678382873535\n",
      "Epoch 0:  10%|▉         | 50/512 [00:09<01:27,  5.26it/s, v_num=11]Training loss: 2.711735725402832\n",
      "Epoch 0:  10%|▉         | 51/512 [00:09<01:27,  5.26it/s, v_num=11]Training loss: 1.726823091506958\n",
      "Epoch 0:  10%|█         | 52/512 [00:09<01:27,  5.26it/s, v_num=11]Training loss: 3.0794215202331543\n",
      "Epoch 0:  10%|█         | 53/512 [00:10<01:27,  5.26it/s, v_num=11]Training loss: 5.023006439208984\n",
      "Epoch 0:  11%|█         | 54/512 [00:10<01:27,  5.25it/s, v_num=11]Training loss: 2.300957202911377\n",
      "Epoch 0:  11%|█         | 55/512 [00:10<01:26,  5.25it/s, v_num=11]Training loss: 2.3255534172058105\n",
      "Epoch 0:  11%|█         | 56/512 [00:10<01:26,  5.25it/s, v_num=11]Training loss: 3.3700389862060547\n",
      "Epoch 0:  11%|█         | 57/512 [00:10<01:26,  5.25it/s, v_num=11]Training loss: 2.7202374935150146\n",
      "Epoch 0:  11%|█▏        | 58/512 [00:11<01:26,  5.25it/s, v_num=11]Training loss: 2.2534499168395996\n",
      "Epoch 0:  12%|█▏        | 59/512 [00:11<01:26,  5.25it/s, v_num=11]Training loss: 2.5687448978424072\n",
      "Epoch 0:  12%|█▏        | 60/512 [00:11<01:26,  5.25it/s, v_num=11]Training loss: 4.4160966873168945\n",
      "Epoch 0:  12%|█▏        | 61/512 [00:11<01:25,  5.25it/s, v_num=11]Training loss: 2.6353249549865723\n",
      "Epoch 0:  12%|█▏        | 62/512 [00:11<01:25,  5.25it/s, v_num=11]Training loss: 2.909193992614746\n",
      "Epoch 0:  12%|█▏        | 63/512 [00:12<01:25,  5.25it/s, v_num=11]Training loss: 3.4016737937927246\n",
      "Epoch 0:  12%|█▎        | 64/512 [00:12<01:25,  5.25it/s, v_num=11]Training loss: 2.963503837585449\n",
      "Epoch 0:  13%|█▎        | 65/512 [00:12<01:25,  5.25it/s, v_num=11]Training loss: 1.638532280921936\n",
      "Epoch 0:  13%|█▎        | 66/512 [00:12<01:25,  5.24it/s, v_num=11]Training loss: 2.0559275150299072\n",
      "Epoch 0:  13%|█▎        | 67/512 [00:12<01:24,  5.24it/s, v_num=11]Training loss: 1.7564003467559814\n",
      "Epoch 0:  13%|█▎        | 68/512 [00:12<01:24,  5.24it/s, v_num=11]Training loss: 3.8313941955566406\n",
      "Epoch 0:  13%|█▎        | 69/512 [00:13<01:24,  5.24it/s, v_num=11]Training loss: 2.0566887855529785\n",
      "Epoch 0:  14%|█▎        | 70/512 [00:13<01:24,  5.24it/s, v_num=11]Training loss: 2.693821430206299\n",
      "Epoch 0:  14%|█▍        | 71/512 [00:13<01:24,  5.24it/s, v_num=11]Training loss: 2.1339755058288574\n",
      "Epoch 0:  14%|█▍        | 72/512 [00:13<01:23,  5.24it/s, v_num=11]Training loss: 3.39547061920166\n",
      "Epoch 0:  14%|█▍        | 73/512 [00:13<01:23,  5.24it/s, v_num=11]Training loss: 2.4345688819885254\n",
      "Epoch 0:  14%|█▍        | 74/512 [00:14<01:23,  5.24it/s, v_num=11]Training loss: 1.8338297605514526\n",
      "Epoch 0:  15%|█▍        | 75/512 [00:14<01:23,  5.24it/s, v_num=11]Training loss: 2.165714740753174\n",
      "Epoch 0:  15%|█▍        | 76/512 [00:14<01:23,  5.24it/s, v_num=11]Training loss: 2.2308058738708496\n",
      "Epoch 0:  15%|█▌        | 77/512 [00:14<01:23,  5.24it/s, v_num=11]Training loss: 3.16121244430542\n",
      "Epoch 0:  15%|█▌        | 78/512 [00:14<01:22,  5.24it/s, v_num=11]Training loss: 2.2872085571289062\n",
      "Epoch 0:  15%|█▌        | 79/512 [00:15<01:22,  5.24it/s, v_num=11]Training loss: 1.8745146989822388\n",
      "Epoch 0:  16%|█▌        | 80/512 [00:15<01:22,  5.24it/s, v_num=11]Training loss: 1.9792193174362183\n",
      "Epoch 0:  16%|█▌        | 81/512 [00:15<01:22,  5.24it/s, v_num=11]Training loss: 2.2118782997131348\n",
      "Epoch 0:  16%|█▌        | 82/512 [00:15<01:22,  5.24it/s, v_num=11]Training loss: 2.2464895248413086\n",
      "Epoch 0:  16%|█▌        | 83/512 [00:15<01:21,  5.24it/s, v_num=11]Training loss: 2.401947498321533\n",
      "Epoch 0:  16%|█▋        | 84/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.6571674346923828\n",
      "Epoch 0:  17%|█▋        | 85/512 [00:16<01:21,  5.23it/s, v_num=11]Training loss: 2.901962995529175\n",
      "Epoch 0:  17%|█▋        | 86/512 [00:16<01:21,  5.23it/s, v_num=11]Training loss: 1.5217466354370117\n",
      "Epoch 0:  17%|█▋        | 87/512 [00:16<01:21,  5.23it/s, v_num=11]Training loss: 2.9478135108947754\n",
      "Epoch 0:  17%|█▋        | 88/512 [00:16<01:21,  5.23it/s, v_num=11]Training loss: 2.8868141174316406\n",
      "Epoch 0:  17%|█▋        | 89/512 [00:17<01:20,  5.23it/s, v_num=11]Training loss: 106.67489624023438\n",
      "Epoch 0:  18%|█▊        | 90/512 [00:17<01:20,  5.23it/s, v_num=11]Training loss: 2.525869846343994\n",
      "Epoch 0:  18%|█▊        | 91/512 [00:17<01:20,  5.23it/s, v_num=11]Training loss: 2.6389975547790527\n",
      "Epoch 0:  18%|█▊        | 92/512 [00:17<01:20,  5.23it/s, v_num=11]Training loss: 2.709709644317627\n",
      "Epoch 0:  18%|█▊        | 93/512 [00:17<01:20,  5.23it/s, v_num=11]Training loss: 2.546428918838501\n",
      "Epoch 0:  18%|█▊        | 94/512 [00:17<01:19,  5.23it/s, v_num=11]Training loss: 2.3681249618530273\n",
      "Epoch 0:  19%|█▊        | 95/512 [00:18<01:19,  5.23it/s, v_num=11]Training loss: 3.11198091506958\n",
      "Epoch 0:  19%|█▉        | 96/512 [00:18<01:19,  5.23it/s, v_num=11]Training loss: 1.8719881772994995\n",
      "Epoch 0:  19%|█▉        | 97/512 [00:18<01:19,  5.23it/s, v_num=11]Training loss: 2.2676753997802734\n",
      "Epoch 0:  19%|█▉        | 98/512 [00:18<01:19,  5.23it/s, v_num=11]Training loss: 2.239037036895752\n",
      "Epoch 0:  19%|█▉        | 99/512 [00:18<01:18,  5.23it/s, v_num=11]Training loss: 2.2562122344970703\n",
      "Epoch 0:  20%|█▉        | 100/512 [00:19<01:18,  5.23it/s, v_num=11]Training loss: 2.4027953147888184\n",
      "Epoch 0:  20%|█▉        | 101/512 [00:19<01:18,  5.23it/s, v_num=11]Training loss: 2.027480363845825\n",
      "Epoch 0:  20%|█▉        | 102/512 [00:19<01:18,  5.23it/s, v_num=11]Training loss: 2.851926565170288\n",
      "Epoch 0:  20%|██        | 103/512 [00:19<01:18,  5.23it/s, v_num=11]Training loss: 1.7542176246643066\n",
      "Epoch 0:  20%|██        | 104/512 [00:19<01:18,  5.23it/s, v_num=11]Training loss: 2.274763584136963\n",
      "Epoch 0:  21%|██        | 105/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.391106605529785\n",
      "Epoch 0:  21%|██        | 106/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.318159580230713\n",
      "Epoch 0:  21%|██        | 107/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.408463954925537\n",
      "Epoch 0:  21%|██        | 108/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.31433367729187\n",
      "Epoch 0:  21%|██▏       | 109/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.6881582736968994\n",
      "Epoch 0:  21%|██▏       | 110/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.1146044731140137\n",
      "Epoch 0:  22%|██▏       | 111/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.2601070404052734\n",
      "Epoch 0:  22%|██▏       | 112/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.9677302837371826\n",
      "Epoch 0:  22%|██▏       | 113/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.6854077577590942\n",
      "Epoch 0:  22%|██▏       | 114/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.8813586235046387\n",
      "Epoch 0:  22%|██▏       | 115/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.6186015605926514\n",
      "Epoch 0:  23%|██▎       | 116/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.871103286743164\n",
      "Epoch 0:  23%|██▎       | 117/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.287522315979004\n",
      "Epoch 0:  23%|██▎       | 118/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.781404972076416\n",
      "Epoch 0:  23%|██▎       | 119/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.2793755531311035\n",
      "Epoch 0:  23%|██▎       | 120/512 [00:22<01:15,  5.22it/s, v_num=11]Training loss: 2.3928675651550293\n",
      "Epoch 0:  24%|██▎       | 121/512 [00:23<01:14,  5.22it/s, v_num=11]Training loss: 1.8370720148086548\n",
      "Epoch 0:  24%|██▍       | 122/512 [00:23<01:14,  5.22it/s, v_num=11]Training loss: 2.4700963497161865\n",
      "Epoch 0:  24%|██▍       | 123/512 [00:23<01:14,  5.22it/s, v_num=11]Training loss: 3.9780900478363037\n",
      "Epoch 0:  24%|██▍       | 124/512 [00:23<01:14,  5.22it/s, v_num=11]Training loss: 5.394703388214111\n",
      "Epoch 0:  24%|██▍       | 125/512 [00:23<01:14,  5.22it/s, v_num=11]Training loss: 1.9092252254486084\n",
      "Epoch 0:  25%|██▍       | 126/512 [00:24<01:13,  5.22it/s, v_num=11]Training loss: 2.2880656719207764\n",
      "Epoch 0:  25%|██▍       | 127/512 [00:24<01:13,  5.22it/s, v_num=11]Training loss: 2.820732593536377\n",
      "Epoch 0:  25%|██▌       | 128/512 [00:24<01:13,  5.22it/s, v_num=11]Training loss: 2.4180445671081543\n",
      "Epoch 0:  25%|██▌       | 129/512 [00:24<01:13,  5.22it/s, v_num=11]Training loss: 1.7706809043884277\n",
      "Epoch 0:  25%|██▌       | 130/512 [00:24<01:13,  5.22it/s, v_num=11]Training loss: 2.97662353515625\n",
      "Epoch 0:  26%|██▌       | 131/512 [00:25<01:12,  5.22it/s, v_num=11]Training loss: 1.7607218027114868\n",
      "Epoch 0:  26%|██▌       | 132/512 [00:25<01:12,  5.22it/s, v_num=11]Training loss: 1.4034138917922974\n",
      "Epoch 0:  26%|██▌       | 133/512 [00:25<01:12,  5.22it/s, v_num=11]Training loss: 2.5584206581115723\n",
      "Epoch 0:  26%|██▌       | 134/512 [00:25<01:12,  5.22it/s, v_num=11]Training loss: 2.185988426208496\n",
      "Epoch 0:  26%|██▋       | 135/512 [00:25<01:12,  5.22it/s, v_num=11]Training loss: 2.551386833190918\n",
      "Epoch 0:  27%|██▋       | 136/512 [00:26<01:12,  5.22it/s, v_num=11]Training loss: 3.3495359420776367\n",
      "Epoch 0:  27%|██▋       | 137/512 [00:26<01:11,  5.22it/s, v_num=11]Training loss: 1.9804611206054688\n",
      "Epoch 0:  27%|██▋       | 138/512 [00:26<01:11,  5.22it/s, v_num=11]Training loss: 2.360415458679199\n",
      "Epoch 0:  27%|██▋       | 139/512 [00:26<01:11,  5.22it/s, v_num=11]Training loss: 2.157538414001465\n",
      "Epoch 0:  27%|██▋       | 140/512 [00:26<01:11,  5.22it/s, v_num=11]Training loss: 2.0172343254089355\n",
      "Epoch 0:  28%|██▊       | 141/512 [00:27<01:11,  5.22it/s, v_num=11]Training loss: 1.748934030532837\n",
      "Epoch 0:  28%|██▊       | 142/512 [00:27<01:10,  5.22it/s, v_num=11]Training loss: 1.7361512184143066\n",
      "Epoch 0:  28%|██▊       | 143/512 [00:27<01:10,  5.22it/s, v_num=11]Training loss: 2.121692180633545\n",
      "Epoch 0:  28%|██▊       | 144/512 [00:27<01:10,  5.22it/s, v_num=11]Training loss: 3.2613964080810547\n",
      "Epoch 0:  28%|██▊       | 145/512 [00:27<01:10,  5.22it/s, v_num=11]Training loss: 1.7582957744598389\n",
      "Epoch 0:  29%|██▊       | 146/512 [00:27<01:10,  5.22it/s, v_num=11]Training loss: 2.310105800628662\n",
      "Epoch 0:  29%|██▊       | 147/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 2.213501453399658\n",
      "Epoch 0:  29%|██▉       | 148/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 1.94135320186615\n",
      "Epoch 0:  29%|██▉       | 149/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 1.6595489978790283\n",
      "Epoch 0:  29%|██▉       | 150/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 1.8796790838241577\n",
      "Epoch 0:  29%|██▉       | 151/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 2.2265076637268066\n",
      "Epoch 0:  30%|██▉       | 152/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.1748199462890625\n",
      "Epoch 0:  30%|██▉       | 153/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.1993420124053955\n",
      "Epoch 0:  30%|███       | 154/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.390167474746704\n",
      "Epoch 0:  30%|███       | 155/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.7491157054901123\n",
      "Epoch 0:  30%|███       | 156/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.113330364227295\n",
      "Epoch 0:  31%|███       | 157/512 [00:30<01:08,  5.22it/s, v_num=11]Training loss: 1.9377310276031494\n",
      "Epoch 0:  31%|███       | 158/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 2.7917375564575195\n",
      "Epoch 0:  31%|███       | 159/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.6181334257125854\n",
      "Epoch 0:  31%|███▏      | 160/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.6295390129089355\n",
      "Epoch 0:  31%|███▏      | 161/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.910701036453247\n",
      "Epoch 0:  32%|███▏      | 162/512 [00:31<01:07,  5.22it/s, v_num=11]Training loss: 2.0683112144470215\n",
      "Epoch 0:  32%|███▏      | 163/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 3.3973159790039062\n",
      "Epoch 0:  32%|███▏      | 164/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 2.00337290763855\n",
      "Epoch 0:  32%|███▏      | 165/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.7495901584625244\n",
      "Epoch 0:  32%|███▏      | 166/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 2.3141627311706543\n",
      "Epoch 0:  33%|███▎      | 167/512 [00:32<01:06,  5.22it/s, v_num=11]Training loss: 1.8302901983261108\n",
      "Epoch 0:  33%|███▎      | 168/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.5748860836029053\n",
      "Epoch 0:  33%|███▎      | 169/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.9245043992996216\n",
      "Epoch 0:  33%|███▎      | 170/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 3.0211002826690674\n",
      "Epoch 0:  33%|███▎      | 171/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 2.004595994949341\n",
      "Epoch 0:  34%|███▎      | 172/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.9803334474563599\n",
      "Epoch 0:  34%|███▍      | 173/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.4993784427642822\n",
      "Epoch 0:  34%|███▍      | 174/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 3.2584569454193115\n",
      "Epoch 0:  34%|███▍      | 175/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.214907646179199\n",
      "Epoch 0:  34%|███▍      | 176/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.175574779510498\n",
      "Epoch 0:  35%|███▍      | 177/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.349381446838379\n",
      "Epoch 0:  35%|███▍      | 178/512 [00:34<01:04,  5.22it/s, v_num=11]Training loss: 2.76741623878479\n",
      "Epoch 0:  35%|███▍      | 179/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 2.237110137939453\n",
      "Epoch 0:  35%|███▌      | 180/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.6899422407150269\n",
      "Epoch 0:  35%|███▌      | 181/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 2.6350040435791016\n",
      "Epoch 0:  36%|███▌      | 182/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.939252495765686\n",
      "Epoch 0:  36%|███▌      | 183/512 [00:35<01:03,  5.22it/s, v_num=11]Training loss: 2.556849479675293\n",
      "Epoch 0:  36%|███▌      | 184/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.657477617263794\n",
      "Epoch 0:  36%|███▌      | 185/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.7599530220031738\n",
      "Epoch 0:  36%|███▋      | 186/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.681278944015503\n",
      "Epoch 0:  37%|███▋      | 187/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.5170390605926514\n",
      "Epoch 0:  37%|███▋      | 188/512 [00:36<01:02,  5.22it/s, v_num=11]Training loss: 1.938887357711792\n",
      "Epoch 0:  37%|███▋      | 189/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.652571678161621\n",
      "Epoch 0:  37%|███▋      | 190/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.011948347091675\n",
      "Epoch 0:  37%|███▋      | 191/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.504913806915283\n",
      "Epoch 0:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.69887113571167\n",
      "Epoch 0:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.3876547813415527\n",
      "Epoch 0:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 2.737109899520874\n",
      "Epoch 0:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 2.3129162788391113\n",
      "Epoch 0:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 2.862720012664795\n",
      "Epoch 0:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.6966993808746338\n",
      "Epoch 0:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 3.2322683334350586\n",
      "Epoch 0:  39%|███▉      | 199/512 [00:38<01:00,  5.22it/s, v_num=11]Training loss: 2.395792007446289\n",
      "Epoch 0:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.7798808813095093\n",
      "Epoch 0:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 3.214158773422241\n",
      "Epoch 0:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.4189319610595703\n",
      "Epoch 0:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.5394408702850342\n",
      "Epoch 0:  40%|███▉      | 204/512 [00:39<00:59,  5.22it/s, v_num=11]Training loss: 2.3401224613189697\n",
      "Epoch 0:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.265584945678711\n",
      "Epoch 0:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.9436886310577393\n",
      "Epoch 0:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.948452949523926\n",
      "Epoch 0:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.6804289817810059\n",
      "Epoch 0:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 2.4927897453308105\n",
      "Epoch 0:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.7407236099243164\n",
      "Epoch 0:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 2.0682451725006104\n",
      "Epoch 0:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 2.5588362216949463\n",
      "Epoch 0:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 2.052927017211914\n",
      "Epoch 0:  42%|████▏     | 214/512 [00:41<00:57,  5.22it/s, v_num=11]Training loss: 4.0848846435546875\n",
      "Epoch 0:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.888001561164856\n",
      "Epoch 0:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 2.5797929763793945\n",
      "Epoch 0:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.7339794635772705\n",
      "Epoch 0:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 2.527475118637085\n",
      "Epoch 0:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.8230770826339722\n",
      "Epoch 0:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.9601691961288452\n",
      "Epoch 0:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.3047311305999756\n",
      "Epoch 0:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.0201869010925293\n",
      "Epoch 0:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.0158467292785645\n",
      "Epoch 0:  44%|████▍     | 224/512 [00:42<00:55,  5.21it/s, v_num=11]Training loss: 1.660219669342041\n",
      "Epoch 0:  44%|████▍     | 225/512 [00:43<00:55,  5.21it/s, v_num=11]Training loss: 2.242579460144043\n",
      "Epoch 0:  44%|████▍     | 226/512 [00:43<00:54,  5.21it/s, v_num=11]Training loss: 2.5678930282592773\n",
      "Epoch 0:  44%|████▍     | 227/512 [00:43<00:54,  5.21it/s, v_num=11]Training loss: 2.4750115871429443\n",
      "Epoch 0:  45%|████▍     | 228/512 [00:43<00:54,  5.21it/s, v_num=11]Training loss: 1.9237949848175049\n",
      "Epoch 0:  45%|████▍     | 229/512 [00:43<00:54,  5.21it/s, v_num=11]Training loss: 1.9070706367492676\n",
      "Epoch 0:  45%|████▍     | 230/512 [00:44<00:54,  5.21it/s, v_num=11]Training loss: 1.9123579263687134\n",
      "Epoch 0:  45%|████▌     | 231/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 3.011496067047119\n",
      "Epoch 0:  45%|████▌     | 232/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 2.022571086883545\n",
      "Epoch 0:  46%|████▌     | 233/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 1.4836831092834473\n",
      "Epoch 0:  46%|████▌     | 234/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 2.1374034881591797\n",
      "Epoch 0:  46%|████▌     | 235/512 [00:45<00:53,  5.21it/s, v_num=11]Training loss: 2.3385305404663086\n",
      "Epoch 0:  46%|████▌     | 236/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.8375117778778076\n",
      "Epoch 0:  46%|████▋     | 237/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.8339945077896118\n",
      "Epoch 0:  46%|████▋     | 238/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 2.073513984680176\n",
      "Epoch 0:  47%|████▋     | 239/512 [00:45<00:52,  5.20it/s, v_num=11]Training loss: 1.8270642757415771\n",
      "Epoch 0:  47%|████▋     | 240/512 [00:46<00:52,  5.20it/s, v_num=11]Training loss: 1.3201594352722168\n",
      "Epoch 0:  47%|████▋     | 241/512 [00:46<00:52,  5.20it/s, v_num=11]Training loss: 1.840439796447754\n",
      "Epoch 0:  47%|████▋     | 242/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 2.3356709480285645\n",
      "Epoch 0:  47%|████▋     | 243/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 2.0000317096710205\n",
      "Epoch 0:  48%|████▊     | 244/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 2.5358383655548096\n",
      "Epoch 0:  48%|████▊     | 245/512 [00:47<00:51,  5.20it/s, v_num=11]Training loss: 1.735045313835144\n",
      "Epoch 0:  48%|████▊     | 246/512 [00:47<00:51,  5.20it/s, v_num=11]Training loss: 2.164602756500244\n",
      "Epoch 0:  48%|████▊     | 247/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 2.4313297271728516\n",
      "Epoch 0:  48%|████▊     | 248/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 2.3278350830078125\n",
      "Epoch 0:  49%|████▊     | 249/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 10.24742317199707\n",
      "Epoch 0:  49%|████▉     | 250/512 [00:48<00:50,  5.20it/s, v_num=11]Training loss: 1.8929035663604736\n",
      "Epoch 0:  49%|████▉     | 251/512 [00:48<00:50,  5.20it/s, v_num=11]Training loss: 1.9321234226226807\n",
      "Epoch 0:  49%|████▉     | 252/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 1.8110640048980713\n",
      "Epoch 0:  49%|████▉     | 253/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 2.0729117393493652\n",
      "Epoch 0:  50%|████▉     | 254/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 2.148622512817383\n",
      "Epoch 0:  50%|████▉     | 255/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 1.9096994400024414\n",
      "Epoch 0:  50%|█████     | 256/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 3.0897390842437744\n",
      "Epoch 0:  50%|█████     | 257/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 1.9259037971496582\n",
      "Epoch 0:  50%|█████     | 258/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 2.078756809234619\n",
      "Epoch 0:  51%|█████     | 259/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 1.7197811603546143\n",
      "Epoch 0:  51%|█████     | 260/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 1.549728274345398\n",
      "Epoch 0:  51%|█████     | 261/512 [00:50<00:48,  5.20it/s, v_num=11]Training loss: 2.085615634918213\n",
      "Epoch 0:  51%|█████     | 262/512 [00:50<00:48,  5.20it/s, v_num=11]Training loss: 1.6925196647644043\n",
      "Epoch 0:  51%|█████▏    | 263/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.8045032024383545\n",
      "Epoch 0:  52%|█████▏    | 264/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.4991859197616577\n",
      "Epoch 0:  52%|█████▏    | 265/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.7316995859146118\n",
      "Epoch 0:  52%|█████▏    | 266/512 [00:51<00:47,  5.20it/s, v_num=11]Training loss: 2.0657899379730225\n",
      "Epoch 0:  52%|█████▏    | 267/512 [00:51<00:47,  5.20it/s, v_num=11]Training loss: 2.1422066688537598\n",
      "Epoch 0:  52%|█████▏    | 268/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 1.8539232015609741\n",
      "Epoch 0:  53%|█████▎    | 269/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 2.6363253593444824\n",
      "Epoch 0:  53%|█████▎    | 270/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 1.6030690670013428\n",
      "Epoch 0:  53%|█████▎    | 271/512 [00:52<00:46,  5.20it/s, v_num=11]Training loss: 2.043553590774536\n",
      "Epoch 0:  53%|█████▎    | 272/512 [00:52<00:46,  5.20it/s, v_num=11]Training loss: 2.224172830581665\n",
      "Epoch 0:  53%|█████▎    | 273/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 1.6851226091384888\n",
      "Epoch 0:  54%|█████▎    | 274/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 3.1864352226257324\n",
      "Epoch 0:  54%|█████▎    | 275/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 2.580866813659668\n",
      "Epoch 0:  54%|█████▍    | 276/512 [00:53<00:45,  5.20it/s, v_num=11]Training loss: 2.3216464519500732\n",
      "Epoch 0:  54%|█████▍    | 277/512 [00:53<00:45,  5.20it/s, v_num=11]Training loss: 2.1236441135406494\n",
      "Epoch 0:  54%|█████▍    | 278/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 3.4141931533813477\n",
      "Epoch 0:  54%|█████▍    | 279/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 1.9620449542999268\n",
      "Epoch 0:  55%|█████▍    | 280/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 1.4934954643249512\n",
      "Epoch 0:  55%|█████▍    | 281/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 1.5546114444732666\n",
      "Epoch 0:  55%|█████▌    | 282/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 1.489732265472412\n",
      "Epoch 0:  55%|█████▌    | 283/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 2.7791147232055664\n",
      "Epoch 0:  55%|█████▌    | 284/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 2.3656325340270996\n",
      "Epoch 0:  56%|█████▌    | 285/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 2.871079683303833\n",
      "Epoch 0:  56%|█████▌    | 286/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 1.4006763696670532\n",
      "Epoch 0:  56%|█████▌    | 287/512 [00:55<00:43,  5.20it/s, v_num=11]Training loss: 1.542845606803894\n",
      "Epoch 0:  56%|█████▋    | 288/512 [00:55<00:43,  5.20it/s, v_num=11]Training loss: 1.953261137008667\n",
      "Epoch 0:  56%|█████▋    | 289/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 1.6067490577697754\n",
      "Epoch 0:  57%|█████▋    | 290/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 2.399488925933838\n",
      "Epoch 0:  57%|█████▋    | 291/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 3.1224112510681152\n",
      "Epoch 0:  57%|█████▋    | 292/512 [00:56<00:42,  5.20it/s, v_num=11]Training loss: 2.7100374698638916\n",
      "Epoch 0:  57%|█████▋    | 293/512 [00:56<00:42,  5.20it/s, v_num=11]Training loss: 2.0710268020629883\n",
      "Epoch 0:  57%|█████▋    | 294/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.7373573780059814\n",
      "Epoch 0:  58%|█████▊    | 295/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.828322410583496\n",
      "Epoch 0:  58%|█████▊    | 296/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.5544970035552979\n",
      "Epoch 0:  58%|█████▊    | 297/512 [00:57<00:41,  5.20it/s, v_num=11]Training loss: 1.557776927947998\n",
      "Epoch 0:  58%|█████▊    | 298/512 [00:57<00:41,  5.20it/s, v_num=11]Training loss: 1.783240556716919\n",
      "Epoch 0:  58%|█████▊    | 299/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 2.153503179550171\n",
      "Epoch 0:  59%|█████▊    | 300/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 2.9777345657348633\n",
      "Epoch 0:  59%|█████▉    | 301/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 2.29349946975708\n",
      "Epoch 0:  59%|█████▉    | 302/512 [00:58<00:40,  5.20it/s, v_num=11]Training loss: 2.8548431396484375\n",
      "Epoch 0:  59%|█████▉    | 303/512 [00:58<00:40,  5.20it/s, v_num=11]Training loss: 1.9939197301864624\n",
      "Epoch 0:  59%|█████▉    | 304/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 1.4950090646743774\n",
      "Epoch 0:  60%|█████▉    | 305/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 2.0297231674194336\n",
      "Epoch 0:  60%|█████▉    | 306/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 2.055203914642334\n",
      "Epoch 0:  60%|█████▉    | 307/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 2.038105010986328\n",
      "Epoch 0:  60%|██████    | 308/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 1.7756109237670898\n",
      "Epoch 0:  60%|██████    | 309/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 2.625492572784424\n",
      "Epoch 0:  61%|██████    | 310/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 1.5910382270812988\n",
      "Epoch 0:  61%|██████    | 311/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 2.2008233070373535\n",
      "Epoch 0:  61%|██████    | 312/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 1.9455323219299316\n",
      "Epoch 0:  61%|██████    | 313/512 [01:00<00:38,  5.20it/s, v_num=11]Training loss: 1.9643943309783936\n",
      "Epoch 0:  61%|██████▏   | 314/512 [01:00<00:38,  5.20it/s, v_num=11]Training loss: 2.6198318004608154\n",
      "Epoch 0:  62%|██████▏   | 315/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 2.404600143432617\n",
      "Epoch 0:  62%|██████▏   | 316/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 1.9708151817321777\n",
      "Epoch 0:  62%|██████▏   | 317/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 1.5939472913742065\n",
      "Epoch 0:  62%|██████▏   | 318/512 [01:01<00:37,  5.20it/s, v_num=11]Training loss: 1.9990367889404297\n",
      "Epoch 0:  62%|██████▏   | 319/512 [01:01<00:37,  5.20it/s, v_num=11]Training loss: 3.956867218017578\n",
      "Epoch 0:  62%|██████▎   | 320/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 1.913101315498352\n",
      "Epoch 0:  63%|██████▎   | 321/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 1.7865676879882812\n",
      "Epoch 0:  63%|██████▎   | 322/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 2.076441526412964\n",
      "Epoch 0:  63%|██████▎   | 323/512 [01:02<00:36,  5.20it/s, v_num=11]Training loss: 1.9561443328857422\n",
      "Epoch 0:  63%|██████▎   | 324/512 [01:02<00:36,  5.20it/s, v_num=11]Training loss: 1.4335451126098633\n",
      "Epoch 0:  63%|██████▎   | 325/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 2.1099400520324707\n",
      "Epoch 0:  64%|██████▎   | 326/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 2.2882981300354004\n",
      "Epoch 0:  64%|██████▍   | 327/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 2.628917694091797\n",
      "Epoch 0:  64%|██████▍   | 328/512 [01:03<00:35,  5.20it/s, v_num=11]Training loss: 1.7541035413742065\n",
      "Epoch 0:  64%|██████▍   | 329/512 [01:03<00:35,  5.20it/s, v_num=11]Training loss: 2.018089532852173\n",
      "Epoch 0:  64%|██████▍   | 330/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 2.248929023742676\n",
      "Epoch 0:  65%|██████▍   | 331/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 2.0334081649780273\n",
      "Epoch 0:  65%|██████▍   | 332/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 2.0806262493133545\n",
      "Epoch 0:  65%|██████▌   | 333/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 1.8769060373306274\n",
      "Epoch 0:  65%|██████▌   | 334/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 2.0435709953308105\n",
      "Epoch 0:  65%|██████▌   | 335/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 1.5050113201141357\n",
      "Epoch 0:  66%|██████▌   | 336/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 2.1975178718566895\n",
      "Epoch 0:  66%|██████▌   | 337/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 1.975355863571167\n",
      "Epoch 0:  66%|██████▌   | 338/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 2.3915395736694336\n",
      "Epoch 0:  66%|██████▌   | 339/512 [01:05<00:33,  5.20it/s, v_num=11]Training loss: 2.0633976459503174\n",
      "Epoch 0:  66%|██████▋   | 340/512 [01:05<00:33,  5.20it/s, v_num=11]Training loss: 2.070152759552002\n",
      "Epoch 0:  67%|██████▋   | 341/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 1.8881498575210571\n",
      "Epoch 0:  67%|██████▋   | 342/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 1.8267736434936523\n",
      "Epoch 0:  67%|██████▋   | 343/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 1.8015961647033691\n",
      "Epoch 0:  67%|██████▋   | 344/512 [01:06<00:32,  5.20it/s, v_num=11]Training loss: 1.850175142288208\n",
      "Epoch 0:  67%|██████▋   | 345/512 [01:06<00:32,  5.20it/s, v_num=11]Training loss: 1.798313856124878\n",
      "Epoch 0:  68%|██████▊   | 346/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 2.00044322013855\n",
      "Epoch 0:  68%|██████▊   | 347/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 1.8517918586730957\n",
      "Epoch 0:  68%|██████▊   | 348/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 1.2997817993164062\n",
      "Epoch 0:  68%|██████▊   | 349/512 [01:07<00:31,  5.20it/s, v_num=11]Training loss: 2.0114524364471436\n",
      "Epoch 0:  68%|██████▊   | 350/512 [01:07<00:31,  5.20it/s, v_num=11]Training loss: 2.264070987701416\n",
      "Epoch 0:  69%|██████▊   | 351/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 2.051687240600586\n",
      "Epoch 0:  69%|██████▉   | 352/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 1.4765846729278564\n",
      "Epoch 0:  69%|██████▉   | 353/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 1.8093204498291016\n",
      "Epoch 0:  69%|██████▉   | 354/512 [01:08<00:30,  5.20it/s, v_num=11]Training loss: 1.378381371498108\n",
      "Epoch 0:  69%|██████▉   | 355/512 [01:08<00:30,  5.20it/s, v_num=11]Training loss: 2.3683457374572754\n",
      "Epoch 0:  70%|██████▉   | 356/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 1.7886450290679932\n",
      "Epoch 0:  70%|██████▉   | 357/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 1.550445556640625\n",
      "Epoch 0:  70%|██████▉   | 358/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 1.9830379486083984\n",
      "Epoch 0:  70%|███████   | 359/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 1.8081012964248657\n",
      "Epoch 0:  70%|███████   | 360/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 1.9716469049453735\n",
      "Epoch 0:  71%|███████   | 361/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 1.8236711025238037\n",
      "Epoch 0:  71%|███████   | 362/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 1.788750410079956\n",
      "Epoch 0:  71%|███████   | 363/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 1.4500709772109985\n",
      "Epoch 0:  71%|███████   | 364/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 2.2369771003723145\n",
      "Epoch 0:  71%|███████▏  | 365/512 [01:10<00:28,  5.20it/s, v_num=11]Training loss: 2.5296194553375244\n",
      "Epoch 0:  71%|███████▏  | 366/512 [01:10<00:28,  5.20it/s, v_num=11]Training loss: 2.3405601978302\n",
      "Epoch 0:  72%|███████▏  | 367/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 1.9764721393585205\n",
      "Epoch 0:  72%|███████▏  | 368/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 3.1519737243652344\n",
      "Epoch 0:  72%|███████▏  | 369/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 2.170632839202881\n",
      "Epoch 0:  72%|███████▏  | 370/512 [01:11<00:27,  5.20it/s, v_num=11]Training loss: 1.6919004917144775\n",
      "Epoch 0:  72%|███████▏  | 371/512 [01:11<00:27,  5.20it/s, v_num=11]Training loss: 1.7717372179031372\n",
      "Epoch 0:  73%|███████▎  | 372/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 1.856797218322754\n",
      "Epoch 0:  73%|███████▎  | 373/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 1.7178394794464111\n",
      "Epoch 0:  73%|███████▎  | 374/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 2.2122700214385986\n",
      "Epoch 0:  73%|███████▎  | 375/512 [01:12<00:26,  5.20it/s, v_num=11]Training loss: 2.7611141204833984\n",
      "Epoch 0:  73%|███████▎  | 376/512 [01:12<00:26,  5.20it/s, v_num=11]Training loss: 1.4799871444702148\n",
      "Epoch 0:  74%|███████▎  | 377/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 1.584913969039917\n",
      "Epoch 0:  74%|███████▍  | 378/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 2.570291519165039\n",
      "Epoch 0:  74%|███████▍  | 379/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 1.617710828781128\n",
      "Epoch 0:  74%|███████▍  | 380/512 [01:13<00:25,  5.20it/s, v_num=11]Training loss: 1.5792090892791748\n",
      "Epoch 0:  74%|███████▍  | 381/512 [01:13<00:25,  5.20it/s, v_num=11]Training loss: 1.5729591846466064\n",
      "Epoch 0:  75%|███████▍  | 382/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 2.092005968093872\n",
      "Epoch 0:  75%|███████▍  | 383/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 2.22442364692688\n",
      "Epoch 0:  75%|███████▌  | 384/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 2.376723051071167\n",
      "Epoch 0:  75%|███████▌  | 385/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 1.6252424716949463\n",
      "Epoch 0:  75%|███████▌  | 386/512 [01:14<00:24,  5.20it/s, v_num=11]Training loss: 7.216025352478027\n",
      "Epoch 0:  76%|███████▌  | 387/512 [01:14<00:24,  5.20it/s, v_num=11]Training loss: 1.7346367835998535\n",
      "Epoch 0:  76%|███████▌  | 388/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 2.279052734375\n",
      "Epoch 0:  76%|███████▌  | 389/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 2.125164031982422\n",
      "Epoch 0:  76%|███████▌  | 390/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 1.6268481016159058\n",
      "Epoch 0:  76%|███████▋  | 391/512 [01:15<00:23,  5.20it/s, v_num=11]Training loss: 2.5434134006500244\n",
      "Epoch 0:  77%|███████▋  | 392/512 [01:15<00:23,  5.20it/s, v_num=11]Training loss: 1.5589041709899902\n",
      "Epoch 0:  77%|███████▋  | 393/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 3.863538980484009\n",
      "Epoch 0:  77%|███████▋  | 394/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 1.6175471544265747\n",
      "Epoch 0:  77%|███████▋  | 395/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 2.3408408164978027\n",
      "Epoch 0:  77%|███████▋  | 396/512 [01:16<00:22,  5.20it/s, v_num=11]Training loss: 1.8101513385772705\n",
      "Epoch 0:  78%|███████▊  | 397/512 [01:16<00:22,  5.20it/s, v_num=11]Training loss: 2.381010055541992\n",
      "Epoch 0:  78%|███████▊  | 398/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 1.607041358947754\n",
      "Epoch 0:  78%|███████▊  | 399/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 2.0359745025634766\n",
      "Epoch 0:  78%|███████▊  | 400/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 1.2753868103027344\n",
      "Epoch 0:  78%|███████▊  | 401/512 [01:17<00:21,  5.20it/s, v_num=11]Training loss: 1.8204824924468994\n",
      "Epoch 0:  79%|███████▊  | 402/512 [01:17<00:21,  5.20it/s, v_num=11]Training loss: 1.6797969341278076\n",
      "Epoch 0:  79%|███████▊  | 403/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 2.2814598083496094\n",
      "Epoch 0:  79%|███████▉  | 404/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 1.7305576801300049\n",
      "Epoch 0:  79%|███████▉  | 405/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 1.7119266986846924\n",
      "Epoch 0:  79%|███████▉  | 406/512 [01:18<00:20,  5.20it/s, v_num=11]Training loss: 1.4615589380264282\n",
      "Epoch 0:  79%|███████▉  | 407/512 [01:18<00:20,  5.20it/s, v_num=11]Training loss: 2.290001392364502\n",
      "Epoch 0:  80%|███████▉  | 408/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 2.6668596267700195\n",
      "Epoch 0:  80%|███████▉  | 409/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 3.646310806274414\n",
      "Epoch 0:  80%|████████  | 410/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 1.787554144859314\n",
      "Epoch 0:  80%|████████  | 411/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 2.521650791168213\n",
      "Epoch 0:  80%|████████  | 412/512 [01:19<00:19,  5.20it/s, v_num=11]Training loss: 1.8356726169586182\n",
      "Epoch 0:  81%|████████  | 413/512 [01:19<00:19,  5.20it/s, v_num=11]Training loss: 2.337924003601074\n",
      "Epoch 0:  81%|████████  | 414/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.4858407974243164\n",
      "Epoch 0:  81%|████████  | 415/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.3908872604370117\n",
      "Epoch 0:  81%|████████▏ | 416/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.9661787748336792\n",
      "Epoch 0:  81%|████████▏ | 417/512 [01:20<00:18,  5.20it/s, v_num=11]Training loss: 1.432143211364746\n",
      "Epoch 0:  82%|████████▏ | 418/512 [01:20<00:18,  5.20it/s, v_num=11]Training loss: 1.8098679780960083\n",
      "Epoch 0:  82%|████████▏ | 419/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 2.785078287124634\n",
      "Epoch 0:  82%|████████▏ | 420/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 1.6808393001556396\n",
      "Epoch 0:  82%|████████▏ | 421/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 2.6745827198028564\n",
      "Epoch 0:  82%|████████▏ | 422/512 [01:21<00:17,  5.20it/s, v_num=11]Training loss: 1.4563190937042236\n",
      "Epoch 0:  83%|████████▎ | 423/512 [01:21<00:17,  5.20it/s, v_num=11]Training loss: 2.181476593017578\n",
      "Epoch 0:  83%|████████▎ | 424/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 1.8274509906768799\n",
      "Epoch 0:  83%|████████▎ | 425/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 1.7298003435134888\n",
      "Epoch 0:  83%|████████▎ | 426/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 2.509474515914917\n",
      "Epoch 0:  83%|████████▎ | 427/512 [01:22<00:16,  5.20it/s, v_num=11]Training loss: 1.843928337097168\n",
      "Epoch 0:  84%|████████▎ | 428/512 [01:22<00:16,  5.20it/s, v_num=11]Training loss: 1.4622995853424072\n",
      "Epoch 0:  84%|████████▍ | 429/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 2.207416296005249\n",
      "Epoch 0:  84%|████████▍ | 430/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 2.183196544647217\n",
      "Epoch 0:  84%|████████▍ | 431/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 1.765127182006836\n",
      "Epoch 0:  84%|████████▍ | 432/512 [01:23<00:15,  5.20it/s, v_num=11]Training loss: 1.3785676956176758\n",
      "Epoch 0:  85%|████████▍ | 433/512 [01:23<00:15,  5.20it/s, v_num=11]Training loss: 1.717704176902771\n",
      "Epoch 0:  85%|████████▍ | 434/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 2.642582416534424\n",
      "Epoch 0:  85%|████████▍ | 435/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 1.3286057710647583\n",
      "Epoch 0:  85%|████████▌ | 436/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 1.6903969049453735\n",
      "Epoch 0:  85%|████████▌ | 437/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 2.0184781551361084\n",
      "Epoch 0:  86%|████████▌ | 438/512 [01:24<00:14,  5.20it/s, v_num=11]Training loss: 1.9585413932800293\n",
      "Epoch 0:  86%|████████▌ | 439/512 [01:24<00:14,  5.20it/s, v_num=11]Training loss: 1.6976993083953857\n",
      "Epoch 0:  86%|████████▌ | 440/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 2.657600164413452\n",
      "Epoch 0:  86%|████████▌ | 441/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 1.857867956161499\n",
      "Epoch 0:  86%|████████▋ | 442/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 2.402505397796631\n",
      "Epoch 0:  87%|████████▋ | 443/512 [01:25<00:13,  5.20it/s, v_num=11]Training loss: 2.7330384254455566\n",
      "Epoch 0:  87%|████████▋ | 444/512 [01:25<00:13,  5.20it/s, v_num=11]Training loss: 1.7264635562896729\n",
      "Epoch 0:  87%|████████▋ | 445/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 2.068960428237915\n",
      "Epoch 0:  87%|████████▋ | 446/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 1.9008421897888184\n",
      "Epoch 0:  87%|████████▋ | 447/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 1.5217169523239136\n",
      "Epoch 0:  88%|████████▊ | 448/512 [01:26<00:12,  5.20it/s, v_num=11]Training loss: 6.122566223144531\n",
      "Epoch 0:  88%|████████▊ | 449/512 [01:26<00:12,  5.20it/s, v_num=11]Training loss: 2.450392484664917\n",
      "Epoch 0:  88%|████████▊ | 450/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 1.8675202131271362\n",
      "Epoch 0:  88%|████████▊ | 451/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 3.11049747467041\n",
      "Epoch 0:  88%|████████▊ | 452/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 2.861121654510498\n",
      "Epoch 0:  88%|████████▊ | 453/512 [01:27<00:11,  5.20it/s, v_num=11]Training loss: 1.7864367961883545\n",
      "Epoch 0:  89%|████████▊ | 454/512 [01:27<00:11,  5.20it/s, v_num=11]Training loss: 1.6503076553344727\n",
      "Epoch 0:  89%|████████▉ | 455/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 2.440216541290283\n",
      "Epoch 0:  89%|████████▉ | 456/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 2.105771541595459\n",
      "Epoch 0:  89%|████████▉ | 457/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 2.222811698913574\n",
      "Epoch 0:  89%|████████▉ | 458/512 [01:28<00:10,  5.20it/s, v_num=11]Training loss: 1.6510365009307861\n",
      "Epoch 0:  90%|████████▉ | 459/512 [01:28<00:10,  5.20it/s, v_num=11]Training loss: 1.9125893115997314\n",
      "Epoch 0:  90%|████████▉ | 460/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 1.584838628768921\n",
      "Epoch 0:  90%|█████████ | 461/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 2.0280344486236572\n",
      "Epoch 0:  90%|█████████ | 462/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 1.7442452907562256\n",
      "Epoch 0:  90%|█████████ | 463/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 2.1656031608581543\n",
      "Epoch 0:  91%|█████████ | 464/512 [01:29<00:09,  5.20it/s, v_num=11]Training loss: 1.639899492263794\n",
      "Epoch 0:  91%|█████████ | 465/512 [01:29<00:09,  5.20it/s, v_num=11]Training loss: 1.9379419088363647\n",
      "Epoch 0:  91%|█████████ | 466/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 1.6062932014465332\n",
      "Epoch 0:  91%|█████████ | 467/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 1.9607911109924316\n",
      "Epoch 0:  91%|█████████▏| 468/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 2.2289657592773438\n",
      "Epoch 0:  92%|█████████▏| 469/512 [01:30<00:08,  5.20it/s, v_num=11]Training loss: 1.4527373313903809\n",
      "Epoch 0:  92%|█████████▏| 470/512 [01:30<00:08,  5.20it/s, v_num=11]Training loss: 1.9998223781585693\n",
      "Epoch 0:  92%|█████████▏| 471/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 2.1113088130950928\n",
      "Epoch 0:  92%|█████████▏| 472/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 1.9713854789733887\n",
      "Epoch 0:  92%|█████████▏| 473/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 1.6139572858810425\n",
      "Epoch 0:  93%|█████████▎| 474/512 [01:31<00:07,  5.20it/s, v_num=11]Training loss: 1.738743782043457\n",
      "Epoch 0:  93%|█████████▎| 475/512 [01:31<00:07,  5.20it/s, v_num=11]Training loss: 2.671548843383789\n",
      "Epoch 0:  93%|█████████▎| 476/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 1.9375817775726318\n",
      "Epoch 0:  93%|█████████▎| 477/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 2.1354010105133057\n",
      "Epoch 0:  93%|█████████▎| 478/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 1.8380422592163086\n",
      "Epoch 0:  94%|█████████▎| 479/512 [01:32<00:06,  5.20it/s, v_num=11]Training loss: 2.0413565635681152\n",
      "Epoch 0:  94%|█████████▍| 480/512 [01:32<00:06,  5.20it/s, v_num=11]Training loss: 2.444873809814453\n",
      "Epoch 0:  94%|█████████▍| 481/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 1.5578868389129639\n",
      "Epoch 0:  94%|█████████▍| 482/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 2.1272521018981934\n",
      "Epoch 0:  94%|█████████▍| 483/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 2.411519765853882\n",
      "Epoch 0:  95%|█████████▍| 484/512 [01:33<00:05,  5.20it/s, v_num=11]Training loss: 2.0568408966064453\n",
      "Epoch 0:  95%|█████████▍| 485/512 [01:33<00:05,  5.20it/s, v_num=11]Training loss: 2.8161163330078125\n",
      "Epoch 0:  95%|█████████▍| 486/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 1.6537787914276123\n",
      "Epoch 0:  95%|█████████▌| 487/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 2.03901743888855\n",
      "Epoch 0:  95%|█████████▌| 488/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 2.887291669845581\n",
      "Epoch 0:  96%|█████████▌| 489/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 2.1988472938537598\n",
      "Epoch 0:  96%|█████████▌| 490/512 [01:34<00:04,  5.20it/s, v_num=11]Training loss: 1.8197585344314575\n",
      "Epoch 0:  96%|█████████▌| 491/512 [01:34<00:04,  5.20it/s, v_num=11]Training loss: 2.6846752166748047\n",
      "Epoch 0:  96%|█████████▌| 492/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 1.9214396476745605\n",
      "Epoch 0:  96%|█████████▋| 493/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 1.9618812799453735\n",
      "Epoch 0:  96%|█████████▋| 494/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 1.7796076536178589\n",
      "Epoch 0:  97%|█████████▋| 495/512 [01:35<00:03,  5.20it/s, v_num=11]Training loss: 1.8305308818817139\n",
      "Epoch 0:  97%|█████████▋| 496/512 [01:35<00:03,  5.20it/s, v_num=11]Training loss: 1.629806637763977\n",
      "Epoch 0:  97%|█████████▋| 497/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 1.9434410333633423\n",
      "Epoch 0:  97%|█████████▋| 498/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 2.1586496829986572\n",
      "Epoch 0:  97%|█████████▋| 499/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 1.7391083240509033\n",
      "Epoch 0:  98%|█████████▊| 500/512 [01:36<00:02,  5.20it/s, v_num=11]Training loss: 1.618520975112915\n",
      "Epoch 0:  98%|█████████▊| 501/512 [01:36<00:02,  5.20it/s, v_num=11]Training loss: 2.034832000732422\n",
      "Epoch 0:  98%|█████████▊| 502/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.9225603342056274\n",
      "Epoch 0:  98%|█████████▊| 503/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.919732689857483\n",
      "Epoch 0:  98%|█████████▊| 504/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.8786530494689941\n",
      "Epoch 0:  99%|█████████▊| 505/512 [01:37<00:01,  5.20it/s, v_num=11]Training loss: 1.7015821933746338\n",
      "Epoch 0:  99%|█████████▉| 506/512 [01:37<00:01,  5.20it/s, v_num=11]Training loss: 2.516879081726074\n",
      "Epoch 0:  99%|█████████▉| 507/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 1.4912195205688477\n",
      "Epoch 0:  99%|█████████▉| 508/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 2.0013184547424316\n",
      "Epoch 0:  99%|█████████▉| 509/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 2.045653820037842\n",
      "Epoch 0: 100%|█████████▉| 510/512 [01:38<00:00,  5.20it/s, v_num=11]Training loss: 3.437472343444824\n",
      "Epoch 0: 100%|█████████▉| 511/512 [01:38<00:00,  5.20it/s, v_num=11]Training loss: 1.331262230873108\n",
      "Epoch 0: 100%|██████████| 512/512 [01:38<00:00,  5.20it/s, v_num=11]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 3.280782461166382\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:02, 49.16it/s]\u001b[AValidation loss: 1.6873259544372559\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:02, 39.46it/s]\u001b[AValidation loss: 1.9775679111480713\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:03, 35.21it/s]\u001b[AValidation loss: 1.6448919773101807\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:03, 33.31it/s]\u001b[AValidation loss: 1.5286903381347656\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:03, 32.05it/s]\u001b[AValidation loss: 2.2390265464782715\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:03, 31.44it/s]\u001b[AValidation loss: 2.3711185455322266\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:03, 31.60it/s]\u001b[AValidation loss: 1.7049884796142578\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:03, 31.97it/s]\u001b[AValidation loss: 2.391754627227783\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:03, 32.10it/s]\u001b[AValidation loss: 1.6911582946777344\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:03, 32.36it/s]\u001b[AValidation loss: 1.6469424962997437\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:03, 32.42it/s]\u001b[AValidation loss: 1.7190451622009277\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:03, 32.48it/s]\u001b[AValidation loss: 1.8337337970733643\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:02, 32.52it/s]\u001b[AValidation loss: 1.58988618850708\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:02, 32.51it/s]\u001b[AValidation loss: 1.6818028688430786\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:02, 32.12it/s]\u001b[AValidation loss: 1.5379054546356201\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:02, 32.16it/s]\u001b[AValidation loss: 1.9056408405303955\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:02, 32.31it/s]\u001b[AValidation loss: 1.735410213470459\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:02, 32.36it/s]\u001b[AValidation loss: 1.8471548557281494\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:02, 32.49it/s]\u001b[AValidation loss: 2.0628347396850586\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:02, 32.52it/s]\u001b[AValidation loss: 2.1023435592651367\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:02, 32.64it/s]\u001b[AValidation loss: 1.64713454246521\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:02, 32.66it/s]\u001b[AValidation loss: 1.994653344154358\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:00<00:02, 32.75it/s]\u001b[AValidation loss: 2.089224100112915\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:00<00:02, 32.77it/s]\u001b[AValidation loss: 1.6638214588165283\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:00<00:02, 32.86it/s]\u001b[AValidation loss: 2.0663278102874756\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:00<00:02, 32.87it/s]\u001b[AValidation loss: 1.5420001745224\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:00<00:02, 32.95it/s]\u001b[AValidation loss: 2.400071144104004\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:00<00:02, 32.96it/s]\u001b[AValidation loss: 2.223221778869629\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:00<00:02, 33.03it/s]\u001b[AValidation loss: 2.6544392108917236\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:00<00:02, 33.04it/s]\u001b[AValidation loss: 1.441694736480713\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:00<00:02, 33.11it/s]\u001b[AValidation loss: 1.924595594406128\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:00<00:02, 33.12it/s]\u001b[AValidation loss: 2.2678709030151367\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:00<00:02, 33.17it/s]\u001b[AValidation loss: 12.053603172302246\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:02, 33.18it/s]\u001b[AValidation loss: 1.7893476486206055\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:02, 33.23it/s]\u001b[AValidation loss: 1.2714521884918213\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:02, 33.23it/s]\u001b[AValidation loss: 2.1914193630218506\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:02, 33.28it/s]\u001b[AValidation loss: 1.9252116680145264\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:02, 33.29it/s]\u001b[AValidation loss: 1.323591709136963\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:02, 33.33it/s]\u001b[AValidation loss: 2.2112345695495605\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:02, 33.34it/s]\u001b[AValidation loss: 1.7978744506835938\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:02, 33.37it/s]\u001b[AValidation loss: 1.7999374866485596\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:02, 33.37it/s]\u001b[AValidation loss: 1.4805264472961426\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 33.41it/s]\u001b[AValidation loss: 2.547778367996216\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:01, 33.41it/s]\u001b[AValidation loss: 2.1848254203796387\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:01<00:01, 33.44it/s]\u001b[AValidation loss: 1.9031767845153809\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:01<00:01, 33.44it/s]\u001b[AValidation loss: 2.049391746520996\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:01<00:01, 33.48it/s]\u001b[AValidation loss: 2.2815680503845215\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:01<00:01, 33.47it/s]\u001b[AValidation loss: 3.146547794342041\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:01<00:01, 33.50it/s]\u001b[AValidation loss: 1.8946205377578735\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:01<00:01, 33.50it/s]\u001b[AValidation loss: 1.6699355840682983\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:01<00:01, 33.53it/s]\u001b[AValidation loss: 1.5052202939987183\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:01<00:01, 33.53it/s]\u001b[AValidation loss: 1.860750436782837\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:01<00:01, 33.56it/s]\u001b[AValidation loss: 2.125422954559326\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:01<00:01, 33.55it/s]\u001b[AValidation loss: 1.810131549835205\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:01<00:01, 33.58it/s]\u001b[AValidation loss: 2.1486549377441406\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:01<00:01, 33.58it/s]\u001b[AValidation loss: 2.2182161808013916\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:01<00:01, 33.60it/s]\u001b[AValidation loss: 1.7637189626693726\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:01<00:01, 33.60it/s]\u001b[AValidation loss: 1.7374153137207031\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:01<00:01, 33.62it/s]\u001b[AValidation loss: 2.006009578704834\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:01<00:01, 33.62it/s]\u001b[AValidation loss: 1.7832751274108887\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:01<00:01, 33.64it/s]\u001b[AValidation loss: 2.1164638996124268\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:01<00:01, 33.64it/s]\u001b[AValidation loss: 1.7076151371002197\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:01<00:01, 33.66it/s]\u001b[AValidation loss: 1.898288607597351\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:01<00:01, 33.66it/s]\u001b[AValidation loss: 1.6250221729278564\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:01<00:01, 33.68it/s]\u001b[AValidation loss: 2.7750606536865234\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:01<00:01, 33.68it/s]\u001b[AValidation loss: 1.4496744871139526\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:01<00:01, 33.70it/s]\u001b[AValidation loss: 1.7948567867279053\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:02<00:01, 33.69it/s]\u001b[AValidation loss: 2.5251948833465576\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:02<00:01, 33.71it/s]\u001b[AValidation loss: 1.922477126121521\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:02<00:01, 33.71it/s]\u001b[AValidation loss: 1.5835009813308716\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:02<00:01, 33.73it/s]\u001b[AValidation loss: 2.005486488342285\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:02<00:01, 33.73it/s]\u001b[AValidation loss: 2.264913558959961\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:02<00:01, 33.74it/s]\u001b[AValidation loss: 2.221412181854248\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:02<00:01, 33.74it/s]\u001b[AValidation loss: 2.872159242630005\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:02<00:01, 33.76it/s]\u001b[AValidation loss: 2.397376775741577\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:02<00:01, 33.75it/s]\u001b[AValidation loss: 1.9934406280517578\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 2.0911777019500732\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:02<00:00, 33.73it/s]\u001b[AValidation loss: 1.8101768493652344\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:02<00:00, 33.74it/s]\u001b[AValidation loss: 1.825951337814331\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:02<00:00, 33.74it/s]\u001b[AValidation loss: 1.7381532192230225\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:02<00:00, 33.76it/s]\u001b[AValidation loss: 1.998267650604248\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:02<00:00, 33.75it/s]\u001b[AValidation loss: 1.5598102807998657\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:02<00:00, 33.76it/s]\u001b[AValidation loss: 1.9630534648895264\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:02<00:00, 33.76it/s]\u001b[AValidation loss: 2.080368757247925\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 1.8720037937164307\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 1.8669391870498657\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 2.056215286254883\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 1.8188146352767944\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:02<00:00, 33.78it/s]\u001b[AValidation loss: 3.092989444732666\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:02<00:00, 33.77it/s]\u001b[AValidation loss: 1.9220106601715088\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:02<00:00, 33.78it/s]\u001b[AValidation loss: 2.0487098693847656\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:02<00:00, 33.78it/s]\u001b[AValidation loss: 3.6807074546813965\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:02<00:00, 33.79it/s]\u001b[AValidation loss: 1.8892872333526611\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:02<00:00, 33.79it/s]\u001b[AValidation loss: 2.3124985694885254\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:02<00:00, 33.80it/s]\u001b[AValidation loss: 1.4478182792663574\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:02<00:00, 33.80it/s]\u001b[AValidation loss: 1.656514048576355\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:02<00:00, 33.81it/s]\u001b[AValidation loss: 1.9786573648452759\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:02<00:00, 33.80it/s]\u001b[AValidation loss: 1.7463581562042236\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:02<00:00, 33.81it/s]\u001b[AValidation loss: 1.5026874542236328\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:02<00:00, 33.81it/s]\u001b[AValidation loss: 2.326052665710449\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:02<00:00, 33.82it/s]\u001b[AValidation loss: 1.6992484331130981\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:03<00:00, 33.82it/s]\u001b[AValidation loss: 1.373553991317749\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:03<00:00, 33.83it/s]\u001b[AValidation loss: 2.1197361946105957\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:03<00:00, 33.83it/s]\u001b[AValidation loss: 1.9214136600494385\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:03<00:00, 33.84it/s]\u001b[AValidation loss: 2.2591183185577393\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:03<00:00, 33.84it/s]\u001b[AValidation loss: 2.0569653511047363\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:03<00:00, 33.85it/s]\u001b[AValidation loss: 1.7022433280944824\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:03<00:00, 33.84it/s]\u001b[AValidation loss: 2.142143726348877\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:03<00:00, 33.85it/s]\u001b[AValidation loss: 3.4053192138671875\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:03<00:00, 33.89it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/512 [00:00<?, ?it/s, v_num=11]                \u001b[ATraining loss: 1.6450309753417969\n",
      "Epoch 1:   0%|          | 1/512 [00:00<00:22, 22.58it/s, v_num=11]Training loss: 2.0286717414855957\n",
      "Epoch 1:   0%|          | 2/512 [00:00<01:01,  8.35it/s, v_num=11]Training loss: 1.937097191810608\n",
      "Epoch 1:   1%|          | 3/512 [00:00<01:13,  6.97it/s, v_num=11]Training loss: 1.470523476600647\n",
      "Epoch 1:   1%|          | 4/512 [00:00<01:19,  6.42it/s, v_num=11]Training loss: 1.4361026287078857\n",
      "Epoch 1:   1%|          | 5/512 [00:00<01:22,  6.13it/s, v_num=11]Training loss: 1.6014180183410645\n",
      "Epoch 1:   1%|          | 6/512 [00:01<01:24,  5.95it/s, v_num=11]Training loss: 1.258589267730713\n",
      "Epoch 1:   1%|▏         | 7/512 [00:01<01:26,  5.83it/s, v_num=11]Training loss: 2.1352901458740234\n",
      "Epoch 1:   2%|▏         | 8/512 [00:01<01:27,  5.74it/s, v_num=11]Training loss: 1.945673942565918\n",
      "Epoch 1:   2%|▏         | 9/512 [00:01<01:28,  5.68it/s, v_num=11]Training loss: 2.052536725997925\n",
      "Epoch 1:   2%|▏         | 10/512 [00:01<01:29,  5.63it/s, v_num=11]Training loss: 1.4844365119934082\n",
      "Epoch 1:   2%|▏         | 11/512 [00:01<01:29,  5.58it/s, v_num=11]Training loss: 1.3563472032546997\n",
      "Epoch 1:   2%|▏         | 12/512 [00:02<01:30,  5.54it/s, v_num=11]Training loss: 1.6841068267822266\n",
      "Epoch 1:   3%|▎         | 13/512 [00:02<01:30,  5.52it/s, v_num=11]Training loss: 2.5651533603668213\n",
      "Epoch 1:   3%|▎         | 14/512 [00:02<01:30,  5.49it/s, v_num=11]Training loss: 2.3782601356506348\n",
      "Epoch 1:   3%|▎         | 15/512 [00:02<01:30,  5.47it/s, v_num=11]Training loss: 2.086972236633301\n",
      "Epoch 1:   3%|▎         | 16/512 [00:02<01:30,  5.45it/s, v_num=11]Training loss: 1.6441559791564941\n",
      "Epoch 1:   3%|▎         | 17/512 [00:03<01:31,  5.44it/s, v_num=11]Training loss: 2.0738062858581543\n",
      "Epoch 1:   4%|▎         | 18/512 [00:03<01:31,  5.42it/s, v_num=11]Training loss: 1.191893219947815\n",
      "Epoch 1:   4%|▎         | 19/512 [00:03<01:31,  5.41it/s, v_num=11]Training loss: 1.374610424041748\n",
      "Epoch 1:   4%|▍         | 20/512 [00:03<01:31,  5.39it/s, v_num=11]Training loss: 1.6296370029449463\n",
      "Epoch 1:   4%|▍         | 21/512 [00:03<01:31,  5.39it/s, v_num=11]Training loss: 2.062387228012085\n",
      "Epoch 1:   4%|▍         | 22/512 [00:04<01:31,  5.38it/s, v_num=11]Training loss: 1.3882603645324707\n",
      "Epoch 1:   4%|▍         | 23/512 [00:04<01:31,  5.37it/s, v_num=11]Training loss: 1.1721811294555664\n",
      "Epoch 1:   5%|▍         | 24/512 [00:04<01:31,  5.36it/s, v_num=11]Training loss: 1.8151191473007202\n",
      "Epoch 1:   5%|▍         | 25/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 1.4447646141052246\n",
      "Epoch 1:   5%|▌         | 26/512 [00:04<01:30,  5.35it/s, v_num=11]Training loss: 1.2962746620178223\n",
      "Epoch 1:   5%|▌         | 27/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.85547935962677\n",
      "Epoch 1:   5%|▌         | 28/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.717264175415039\n",
      "Epoch 1:   6%|▌         | 29/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.836706519126892\n",
      "Epoch 1:   6%|▌         | 30/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 2.3459088802337646\n",
      "Epoch 1:   6%|▌         | 31/512 [00:05<01:30,  5.32it/s, v_num=11]Training loss: 2.125816583633423\n",
      "Epoch 1:   6%|▋         | 32/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 2.390126943588257\n",
      "Epoch 1:   6%|▋         | 33/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 1.8431018590927124\n",
      "Epoch 1:   7%|▋         | 34/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 2.507626533508301\n",
      "Epoch 1:   7%|▋         | 35/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.7191615104675293\n",
      "Epoch 1:   7%|▋         | 36/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.359900712966919\n",
      "Epoch 1:   7%|▋         | 37/512 [00:06<01:29,  5.30it/s, v_num=11]Training loss: 1.7696409225463867\n",
      "Epoch 1:   7%|▋         | 38/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 2.4195916652679443\n",
      "Epoch 1:   8%|▊         | 39/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.6283098459243774\n",
      "Epoch 1:   8%|▊         | 40/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.3161202669143677\n",
      "Epoch 1:   8%|▊         | 41/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 1.582080602645874\n",
      "Epoch 1:   8%|▊         | 42/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 1.4728386402130127\n",
      "Epoch 1:   8%|▊         | 43/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 2.218299627304077\n",
      "Epoch 1:   9%|▊         | 44/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.666729211807251\n",
      "Epoch 1:   9%|▉         | 45/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.6603299379348755\n",
      "Epoch 1:   9%|▉         | 46/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 1.3818491697311401\n",
      "Epoch 1:   9%|▉         | 47/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 2.0042998790740967\n",
      "Epoch 1:   9%|▉         | 48/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 2.162590503692627\n",
      "Epoch 1:  10%|▉         | 49/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 2.3189539909362793\n",
      "Epoch 1:  10%|▉         | 50/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 2.6719722747802734\n",
      "Epoch 1:  10%|▉         | 51/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.9156533479690552\n",
      "Epoch 1:  10%|█         | 52/512 [00:09<01:27,  5.27it/s, v_num=11]Training loss: 1.5436742305755615\n",
      "Epoch 1:  10%|█         | 53/512 [00:10<01:27,  5.27it/s, v_num=11]Training loss: 1.6675848960876465\n",
      "Epoch 1:  11%|█         | 54/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.711735725402832\n",
      "Epoch 1:  11%|█         | 55/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 3.480696678161621\n",
      "Epoch 1:  11%|█         | 56/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 2.041104793548584\n",
      "Epoch 1:  11%|█         | 57/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.614431619644165\n",
      "Epoch 1:  11%|█▏        | 58/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 1.4195480346679688\n",
      "Epoch 1:  12%|█▏        | 59/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 1.6254749298095703\n",
      "Epoch 1:  12%|█▏        | 60/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.8271293640136719\n",
      "Epoch 1:  12%|█▏        | 61/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.4945974349975586\n",
      "Epoch 1:  12%|█▏        | 62/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.2857402563095093\n",
      "Epoch 1:  12%|█▏        | 63/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 2.6354119777679443\n",
      "Epoch 1:  12%|█▎        | 64/512 [00:12<01:25,  5.26it/s, v_num=11]Training loss: 1.223968267440796\n",
      "Epoch 1:  13%|█▎        | 65/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.6448522806167603\n",
      "Epoch 1:  13%|█▎        | 66/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.688197374343872\n",
      "Epoch 1:  13%|█▎        | 67/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.6100691556930542\n",
      "Epoch 1:  13%|█▎        | 68/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.5807905197143555\n",
      "Epoch 1:  13%|█▎        | 69/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 1.664177656173706\n",
      "Epoch 1:  14%|█▎        | 70/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 2.0731546878814697\n",
      "Epoch 1:  14%|█▍        | 71/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.9672632217407227\n",
      "Epoch 1:  14%|█▍        | 72/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.4825279712677002\n",
      "Epoch 1:  14%|█▍        | 73/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.9103097915649414\n",
      "Epoch 1:  14%|█▍        | 74/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.9726330041885376\n",
      "Epoch 1:  15%|█▍        | 75/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.9368255138397217\n",
      "Epoch 1:  15%|█▍        | 76/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.930560827255249\n",
      "Epoch 1:  15%|█▌        | 77/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 2.2989296913146973\n",
      "Epoch 1:  15%|█▌        | 78/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.7921520471572876\n",
      "Epoch 1:  15%|█▌        | 79/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.77262544631958\n",
      "Epoch 1:  16%|█▌        | 80/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 2.0645480155944824\n",
      "Epoch 1:  16%|█▌        | 81/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.5586594343185425\n",
      "Epoch 1:  16%|█▌        | 82/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.1155487298965454\n",
      "Epoch 1:  16%|█▌        | 83/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.9311994314193726\n",
      "Epoch 1:  16%|█▋        | 84/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.5204260349273682\n",
      "Epoch 1:  17%|█▋        | 85/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.2992231845855713\n",
      "Epoch 1:  17%|█▋        | 86/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.9232044219970703\n",
      "Epoch 1:  17%|█▋        | 87/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.3218951225280762\n",
      "Epoch 1:  17%|█▋        | 88/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.847153902053833\n",
      "Epoch 1:  17%|█▋        | 89/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.1204910278320312\n",
      "Epoch 1:  18%|█▊        | 90/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 2.768444538116455\n",
      "Epoch 1:  18%|█▊        | 91/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.859588623046875\n",
      "Epoch 1:  18%|█▊        | 92/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.2785956859588623\n",
      "Epoch 1:  18%|█▊        | 93/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 2.0081076622009277\n",
      "Epoch 1:  18%|█▊        | 94/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 2.0943334102630615\n",
      "Epoch 1:  19%|█▊        | 95/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 2.0038328170776367\n",
      "Epoch 1:  19%|█▉        | 96/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.2205477952957153\n",
      "Epoch 1:  19%|█▉        | 97/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 2.1272616386413574\n",
      "Epoch 1:  19%|█▉        | 98/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.2808657884597778\n",
      "Epoch 1:  19%|█▉        | 99/512 [00:18<01:18,  5.24it/s, v_num=11]Training loss: 2.164555072784424\n",
      "Epoch 1:  20%|█▉        | 100/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 2.2977054119110107\n",
      "Epoch 1:  20%|█▉        | 101/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 2.310818910598755\n",
      "Epoch 1:  20%|█▉        | 102/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 2.315186023712158\n",
      "Epoch 1:  20%|██        | 103/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.6313928365707397\n",
      "Epoch 1:  20%|██        | 104/512 [00:19<01:17,  5.23it/s, v_num=11]Training loss: 1.7834718227386475\n",
      "Epoch 1:  21%|██        | 105/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 1.841521978378296\n",
      "Epoch 1:  21%|██        | 106/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.112886428833008\n",
      "Epoch 1:  21%|██        | 107/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 2.1940298080444336\n",
      "Epoch 1:  21%|██        | 108/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 1.6712573766708374\n",
      "Epoch 1:  21%|██▏       | 109/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 1.723447561264038\n",
      "Epoch 1:  21%|██▏       | 110/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.11743426322937\n",
      "Epoch 1:  22%|██▏       | 111/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.7662221193313599\n",
      "Epoch 1:  22%|██▏       | 112/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.0621719360351562\n",
      "Epoch 1:  22%|██▏       | 113/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.327359914779663\n",
      "Epoch 1:  22%|██▏       | 114/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.029362201690674\n",
      "Epoch 1:  22%|██▏       | 115/512 [00:21<01:15,  5.23it/s, v_num=11]Training loss: 2.1127724647521973\n",
      "Epoch 1:  23%|██▎       | 116/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.6875267028808594\n",
      "Epoch 1:  23%|██▎       | 117/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.011989116668701\n",
      "Epoch 1:  23%|██▎       | 118/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.545960783958435\n",
      "Epoch 1:  23%|██▎       | 119/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.9918588399887085\n",
      "Epoch 1:  23%|██▎       | 120/512 [00:22<01:14,  5.23it/s, v_num=11]Training loss: 2.2350194454193115\n",
      "Epoch 1:  24%|██▎       | 121/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 2.4879813194274902\n",
      "Epoch 1:  24%|██▍       | 122/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 2.0230252742767334\n",
      "Epoch 1:  24%|██▍       | 123/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.6517821550369263\n",
      "Epoch 1:  24%|██▍       | 124/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 2.318349599838257\n",
      "Epoch 1:  24%|██▍       | 125/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.9207478761672974\n",
      "Epoch 1:  25%|██▍       | 126/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.9064029455184937\n",
      "Epoch 1:  25%|██▍       | 127/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.7160758972167969\n",
      "Epoch 1:  25%|██▌       | 128/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.7102882862091064\n",
      "Epoch 1:  25%|██▌       | 129/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5002578496932983\n",
      "Epoch 1:  25%|██▌       | 130/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.1161952018737793\n",
      "Epoch 1:  26%|██▌       | 131/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.848242998123169\n",
      "Epoch 1:  26%|██▌       | 132/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 2.4587411880493164\n",
      "Epoch 1:  26%|██▌       | 133/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.621298909187317\n",
      "Epoch 1:  26%|██▌       | 134/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.6641967296600342\n",
      "Epoch 1:  26%|██▋       | 135/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.5672541856765747\n",
      "Epoch 1:  27%|██▋       | 136/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.9707868099212646\n",
      "Epoch 1:  27%|██▋       | 137/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.6268274784088135\n",
      "Epoch 1:  27%|██▋       | 138/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.7654093503952026\n",
      "Epoch 1:  27%|██▋       | 139/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.3272981643676758\n",
      "Epoch 1:  27%|██▋       | 140/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 2.3308799266815186\n",
      "Epoch 1:  28%|██▊       | 141/512 [00:26<01:10,  5.23it/s, v_num=11]Training loss: 1.8397867679595947\n",
      "Epoch 1:  28%|██▊       | 142/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.6090340614318848\n",
      "Epoch 1:  28%|██▊       | 143/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 2.274256706237793\n",
      "Epoch 1:  28%|██▊       | 144/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.5412812232971191\n",
      "Epoch 1:  28%|██▊       | 145/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.7469021081924438\n",
      "Epoch 1:  29%|██▊       | 146/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.0265274047851562\n",
      "Epoch 1:  29%|██▊       | 147/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.6071678400039673\n",
      "Epoch 1:  29%|██▉       | 148/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.7687053680419922\n",
      "Epoch 1:  29%|██▉       | 149/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.8100435733795166\n",
      "Epoch 1:  29%|██▉       | 150/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.7249573469161987\n",
      "Epoch 1:  29%|██▉       | 151/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.6912806034088135\n",
      "Epoch 1:  30%|██▉       | 152/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 2.577730178833008\n",
      "Epoch 1:  30%|██▉       | 153/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.0464510917663574\n",
      "Epoch 1:  30%|███       | 154/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.9504135847091675\n",
      "Epoch 1:  30%|███       | 155/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.4418489933013916\n",
      "Epoch 1:  30%|███       | 156/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.648792028427124\n",
      "Epoch 1:  31%|███       | 157/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.3859338760375977\n",
      "Epoch 1:  31%|███       | 158/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.9716179370880127\n",
      "Epoch 1:  31%|███       | 159/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.3502838611602783\n",
      "Epoch 1:  31%|███▏      | 160/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.5260090827941895\n",
      "Epoch 1:  31%|███▏      | 161/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.7207558155059814\n",
      "Epoch 1:  32%|███▏      | 162/512 [00:31<01:07,  5.22it/s, v_num=11]Training loss: 1.7126134634017944\n",
      "Epoch 1:  32%|███▏      | 163/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.7967920303344727\n",
      "Epoch 1:  32%|███▏      | 164/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 2.0350685119628906\n",
      "Epoch 1:  32%|███▏      | 165/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.9749610424041748\n",
      "Epoch 1:  32%|███▏      | 166/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.4301934242248535\n",
      "Epoch 1:  33%|███▎      | 167/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.905964970588684\n",
      "Epoch 1:  33%|███▎      | 168/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.5856497287750244\n",
      "Epoch 1:  33%|███▎      | 169/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.5146188735961914\n",
      "Epoch 1:  33%|███▎      | 170/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.5895617008209229\n",
      "Epoch 1:  33%|███▎      | 171/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.251144289970398\n",
      "Epoch 1:  34%|███▎      | 172/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 2.4106597900390625\n",
      "Epoch 1:  34%|███▍      | 173/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.3787734508514404\n",
      "Epoch 1:  34%|███▍      | 174/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.392104387283325\n",
      "Epoch 1:  34%|███▍      | 175/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.3283183574676514\n",
      "Epoch 1:  34%|███▍      | 176/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 76.02826690673828\n",
      "Epoch 1:  35%|███▍      | 177/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.1214044094085693\n",
      "Epoch 1:  35%|███▍      | 178/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 2.510730266571045\n",
      "Epoch 1:  35%|███▍      | 179/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 2.133842945098877\n",
      "Epoch 1:  35%|███▌      | 180/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.878319263458252\n",
      "Epoch 1:  35%|███▌      | 181/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.694401502609253\n",
      "Epoch 1:  36%|███▌      | 182/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 36.657894134521484\n",
      "Epoch 1:  36%|███▌      | 183/512 [00:35<01:03,  5.22it/s, v_num=11]Training loss: 1.7447283267974854\n",
      "Epoch 1:  36%|███▌      | 184/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.5315568447113037\n",
      "Epoch 1:  36%|███▌      | 185/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.6335350275039673\n",
      "Epoch 1:  36%|███▋      | 186/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.8687869310379028\n",
      "Epoch 1:  37%|███▋      | 187/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.4438390731811523\n",
      "Epoch 1:  37%|███▋      | 188/512 [00:36<01:02,  5.22it/s, v_num=11]Training loss: 1.628601312637329\n",
      "Epoch 1:  37%|███▋      | 189/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.6185696125030518\n",
      "Epoch 1:  37%|███▋      | 190/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.8939638137817383\n",
      "Epoch 1:  37%|███▋      | 191/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.2595388889312744\n",
      "Epoch 1:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.5664769411087036\n",
      "Epoch 1:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.4718968868255615\n",
      "Epoch 1:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.3557384014129639\n",
      "Epoch 1:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.5875427722930908\n",
      "Epoch 1:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.7437329292297363\n",
      "Epoch 1:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.2675669193267822\n",
      "Epoch 1:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.4455862045288086\n",
      "Epoch 1:  39%|███▉      | 199/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.14216685295105\n",
      "Epoch 1:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.430504322052002\n",
      "Epoch 1:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.342626690864563\n",
      "Epoch 1:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.1932895183563232\n",
      "Epoch 1:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.7401437759399414\n",
      "Epoch 1:  40%|███▉      | 204/512 [00:39<00:59,  5.22it/s, v_num=11]Training loss: 1.3689792156219482\n",
      "Epoch 1:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.1798486709594727\n",
      "Epoch 1:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.5174624919891357\n",
      "Epoch 1:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.8246337175369263\n",
      "Epoch 1:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.494516372680664\n",
      "Epoch 1:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 2.5630764961242676\n",
      "Epoch 1:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.3260016441345215\n",
      "Epoch 1:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.31208336353302\n",
      "Epoch 1:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.7804508209228516\n",
      "Epoch 1:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.726599931716919\n",
      "Epoch 1:  42%|████▏     | 214/512 [00:41<00:57,  5.22it/s, v_num=11]Training loss: 1.5527722835540771\n",
      "Epoch 1:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.5619680881500244\n",
      "Epoch 1:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.6319363117218018\n",
      "Epoch 1:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 2.5901424884796143\n",
      "Epoch 1:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4099180698394775\n",
      "Epoch 1:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.2858726978302002\n",
      "Epoch 1:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.6757439374923706\n",
      "Epoch 1:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.068563938140869\n",
      "Epoch 1:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.368654251098633\n",
      "Epoch 1:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.8636770248413086\n",
      "Epoch 1:  44%|████▍     | 224/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.7911605834960938\n",
      "Epoch 1:  44%|████▍     | 225/512 [00:43<00:55,  5.22it/s, v_num=11]Training loss: 1.5965139865875244\n",
      "Epoch 1:  44%|████▍     | 226/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.9947655200958252\n",
      "Epoch 1:  44%|████▍     | 227/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.3149502277374268\n",
      "Epoch 1:  45%|████▍     | 228/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 2.0166478157043457\n",
      "Epoch 1:  45%|████▍     | 229/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.638393759727478\n",
      "Epoch 1:  45%|████▍     | 230/512 [00:44<00:54,  5.22it/s, v_num=11]Training loss: 1.7694480419158936\n",
      "Epoch 1:  45%|████▌     | 231/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.870364785194397\n",
      "Epoch 1:  45%|████▌     | 232/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 2.0557332038879395\n",
      "Epoch 1:  46%|████▌     | 233/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.3509413003921509\n",
      "Epoch 1:  46%|████▌     | 234/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 2.036269187927246\n",
      "Epoch 1:  46%|████▌     | 235/512 [00:45<00:53,  5.22it/s, v_num=11]Training loss: 1.7666335105895996\n",
      "Epoch 1:  46%|████▌     | 236/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.5416096448898315\n",
      "Epoch 1:  46%|████▋     | 237/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.8164420127868652\n",
      "Epoch 1:  46%|████▋     | 238/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.6388566493988037\n",
      "Epoch 1:  47%|████▋     | 239/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.4932725429534912\n",
      "Epoch 1:  47%|████▋     | 240/512 [00:46<00:52,  5.21it/s, v_num=11]Training loss: 1.356454849243164\n",
      "Epoch 1:  47%|████▋     | 241/512 [00:46<00:52,  5.21it/s, v_num=11]Training loss: 1.8700135946273804\n",
      "Epoch 1:  47%|████▋     | 242/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 1.7742619514465332\n",
      "Epoch 1:  47%|████▋     | 243/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 1.4949467182159424\n",
      "Epoch 1:  48%|████▊     | 244/512 [00:46<00:51,  5.20it/s, v_num=11]Training loss: 1.6279226541519165\n",
      "Epoch 1:  48%|████▊     | 245/512 [00:47<00:51,  5.20it/s, v_num=11]Training loss: 1.7058779001235962\n",
      "Epoch 1:  48%|████▊     | 246/512 [00:47<00:51,  5.20it/s, v_num=11]Training loss: 1.379191517829895\n",
      "Epoch 1:  48%|████▊     | 247/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 1.7405076026916504\n",
      "Epoch 1:  48%|████▊     | 248/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 1.6093703508377075\n",
      "Epoch 1:  49%|████▊     | 249/512 [00:47<00:50,  5.20it/s, v_num=11]Training loss: 3.0198633670806885\n",
      "Epoch 1:  49%|████▉     | 250/512 [00:48<00:50,  5.20it/s, v_num=11]Training loss: 1.4532232284545898\n",
      "Epoch 1:  49%|████▉     | 251/512 [00:48<00:50,  5.20it/s, v_num=11]Training loss: 2.0112009048461914\n",
      "Epoch 1:  49%|████▉     | 252/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 2.003683090209961\n",
      "Epoch 1:  49%|████▉     | 253/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 1.6730438470840454\n",
      "Epoch 1:  50%|████▉     | 254/512 [00:48<00:49,  5.20it/s, v_num=11]Training loss: 2.227536201477051\n",
      "Epoch 1:  50%|████▉     | 255/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 2.9600307941436768\n",
      "Epoch 1:  50%|█████     | 256/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 1.7386329174041748\n",
      "Epoch 1:  50%|█████     | 257/512 [00:49<00:49,  5.20it/s, v_num=11]Training loss: 1.8589924573898315\n",
      "Epoch 1:  50%|█████     | 258/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 1.3079909086227417\n",
      "Epoch 1:  51%|█████     | 259/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 2.063267707824707\n",
      "Epoch 1:  51%|█████     | 260/512 [00:49<00:48,  5.20it/s, v_num=11]Training loss: 1.7757062911987305\n",
      "Epoch 1:  51%|█████     | 261/512 [00:50<00:48,  5.20it/s, v_num=11]Training loss: 1.9897481203079224\n",
      "Epoch 1:  51%|█████     | 262/512 [00:50<00:48,  5.20it/s, v_num=11]Training loss: 1.6492654085159302\n",
      "Epoch 1:  51%|█████▏    | 263/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.794409990310669\n",
      "Epoch 1:  52%|█████▏    | 264/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.701833963394165\n",
      "Epoch 1:  52%|█████▏    | 265/512 [00:50<00:47,  5.20it/s, v_num=11]Training loss: 1.7085461616516113\n",
      "Epoch 1:  52%|█████▏    | 266/512 [00:51<00:47,  5.20it/s, v_num=11]Training loss: 1.9522199630737305\n",
      "Epoch 1:  52%|█████▏    | 267/512 [00:51<00:47,  5.20it/s, v_num=11]Training loss: 1.5268006324768066\n",
      "Epoch 1:  52%|█████▏    | 268/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 1.8687927722930908\n",
      "Epoch 1:  53%|█████▎    | 269/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 1.4993407726287842\n",
      "Epoch 1:  53%|█████▎    | 270/512 [00:51<00:46,  5.20it/s, v_num=11]Training loss: 1.3938307762145996\n",
      "Epoch 1:  53%|█████▎    | 271/512 [00:52<00:46,  5.20it/s, v_num=11]Training loss: 1.779932975769043\n",
      "Epoch 1:  53%|█████▎    | 272/512 [00:52<00:46,  5.20it/s, v_num=11]Training loss: 1.3869786262512207\n",
      "Epoch 1:  53%|█████▎    | 273/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 2.0694022178649902\n",
      "Epoch 1:  54%|█████▎    | 274/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 2.189779043197632\n",
      "Epoch 1:  54%|█████▎    | 275/512 [00:52<00:45,  5.20it/s, v_num=11]Training loss: 1.4939631223678589\n",
      "Epoch 1:  54%|█████▍    | 276/512 [00:53<00:45,  5.20it/s, v_num=11]Training loss: 1.7236160039901733\n",
      "Epoch 1:  54%|█████▍    | 277/512 [00:53<00:45,  5.20it/s, v_num=11]Training loss: 1.925910472869873\n",
      "Epoch 1:  54%|█████▍    | 278/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 1.8469038009643555\n",
      "Epoch 1:  54%|█████▍    | 279/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 1.440223217010498\n",
      "Epoch 1:  55%|█████▍    | 280/512 [00:53<00:44,  5.20it/s, v_num=11]Training loss: 1.544555425643921\n",
      "Epoch 1:  55%|█████▍    | 281/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 2.043997049331665\n",
      "Epoch 1:  55%|█████▌    | 282/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 2.0610129833221436\n",
      "Epoch 1:  55%|█████▌    | 283/512 [00:54<00:44,  5.20it/s, v_num=11]Training loss: 1.7329039573669434\n",
      "Epoch 1:  55%|█████▌    | 284/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 1.6462700366973877\n",
      "Epoch 1:  56%|█████▌    | 285/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 2.1596627235412598\n",
      "Epoch 1:  56%|█████▌    | 286/512 [00:54<00:43,  5.20it/s, v_num=11]Training loss: 1.7257815599441528\n",
      "Epoch 1:  56%|█████▌    | 287/512 [00:55<00:43,  5.20it/s, v_num=11]Training loss: 1.949047565460205\n",
      "Epoch 1:  56%|█████▋    | 288/512 [00:55<00:43,  5.20it/s, v_num=11]Training loss: 1.2088091373443604\n",
      "Epoch 1:  56%|█████▋    | 289/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 1.6977037191390991\n",
      "Epoch 1:  57%|█████▋    | 290/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 1.848507046699524\n",
      "Epoch 1:  57%|█████▋    | 291/512 [00:55<00:42,  5.20it/s, v_num=11]Training loss: 1.6077511310577393\n",
      "Epoch 1:  57%|█████▋    | 292/512 [00:56<00:42,  5.20it/s, v_num=11]Training loss: 1.2949877977371216\n",
      "Epoch 1:  57%|█████▋    | 293/512 [00:56<00:42,  5.20it/s, v_num=11]Training loss: 1.717342734336853\n",
      "Epoch 1:  57%|█████▋    | 294/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.8223453760147095\n",
      "Epoch 1:  58%|█████▊    | 295/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.957029104232788\n",
      "Epoch 1:  58%|█████▊    | 296/512 [00:56<00:41,  5.20it/s, v_num=11]Training loss: 1.5743467807769775\n",
      "Epoch 1:  58%|█████▊    | 297/512 [00:57<00:41,  5.20it/s, v_num=11]Training loss: 1.9185950756072998\n",
      "Epoch 1:  58%|█████▊    | 298/512 [00:57<00:41,  5.20it/s, v_num=11]Training loss: 1.5826997756958008\n",
      "Epoch 1:  58%|█████▊    | 299/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 1.3689261674880981\n",
      "Epoch 1:  59%|█████▊    | 300/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 1.3838958740234375\n",
      "Epoch 1:  59%|█████▉    | 301/512 [00:57<00:40,  5.20it/s, v_num=11]Training loss: 1.744154691696167\n",
      "Epoch 1:  59%|█████▉    | 302/512 [00:58<00:40,  5.20it/s, v_num=11]Training loss: 1.862677812576294\n",
      "Epoch 1:  59%|█████▉    | 303/512 [00:58<00:40,  5.20it/s, v_num=11]Training loss: 1.5980470180511475\n",
      "Epoch 1:  59%|█████▉    | 304/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 1.8453950881958008\n",
      "Epoch 1:  60%|█████▉    | 305/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 1.5880333185195923\n",
      "Epoch 1:  60%|█████▉    | 306/512 [00:58<00:39,  5.20it/s, v_num=11]Training loss: 1.91813325881958\n",
      "Epoch 1:  60%|█████▉    | 307/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 1.850799560546875\n",
      "Epoch 1:  60%|██████    | 308/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 1.569168210029602\n",
      "Epoch 1:  60%|██████    | 309/512 [00:59<00:39,  5.20it/s, v_num=11]Training loss: 1.8615117073059082\n",
      "Epoch 1:  61%|██████    | 310/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 2.404090166091919\n",
      "Epoch 1:  61%|██████    | 311/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 3.0899057388305664\n",
      "Epoch 1:  61%|██████    | 312/512 [00:59<00:38,  5.20it/s, v_num=11]Training loss: 2.1191468238830566\n",
      "Epoch 1:  61%|██████    | 313/512 [01:00<00:38,  5.20it/s, v_num=11]Training loss: 1.810833215713501\n",
      "Epoch 1:  61%|██████▏   | 314/512 [01:00<00:38,  5.20it/s, v_num=11]Training loss: 1.4107681512832642\n",
      "Epoch 1:  62%|██████▏   | 315/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 1.339494228363037\n",
      "Epoch 1:  62%|██████▏   | 316/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 1.8171273469924927\n",
      "Epoch 1:  62%|██████▏   | 317/512 [01:00<00:37,  5.20it/s, v_num=11]Training loss: 1.626233458518982\n",
      "Epoch 1:  62%|██████▏   | 318/512 [01:01<00:37,  5.20it/s, v_num=11]Training loss: 1.5480586290359497\n",
      "Epoch 1:  62%|██████▏   | 319/512 [01:01<00:37,  5.20it/s, v_num=11]Training loss: 1.5738693475723267\n",
      "Epoch 1:  62%|██████▎   | 320/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 2.379587411880493\n",
      "Epoch 1:  63%|██████▎   | 321/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 2.320154905319214\n",
      "Epoch 1:  63%|██████▎   | 322/512 [01:01<00:36,  5.20it/s, v_num=11]Training loss: 2.234027624130249\n",
      "Epoch 1:  63%|██████▎   | 323/512 [01:02<00:36,  5.20it/s, v_num=11]Training loss: 1.893566370010376\n",
      "Epoch 1:  63%|██████▎   | 324/512 [01:02<00:36,  5.20it/s, v_num=11]Training loss: 1.7293591499328613\n",
      "Epoch 1:  63%|██████▎   | 325/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 1.9098087549209595\n",
      "Epoch 1:  64%|██████▎   | 326/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 2.542449712753296\n",
      "Epoch 1:  64%|██████▍   | 327/512 [01:02<00:35,  5.20it/s, v_num=11]Training loss: 1.9683420658111572\n",
      "Epoch 1:  64%|██████▍   | 328/512 [01:03<00:35,  5.20it/s, v_num=11]Training loss: 1.7064361572265625\n",
      "Epoch 1:  64%|██████▍   | 329/512 [01:03<00:35,  5.20it/s, v_num=11]Training loss: 1.4486382007598877\n",
      "Epoch 1:  64%|██████▍   | 330/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 1.468092679977417\n",
      "Epoch 1:  65%|██████▍   | 331/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 1.4941933155059814\n",
      "Epoch 1:  65%|██████▍   | 332/512 [01:03<00:34,  5.20it/s, v_num=11]Training loss: 1.1226317882537842\n",
      "Epoch 1:  65%|██████▌   | 333/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 1.4628589153289795\n",
      "Epoch 1:  65%|██████▌   | 334/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 1.2284305095672607\n",
      "Epoch 1:  65%|██████▌   | 335/512 [01:04<00:34,  5.20it/s, v_num=11]Training loss: 1.7917969226837158\n",
      "Epoch 1:  66%|██████▌   | 336/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 2.0048584938049316\n",
      "Epoch 1:  66%|██████▌   | 337/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 1.6078226566314697\n",
      "Epoch 1:  66%|██████▌   | 338/512 [01:04<00:33,  5.20it/s, v_num=11]Training loss: 1.2013237476348877\n",
      "Epoch 1:  66%|██████▌   | 339/512 [01:05<00:33,  5.20it/s, v_num=11]Training loss: 2.147275447845459\n",
      "Epoch 1:  66%|██████▋   | 340/512 [01:05<00:33,  5.20it/s, v_num=11]Training loss: 1.514114499092102\n",
      "Epoch 1:  67%|██████▋   | 341/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 2.2246880531311035\n",
      "Epoch 1:  67%|██████▋   | 342/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 1.8668161630630493\n",
      "Epoch 1:  67%|██████▋   | 343/512 [01:05<00:32,  5.20it/s, v_num=11]Training loss: 1.7697267532348633\n",
      "Epoch 1:  67%|██████▋   | 344/512 [01:06<00:32,  5.20it/s, v_num=11]Training loss: 1.480048656463623\n",
      "Epoch 1:  67%|██████▋   | 345/512 [01:06<00:32,  5.20it/s, v_num=11]Training loss: 1.4607961177825928\n",
      "Epoch 1:  68%|██████▊   | 346/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 2.2773373126983643\n",
      "Epoch 1:  68%|██████▊   | 347/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 1.6428337097167969\n",
      "Epoch 1:  68%|██████▊   | 348/512 [01:06<00:31,  5.20it/s, v_num=11]Training loss: 1.722733974456787\n",
      "Epoch 1:  68%|██████▊   | 349/512 [01:07<00:31,  5.20it/s, v_num=11]Training loss: 2.145009994506836\n",
      "Epoch 1:  68%|██████▊   | 350/512 [01:07<00:31,  5.20it/s, v_num=11]Training loss: 2.0711183547973633\n",
      "Epoch 1:  69%|██████▊   | 351/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 1.9476631879806519\n",
      "Epoch 1:  69%|██████▉   | 352/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 1.5280752182006836\n",
      "Epoch 1:  69%|██████▉   | 353/512 [01:07<00:30,  5.20it/s, v_num=11]Training loss: 2.209984302520752\n",
      "Epoch 1:  69%|██████▉   | 354/512 [01:08<00:30,  5.20it/s, v_num=11]Training loss: 2.833064317703247\n",
      "Epoch 1:  69%|██████▉   | 355/512 [01:08<00:30,  5.20it/s, v_num=11]Training loss: 1.750891923904419\n",
      "Epoch 1:  70%|██████▉   | 356/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 1.9865937232971191\n",
      "Epoch 1:  70%|██████▉   | 357/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 2.0465950965881348\n",
      "Epoch 1:  70%|██████▉   | 358/512 [01:08<00:29,  5.20it/s, v_num=11]Training loss: 1.294435977935791\n",
      "Epoch 1:  70%|███████   | 359/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 2.0897574424743652\n",
      "Epoch 1:  70%|███████   | 360/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 1.5532431602478027\n",
      "Epoch 1:  71%|███████   | 361/512 [01:09<00:29,  5.20it/s, v_num=11]Training loss: 1.3061130046844482\n",
      "Epoch 1:  71%|███████   | 362/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 1.383484125137329\n",
      "Epoch 1:  71%|███████   | 363/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 1.4822413921356201\n",
      "Epoch 1:  71%|███████   | 364/512 [01:09<00:28,  5.20it/s, v_num=11]Training loss: 2.3665037155151367\n",
      "Epoch 1:  71%|███████▏  | 365/512 [01:10<00:28,  5.20it/s, v_num=11]Training loss: 1.5468792915344238\n",
      "Epoch 1:  71%|███████▏  | 366/512 [01:10<00:28,  5.20it/s, v_num=11]Training loss: 1.1551249027252197\n",
      "Epoch 1:  72%|███████▏  | 367/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 1.9995521306991577\n",
      "Epoch 1:  72%|███████▏  | 368/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 1.4682509899139404\n",
      "Epoch 1:  72%|███████▏  | 369/512 [01:10<00:27,  5.20it/s, v_num=11]Training loss: 1.4883384704589844\n",
      "Epoch 1:  72%|███████▏  | 370/512 [01:11<00:27,  5.20it/s, v_num=11]Training loss: 1.4304243326187134\n",
      "Epoch 1:  72%|███████▏  | 371/512 [01:11<00:27,  5.20it/s, v_num=11]Training loss: 2.1658668518066406\n",
      "Epoch 1:  73%|███████▎  | 372/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 1.6579792499542236\n",
      "Epoch 1:  73%|███████▎  | 373/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 1.5071773529052734\n",
      "Epoch 1:  73%|███████▎  | 374/512 [01:11<00:26,  5.20it/s, v_num=11]Training loss: 1.1008044481277466\n",
      "Epoch 1:  73%|███████▎  | 375/512 [01:12<00:26,  5.20it/s, v_num=11]Training loss: 1.507474660873413\n",
      "Epoch 1:  73%|███████▎  | 376/512 [01:12<00:26,  5.20it/s, v_num=11]Training loss: 1.8255329132080078\n",
      "Epoch 1:  74%|███████▎  | 377/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 1.5592796802520752\n",
      "Epoch 1:  74%|███████▍  | 378/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 1.3713762760162354\n",
      "Epoch 1:  74%|███████▍  | 379/512 [01:12<00:25,  5.20it/s, v_num=11]Training loss: 1.6316089630126953\n",
      "Epoch 1:  74%|███████▍  | 380/512 [01:13<00:25,  5.20it/s, v_num=11]Training loss: 2.2782514095306396\n",
      "Epoch 1:  74%|███████▍  | 381/512 [01:13<00:25,  5.20it/s, v_num=11]Training loss: 1.4826581478118896\n",
      "Epoch 1:  75%|███████▍  | 382/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 4.06004524230957\n",
      "Epoch 1:  75%|███████▍  | 383/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 2.0789151191711426\n",
      "Epoch 1:  75%|███████▌  | 384/512 [01:13<00:24,  5.20it/s, v_num=11]Training loss: 1.8800874948501587\n",
      "Epoch 1:  75%|███████▌  | 385/512 [01:14<00:24,  5.20it/s, v_num=11]Training loss: 1.3145709037780762\n",
      "Epoch 1:  75%|███████▌  | 386/512 [01:14<00:24,  5.20it/s, v_num=11]Training loss: 1.2460417747497559\n",
      "Epoch 1:  76%|███████▌  | 387/512 [01:14<00:24,  5.20it/s, v_num=11]Training loss: 1.801135778427124\n",
      "Epoch 1:  76%|███████▌  | 388/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 2.1101951599121094\n",
      "Epoch 1:  76%|███████▌  | 389/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 2.1408426761627197\n",
      "Epoch 1:  76%|███████▌  | 390/512 [01:14<00:23,  5.20it/s, v_num=11]Training loss: 1.4073476791381836\n",
      "Epoch 1:  76%|███████▋  | 391/512 [01:15<00:23,  5.20it/s, v_num=11]Training loss: 1.5154647827148438\n",
      "Epoch 1:  77%|███████▋  | 392/512 [01:15<00:23,  5.20it/s, v_num=11]Training loss: 1.290997862815857\n",
      "Epoch 1:  77%|███████▋  | 393/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 1.6895220279693604\n",
      "Epoch 1:  77%|███████▋  | 394/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 1.4540281295776367\n",
      "Epoch 1:  77%|███████▋  | 395/512 [01:15<00:22,  5.20it/s, v_num=11]Training loss: 1.7864265441894531\n",
      "Epoch 1:  77%|███████▋  | 396/512 [01:16<00:22,  5.20it/s, v_num=11]Training loss: 1.6331062316894531\n",
      "Epoch 1:  78%|███████▊  | 397/512 [01:16<00:22,  5.20it/s, v_num=11]Training loss: 1.4053940773010254\n",
      "Epoch 1:  78%|███████▊  | 398/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 1.7209117412567139\n",
      "Epoch 1:  78%|███████▊  | 399/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 1.6465109586715698\n",
      "Epoch 1:  78%|███████▊  | 400/512 [01:16<00:21,  5.20it/s, v_num=11]Training loss: 1.4770722389221191\n",
      "Epoch 1:  78%|███████▊  | 401/512 [01:17<00:21,  5.20it/s, v_num=11]Training loss: 1.8825876712799072\n",
      "Epoch 1:  79%|███████▊  | 402/512 [01:17<00:21,  5.20it/s, v_num=11]Training loss: 1.6073694229125977\n",
      "Epoch 1:  79%|███████▊  | 403/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 2.4035072326660156\n",
      "Epoch 1:  79%|███████▉  | 404/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 1.4613709449768066\n",
      "Epoch 1:  79%|███████▉  | 405/512 [01:17<00:20,  5.20it/s, v_num=11]Training loss: 1.620619773864746\n",
      "Epoch 1:  79%|███████▉  | 406/512 [01:18<00:20,  5.20it/s, v_num=11]Training loss: 2.1226468086242676\n",
      "Epoch 1:  79%|███████▉  | 407/512 [01:18<00:20,  5.20it/s, v_num=11]Training loss: 2.1480765342712402\n",
      "Epoch 1:  80%|███████▉  | 408/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 1.8757933378219604\n",
      "Epoch 1:  80%|███████▉  | 409/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 1.506861686706543\n",
      "Epoch 1:  80%|████████  | 410/512 [01:18<00:19,  5.20it/s, v_num=11]Training loss: 2.099597215652466\n",
      "Epoch 1:  80%|████████  | 411/512 [01:19<00:19,  5.20it/s, v_num=11]Training loss: 2.7073922157287598\n",
      "Epoch 1:  80%|████████  | 412/512 [01:19<00:19,  5.20it/s, v_num=11]Training loss: 1.5623111724853516\n",
      "Epoch 1:  81%|████████  | 413/512 [01:19<00:19,  5.20it/s, v_num=11]Training loss: 1.6165671348571777\n",
      "Epoch 1:  81%|████████  | 414/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.7892630100250244\n",
      "Epoch 1:  81%|████████  | 415/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.8190033435821533\n",
      "Epoch 1:  81%|████████▏ | 416/512 [01:19<00:18,  5.20it/s, v_num=11]Training loss: 1.6516194343566895\n",
      "Epoch 1:  81%|████████▏ | 417/512 [01:20<00:18,  5.20it/s, v_num=11]Training loss: 1.854694128036499\n",
      "Epoch 1:  82%|████████▏ | 418/512 [01:20<00:18,  5.20it/s, v_num=11]Training loss: 2.033318281173706\n",
      "Epoch 1:  82%|████████▏ | 419/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 1.6954262256622314\n",
      "Epoch 1:  82%|████████▏ | 420/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 1.7701902389526367\n",
      "Epoch 1:  82%|████████▏ | 421/512 [01:20<00:17,  5.20it/s, v_num=11]Training loss: 1.4507718086242676\n",
      "Epoch 1:  82%|████████▏ | 422/512 [01:21<00:17,  5.20it/s, v_num=11]Training loss: 1.5265220403671265\n",
      "Epoch 1:  83%|████████▎ | 423/512 [01:21<00:17,  5.20it/s, v_num=11]Training loss: 1.5322577953338623\n",
      "Epoch 1:  83%|████████▎ | 424/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 1.5115137100219727\n",
      "Epoch 1:  83%|████████▎ | 425/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 1.6071152687072754\n",
      "Epoch 1:  83%|████████▎ | 426/512 [01:21<00:16,  5.20it/s, v_num=11]Training loss: 1.9217299222946167\n",
      "Epoch 1:  83%|████████▎ | 427/512 [01:22<00:16,  5.20it/s, v_num=11]Training loss: 1.941578984260559\n",
      "Epoch 1:  84%|████████▎ | 428/512 [01:22<00:16,  5.20it/s, v_num=11]Training loss: 1.3707901239395142\n",
      "Epoch 1:  84%|████████▍ | 429/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 1.9778635501861572\n",
      "Epoch 1:  84%|████████▍ | 430/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 1.1711540222167969\n",
      "Epoch 1:  84%|████████▍ | 431/512 [01:22<00:15,  5.20it/s, v_num=11]Training loss: 1.7648167610168457\n",
      "Epoch 1:  84%|████████▍ | 432/512 [01:23<00:15,  5.20it/s, v_num=11]Training loss: 1.6140000820159912\n",
      "Epoch 1:  85%|████████▍ | 433/512 [01:23<00:15,  5.20it/s, v_num=11]Training loss: 1.3150389194488525\n",
      "Epoch 1:  85%|████████▍ | 434/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 1.9358885288238525\n",
      "Epoch 1:  85%|████████▍ | 435/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 1.4732521772384644\n",
      "Epoch 1:  85%|████████▌ | 436/512 [01:23<00:14,  5.20it/s, v_num=11]Training loss: 1.7184545993804932\n",
      "Epoch 1:  85%|████████▌ | 437/512 [01:24<00:14,  5.20it/s, v_num=11]Training loss: 1.9226136207580566\n",
      "Epoch 1:  86%|████████▌ | 438/512 [01:24<00:14,  5.20it/s, v_num=11]Training loss: 1.490384578704834\n",
      "Epoch 1:  86%|████████▌ | 439/512 [01:24<00:14,  5.20it/s, v_num=11]Training loss: 1.5460271835327148\n",
      "Epoch 1:  86%|████████▌ | 440/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 2.2398900985717773\n",
      "Epoch 1:  86%|████████▌ | 441/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 1.3401669263839722\n",
      "Epoch 1:  86%|████████▋ | 442/512 [01:24<00:13,  5.20it/s, v_num=11]Training loss: 1.2276310920715332\n",
      "Epoch 1:  87%|████████▋ | 443/512 [01:25<00:13,  5.20it/s, v_num=11]Training loss: 1.8555068969726562\n",
      "Epoch 1:  87%|████████▋ | 444/512 [01:25<00:13,  5.20it/s, v_num=11]Training loss: 1.35870361328125\n",
      "Epoch 1:  87%|████████▋ | 445/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 1.7670317888259888\n",
      "Epoch 1:  87%|████████▋ | 446/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 1.341536521911621\n",
      "Epoch 1:  87%|████████▋ | 447/512 [01:25<00:12,  5.20it/s, v_num=11]Training loss: 2.2649474143981934\n",
      "Epoch 1:  88%|████████▊ | 448/512 [01:26<00:12,  5.20it/s, v_num=11]Training loss: 1.350313425064087\n",
      "Epoch 1:  88%|████████▊ | 449/512 [01:26<00:12,  5.20it/s, v_num=11]Training loss: 2.0759012699127197\n",
      "Epoch 1:  88%|████████▊ | 450/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 2.330077648162842\n",
      "Epoch 1:  88%|████████▊ | 451/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 1.4888529777526855\n",
      "Epoch 1:  88%|████████▊ | 452/512 [01:26<00:11,  5.20it/s, v_num=11]Training loss: 1.6418631076812744\n",
      "Epoch 1:  88%|████████▊ | 453/512 [01:27<00:11,  5.20it/s, v_num=11]Training loss: 1.815755844116211\n",
      "Epoch 1:  89%|████████▊ | 454/512 [01:27<00:11,  5.20it/s, v_num=11]Training loss: 1.9129455089569092\n",
      "Epoch 1:  89%|████████▉ | 455/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 2.1692209243774414\n",
      "Epoch 1:  89%|████████▉ | 456/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 2.117544651031494\n",
      "Epoch 1:  89%|████████▉ | 457/512 [01:27<00:10,  5.20it/s, v_num=11]Training loss: 1.3961217403411865\n",
      "Epoch 1:  89%|████████▉ | 458/512 [01:28<00:10,  5.20it/s, v_num=11]Training loss: 1.4944655895233154\n",
      "Epoch 1:  90%|████████▉ | 459/512 [01:28<00:10,  5.20it/s, v_num=11]Training loss: 2.1824989318847656\n",
      "Epoch 1:  90%|████████▉ | 460/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 1.3763339519500732\n",
      "Epoch 1:  90%|█████████ | 461/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 1.90788733959198\n",
      "Epoch 1:  90%|█████████ | 462/512 [01:28<00:09,  5.20it/s, v_num=11]Training loss: 1.4016804695129395\n",
      "Epoch 1:  90%|█████████ | 463/512 [01:29<00:09,  5.20it/s, v_num=11]Training loss: 2.008265972137451\n",
      "Epoch 1:  91%|█████████ | 464/512 [01:29<00:09,  5.20it/s, v_num=11]Training loss: 1.8001556396484375\n",
      "Epoch 1:  91%|█████████ | 465/512 [01:29<00:09,  5.20it/s, v_num=11]Training loss: 1.8908369541168213\n",
      "Epoch 1:  91%|█████████ | 466/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 1.6984906196594238\n",
      "Epoch 1:  91%|█████████ | 467/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 1.5905312299728394\n",
      "Epoch 1:  91%|█████████▏| 468/512 [01:29<00:08,  5.20it/s, v_num=11]Training loss: 1.8593111038208008\n",
      "Epoch 1:  92%|█████████▏| 469/512 [01:30<00:08,  5.20it/s, v_num=11]Training loss: 1.7444599866867065\n",
      "Epoch 1:  92%|█████████▏| 470/512 [01:30<00:08,  5.20it/s, v_num=11]Training loss: 1.750901222229004\n",
      "Epoch 1:  92%|█████████▏| 471/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 1.8891487121582031\n",
      "Epoch 1:  92%|█████████▏| 472/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 1.543321132659912\n",
      "Epoch 1:  92%|█████████▏| 473/512 [01:30<00:07,  5.20it/s, v_num=11]Training loss: 1.4107120037078857\n",
      "Epoch 1:  93%|█████████▎| 474/512 [01:31<00:07,  5.20it/s, v_num=11]Training loss: 1.2778751850128174\n",
      "Epoch 1:  93%|█████████▎| 475/512 [01:31<00:07,  5.20it/s, v_num=11]Training loss: 1.9451673030853271\n",
      "Epoch 1:  93%|█████████▎| 476/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 1.8213000297546387\n",
      "Epoch 1:  93%|█████████▎| 477/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 4.169462203979492\n",
      "Epoch 1:  93%|█████████▎| 478/512 [01:31<00:06,  5.20it/s, v_num=11]Training loss: 2.692293167114258\n",
      "Epoch 1:  94%|█████████▎| 479/512 [01:32<00:06,  5.20it/s, v_num=11]Training loss: 1.546858549118042\n",
      "Epoch 1:  94%|█████████▍| 480/512 [01:32<00:06,  5.20it/s, v_num=11]Training loss: 1.7322165966033936\n",
      "Epoch 1:  94%|█████████▍| 481/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 1.752760887145996\n",
      "Epoch 1:  94%|█████████▍| 482/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 1.953162431716919\n",
      "Epoch 1:  94%|█████████▍| 483/512 [01:32<00:05,  5.20it/s, v_num=11]Training loss: 2.045630693435669\n",
      "Epoch 1:  95%|█████████▍| 484/512 [01:33<00:05,  5.20it/s, v_num=11]Training loss: 1.689112663269043\n",
      "Epoch 1:  95%|█████████▍| 485/512 [01:33<00:05,  5.20it/s, v_num=11]Training loss: 1.362368106842041\n",
      "Epoch 1:  95%|█████████▍| 486/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 1.6786869764328003\n",
      "Epoch 1:  95%|█████████▌| 487/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 1.6684082746505737\n",
      "Epoch 1:  95%|█████████▌| 488/512 [01:33<00:04,  5.20it/s, v_num=11]Training loss: 1.7200851440429688\n",
      "Epoch 1:  96%|█████████▌| 489/512 [01:34<00:04,  5.20it/s, v_num=11]Training loss: 1.6756396293640137\n",
      "Epoch 1:  96%|█████████▌| 490/512 [01:34<00:04,  5.20it/s, v_num=11]Training loss: 1.3959999084472656\n",
      "Epoch 1:  96%|█████████▌| 491/512 [01:34<00:04,  5.20it/s, v_num=11]Training loss: 1.7928282022476196\n",
      "Epoch 1:  96%|█████████▌| 492/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 1.9516371488571167\n",
      "Epoch 1:  96%|█████████▋| 493/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 1.5188462734222412\n",
      "Epoch 1:  96%|█████████▋| 494/512 [01:34<00:03,  5.20it/s, v_num=11]Training loss: 2.1918411254882812\n",
      "Epoch 1:  97%|█████████▋| 495/512 [01:35<00:03,  5.20it/s, v_num=11]Training loss: 1.7109434604644775\n",
      "Epoch 1:  97%|█████████▋| 496/512 [01:35<00:03,  5.20it/s, v_num=11]Training loss: 1.2624939680099487\n",
      "Epoch 1:  97%|█████████▋| 497/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 1.8378061056137085\n",
      "Epoch 1:  97%|█████████▋| 498/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 6.341535568237305\n",
      "Epoch 1:  97%|█████████▋| 499/512 [01:35<00:02,  5.20it/s, v_num=11]Training loss: 1.6855121850967407\n",
      "Epoch 1:  98%|█████████▊| 500/512 [01:36<00:02,  5.20it/s, v_num=11]Training loss: 2.2699739933013916\n",
      "Epoch 1:  98%|█████████▊| 501/512 [01:36<00:02,  5.20it/s, v_num=11]Training loss: 1.8498547077178955\n",
      "Epoch 1:  98%|█████████▊| 502/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.2925609350204468\n",
      "Epoch 1:  98%|█████████▊| 503/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.9368809461593628\n",
      "Epoch 1:  98%|█████████▊| 504/512 [01:36<00:01,  5.20it/s, v_num=11]Training loss: 1.2268927097320557\n",
      "Epoch 1:  99%|█████████▊| 505/512 [01:37<00:01,  5.20it/s, v_num=11]Training loss: 2.2784249782562256\n",
      "Epoch 1:  99%|█████████▉| 506/512 [01:37<00:01,  5.20it/s, v_num=11]Training loss: 1.808210849761963\n",
      "Epoch 1:  99%|█████████▉| 507/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 1.7092459201812744\n",
      "Epoch 1:  99%|█████████▉| 508/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 1.6540991067886353\n",
      "Epoch 1:  99%|█████████▉| 509/512 [01:37<00:00,  5.20it/s, v_num=11]Training loss: 1.7679693698883057\n",
      "Epoch 1: 100%|█████████▉| 510/512 [01:38<00:00,  5.20it/s, v_num=11]Training loss: 1.201918125152588\n",
      "Epoch 1: 100%|█████████▉| 511/512 [01:38<00:00,  5.20it/s, v_num=11]Training loss: 1.6576972007751465\n",
      "Epoch 1: 100%|██████████| 512/512 [01:38<00:00,  5.20it/s, v_num=11]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 3.1179261207580566\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:02, 49.36it/s]\u001b[AValidation loss: 1.597834587097168\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:02, 36.37it/s]\u001b[AValidation loss: 1.8795604705810547\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:03, 33.50it/s]\u001b[AValidation loss: 1.4902863502502441\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:03, 32.11it/s]\u001b[AValidation loss: 1.3715319633483887\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:03, 31.41it/s]\u001b[AValidation loss: 2.0802197456359863\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:03, 30.87it/s]\u001b[AValidation loss: 2.417675256729126\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:03, 30.60it/s]\u001b[AValidation loss: 1.5557883977890015\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:03, 30.86it/s]\u001b[AValidation loss: 2.46748685836792\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:03, 31.26it/s]\u001b[AValidation loss: 1.5806201696395874\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:03, 31.44it/s]\u001b[AValidation loss: 1.580608606338501\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:03, 31.57it/s]\u001b[AValidation loss: 1.568033218383789\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:03, 31.70it/s]\u001b[AValidation loss: 1.6965930461883545\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:03, 31.81it/s]\u001b[AValidation loss: 1.4422415494918823\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:03, 31.92it/s]\u001b[AValidation loss: 1.63844633102417\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:02, 32.00it/s]\u001b[AValidation loss: 1.4030916690826416\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:02, 32.08it/s]\u001b[AValidation loss: 1.7359801530838013\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:02, 32.14it/s]\u001b[AValidation loss: 1.5537010431289673\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:02, 32.21it/s]\u001b[AValidation loss: 1.7501270771026611\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:02, 32.26it/s]\u001b[AValidation loss: 2.0153393745422363\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:02, 32.32it/s]\u001b[AValidation loss: 1.9562625885009766\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:02, 32.36it/s]\u001b[AValidation loss: 1.559009075164795\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:02, 32.41it/s]\u001b[AValidation loss: 1.7958855628967285\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:00<00:02, 32.45it/s]\u001b[AValidation loss: 2.019537925720215\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:00<00:02, 32.49it/s]\u001b[AValidation loss: 1.585715651512146\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:00<00:02, 32.53it/s]\u001b[AValidation loss: 2.058779239654541\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:00<00:02, 32.56it/s]\u001b[AValidation loss: 1.3701894283294678\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:00<00:02, 32.59it/s]\u001b[AValidation loss: 2.181009292602539\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:00<00:02, 32.63it/s]\u001b[AValidation loss: 2.1175363063812256\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:00<00:02, 32.65it/s]\u001b[AValidation loss: 2.659201145172119\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:00<00:02, 32.68it/s]\u001b[AValidation loss: 1.2846101522445679\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:00<00:02, 32.71it/s]\u001b[AValidation loss: 1.815246820449829\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:00<00:02, 32.73it/s]\u001b[AValidation loss: 2.263298749923706\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:02, 32.76it/s]\u001b[AValidation loss: 9.946245193481445\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:02, 32.77it/s]\u001b[AValidation loss: 1.7231351137161255\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:02, 32.79it/s]\u001b[AValidation loss: 1.1527256965637207\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:02, 32.81it/s]\u001b[AValidation loss: 2.1257240772247314\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:02, 32.82it/s]\u001b[AValidation loss: 1.7634837627410889\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:02, 32.84it/s]\u001b[AValidation loss: 1.1596128940582275\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:02, 32.86it/s]\u001b[AValidation loss: 2.048941135406494\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:02, 32.88it/s]\u001b[AValidation loss: 1.6828498840332031\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:02, 32.89it/s]\u001b[AValidation loss: 1.6862016916275024\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:02, 32.91it/s]\u001b[AValidation loss: 1.3441990613937378\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 32.92it/s]\u001b[AValidation loss: 2.442009449005127\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 32.94it/s]\u001b[AValidation loss: 2.1213204860687256\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:01<00:01, 32.95it/s]\u001b[AValidation loss: 1.7558451890945435\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:01<00:01, 32.96it/s]\u001b[AValidation loss: 1.848623514175415\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:01<00:01, 32.97it/s]\u001b[AValidation loss: 2.325376033782959\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:01<00:01, 32.98it/s]\u001b[AValidation loss: 3.2220640182495117\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:01<00:01, 33.00it/s]\u001b[AValidation loss: 1.6927640438079834\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:01<00:01, 33.01it/s]\u001b[AValidation loss: 1.4968900680541992\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:01<00:01, 33.02it/s]\u001b[AValidation loss: 1.3495601415634155\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:01<00:01, 33.03it/s]\u001b[AValidation loss: 1.7147045135498047\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:01<00:01, 33.04it/s]\u001b[AValidation loss: 2.0040156841278076\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:01<00:01, 33.05it/s]\u001b[AValidation loss: 1.7738068103790283\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:01<00:01, 33.06it/s]\u001b[AValidation loss: 2.052438735961914\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:01<00:01, 33.07it/s]\u001b[AValidation loss: 2.166734457015991\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:01<00:01, 33.08it/s]\u001b[AValidation loss: 1.543332576751709\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:01<00:01, 33.03it/s]\u001b[AValidation loss: 1.5515155792236328\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:01<00:01, 33.04it/s]\u001b[AValidation loss: 1.8499435186386108\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:01<00:01, 33.05it/s]\u001b[AValidation loss: 1.6639209985733032\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:01<00:01, 33.06it/s]\u001b[AValidation loss: 1.967524528503418\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:01<00:01, 33.07it/s]\u001b[AValidation loss: 1.5020320415496826\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:01<00:01, 33.03it/s]\u001b[AValidation loss: 1.7439861297607422\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:01<00:01, 33.04it/s]\u001b[AValidation loss: 1.4757304191589355\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:01<00:01, 33.05it/s]\u001b[AValidation loss: 2.666017532348633\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:01<00:01, 33.05it/s]\u001b[AValidation loss: 1.296040415763855\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:02<00:01, 33.06it/s]\u001b[AValidation loss: 1.5898714065551758\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:02<00:01, 33.07it/s]\u001b[AValidation loss: 2.411202907562256\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:02<00:01, 33.08it/s]\u001b[AValidation loss: 1.848025918006897\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:02<00:01, 33.09it/s]\u001b[AValidation loss: 1.5269384384155273\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:02<00:01, 33.09it/s]\u001b[AValidation loss: 1.9778931140899658\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:02<00:01, 33.10it/s]\u001b[AValidation loss: 2.1458754539489746\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:02<00:01, 33.10it/s]\u001b[AValidation loss: 2.0831637382507324\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:02<00:01, 33.10it/s]\u001b[AValidation loss: 2.5115485191345215\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:02<00:01, 33.11it/s]\u001b[AValidation loss: 2.2992653846740723\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:02<00:01, 33.12it/s]\u001b[AValidation loss: 1.818225622177124\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:02<00:00, 33.12it/s]\u001b[AValidation loss: 1.8860242366790771\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:02<00:00, 33.13it/s]\u001b[AValidation loss: 1.67603600025177\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:02<00:00, 33.14it/s]\u001b[AValidation loss: 1.7083944082260132\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:02<00:00, 33.15it/s]\u001b[AValidation loss: 1.6789929866790771\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:02<00:00, 33.15it/s]\u001b[AValidation loss: 1.7980486154556274\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:02<00:00, 33.16it/s]\u001b[AValidation loss: 1.4879286289215088\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.9160423278808594\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.9805022478103638\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.779953956604004\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.7896227836608887\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.9040870666503906\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.6672861576080322\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:02<00:00, 33.19it/s]\u001b[AValidation loss: 3.097522497177124\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 1.7790347337722778\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 2.0710582733154297\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 3.551754951477051\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.7137037515640259\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:02<00:00, 33.19it/s]\u001b[AValidation loss: 2.119051218032837\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 1.2919554710388184\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 1.5312093496322632\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 1.835516333580017\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 1.6659648418426514\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 1.4310269355773926\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:03<00:00, 33.22it/s]\u001b[AValidation loss: 2.3207733631134033\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:03<00:00, 33.22it/s]\u001b[AValidation loss: 1.55782151222229\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:03<00:00, 33.21it/s]\u001b[AValidation loss: 1.238513708114624\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:03<00:00, 33.21it/s]\u001b[AValidation loss: 2.025855541229248\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:03<00:00, 33.22it/s]\u001b[AValidation loss: 1.8288812637329102\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:03<00:00, 33.22it/s]\u001b[AValidation loss: 2.1641957759857178\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:03<00:00, 33.22it/s]\u001b[AValidation loss: 1.920248031616211\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:03<00:00, 33.23it/s]\u001b[AValidation loss: 1.6689447164535522\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:03<00:00, 33.23it/s]\u001b[AValidation loss: 2.0412356853485107\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:03<00:00, 33.24it/s]\u001b[AValidation loss: 2.978386878967285\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:03<00:00, 33.29it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/512 [00:00<?, ?it/s, v_num=11]                \u001b[ATraining loss: 1.530665397644043\n",
      "Epoch 2:   0%|          | 1/512 [00:00<00:24, 20.51it/s, v_num=11]Training loss: 2.577725410461426\n",
      "Epoch 2:   0%|          | 2/512 [00:00<01:01,  8.33it/s, v_num=11]Training loss: 1.378873348236084\n",
      "Epoch 2:   1%|          | 3/512 [00:00<01:13,  6.94it/s, v_num=11]Training loss: 1.1252222061157227\n",
      "Epoch 2:   1%|          | 4/512 [00:00<01:19,  6.41it/s, v_num=11]Training loss: 1.8175532817840576\n",
      "Epoch 2:   1%|          | 5/512 [00:00<01:22,  6.12it/s, v_num=11]Training loss: 1.507849931716919\n",
      "Epoch 2:   1%|          | 6/512 [00:01<01:25,  5.95it/s, v_num=11]Training loss: 1.2020647525787354\n",
      "Epoch 2:   1%|▏         | 7/512 [00:01<01:26,  5.83it/s, v_num=11]Training loss: 1.5334465503692627\n",
      "Epoch 2:   2%|▏         | 8/512 [00:01<01:27,  5.74it/s, v_num=11]Training loss: 1.2871997356414795\n",
      "Epoch 2:   2%|▏         | 9/512 [00:01<01:28,  5.68it/s, v_num=11]Training loss: 1.4953949451446533\n",
      "Epoch 2:   2%|▏         | 10/512 [00:01<01:29,  5.63it/s, v_num=11]Training loss: 2.2067854404449463\n",
      "Epoch 2:   2%|▏         | 11/512 [00:01<01:29,  5.59it/s, v_num=11]Training loss: 1.5042216777801514\n",
      "Epoch 2:   2%|▏         | 12/512 [00:02<01:30,  5.55it/s, v_num=11]Training loss: 1.6920347213745117\n",
      "Epoch 2:   3%|▎         | 13/512 [00:02<01:30,  5.52it/s, v_num=11]Training loss: 1.590019941329956\n",
      "Epoch 2:   3%|▎         | 14/512 [00:02<01:30,  5.50it/s, v_num=11]Training loss: 1.582694411277771\n",
      "Epoch 2:   3%|▎         | 15/512 [00:02<01:30,  5.48it/s, v_num=11]Training loss: 2.242194652557373\n",
      "Epoch 2:   3%|▎         | 16/512 [00:02<01:30,  5.46it/s, v_num=11]Training loss: 1.2621169090270996\n",
      "Epoch 2:   3%|▎         | 17/512 [00:03<01:30,  5.44it/s, v_num=11]Training loss: 1.3828608989715576\n",
      "Epoch 2:   4%|▎         | 18/512 [00:03<01:31,  5.43it/s, v_num=11]Training loss: 1.764099359512329\n",
      "Epoch 2:   4%|▎         | 19/512 [00:03<01:31,  5.41it/s, v_num=11]Training loss: 1.472383975982666\n",
      "Epoch 2:   4%|▍         | 20/512 [00:03<01:31,  5.40it/s, v_num=11]Training loss: 1.5846443176269531\n",
      "Epoch 2:   4%|▍         | 21/512 [00:03<01:31,  5.39it/s, v_num=11]Training loss: 1.7674040794372559\n",
      "Epoch 2:   4%|▍         | 22/512 [00:04<01:31,  5.38it/s, v_num=11]Training loss: 1.246936559677124\n",
      "Epoch 2:   4%|▍         | 23/512 [00:04<01:31,  5.37it/s, v_num=11]Training loss: 1.649336814880371\n",
      "Epoch 2:   5%|▍         | 24/512 [00:04<01:30,  5.37it/s, v_num=11]Training loss: 1.6540920734405518\n",
      "Epoch 2:   5%|▍         | 25/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 1.8331575393676758\n",
      "Epoch 2:   5%|▌         | 26/512 [00:04<01:30,  5.35it/s, v_num=11]Training loss: 2.1434872150421143\n",
      "Epoch 2:   5%|▌         | 27/512 [00:05<01:30,  5.35it/s, v_num=11]Training loss: 1.2907750606536865\n",
      "Epoch 2:   5%|▌         | 28/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.1070607900619507\n",
      "Epoch 2:   6%|▌         | 29/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 2.190694570541382\n",
      "Epoch 2:   6%|▌         | 30/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.4887831211090088\n",
      "Epoch 2:   6%|▌         | 31/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.819346308708191\n",
      "Epoch 2:   6%|▋         | 32/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 1.1277835369110107\n",
      "Epoch 2:   6%|▋         | 33/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 2.0815443992614746\n",
      "Epoch 2:   7%|▋         | 34/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.7388837337493896\n",
      "Epoch 2:   7%|▋         | 35/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.3262094259262085\n",
      "Epoch 2:   7%|▋         | 36/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.5200148820877075\n",
      "Epoch 2:   7%|▋         | 37/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.5993928909301758\n",
      "Epoch 2:   7%|▋         | 38/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.5062745809555054\n",
      "Epoch 2:   8%|▊         | 39/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.7677209377288818\n",
      "Epoch 2:   8%|▊         | 40/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.3769550323486328\n",
      "Epoch 2:   8%|▊         | 41/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 1.6415823698043823\n",
      "Epoch 2:   8%|▊         | 42/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 2.5093541145324707\n",
      "Epoch 2:   8%|▊         | 43/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.9199717044830322\n",
      "Epoch 2:   9%|▊         | 44/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.9560526609420776\n",
      "Epoch 2:   9%|▉         | 45/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.1823643445968628\n",
      "Epoch 2:   9%|▉         | 46/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 1.8664928674697876\n",
      "Epoch 2:   9%|▉         | 47/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 2.2485697269439697\n",
      "Epoch 2:   9%|▉         | 48/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 2.0897021293640137\n",
      "Epoch 2:  10%|▉         | 49/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.5528919696807861\n",
      "Epoch 2:  10%|▉         | 50/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.8788520097732544\n",
      "Epoch 2:  10%|▉         | 51/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.6265311241149902\n",
      "Epoch 2:  10%|█         | 52/512 [00:09<01:27,  5.27it/s, v_num=11]Training loss: 1.832135558128357\n",
      "Epoch 2:  10%|█         | 53/512 [00:10<01:27,  5.27it/s, v_num=11]Training loss: 1.5611767768859863\n",
      "Epoch 2:  11%|█         | 54/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.7860726118087769\n",
      "Epoch 2:  11%|█         | 55/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.788397192955017\n",
      "Epoch 2:  11%|█         | 56/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.5235638618469238\n",
      "Epoch 2:  11%|█         | 57/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 2.5635108947753906\n",
      "Epoch 2:  11%|█▏        | 58/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 1.14534330368042\n",
      "Epoch 2:  12%|█▏        | 59/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 1.5486392974853516\n",
      "Epoch 2:  12%|█▏        | 60/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 2.029810905456543\n",
      "Epoch 2:  12%|█▏        | 61/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 2.0394654273986816\n",
      "Epoch 2:  12%|█▏        | 62/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.3127293586730957\n",
      "Epoch 2:  12%|█▏        | 63/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 3.1772871017456055\n",
      "Epoch 2:  12%|█▎        | 64/512 [00:12<01:25,  5.26it/s, v_num=11]Training loss: 1.3284635543823242\n",
      "Epoch 2:  13%|█▎        | 65/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.4292857646942139\n",
      "Epoch 2:  13%|█▎        | 66/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.6846463680267334\n",
      "Epoch 2:  13%|█▎        | 67/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.7262831926345825\n",
      "Epoch 2:  13%|█▎        | 68/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.8668551445007324\n",
      "Epoch 2:  13%|█▎        | 69/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 2.2033495903015137\n",
      "Epoch 2:  14%|█▎        | 70/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 1.8141270875930786\n",
      "Epoch 2:  14%|█▍        | 71/512 [00:13<01:23,  5.26it/s, v_num=11]Training loss: 1.6003468036651611\n",
      "Epoch 2:  14%|█▍        | 72/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 2.102707862854004\n",
      "Epoch 2:  14%|█▍        | 73/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.252349615097046\n",
      "Epoch 2:  14%|█▍        | 74/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 2.3704326152801514\n",
      "Epoch 2:  15%|█▍        | 75/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 2.0885660648345947\n",
      "Epoch 2:  15%|█▍        | 76/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.3422889709472656\n",
      "Epoch 2:  15%|█▌        | 77/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.1823288202285767\n",
      "Epoch 2:  15%|█▌        | 78/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.270059585571289\n",
      "Epoch 2:  15%|█▌        | 79/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.6959269046783447\n",
      "Epoch 2:  16%|█▌        | 80/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.359352469444275\n",
      "Epoch 2:  16%|█▌        | 81/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 2.8522629737854004\n",
      "Epoch 2:  16%|█▌        | 82/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.4640249013900757\n",
      "Epoch 2:  16%|█▌        | 83/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.937395453453064\n",
      "Epoch 2:  16%|█▋        | 84/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.5193727016448975\n",
      "Epoch 2:  17%|█▋        | 85/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.9975073337554932\n",
      "Epoch 2:  17%|█▋        | 86/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 2.3126797676086426\n",
      "Epoch 2:  17%|█▋        | 87/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.9660277366638184\n",
      "Epoch 2:  17%|█▋        | 88/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.4256505966186523\n",
      "Epoch 2:  17%|█▋        | 89/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.6604838371276855\n",
      "Epoch 2:  18%|█▊        | 90/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.3035441637039185\n",
      "Epoch 2:  18%|█▊        | 91/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 2.028909683227539\n",
      "Epoch 2:  18%|█▊        | 92/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.6603286266326904\n",
      "Epoch 2:  18%|█▊        | 93/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 1.5289381742477417\n",
      "Epoch 2:  18%|█▊        | 94/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 2.043806791305542\n",
      "Epoch 2:  19%|█▊        | 95/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.9503310918807983\n",
      "Epoch 2:  19%|█▉        | 96/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.5901992321014404\n",
      "Epoch 2:  19%|█▉        | 97/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 2.167750358581543\n",
      "Epoch 2:  19%|█▉        | 98/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.5550856590270996\n",
      "Epoch 2:  19%|█▉        | 99/512 [00:18<01:18,  5.24it/s, v_num=11]Training loss: 0.9916142225265503\n",
      "Epoch 2:  20%|█▉        | 100/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.5774054527282715\n",
      "Epoch 2:  20%|█▉        | 101/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.4754834175109863\n",
      "Epoch 2:  20%|█▉        | 102/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.4220118522644043\n",
      "Epoch 2:  20%|██        | 103/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.8504588603973389\n",
      "Epoch 2:  20%|██        | 104/512 [00:19<01:17,  5.24it/s, v_num=11]Training loss: 1.1521358489990234\n",
      "Epoch 2:  21%|██        | 105/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.569568157196045\n",
      "Epoch 2:  21%|██        | 106/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.3271808624267578\n",
      "Epoch 2:  21%|██        | 107/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.941408634185791\n",
      "Epoch 2:  21%|██        | 108/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 0.9587838649749756\n",
      "Epoch 2:  21%|██▏       | 109/512 [00:20<01:16,  5.24it/s, v_num=11]Training loss: 1.8329503536224365\n",
      "Epoch 2:  21%|██▏       | 110/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.1417187452316284\n",
      "Epoch 2:  22%|██▏       | 111/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.635345220565796\n",
      "Epoch 2:  22%|██▏       | 112/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.397953987121582\n",
      "Epoch 2:  22%|██▏       | 113/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.7904727458953857\n",
      "Epoch 2:  22%|██▏       | 114/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 2.366227149963379\n",
      "Epoch 2:  22%|██▏       | 115/512 [00:21<01:15,  5.23it/s, v_num=11]Training loss: 1.7754569053649902\n",
      "Epoch 2:  23%|██▎       | 116/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.2764394283294678\n",
      "Epoch 2:  23%|██▎       | 117/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.155295968055725\n",
      "Epoch 2:  23%|██▎       | 118/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.698748230934143\n",
      "Epoch 2:  23%|██▎       | 119/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.458878755569458\n",
      "Epoch 2:  23%|██▎       | 120/512 [00:22<01:14,  5.23it/s, v_num=11]Training loss: 1.1191391944885254\n",
      "Epoch 2:  24%|██▎       | 121/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 2.343625783920288\n",
      "Epoch 2:  24%|██▍       | 122/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.446856141090393\n",
      "Epoch 2:  24%|██▍       | 123/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.7419695854187012\n",
      "Epoch 2:  24%|██▍       | 124/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.1511006355285645\n",
      "Epoch 2:  24%|██▍       | 125/512 [00:23<01:13,  5.23it/s, v_num=11]Training loss: 1.2161812782287598\n",
      "Epoch 2:  25%|██▍       | 126/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5124406814575195\n",
      "Epoch 2:  25%|██▍       | 127/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5387974977493286\n",
      "Epoch 2:  25%|██▌       | 128/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.0667839050292969\n",
      "Epoch 2:  25%|██▌       | 129/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5363097190856934\n",
      "Epoch 2:  25%|██▌       | 130/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.6032192707061768\n",
      "Epoch 2:  26%|██▌       | 131/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.8912991285324097\n",
      "Epoch 2:  26%|██▌       | 132/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.448572039604187\n",
      "Epoch 2:  26%|██▌       | 133/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 5.038193225860596\n",
      "Epoch 2:  26%|██▌       | 134/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.8213783502578735\n",
      "Epoch 2:  26%|██▋       | 135/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.4079694747924805\n",
      "Epoch 2:  27%|██▋       | 136/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.2110695838928223\n",
      "Epoch 2:  27%|██▋       | 137/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.9386893510818481\n",
      "Epoch 2:  27%|██▋       | 138/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.8878780603408813\n",
      "Epoch 2:  27%|██▋       | 139/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.4272267818450928\n",
      "Epoch 2:  27%|██▋       | 140/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.428849697113037\n",
      "Epoch 2:  28%|██▊       | 141/512 [00:26<01:10,  5.23it/s, v_num=11]Training loss: 1.818412184715271\n",
      "Epoch 2:  28%|██▊       | 142/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.7548081874847412\n",
      "Epoch 2:  28%|██▊       | 143/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.2274118661880493\n",
      "Epoch 2:  28%|██▊       | 144/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.3612818717956543\n",
      "Epoch 2:  28%|██▊       | 145/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.719407558441162\n",
      "Epoch 2:  29%|██▊       | 146/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.2648606300354004\n",
      "Epoch 2:  29%|██▊       | 147/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.5211679935455322\n",
      "Epoch 2:  29%|██▉       | 148/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.489162802696228\n",
      "Epoch 2:  29%|██▉       | 149/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.7028648853302002\n",
      "Epoch 2:  29%|██▉       | 150/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.4233477115631104\n",
      "Epoch 2:  29%|██▉       | 151/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.5411181449890137\n",
      "Epoch 2:  30%|██▉       | 152/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.6986122131347656\n",
      "Epoch 2:  30%|██▉       | 153/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.7660282850265503\n",
      "Epoch 2:  30%|███       | 154/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.7131547927856445\n",
      "Epoch 2:  30%|███       | 155/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.2656495571136475\n",
      "Epoch 2:  30%|███       | 156/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.8888840675354004\n",
      "Epoch 2:  31%|███       | 157/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.7694692611694336\n",
      "Epoch 2:  31%|███       | 158/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.3960222005844116\n",
      "Epoch 2:  31%|███       | 159/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.1819250583648682\n",
      "Epoch 2:  31%|███▏      | 160/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.7177366018295288\n",
      "Epoch 2:  31%|███▏      | 161/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.6296937465667725\n",
      "Epoch 2:  32%|███▏      | 162/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.5727622509002686\n",
      "Epoch 2:  32%|███▏      | 163/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.441648244857788\n",
      "Epoch 2:  32%|███▏      | 164/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.3018739223480225\n",
      "Epoch 2:  32%|███▏      | 165/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.4252655506134033\n",
      "Epoch 2:  32%|███▏      | 166/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.9007012844085693\n",
      "Epoch 2:  33%|███▎      | 167/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.405663251876831\n",
      "Epoch 2:  33%|███▎      | 168/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.8627647161483765\n",
      "Epoch 2:  33%|███▎      | 169/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.7574701309204102\n",
      "Epoch 2:  33%|███▎      | 170/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.2150849103927612\n",
      "Epoch 2:  33%|███▎      | 171/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.5439926385879517\n",
      "Epoch 2:  34%|███▎      | 172/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.3213989734649658\n",
      "Epoch 2:  34%|███▍      | 173/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.937386393547058\n",
      "Epoch 2:  34%|███▍      | 174/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.708651065826416\n",
      "Epoch 2:  34%|███▍      | 175/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.9256839752197266\n",
      "Epoch 2:  34%|███▍      | 176/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.8166519403457642\n",
      "Epoch 2:  35%|███▍      | 177/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.2891359329223633\n",
      "Epoch 2:  35%|███▍      | 178/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.5816681385040283\n",
      "Epoch 2:  35%|███▍      | 179/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.7961459159851074\n",
      "Epoch 2:  35%|███▌      | 180/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.3964238166809082\n",
      "Epoch 2:  35%|███▌      | 181/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.505446434020996\n",
      "Epoch 2:  36%|███▌      | 182/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.0459686517715454\n",
      "Epoch 2:  36%|███▌      | 183/512 [00:35<01:03,  5.22it/s, v_num=11]Training loss: 2.2513587474823\n",
      "Epoch 2:  36%|███▌      | 184/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 2.1007754802703857\n",
      "Epoch 2:  36%|███▌      | 185/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.5153733491897583\n",
      "Epoch 2:  36%|███▋      | 186/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.6001981496810913\n",
      "Epoch 2:  37%|███▋      | 187/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.4128444194793701\n",
      "Epoch 2:  37%|███▋      | 188/512 [00:36<01:02,  5.22it/s, v_num=11]Training loss: 1.5673750638961792\n",
      "Epoch 2:  37%|███▋      | 189/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.3102154731750488\n",
      "Epoch 2:  37%|███▋      | 190/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.7415428161621094\n",
      "Epoch 2:  37%|███▋      | 191/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.683356523513794\n",
      "Epoch 2:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.7067904472351074\n",
      "Epoch 2:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.2241594791412354\n",
      "Epoch 2:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.2959738969802856\n",
      "Epoch 2:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.4118279218673706\n",
      "Epoch 2:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.550107717514038\n",
      "Epoch 2:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.2709709405899048\n",
      "Epoch 2:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.2497305870056152\n",
      "Epoch 2:  39%|███▉      | 199/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.8485904932022095\n",
      "Epoch 2:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.5976710319519043\n",
      "Epoch 2:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.0670251846313477\n",
      "Epoch 2:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.2492759227752686\n",
      "Epoch 2:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.9970145225524902\n",
      "Epoch 2:  40%|███▉      | 204/512 [00:39<00:59,  5.22it/s, v_num=11]Training loss: 1.7348017692565918\n",
      "Epoch 2:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.472209095954895\n",
      "Epoch 2:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.6381477117538452\n",
      "Epoch 2:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.360933780670166\n",
      "Epoch 2:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.8776105642318726\n",
      "Epoch 2:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 2.75241756439209\n",
      "Epoch 2:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.733648657798767\n",
      "Epoch 2:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.681761622428894\n",
      "Epoch 2:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.5221573114395142\n",
      "Epoch 2:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.7699620723724365\n",
      "Epoch 2:  42%|████▏     | 214/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.7723326683044434\n",
      "Epoch 2:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4537951946258545\n",
      "Epoch 2:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 2.1022019386291504\n",
      "Epoch 2:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.3652446269989014\n",
      "Epoch 2:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.842586874961853\n",
      "Epoch 2:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4939360618591309\n",
      "Epoch 2:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.317111611366272\n",
      "Epoch 2:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.6890764236450195\n",
      "Epoch 2:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.3475914001464844\n",
      "Epoch 2:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.4191746711730957\n",
      "Epoch 2:  44%|████▍     | 224/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.549288034439087\n",
      "Epoch 2:  44%|████▍     | 225/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.6777386665344238\n",
      "Epoch 2:  44%|████▍     | 226/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.0923020839691162\n",
      "Epoch 2:  44%|████▍     | 227/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.6860554218292236\n",
      "Epoch 2:  45%|████▍     | 228/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 2.2080609798431396\n",
      "Epoch 2:  45%|████▍     | 229/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.5669522285461426\n",
      "Epoch 2:  45%|████▍     | 230/512 [00:44<00:54,  5.22it/s, v_num=11]Training loss: 1.531567096710205\n",
      "Epoch 2:  45%|████▌     | 231/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.4174057245254517\n",
      "Epoch 2:  45%|████▌     | 232/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.0854644775390625\n",
      "Epoch 2:  46%|████▌     | 233/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.1110140085220337\n",
      "Epoch 2:  46%|████▌     | 234/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.4203038215637207\n",
      "Epoch 2:  46%|████▌     | 235/512 [00:45<00:53,  5.22it/s, v_num=11]Training loss: 1.2268626689910889\n",
      "Epoch 2:  46%|████▌     | 236/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.2118744850158691\n",
      "Epoch 2:  46%|████▋     | 237/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.3679790496826172\n",
      "Epoch 2:  46%|████▋     | 238/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.360596776008606\n",
      "Epoch 2:  47%|████▋     | 239/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.2425124645233154\n",
      "Epoch 2:  47%|████▋     | 240/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.3050509691238403\n",
      "Epoch 2:  47%|████▋     | 241/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.2707459926605225\n",
      "Epoch 2:  47%|████▋     | 242/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.385430097579956\n",
      "Epoch 2:  47%|████▋     | 243/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 10.23159122467041\n",
      "Epoch 2:  48%|████▊     | 244/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.5164674520492554\n",
      "Epoch 2:  48%|████▊     | 245/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.4505393505096436\n",
      "Epoch 2:  48%|████▊     | 246/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.382571816444397\n",
      "Epoch 2:  48%|████▊     | 247/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.640212893486023\n",
      "Epoch 2:  48%|████▊     | 248/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 2.298719882965088\n",
      "Epoch 2:  49%|████▊     | 249/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 2.015763521194458\n",
      "Epoch 2:  49%|████▉     | 250/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.6384029388427734\n",
      "Epoch 2:  49%|████▉     | 251/512 [00:48<00:50,  5.22it/s, v_num=11]Training loss: 1.6997056007385254\n",
      "Epoch 2:  49%|████▉     | 252/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 2.48870849609375\n",
      "Epoch 2:  49%|████▉     | 253/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.5262905359268188\n",
      "Epoch 2:  50%|████▉     | 254/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.8882699012756348\n",
      "Epoch 2:  50%|████▉     | 255/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 2.012495517730713\n",
      "Epoch 2:  50%|█████     | 256/512 [00:49<00:49,  5.22it/s, v_num=11]Training loss: 1.7961851358413696\n",
      "Epoch 2:  50%|█████     | 257/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.1636244058609009\n",
      "Epoch 2:  50%|█████     | 258/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.054095983505249\n",
      "Epoch 2:  51%|█████     | 259/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.565423846244812\n",
      "Epoch 2:  51%|█████     | 260/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.3459053039550781\n",
      "Epoch 2:  51%|█████     | 261/512 [00:50<00:48,  5.22it/s, v_num=11]Training loss: 1.0412765741348267\n",
      "Epoch 2:  51%|█████     | 262/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.3713799715042114\n",
      "Epoch 2:  51%|█████▏    | 263/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.147095799446106\n",
      "Epoch 2:  52%|█████▏    | 264/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.3907538652420044\n",
      "Epoch 2:  52%|█████▏    | 265/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.306113839149475\n",
      "Epoch 2:  52%|█████▏    | 266/512 [00:51<00:47,  5.21it/s, v_num=11]Training loss: 1.6095465421676636\n",
      "Epoch 2:  52%|█████▏    | 267/512 [00:51<00:47,  5.21it/s, v_num=11]Training loss: 33.7943229675293\n",
      "Epoch 2:  52%|█████▏    | 268/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.578965663909912\n",
      "Epoch 2:  53%|█████▎    | 269/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.2221907377243042\n",
      "Epoch 2:  53%|█████▎    | 270/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.4160232543945312\n",
      "Epoch 2:  53%|█████▎    | 271/512 [00:52<00:46,  5.21it/s, v_num=11]Training loss: 2.1989455223083496\n",
      "Epoch 2:  53%|█████▎    | 272/512 [00:52<00:46,  5.21it/s, v_num=11]Training loss: 1.3176764249801636\n",
      "Epoch 2:  53%|█████▎    | 273/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.716353178024292\n",
      "Epoch 2:  54%|█████▎    | 274/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.5229246616363525\n",
      "Epoch 2:  54%|█████▎    | 275/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.2938690185546875\n",
      "Epoch 2:  54%|█████▍    | 276/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.7224407196044922\n",
      "Epoch 2:  54%|█████▍    | 277/512 [00:53<00:45,  5.21it/s, v_num=11]Training loss: 1.4448144435882568\n",
      "Epoch 2:  54%|█████▍    | 278/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.0265196561813354\n",
      "Epoch 2:  54%|█████▍    | 279/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.7620309591293335\n",
      "Epoch 2:  55%|█████▍    | 280/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.6903436183929443\n",
      "Epoch 2:  55%|█████▍    | 281/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.937261939048767\n",
      "Epoch 2:  55%|█████▌    | 282/512 [00:54<00:44,  5.21it/s, v_num=11]Training loss: 2.356632709503174\n",
      "Epoch 2:  55%|█████▌    | 283/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 1.8133440017700195\n",
      "Epoch 2:  55%|█████▌    | 284/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 2.4666061401367188\n",
      "Epoch 2:  56%|█████▌    | 285/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 2.194833755493164\n",
      "Epoch 2:  56%|█████▌    | 286/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 2.039443254470825\n",
      "Epoch 2:  56%|█████▌    | 287/512 [00:55<00:43,  5.21it/s, v_num=11]Training loss: 1.470490574836731\n",
      "Epoch 2:  56%|█████▋    | 288/512 [00:55<00:43,  5.21it/s, v_num=11]Training loss: 1.0917797088623047\n",
      "Epoch 2:  56%|█████▋    | 289/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.601832628250122\n",
      "Epoch 2:  57%|█████▋    | 290/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.8033711910247803\n",
      "Epoch 2:  57%|█████▋    | 291/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.8589832782745361\n",
      "Epoch 2:  57%|█████▋    | 292/512 [00:56<00:42,  5.21it/s, v_num=11]Training loss: 1.4098906517028809\n",
      "Epoch 2:  57%|█████▋    | 293/512 [00:56<00:42,  5.21it/s, v_num=11]Training loss: 1.1738702058792114\n",
      "Epoch 2:  57%|█████▋    | 294/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 1.069332480430603\n",
      "Epoch 2:  58%|█████▊    | 295/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 1.354913592338562\n",
      "Epoch 2:  58%|█████▊    | 296/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 1.7714805603027344\n",
      "Epoch 2:  58%|█████▊    | 297/512 [00:57<00:41,  5.21it/s, v_num=11]Training loss: 1.4736473560333252\n",
      "Epoch 2:  58%|█████▊    | 298/512 [00:57<00:41,  5.21it/s, v_num=11]Training loss: 1.5201756954193115\n",
      "Epoch 2:  58%|█████▊    | 299/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.5129835605621338\n",
      "Epoch 2:  59%|█████▊    | 300/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.3947279453277588\n",
      "Epoch 2:  59%|█████▉    | 301/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.9685983657836914\n",
      "Epoch 2:  59%|█████▉    | 302/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.0459734201431274\n",
      "Epoch 2:  59%|█████▉    | 303/512 [00:58<00:40,  5.21it/s, v_num=11]Training loss: 1.3988797664642334\n",
      "Epoch 2:  59%|█████▉    | 304/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 2.2296319007873535\n",
      "Epoch 2:  60%|█████▉    | 305/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.947541356086731\n",
      "Epoch 2:  60%|█████▉    | 306/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.5829887390136719\n",
      "Epoch 2:  60%|█████▉    | 307/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.6413462162017822\n",
      "Epoch 2:  60%|██████    | 308/512 [00:59<00:39,  5.21it/s, v_num=11]Training loss: 0.9239028692245483\n",
      "Epoch 2:  60%|██████    | 309/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 2.361013174057007\n",
      "Epoch 2:  61%|██████    | 310/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.2743921279907227\n",
      "Epoch 2:  61%|██████    | 311/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.7008930444717407\n",
      "Epoch 2:  61%|██████    | 312/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.6970484256744385\n",
      "Epoch 2:  61%|██████    | 313/512 [01:00<00:38,  5.21it/s, v_num=11]Training loss: 1.6692752838134766\n",
      "Epoch 2:  61%|██████▏   | 314/512 [01:00<00:38,  5.21it/s, v_num=11]Training loss: 1.5218617916107178\n",
      "Epoch 2:  62%|██████▏   | 315/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 1.4070993661880493\n",
      "Epoch 2:  62%|██████▏   | 316/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 1.54568612575531\n",
      "Epoch 2:  62%|██████▏   | 317/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 1.420461893081665\n",
      "Epoch 2:  62%|██████▏   | 318/512 [01:01<00:37,  5.21it/s, v_num=11]Training loss: 1.3906238079071045\n",
      "Epoch 2:  62%|██████▏   | 319/512 [01:01<00:37,  5.21it/s, v_num=11]Training loss: 1.1285768747329712\n",
      "Epoch 2:  62%|██████▎   | 320/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.7309622764587402\n",
      "Epoch 2:  63%|██████▎   | 321/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.3454400300979614\n",
      "Epoch 2:  63%|██████▎   | 322/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.8933031558990479\n",
      "Epoch 2:  63%|██████▎   | 323/512 [01:02<00:36,  5.21it/s, v_num=11]Training loss: 1.3824857473373413\n",
      "Epoch 2:  63%|██████▎   | 324/512 [01:02<00:36,  5.21it/s, v_num=11]Training loss: 1.2468048334121704\n",
      "Epoch 2:  63%|██████▎   | 325/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.661226511001587\n",
      "Epoch 2:  64%|██████▎   | 326/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.6400996446609497\n",
      "Epoch 2:  64%|██████▍   | 327/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.21965754032135\n",
      "Epoch 2:  64%|██████▍   | 328/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.9182798862457275\n",
      "Epoch 2:  64%|██████▍   | 329/512 [01:03<00:35,  5.21it/s, v_num=11]Training loss: 2.067502975463867\n",
      "Epoch 2:  64%|██████▍   | 330/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.7161502838134766\n",
      "Epoch 2:  65%|██████▍   | 331/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.4082353115081787\n",
      "Epoch 2:  65%|██████▍   | 332/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.7212872505187988\n",
      "Epoch 2:  65%|██████▌   | 333/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.9233789443969727\n",
      "Epoch 2:  65%|██████▌   | 334/512 [01:04<00:34,  5.21it/s, v_num=11]Training loss: 1.7091622352600098\n",
      "Epoch 2:  65%|██████▌   | 335/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.3694298267364502\n",
      "Epoch 2:  66%|██████▌   | 336/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.8357901573181152\n",
      "Epoch 2:  66%|██████▌   | 337/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.261265754699707\n",
      "Epoch 2:  66%|██████▌   | 338/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.5057334899902344\n",
      "Epoch 2:  66%|██████▌   | 339/512 [01:05<00:33,  5.21it/s, v_num=11]Training loss: 1.5943739414215088\n",
      "Epoch 2:  66%|██████▋   | 340/512 [01:05<00:33,  5.21it/s, v_num=11]Training loss: 1.6147053241729736\n",
      "Epoch 2:  67%|██████▋   | 341/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.02748703956604\n",
      "Epoch 2:  67%|██████▋   | 342/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 2.6797139644622803\n",
      "Epoch 2:  67%|██████▋   | 343/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.3120425939559937\n",
      "Epoch 2:  67%|██████▋   | 344/512 [01:06<00:32,  5.21it/s, v_num=11]Training loss: 1.8853962421417236\n",
      "Epoch 2:  67%|██████▋   | 345/512 [01:06<00:32,  5.21it/s, v_num=11]Training loss: 1.8970723152160645\n",
      "Epoch 2:  68%|██████▊   | 346/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.6002800464630127\n",
      "Epoch 2:  68%|██████▊   | 347/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.3818433284759521\n",
      "Epoch 2:  68%|██████▊   | 348/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.5453225374221802\n",
      "Epoch 2:  68%|██████▊   | 349/512 [01:07<00:31,  5.21it/s, v_num=11]Training loss: 1.6373178958892822\n",
      "Epoch 2:  68%|██████▊   | 350/512 [01:07<00:31,  5.21it/s, v_num=11]Training loss: 1.847915530204773\n",
      "Epoch 2:  69%|██████▊   | 351/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.413926601409912\n",
      "Epoch 2:  69%|██████▉   | 352/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.6539463996887207\n",
      "Epoch 2:  69%|██████▉   | 353/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.9230260848999023\n",
      "Epoch 2:  69%|██████▉   | 354/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.058840274810791\n",
      "Epoch 2:  69%|██████▉   | 355/512 [01:08<00:30,  5.21it/s, v_num=11]Training loss: 1.7760717868804932\n",
      "Epoch 2:  70%|██████▉   | 356/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.0459610223770142\n",
      "Epoch 2:  70%|██████▉   | 357/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.294937252998352\n",
      "Epoch 2:  70%|██████▉   | 358/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 2.266033411026001\n",
      "Epoch 2:  70%|███████   | 359/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 2.4631755352020264\n",
      "Epoch 2:  70%|███████   | 360/512 [01:09<00:29,  5.21it/s, v_num=11]Training loss: 1.647885799407959\n",
      "Epoch 2:  71%|███████   | 361/512 [01:09<00:29,  5.21it/s, v_num=11]Training loss: 1.8220707178115845\n",
      "Epoch 2:  71%|███████   | 362/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.6988630294799805\n",
      "Epoch 2:  71%|███████   | 363/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.4602028131484985\n",
      "Epoch 2:  71%|███████   | 364/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.9874484539031982\n",
      "Epoch 2:  71%|███████▏  | 365/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 1.4849894046783447\n",
      "Epoch 2:  71%|███████▏  | 366/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 2.051964282989502\n",
      "Epoch 2:  72%|███████▏  | 367/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.2303509712219238\n",
      "Epoch 2:  72%|███████▏  | 368/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.4433016777038574\n",
      "Epoch 2:  72%|███████▏  | 369/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.9811664819717407\n",
      "Epoch 2:  72%|███████▏  | 370/512 [01:11<00:27,  5.21it/s, v_num=11]Training loss: 1.5720785856246948\n",
      "Epoch 2:  72%|███████▏  | 371/512 [01:11<00:27,  5.21it/s, v_num=11]Training loss: 2.2140541076660156\n",
      "Epoch 2:  73%|███████▎  | 372/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.5678174495697021\n",
      "Epoch 2:  73%|███████▎  | 373/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.3602118492126465\n",
      "Epoch 2:  73%|███████▎  | 374/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.751299500465393\n",
      "Epoch 2:  73%|███████▎  | 375/512 [01:12<00:26,  5.21it/s, v_num=11]Training loss: 2.293480396270752\n",
      "Epoch 2:  73%|███████▎  | 376/512 [01:12<00:26,  5.21it/s, v_num=11]Training loss: 1.3939330577850342\n",
      "Epoch 2:  74%|███████▎  | 377/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.029992699623108\n",
      "Epoch 2:  74%|███████▍  | 378/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.5558922290802002\n",
      "Epoch 2:  74%|███████▍  | 379/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.4938175678253174\n",
      "Epoch 2:  74%|███████▍  | 380/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 43.5436897277832\n",
      "Epoch 2:  74%|███████▍  | 381/512 [01:13<00:25,  5.21it/s, v_num=11]Training loss: 1.7294843196868896\n",
      "Epoch 2:  75%|███████▍  | 382/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.4405863285064697\n",
      "Epoch 2:  75%|███████▍  | 383/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.8183228969573975\n",
      "Epoch 2:  75%|███████▌  | 384/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.9106159210205078\n",
      "Epoch 2:  75%|███████▌  | 385/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.5613369941711426\n",
      "Epoch 2:  75%|███████▌  | 386/512 [01:14<00:24,  5.21it/s, v_num=11]Training loss: 1.4418425559997559\n",
      "Epoch 2:  76%|███████▌  | 387/512 [01:14<00:24,  5.21it/s, v_num=11]Training loss: 1.545959234237671\n",
      "Epoch 2:  76%|███████▌  | 388/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.1961979866027832\n",
      "Epoch 2:  76%|███████▌  | 389/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.8155535459518433\n",
      "Epoch 2:  76%|███████▌  | 390/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.1701738834381104\n",
      "Epoch 2:  76%|███████▋  | 391/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 1.503279685974121\n",
      "Epoch 2:  77%|███████▋  | 392/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 1.8342747688293457\n",
      "Epoch 2:  77%|███████▋  | 393/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.3670222759246826\n",
      "Epoch 2:  77%|███████▋  | 394/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 2.2462239265441895\n",
      "Epoch 2:  77%|███████▋  | 395/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.7635536193847656\n",
      "Epoch 2:  77%|███████▋  | 396/512 [01:16<00:22,  5.21it/s, v_num=11]Training loss: 2.1761646270751953\n",
      "Epoch 2:  78%|███████▊  | 397/512 [01:16<00:22,  5.21it/s, v_num=11]Training loss: 1.839668869972229\n",
      "Epoch 2:  78%|███████▊  | 398/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.1933411359786987\n",
      "Epoch 2:  78%|███████▊  | 399/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.2342908382415771\n",
      "Epoch 2:  78%|███████▊  | 400/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 3.1349754333496094\n",
      "Epoch 2:  78%|███████▊  | 401/512 [01:17<00:21,  5.21it/s, v_num=11]Training loss: 1.0417890548706055\n",
      "Epoch 2:  79%|███████▊  | 402/512 [01:17<00:21,  5.21it/s, v_num=11]Training loss: 1.620795726776123\n",
      "Epoch 2:  79%|███████▊  | 403/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.3766134977340698\n",
      "Epoch 2:  79%|███████▉  | 404/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.577131748199463\n",
      "Epoch 2:  79%|███████▉  | 405/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.2840193510055542\n",
      "Epoch 2:  79%|███████▉  | 406/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.556991696357727\n",
      "Epoch 2:  79%|███████▉  | 407/512 [01:18<00:20,  5.21it/s, v_num=11]Training loss: 1.352083683013916\n",
      "Epoch 2:  80%|███████▉  | 408/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.159862756729126\n",
      "Epoch 2:  80%|███████▉  | 409/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.4840993881225586\n",
      "Epoch 2:  80%|████████  | 410/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.343503713607788\n",
      "Epoch 2:  80%|████████  | 411/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.0826196670532227\n",
      "Epoch 2:  80%|████████  | 412/512 [01:19<00:19,  5.21it/s, v_num=11]Training loss: 1.2701797485351562\n",
      "Epoch 2:  81%|████████  | 413/512 [01:19<00:19,  5.21it/s, v_num=11]Training loss: 1.6138464212417603\n",
      "Epoch 2:  81%|████████  | 414/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 2.976823329925537\n",
      "Epoch 2:  81%|████████  | 415/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.4838898181915283\n",
      "Epoch 2:  81%|████████▏ | 416/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.7645387649536133\n",
      "Epoch 2:  81%|████████▏ | 417/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 1.0740288496017456\n",
      "Epoch 2:  82%|████████▏ | 418/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 1.4579339027404785\n",
      "Epoch 2:  82%|████████▏ | 419/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.4395735263824463\n",
      "Epoch 2:  82%|████████▏ | 420/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 2.213927984237671\n",
      "Epoch 2:  82%|████████▏ | 421/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.8722140789031982\n",
      "Epoch 2:  82%|████████▏ | 422/512 [01:21<00:17,  5.21it/s, v_num=11]Training loss: 2.151808261871338\n",
      "Epoch 2:  83%|████████▎ | 423/512 [01:21<00:17,  5.21it/s, v_num=11]Training loss: 1.8351786136627197\n",
      "Epoch 2:  83%|████████▎ | 424/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.5423496961593628\n",
      "Epoch 2:  83%|████████▎ | 425/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.8023977279663086\n",
      "Epoch 2:  83%|████████▎ | 426/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.8864794969558716\n",
      "Epoch 2:  83%|████████▎ | 427/512 [01:22<00:16,  5.21it/s, v_num=11]Training loss: 1.5848147869110107\n",
      "Epoch 2:  84%|████████▎ | 428/512 [01:22<00:16,  5.21it/s, v_num=11]Training loss: 1.8710272312164307\n",
      "Epoch 2:  84%|████████▍ | 429/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.431584358215332\n",
      "Epoch 2:  84%|████████▍ | 430/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 2.5178000926971436\n",
      "Epoch 2:  84%|████████▍ | 431/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.3123314380645752\n",
      "Epoch 2:  84%|████████▍ | 432/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.7049877643585205\n",
      "Epoch 2:  85%|████████▍ | 433/512 [01:23<00:15,  5.21it/s, v_num=11]Training loss: 2.2593817710876465\n",
      "Epoch 2:  85%|████████▍ | 434/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.3252248764038086\n",
      "Epoch 2:  85%|████████▍ | 435/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.3286890983581543\n",
      "Epoch 2:  85%|████████▌ | 436/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.7969145774841309\n",
      "Epoch 2:  85%|████████▌ | 437/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 2.416595458984375\n",
      "Epoch 2:  86%|████████▌ | 438/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 1.6405649185180664\n",
      "Epoch 2:  86%|████████▌ | 439/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 1.4372954368591309\n",
      "Epoch 2:  86%|████████▌ | 440/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 2.0623021125793457\n",
      "Epoch 2:  86%|████████▌ | 441/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.422948956489563\n",
      "Epoch 2:  86%|████████▋ | 442/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.3556541204452515\n",
      "Epoch 2:  87%|████████▋ | 443/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.353180170059204\n",
      "Epoch 2:  87%|████████▋ | 444/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.6201348304748535\n",
      "Epoch 2:  87%|████████▋ | 445/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.738888144493103\n",
      "Epoch 2:  87%|████████▋ | 446/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.46859872341156\n",
      "Epoch 2:  87%|████████▋ | 447/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.4929617643356323\n",
      "Epoch 2:  88%|████████▊ | 448/512 [01:26<00:12,  5.21it/s, v_num=11]Training loss: 1.7955231666564941\n",
      "Epoch 2:  88%|████████▊ | 449/512 [01:26<00:12,  5.21it/s, v_num=11]Training loss: 1.5128347873687744\n",
      "Epoch 2:  88%|████████▊ | 450/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.3036527633666992\n",
      "Epoch 2:  88%|████████▊ | 451/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.2121553421020508\n",
      "Epoch 2:  88%|████████▊ | 452/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.2117869853973389\n",
      "Epoch 2:  88%|████████▊ | 453/512 [01:27<00:11,  5.21it/s, v_num=11]Training loss: 2.1892545223236084\n",
      "Epoch 2:  89%|████████▊ | 454/512 [01:27<00:11,  5.21it/s, v_num=11]Training loss: 2.3709816932678223\n",
      "Epoch 2:  89%|████████▉ | 455/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.8843929767608643\n",
      "Epoch 2:  89%|████████▉ | 456/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.2458076477050781\n",
      "Epoch 2:  89%|████████▉ | 457/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.387152910232544\n",
      "Epoch 2:  89%|████████▉ | 458/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.3312921524047852\n",
      "Epoch 2:  90%|████████▉ | 459/512 [01:28<00:10,  5.21it/s, v_num=11]Training loss: 1.3097608089447021\n",
      "Epoch 2:  90%|████████▉ | 460/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.8363606929779053\n",
      "Epoch 2:  90%|█████████ | 461/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.3665481805801392\n",
      "Epoch 2:  90%|█████████ | 462/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.379287838935852\n",
      "Epoch 2:  90%|█████████ | 463/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.200103521347046\n",
      "Epoch 2:  91%|█████████ | 464/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 1.5956759452819824\n",
      "Epoch 2:  91%|█████████ | 465/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 2.0148589611053467\n",
      "Epoch 2:  91%|█████████ | 466/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.2874698638916016\n",
      "Epoch 2:  91%|█████████ | 467/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.4174964427947998\n",
      "Epoch 2:  91%|█████████▏| 468/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.3015981912612915\n",
      "Epoch 2:  92%|█████████▏| 469/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.8486244678497314\n",
      "Epoch 2:  92%|█████████▏| 470/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.7326939105987549\n",
      "Epoch 2:  92%|█████████▏| 471/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.1940889358520508\n",
      "Epoch 2:  92%|█████████▏| 472/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.6692231893539429\n",
      "Epoch 2:  92%|█████████▏| 473/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.341325044631958\n",
      "Epoch 2:  93%|█████████▎| 474/512 [01:31<00:07,  5.21it/s, v_num=11]Training loss: 1.2956212759017944\n",
      "Epoch 2:  93%|█████████▎| 475/512 [01:31<00:07,  5.21it/s, v_num=11]Training loss: 1.4513459205627441\n",
      "Epoch 2:  93%|█████████▎| 476/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.3965070247650146\n",
      "Epoch 2:  93%|█████████▎| 477/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 2.132579803466797\n",
      "Epoch 2:  93%|█████████▎| 478/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.5321311950683594\n",
      "Epoch 2:  94%|█████████▎| 479/512 [01:32<00:06,  5.21it/s, v_num=11]Training loss: 1.5742548704147339\n",
      "Epoch 2:  94%|█████████▍| 480/512 [01:32<00:06,  5.21it/s, v_num=11]Training loss: 1.699152946472168\n",
      "Epoch 2:  94%|█████████▍| 481/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.514229416847229\n",
      "Epoch 2:  94%|█████████▍| 482/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.0510600805282593\n",
      "Epoch 2:  94%|█████████▍| 483/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.7524878978729248\n",
      "Epoch 2:  95%|█████████▍| 484/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 2.0500731468200684\n",
      "Epoch 2:  95%|█████████▍| 485/512 [01:33<00:05,  5.21it/s, v_num=11]Training loss: 1.6692476272583008\n",
      "Epoch 2:  95%|█████████▍| 486/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.7258163690567017\n",
      "Epoch 2:  95%|█████████▌| 487/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.6213274002075195\n",
      "Epoch 2:  95%|█████████▌| 488/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.4206596612930298\n",
      "Epoch 2:  96%|█████████▌| 489/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 2.1069295406341553\n",
      "Epoch 2:  96%|█████████▌| 490/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 1.8109132051467896\n",
      "Epoch 2:  96%|█████████▌| 491/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 1.191657543182373\n",
      "Epoch 2:  96%|█████████▌| 492/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.372990608215332\n",
      "Epoch 2:  96%|█████████▋| 493/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.7035484313964844\n",
      "Epoch 2:  96%|█████████▋| 494/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.2650164365768433\n",
      "Epoch 2:  97%|█████████▋| 495/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.3380851745605469\n",
      "Epoch 2:  97%|█████████▋| 496/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.1424200534820557\n",
      "Epoch 2:  97%|█████████▋| 497/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.5898092985153198\n",
      "Epoch 2:  97%|█████████▋| 498/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.6935901641845703\n",
      "Epoch 2:  97%|█████████▋| 499/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.5871598720550537\n",
      "Epoch 2:  98%|█████████▊| 500/512 [01:36<00:02,  5.21it/s, v_num=11]Training loss: 1.6340458393096924\n",
      "Epoch 2:  98%|█████████▊| 501/512 [01:36<00:02,  5.21it/s, v_num=11]Training loss: 1.5150692462921143\n",
      "Epoch 2:  98%|█████████▊| 502/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.8616567850112915\n",
      "Epoch 2:  98%|█████████▊| 503/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.752166986465454\n",
      "Epoch 2:  98%|█████████▊| 504/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 4.395976543426514\n",
      "Epoch 2:  99%|█████████▊| 505/512 [01:37<00:01,  5.21it/s, v_num=11]Training loss: 1.574806809425354\n",
      "Epoch 2:  99%|█████████▉| 506/512 [01:37<00:01,  5.21it/s, v_num=11]Training loss: 1.5319623947143555\n",
      "Epoch 2:  99%|█████████▉| 507/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.882594347000122\n",
      "Epoch 2:  99%|█████████▉| 508/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.2108838558197021\n",
      "Epoch 2:  99%|█████████▉| 509/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.8993916511535645\n",
      "Epoch 2: 100%|█████████▉| 510/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.1428720951080322\n",
      "Epoch 2: 100%|█████████▉| 511/512 [01:38<00:00,  5.21it/s, v_num=11]Training loss: 1.2423263788223267\n",
      "Epoch 2: 100%|██████████| 512/512 [01:38<00:00,  5.21it/s, v_num=11]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 2.9019269943237305\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:02, 49.34it/s]\u001b[AValidation loss: 1.5181219577789307\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:02, 36.33it/s]\u001b[AValidation loss: 1.7347581386566162\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:03, 33.36it/s]\u001b[AValidation loss: 1.4628615379333496\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:03, 31.94it/s]\u001b[AValidation loss: 1.328658103942871\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:03, 31.22it/s]\u001b[AValidation loss: 2.1651906967163086\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:03, 31.49it/s]\u001b[AValidation loss: 2.1916258335113525\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:03, 31.88it/s]\u001b[AValidation loss: 1.4811456203460693\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:03, 32.04it/s]\u001b[AValidation loss: 2.2660388946533203\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:03, 32.16it/s]\u001b[AValidation loss: 1.5026822090148926\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:03, 32.41it/s]\u001b[AValidation loss: 1.504915475845337\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:03, 32.47it/s]\u001b[AValidation loss: 1.5157124996185303\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:03, 32.52it/s]\u001b[AValidation loss: 1.6787497997283936\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:02, 32.58it/s]\u001b[AValidation loss: 1.4459409713745117\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:02, 32.55it/s]\u001b[AValidation loss: 1.5546696186065674\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:02, 32.34it/s]\u001b[AValidation loss: 1.3636118173599243\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:02, 32.37it/s]\u001b[AValidation loss: 1.6597580909729004\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:02, 32.51it/s]\u001b[AValidation loss: 1.5023362636566162\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:02, 32.54it/s]\u001b[AValidation loss: 1.6555607318878174\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:02, 32.67it/s]\u001b[AValidation loss: 1.8739606142044067\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:02, 32.69it/s]\u001b[AValidation loss: 1.8094499111175537\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:02, 32.80it/s]\u001b[AValidation loss: 1.523280143737793\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:02, 32.81it/s]\u001b[AValidation loss: 1.768046498298645\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:00<00:02, 32.90it/s]\u001b[AValidation loss: 1.865519642829895\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:00<00:02, 32.91it/s]\u001b[AValidation loss: 1.5385377407073975\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:00<00:02, 32.99it/s]\u001b[AValidation loss: 1.8964688777923584\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:00<00:02, 33.00it/s]\u001b[AValidation loss: 1.455946922302246\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:00<00:02, 33.08it/s]\u001b[AValidation loss: 2.0405373573303223\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:00<00:02, 33.09it/s]\u001b[AValidation loss: 2.0206003189086914\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:00<00:02, 33.10it/s]\u001b[AValidation loss: 2.433385133743286\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:00<00:02, 33.16it/s]\u001b[AValidation loss: 1.2247130870819092\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:00<00:02, 33.00it/s]\u001b[AValidation loss: 1.8855913877487183\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:00<00:02, 32.84it/s]\u001b[AValidation loss: 2.138455390930176\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:02, 32.68it/s]\u001b[AValidation loss: 8.797224044799805\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:02, 32.68it/s]\u001b[AValidation loss: 1.782517910003662\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:02, 32.74it/s]\u001b[AValidation loss: 1.0932644605636597\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:02, 32.75it/s]\u001b[AValidation loss: 1.9609143733978271\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:02, 32.80it/s]\u001b[AValidation loss: 1.7543050050735474\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:02, 32.62it/s]\u001b[AValidation loss: 1.2732981443405151\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:02, 32.67it/s]\u001b[AValidation loss: 1.943538784980774\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:02, 32.68it/s]\u001b[AValidation loss: 1.632364273071289\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:02, 32.69it/s]\u001b[AValidation loss: 1.6172592639923096\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:02, 32.65it/s]\u001b[AValidation loss: 1.3469266891479492\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 32.66it/s]\u001b[AValidation loss: 2.368178606033325\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 32.64it/s]\u001b[AValidation loss: 1.9915611743927002\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:01<00:01, 32.55it/s]\u001b[AValidation loss: 1.6435799598693848\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:01<00:01, 32.46it/s]\u001b[AValidation loss: 1.7446149587631226\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:01<00:01, 32.37it/s]\u001b[AValidation loss: 2.1459946632385254\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:01<00:01, 32.28it/s]\u001b[AValidation loss: 2.847254753112793\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:01<00:01, 32.19it/s]\u001b[AValidation loss: 1.6487910747528076\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:01<00:01, 32.11it/s]\u001b[AValidation loss: 1.467573881149292\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:01<00:01, 32.03it/s]\u001b[AValidation loss: 1.3725956678390503\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:01<00:01, 32.05it/s]\u001b[AValidation loss: 1.5881562232971191\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:01<00:01, 32.08it/s]\u001b[AValidation loss: 1.9266449213027954\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:01<00:01, 32.09it/s]\u001b[AValidation loss: 1.6705116033554077\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:01<00:01, 32.13it/s]\u001b[AValidation loss: 2.0589380264282227\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:01<00:01, 32.15it/s]\u001b[AValidation loss: 2.2367215156555176\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:01<00:01, 32.19it/s]\u001b[AValidation loss: 1.5490748882293701\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:01<00:01, 32.20it/s]\u001b[AValidation loss: 1.5420477390289307\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:01<00:01, 32.22it/s]\u001b[AValidation loss: 1.980246663093567\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:01<00:01, 32.24it/s]\u001b[AValidation loss: 1.580359697341919\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:01<00:01, 32.25it/s]\u001b[AValidation loss: 1.90433931350708\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:01<00:01, 32.27it/s]\u001b[AValidation loss: 1.4909522533416748\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:01<00:01, 32.29it/s]\u001b[AValidation loss: 1.6991645097732544\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:01<00:01, 32.30it/s]\u001b[AValidation loss: 1.4304423332214355\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:01, 32.31it/s]\u001b[AValidation loss: 2.5682239532470703\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:02<00:01, 32.32it/s]\u001b[AValidation loss: 1.2550634145736694\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:02<00:01, 32.34it/s]\u001b[AValidation loss: 1.6020926237106323\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:02<00:01, 32.33it/s]\u001b[AValidation loss: 2.4536609649658203\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:02<00:01, 32.34it/s]\u001b[AValidation loss: 1.761045217514038\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:02<00:01, 32.36it/s]\u001b[AValidation loss: 1.4110081195831299\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:02<00:01, 32.37it/s]\u001b[AValidation loss: 1.8936779499053955\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:02<00:01, 32.38it/s]\u001b[AValidation loss: 2.050344467163086\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:02<00:01, 32.39it/s]\u001b[AValidation loss: 2.0453784465789795\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:02<00:01, 32.41it/s]\u001b[AValidation loss: 2.459143877029419\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:02<00:01, 32.42it/s]\u001b[AValidation loss: 2.2042150497436523\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:02<00:01, 32.43it/s]\u001b[AValidation loss: 1.7762436866760254\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:02<00:01, 32.44it/s]\u001b[AValidation loss: 1.8249891996383667\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:02<00:00, 32.45it/s]\u001b[AValidation loss: 1.793274164199829\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:02<00:00, 32.47it/s]\u001b[AValidation loss: 1.6244176626205444\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:02<00:00, 32.48it/s]\u001b[AValidation loss: 1.5870087146759033\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:02<00:00, 32.49it/s]\u001b[AValidation loss: 1.7490894794464111\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:02<00:00, 32.45it/s]\u001b[AValidation loss: 1.4125902652740479\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:02<00:00, 32.39it/s]\u001b[AValidation loss: 1.9064388275146484\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:02<00:00, 32.39it/s]\u001b[AValidation loss: 1.8553829193115234\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:02<00:00, 32.42it/s]\u001b[AValidation loss: 1.7082399129867554\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:02<00:00, 32.43it/s]\u001b[AValidation loss: 1.7029547691345215\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:02<00:00, 32.45it/s]\u001b[AValidation loss: 1.8751518726348877\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:02<00:00, 32.46it/s]\u001b[AValidation loss: 1.7987802028656006\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:02<00:00, 32.49it/s]\u001b[AValidation loss: 2.9319519996643066\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:02<00:00, 32.50it/s]\u001b[AValidation loss: 1.7033629417419434\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:02<00:00, 32.52it/s]\u001b[AValidation loss: 1.8421095609664917\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:02<00:00, 32.53it/s]\u001b[AValidation loss: 3.505173683166504\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:02<00:00, 32.56it/s]\u001b[AValidation loss: 1.6216824054718018\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:02<00:00, 32.57it/s]\u001b[AValidation loss: 1.9893567562103271\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:02<00:00, 32.59it/s]\u001b[AValidation loss: 1.2720041275024414\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:02<00:00, 32.60it/s]\u001b[AValidation loss: 1.505718469619751\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:02<00:00, 32.62it/s]\u001b[AValidation loss: 1.8467131853103638\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:03<00:00, 32.63it/s]\u001b[AValidation loss: 1.6283340454101562\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:03<00:00, 32.64it/s]\u001b[AValidation loss: 1.3542187213897705\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:03<00:00, 32.66it/s]\u001b[AValidation loss: 2.1930794715881348\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:03<00:00, 32.66it/s]\u001b[AValidation loss: 1.5549886226654053\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:03<00:00, 32.69it/s]\u001b[AValidation loss: 1.2458596229553223\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:03<00:00, 32.69it/s]\u001b[AValidation loss: 1.9367849826812744\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:03<00:00, 32.70it/s]\u001b[AValidation loss: 1.7406963109970093\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:03<00:00, 32.70it/s]\u001b[AValidation loss: 2.154999017715454\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:03<00:00, 32.70it/s]\u001b[AValidation loss: 1.8374525308609009\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:03<00:00, 32.67it/s]\u001b[AValidation loss: 1.7146215438842773\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:03<00:00, 32.62it/s]\u001b[AValidation loss: 2.055344820022583\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:03<00:00, 32.58it/s]\u001b[AValidation loss: 2.977423906326294\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:03<00:00, 32.63it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/512 [00:00<?, ?it/s, v_num=11]                \u001b[ATraining loss: 1.8576459884643555\n",
      "Epoch 3:   0%|          | 1/512 [00:00<00:23, 22.02it/s, v_num=11]Training loss: 1.2901972532272339\n",
      "Epoch 3:   0%|          | 2/512 [00:00<01:01,  8.30it/s, v_num=11]Training loss: 2.2724626064300537\n",
      "Epoch 3:   1%|          | 3/512 [00:00<01:13,  6.96it/s, v_num=11]Training loss: 1.9175009727478027\n",
      "Epoch 3:   1%|          | 4/512 [00:00<01:19,  6.41it/s, v_num=11]Training loss: 1.4193177223205566\n",
      "Epoch 3:   1%|          | 5/512 [00:00<01:22,  6.13it/s, v_num=11]Training loss: 1.3969030380249023\n",
      "Epoch 3:   1%|          | 6/512 [00:01<01:25,  5.95it/s, v_num=11]Training loss: 1.3719823360443115\n",
      "Epoch 3:   1%|▏         | 7/512 [00:01<01:26,  5.83it/s, v_num=11]Training loss: 1.3654274940490723\n",
      "Epoch 3:   2%|▏         | 8/512 [00:01<01:27,  5.74it/s, v_num=11]Training loss: 1.6752591133117676\n",
      "Epoch 3:   2%|▏         | 9/512 [00:01<01:28,  5.68it/s, v_num=11]Training loss: 2.062284469604492\n",
      "Epoch 3:   2%|▏         | 10/512 [00:01<01:29,  5.62it/s, v_num=11]Training loss: 1.4608118534088135\n",
      "Epoch 3:   2%|▏         | 11/512 [00:01<01:29,  5.58it/s, v_num=11]Training loss: 1.4763729572296143\n",
      "Epoch 3:   2%|▏         | 12/512 [00:02<01:30,  5.54it/s, v_num=11]Training loss: 1.1039072275161743\n",
      "Epoch 3:   3%|▎         | 13/512 [00:02<01:30,  5.52it/s, v_num=11]Training loss: 1.5693254470825195\n",
      "Epoch 3:   3%|▎         | 14/512 [00:02<01:30,  5.49it/s, v_num=11]Training loss: 1.2797553539276123\n",
      "Epoch 3:   3%|▎         | 15/512 [00:02<01:30,  5.47it/s, v_num=11]Training loss: 1.400987148284912\n",
      "Epoch 3:   3%|▎         | 16/512 [00:02<01:31,  5.45it/s, v_num=11]Training loss: 1.846652865409851\n",
      "Epoch 3:   3%|▎         | 17/512 [00:03<01:31,  5.43it/s, v_num=11]Training loss: 1.4172502756118774\n",
      "Epoch 3:   4%|▎         | 18/512 [00:03<01:31,  5.42it/s, v_num=11]Training loss: 1.2346177101135254\n",
      "Epoch 3:   4%|▎         | 19/512 [00:03<01:31,  5.41it/s, v_num=11]Training loss: 1.527907371520996\n",
      "Epoch 3:   4%|▍         | 20/512 [00:03<01:31,  5.40it/s, v_num=11]Training loss: 1.2391459941864014\n",
      "Epoch 3:   4%|▍         | 21/512 [00:03<01:31,  5.39it/s, v_num=11]Training loss: 1.8089754581451416\n",
      "Epoch 3:   4%|▍         | 22/512 [00:04<01:31,  5.38it/s, v_num=11]Training loss: 1.6749082803726196\n",
      "Epoch 3:   4%|▍         | 23/512 [00:04<01:31,  5.37it/s, v_num=11]Training loss: 2.735200881958008\n",
      "Epoch 3:   5%|▍         | 24/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 1.7501890659332275\n",
      "Epoch 3:   5%|▍         | 25/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 2.379335880279541\n",
      "Epoch 3:   5%|▌         | 26/512 [00:04<01:30,  5.35it/s, v_num=11]Training loss: 1.4382508993148804\n",
      "Epoch 3:   5%|▌         | 27/512 [00:05<01:30,  5.35it/s, v_num=11]Training loss: 1.2819373607635498\n",
      "Epoch 3:   5%|▌         | 28/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.6544907093048096\n",
      "Epoch 3:   6%|▌         | 29/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.6959242820739746\n",
      "Epoch 3:   6%|▌         | 30/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.809936285018921\n",
      "Epoch 3:   6%|▌         | 31/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.6934928894042969\n",
      "Epoch 3:   6%|▋         | 32/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 1.5877935886383057\n",
      "Epoch 3:   6%|▋         | 33/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 1.219228744506836\n",
      "Epoch 3:   7%|▋         | 34/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.1629002094268799\n",
      "Epoch 3:   7%|▋         | 35/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.9142134189605713\n",
      "Epoch 3:   7%|▋         | 36/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.1756064891815186\n",
      "Epoch 3:   7%|▋         | 37/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.148451805114746\n",
      "Epoch 3:   7%|▋         | 38/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 6.011496543884277\n",
      "Epoch 3:   8%|▊         | 39/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.4004361629486084\n",
      "Epoch 3:   8%|▊         | 40/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.5083935260772705\n",
      "Epoch 3:   8%|▊         | 41/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 2.3649144172668457\n",
      "Epoch 3:   8%|▊         | 42/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 1.6531862020492554\n",
      "Epoch 3:   8%|▊         | 43/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.3227661848068237\n",
      "Epoch 3:   9%|▊         | 44/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.3659725189208984\n",
      "Epoch 3:   9%|▉         | 45/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 1.6107990741729736\n",
      "Epoch 3:   9%|▉         | 46/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 1.8554389476776123\n",
      "Epoch 3:   9%|▉         | 47/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 2.5016918182373047\n",
      "Epoch 3:   9%|▉         | 48/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.1066453456878662\n",
      "Epoch 3:  10%|▉         | 49/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.5373084545135498\n",
      "Epoch 3:  10%|▉         | 50/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.4525810480117798\n",
      "Epoch 3:  10%|▉         | 51/512 [00:09<01:27,  5.27it/s, v_num=11]Training loss: 1.413564920425415\n",
      "Epoch 3:  10%|█         | 52/512 [00:09<01:27,  5.27it/s, v_num=11]Training loss: 1.8792058229446411\n",
      "Epoch 3:  10%|█         | 53/512 [00:10<01:27,  5.27it/s, v_num=11]Training loss: 0.9738776087760925\n",
      "Epoch 3:  11%|█         | 54/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.9131141901016235\n",
      "Epoch 3:  11%|█         | 55/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.0085806846618652\n",
      "Epoch 3:  11%|█         | 56/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.3907058238983154\n",
      "Epoch 3:  11%|█         | 57/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.6610255241394043\n",
      "Epoch 3:  11%|█▏        | 58/512 [00:11<01:26,  5.26it/s, v_num=11]Training loss: 3.0403895378112793\n",
      "Epoch 3:  12%|█▏        | 59/512 [00:11<01:26,  5.26it/s, v_num=11]Training loss: 1.0096018314361572\n",
      "Epoch 3:  12%|█▏        | 60/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.0687146186828613\n",
      "Epoch 3:  12%|█▏        | 61/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 2.070434331893921\n",
      "Epoch 3:  12%|█▏        | 62/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 2.2268309593200684\n",
      "Epoch 3:  12%|█▏        | 63/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.601283073425293\n",
      "Epoch 3:  12%|█▎        | 64/512 [00:12<01:25,  5.26it/s, v_num=11]Training loss: 0.988980770111084\n",
      "Epoch 3:  13%|█▎        | 65/512 [00:12<01:25,  5.26it/s, v_num=11]Training loss: 1.522603154182434\n",
      "Epoch 3:  13%|█▎        | 66/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 2.476468324661255\n",
      "Epoch 3:  13%|█▎        | 67/512 [00:12<01:24,  5.25it/s, v_num=11]Training loss: 2.233562469482422\n",
      "Epoch 3:  13%|█▎        | 68/512 [00:12<01:24,  5.25it/s, v_num=11]Training loss: 1.2466727495193481\n",
      "Epoch 3:  13%|█▎        | 69/512 [00:13<01:24,  5.25it/s, v_num=11]Training loss: 1.0021559000015259\n",
      "Epoch 3:  14%|█▎        | 70/512 [00:13<01:24,  5.25it/s, v_num=11]Training loss: 1.5613197088241577\n",
      "Epoch 3:  14%|█▍        | 71/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.6433547735214233\n",
      "Epoch 3:  14%|█▍        | 72/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.8165907859802246\n",
      "Epoch 3:  14%|█▍        | 73/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.4892854690551758\n",
      "Epoch 3:  14%|█▍        | 74/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 2.082058906555176\n",
      "Epoch 3:  15%|█▍        | 75/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.503966212272644\n",
      "Epoch 3:  15%|█▍        | 76/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.911374807357788\n",
      "Epoch 3:  15%|█▌        | 77/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.4276719093322754\n",
      "Epoch 3:  15%|█▌        | 78/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 35.95857620239258\n",
      "Epoch 3:  15%|█▌        | 79/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.514829397201538\n",
      "Epoch 3:  16%|█▌        | 80/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 2.0371382236480713\n",
      "Epoch 3:  16%|█▌        | 81/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 0.9934455752372742\n",
      "Epoch 3:  16%|█▌        | 82/512 [00:15<01:21,  5.24it/s, v_num=11]Training loss: 1.2097285985946655\n",
      "Epoch 3:  16%|█▌        | 83/512 [00:15<01:21,  5.24it/s, v_num=11]Training loss: 1.5907273292541504\n",
      "Epoch 3:  16%|█▋        | 84/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.450624704360962\n",
      "Epoch 3:  17%|█▋        | 85/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.277517557144165\n",
      "Epoch 3:  17%|█▋        | 86/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.2407631874084473\n",
      "Epoch 3:  17%|█▋        | 87/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.8509846925735474\n",
      "Epoch 3:  17%|█▋        | 88/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 0.9594980478286743\n",
      "Epoch 3:  17%|█▋        | 89/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.0302186012268066\n",
      "Epoch 3:  18%|█▊        | 90/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.6267309188842773\n",
      "Epoch 3:  18%|█▊        | 91/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.7270989418029785\n",
      "Epoch 3:  18%|█▊        | 92/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.3639297485351562\n",
      "Epoch 3:  18%|█▊        | 93/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 1.189530372619629\n",
      "Epoch 3:  18%|█▊        | 94/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 1.4301369190216064\n",
      "Epoch 3:  19%|█▊        | 95/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.846800684928894\n",
      "Epoch 3:  19%|█▉        | 96/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.307747483253479\n",
      "Epoch 3:  19%|█▉        | 97/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.181358814239502\n",
      "Epoch 3:  19%|█▉        | 98/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.3263565301895142\n",
      "Epoch 3:  19%|█▉        | 99/512 [00:18<01:18,  5.24it/s, v_num=11]Training loss: 0.9711461067199707\n",
      "Epoch 3:  20%|█▉        | 100/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.3759336471557617\n",
      "Epoch 3:  20%|█▉        | 101/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.595108151435852\n",
      "Epoch 3:  20%|█▉        | 102/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.2591211795806885\n",
      "Epoch 3:  20%|██        | 103/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.0316722393035889\n",
      "Epoch 3:  20%|██        | 104/512 [00:19<01:17,  5.24it/s, v_num=11]Training loss: 1.1783299446105957\n",
      "Epoch 3:  21%|██        | 105/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.4982810020446777\n",
      "Epoch 3:  21%|██        | 106/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.775808334350586\n",
      "Epoch 3:  21%|██        | 107/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 1.763214111328125\n",
      "Epoch 3:  21%|██        | 108/512 [00:20<01:17,  5.23it/s, v_num=11]Training loss: 1.353493094444275\n",
      "Epoch 3:  21%|██▏       | 109/512 [00:20<01:16,  5.23it/s, v_num=11]Training loss: 1.2783169746398926\n",
      "Epoch 3:  21%|██▏       | 110/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.3254722356796265\n",
      "Epoch 3:  22%|██▏       | 111/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.478607416152954\n",
      "Epoch 3:  22%|██▏       | 112/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.8696802854537964\n",
      "Epoch 3:  22%|██▏       | 113/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.4872832298278809\n",
      "Epoch 3:  22%|██▏       | 114/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.1239014863967896\n",
      "Epoch 3:  22%|██▏       | 115/512 [00:21<01:15,  5.23it/s, v_num=11]Training loss: 1.587729811668396\n",
      "Epoch 3:  23%|██▎       | 116/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.1148879528045654\n",
      "Epoch 3:  23%|██▎       | 117/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 2.2236733436584473\n",
      "Epoch 3:  23%|██▎       | 118/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.715987205505371\n",
      "Epoch 3:  23%|██▎       | 119/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.5717551708221436\n",
      "Epoch 3:  23%|██▎       | 120/512 [00:22<01:14,  5.23it/s, v_num=11]Training loss: 1.1503183841705322\n",
      "Epoch 3:  24%|██▎       | 121/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.2568529844284058\n",
      "Epoch 3:  24%|██▍       | 122/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.0476282835006714\n",
      "Epoch 3:  24%|██▍       | 123/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.1641581058502197\n",
      "Epoch 3:  24%|██▍       | 124/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.932112693786621\n",
      "Epoch 3:  24%|██▍       | 125/512 [00:23<01:13,  5.23it/s, v_num=11]Training loss: 1.7064615488052368\n",
      "Epoch 3:  25%|██▍       | 126/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.6592578887939453\n",
      "Epoch 3:  25%|██▍       | 127/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.0550488233566284\n",
      "Epoch 3:  25%|██▌       | 128/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.6763383150100708\n",
      "Epoch 3:  25%|██▌       | 129/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5828016996383667\n",
      "Epoch 3:  25%|██▌       | 130/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.5189566612243652\n",
      "Epoch 3:  26%|██▌       | 131/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.097750186920166\n",
      "Epoch 3:  26%|██▌       | 132/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.375472068786621\n",
      "Epoch 3:  26%|██▌       | 133/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.597314715385437\n",
      "Epoch 3:  26%|██▌       | 134/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.6567176580429077\n",
      "Epoch 3:  26%|██▋       | 135/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 2.0129613876342773\n",
      "Epoch 3:  27%|██▋       | 136/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.158674716949463\n",
      "Epoch 3:  27%|██▋       | 137/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.0339280366897583\n",
      "Epoch 3:  27%|██▋       | 138/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.972257137298584\n",
      "Epoch 3:  27%|██▋       | 139/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.7497634887695312\n",
      "Epoch 3:  27%|██▋       | 140/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.528385877609253\n",
      "Epoch 3:  28%|██▊       | 141/512 [00:26<01:10,  5.23it/s, v_num=11]Training loss: 1.6749210357666016\n",
      "Epoch 3:  28%|██▊       | 142/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.8260810375213623\n",
      "Epoch 3:  28%|██▊       | 143/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.2176852226257324\n",
      "Epoch 3:  28%|██▊       | 144/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 2.04638671875\n",
      "Epoch 3:  28%|██▊       | 145/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.5280240774154663\n",
      "Epoch 3:  29%|██▊       | 146/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 2.0192761421203613\n",
      "Epoch 3:  29%|██▊       | 147/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.8850946426391602\n",
      "Epoch 3:  29%|██▉       | 148/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.7018766403198242\n",
      "Epoch 3:  29%|██▉       | 149/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.958167314529419\n",
      "Epoch 3:  29%|██▉       | 150/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.2398252487182617\n",
      "Epoch 3:  29%|██▉       | 151/512 [00:28<01:09,  5.22it/s, v_num=11]Training loss: 1.2605324983596802\n",
      "Epoch 3:  30%|██▉       | 152/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.987828016281128\n",
      "Epoch 3:  30%|██▉       | 153/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.4541027545928955\n",
      "Epoch 3:  30%|███       | 154/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 2.0265285968780518\n",
      "Epoch 3:  30%|███       | 155/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.2390682697296143\n",
      "Epoch 3:  30%|███       | 156/512 [00:29<01:08,  5.22it/s, v_num=11]Training loss: 1.5575803518295288\n",
      "Epoch 3:  31%|███       | 157/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 2.2146148681640625\n",
      "Epoch 3:  31%|███       | 158/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.8161110877990723\n",
      "Epoch 3:  31%|███       | 159/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.5334835052490234\n",
      "Epoch 3:  31%|███▏      | 160/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.436598539352417\n",
      "Epoch 3:  31%|███▏      | 161/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.24093759059906\n",
      "Epoch 3:  32%|███▏      | 162/512 [00:31<01:07,  5.22it/s, v_num=11]Training loss: 2.2493433952331543\n",
      "Epoch 3:  32%|███▏      | 163/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.0293965339660645\n",
      "Epoch 3:  32%|███▏      | 164/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.1249043941497803\n",
      "Epoch 3:  32%|███▏      | 165/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.7867810726165771\n",
      "Epoch 3:  32%|███▏      | 166/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.197324275970459\n",
      "Epoch 3:  33%|███▎      | 167/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.4356192350387573\n",
      "Epoch 3:  33%|███▎      | 168/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.239549160003662\n",
      "Epoch 3:  33%|███▎      | 169/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.7548327445983887\n",
      "Epoch 3:  33%|███▎      | 170/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.187760591506958\n",
      "Epoch 3:  33%|███▎      | 171/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.4751145839691162\n",
      "Epoch 3:  34%|███▎      | 172/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.2392797470092773\n",
      "Epoch 3:  34%|███▍      | 173/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.1031112670898438\n",
      "Epoch 3:  34%|███▍      | 174/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.0236058235168457\n",
      "Epoch 3:  34%|███▍      | 175/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.1547534465789795\n",
      "Epoch 3:  34%|███▍      | 176/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.9317439794540405\n",
      "Epoch 3:  35%|███▍      | 177/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.5688104629516602\n",
      "Epoch 3:  35%|███▍      | 178/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.1703822612762451\n",
      "Epoch 3:  35%|███▍      | 179/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 0.8662445545196533\n",
      "Epoch 3:  35%|███▌      | 180/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.4971709251403809\n",
      "Epoch 3:  35%|███▌      | 181/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.29086172580719\n",
      "Epoch 3:  36%|███▌      | 182/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.8451735973358154\n",
      "Epoch 3:  36%|███▌      | 183/512 [00:35<01:03,  5.22it/s, v_num=11]Training loss: 1.2620441913604736\n",
      "Epoch 3:  36%|███▌      | 184/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 2.1437931060791016\n",
      "Epoch 3:  36%|███▌      | 185/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.496326208114624\n",
      "Epoch 3:  36%|███▋      | 186/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.2645128965377808\n",
      "Epoch 3:  37%|███▋      | 187/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.7963396310806274\n",
      "Epoch 3:  37%|███▋      | 188/512 [00:36<01:02,  5.22it/s, v_num=11]Training loss: 1.9147758483886719\n",
      "Epoch 3:  37%|███▋      | 189/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.6775000095367432\n",
      "Epoch 3:  37%|███▋      | 190/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.45859694480896\n",
      "Epoch 3:  37%|███▋      | 191/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.6990776062011719\n",
      "Epoch 3:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.9162275791168213\n",
      "Epoch 3:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.5165029764175415\n",
      "Epoch 3:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 2.1522409915924072\n",
      "Epoch 3:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.2728657722473145\n",
      "Epoch 3:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.5566222667694092\n",
      "Epoch 3:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.819952130317688\n",
      "Epoch 3:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.4186620712280273\n",
      "Epoch 3:  39%|███▉      | 199/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.3391473293304443\n",
      "Epoch 3:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.139678955078125\n",
      "Epoch 3:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.9759210348129272\n",
      "Epoch 3:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.6122632026672363\n",
      "Epoch 3:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.445434808731079\n",
      "Epoch 3:  40%|███▉      | 204/512 [00:39<00:59,  5.22it/s, v_num=11]Training loss: 1.2748544216156006\n",
      "Epoch 3:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.276113510131836\n",
      "Epoch 3:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.163596272468567\n",
      "Epoch 3:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 0.9648061990737915\n",
      "Epoch 3:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.2882575988769531\n",
      "Epoch 3:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 1.9228659868240356\n",
      "Epoch 3:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.6409342288970947\n",
      "Epoch 3:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.3157455921173096\n",
      "Epoch 3:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.2356575727462769\n",
      "Epoch 3:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.4551842212677002\n",
      "Epoch 3:  42%|████▏     | 214/512 [00:41<00:57,  5.22it/s, v_num=11]Training loss: 1.584986925125122\n",
      "Epoch 3:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.328399419784546\n",
      "Epoch 3:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.3575674295425415\n",
      "Epoch 3:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.345531940460205\n",
      "Epoch 3:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4019473791122437\n",
      "Epoch 3:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.1286075115203857\n",
      "Epoch 3:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.3741490840911865\n",
      "Epoch 3:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.1884181499481201\n",
      "Epoch 3:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.1421504020690918\n",
      "Epoch 3:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.6767113208770752\n",
      "Epoch 3:  44%|████▍     | 224/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.7895214557647705\n",
      "Epoch 3:  44%|████▍     | 225/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.1117441654205322\n",
      "Epoch 3:  44%|████▍     | 226/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.301956057548523\n",
      "Epoch 3:  44%|████▍     | 227/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.5005178451538086\n",
      "Epoch 3:  45%|████▍     | 228/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.434741735458374\n",
      "Epoch 3:  45%|████▍     | 229/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.8354405164718628\n",
      "Epoch 3:  45%|████▍     | 230/512 [00:44<00:54,  5.22it/s, v_num=11]Training loss: 1.5631294250488281\n",
      "Epoch 3:  45%|████▌     | 231/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.5355479717254639\n",
      "Epoch 3:  45%|████▌     | 232/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.3957622051239014\n",
      "Epoch 3:  46%|████▌     | 233/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.486263632774353\n",
      "Epoch 3:  46%|████▌     | 234/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.4027915000915527\n",
      "Epoch 3:  46%|████▌     | 235/512 [00:45<00:53,  5.22it/s, v_num=11]Training loss: 2.195388078689575\n",
      "Epoch 3:  46%|████▌     | 236/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.3973748683929443\n",
      "Epoch 3:  46%|████▋     | 237/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.9056134223937988\n",
      "Epoch 3:  46%|████▋     | 238/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.9887034893035889\n",
      "Epoch 3:  47%|████▋     | 239/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.2778925895690918\n",
      "Epoch 3:  47%|████▋     | 240/512 [00:46<00:52,  5.22it/s, v_num=11]Training loss: 1.5707398653030396\n",
      "Epoch 3:  47%|████▋     | 241/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.3698756694793701\n",
      "Epoch 3:  47%|████▋     | 242/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.7517380714416504\n",
      "Epoch 3:  47%|████▋     | 243/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 2.058577299118042\n",
      "Epoch 3:  48%|████▊     | 244/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.107590675354004\n",
      "Epoch 3:  48%|████▊     | 245/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 3.978367805480957\n",
      "Epoch 3:  48%|████▊     | 246/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.8641411066055298\n",
      "Epoch 3:  48%|████▊     | 247/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.3653481006622314\n",
      "Epoch 3:  48%|████▊     | 248/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.4977126121520996\n",
      "Epoch 3:  49%|████▊     | 249/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 2.9096951484680176\n",
      "Epoch 3:  49%|████▉     | 250/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.5970580577850342\n",
      "Epoch 3:  49%|████▉     | 251/512 [00:48<00:50,  5.22it/s, v_num=11]Training loss: 1.953479290008545\n",
      "Epoch 3:  49%|████▉     | 252/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.318021297454834\n",
      "Epoch 3:  49%|████▉     | 253/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.0033020973205566\n",
      "Epoch 3:  50%|████▉     | 254/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.682558298110962\n",
      "Epoch 3:  50%|████▉     | 255/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.4481918811798096\n",
      "Epoch 3:  50%|█████     | 256/512 [00:49<00:49,  5.22it/s, v_num=11]Training loss: 1.6024632453918457\n",
      "Epoch 3:  50%|█████     | 257/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.632743000984192\n",
      "Epoch 3:  50%|█████     | 258/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.9915778636932373\n",
      "Epoch 3:  51%|█████     | 259/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.4137139320373535\n",
      "Epoch 3:  51%|█████     | 260/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.0194859504699707\n",
      "Epoch 3:  51%|█████     | 261/512 [00:50<00:48,  5.22it/s, v_num=11]Training loss: 1.5348607301712036\n",
      "Epoch 3:  51%|█████     | 262/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.2815403938293457\n",
      "Epoch 3:  51%|█████▏    | 263/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.2649680376052856\n",
      "Epoch 3:  52%|█████▏    | 264/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.3430427312850952\n",
      "Epoch 3:  52%|█████▏    | 265/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.228238582611084\n",
      "Epoch 3:  52%|█████▏    | 266/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.3097047805786133\n",
      "Epoch 3:  52%|█████▏    | 267/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 2.3480329513549805\n",
      "Epoch 3:  52%|█████▏    | 268/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.5856655836105347\n",
      "Epoch 3:  53%|█████▎    | 269/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.8324761390686035\n",
      "Epoch 3:  53%|█████▎    | 270/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.5582427978515625\n",
      "Epoch 3:  53%|█████▎    | 271/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.7478947639465332\n",
      "Epoch 3:  53%|█████▎    | 272/512 [00:52<00:46,  5.22it/s, v_num=11]Training loss: 1.4287270307540894\n",
      "Epoch 3:  53%|█████▎    | 273/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.7443565130233765\n",
      "Epoch 3:  54%|█████▎    | 274/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.9626829624176025\n",
      "Epoch 3:  54%|█████▎    | 275/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 0.8422833681106567\n",
      "Epoch 3:  54%|█████▍    | 276/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.3155410289764404\n",
      "Epoch 3:  54%|█████▍    | 277/512 [00:53<00:45,  5.22it/s, v_num=11]Training loss: 1.7888588905334473\n",
      "Epoch 3:  54%|█████▍    | 278/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 2.4004852771759033\n",
      "Epoch 3:  54%|█████▍    | 279/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.4676682949066162\n",
      "Epoch 3:  55%|█████▍    | 280/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.4564979076385498\n",
      "Epoch 3:  55%|█████▍    | 281/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.4391143321990967\n",
      "Epoch 3:  55%|█████▌    | 282/512 [00:54<00:44,  5.22it/s, v_num=11]Training loss: 1.89718759059906\n",
      "Epoch 3:  55%|█████▌    | 283/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.525498628616333\n",
      "Epoch 3:  55%|█████▌    | 284/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.3004703521728516\n",
      "Epoch 3:  56%|█████▌    | 285/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.5477997064590454\n",
      "Epoch 3:  56%|█████▌    | 286/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 1.6324446201324463\n",
      "Epoch 3:  56%|█████▌    | 287/512 [00:55<00:43,  5.21it/s, v_num=11]Training loss: 1.4602655172348022\n",
      "Epoch 3:  56%|█████▋    | 288/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.9818483591079712\n",
      "Epoch 3:  56%|█████▋    | 289/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.2240464687347412\n",
      "Epoch 3:  57%|█████▋    | 290/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.4873765707015991\n",
      "Epoch 3:  57%|█████▋    | 291/512 [00:55<00:42,  5.21it/s, v_num=11]Training loss: 1.5472512245178223\n",
      "Epoch 3:  57%|█████▋    | 292/512 [00:56<00:42,  5.21it/s, v_num=11]Training loss: 1.158300757408142\n",
      "Epoch 3:  57%|█████▋    | 293/512 [00:56<00:42,  5.21it/s, v_num=11]Training loss: 1.245812177658081\n",
      "Epoch 3:  57%|█████▋    | 294/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 1.5561226606369019\n",
      "Epoch 3:  58%|█████▊    | 295/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 1.6122989654541016\n",
      "Epoch 3:  58%|█████▊    | 296/512 [00:56<00:41,  5.21it/s, v_num=11]Training loss: 0.9144548177719116\n",
      "Epoch 3:  58%|█████▊    | 297/512 [00:57<00:41,  5.21it/s, v_num=11]Training loss: 1.9345206022262573\n",
      "Epoch 3:  58%|█████▊    | 298/512 [00:57<00:41,  5.21it/s, v_num=11]Training loss: 1.1966077089309692\n",
      "Epoch 3:  58%|█████▊    | 299/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.803403615951538\n",
      "Epoch 3:  59%|█████▊    | 300/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.3869273662567139\n",
      "Epoch 3:  59%|█████▉    | 301/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.503554344177246\n",
      "Epoch 3:  59%|█████▉    | 302/512 [00:57<00:40,  5.21it/s, v_num=11]Training loss: 1.9089319705963135\n",
      "Epoch 3:  59%|█████▉    | 303/512 [00:58<00:40,  5.21it/s, v_num=11]Training loss: 2.278945207595825\n",
      "Epoch 3:  59%|█████▉    | 304/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.6285400390625\n",
      "Epoch 3:  60%|█████▉    | 305/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.9143128395080566\n",
      "Epoch 3:  60%|█████▉    | 306/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.6899259090423584\n",
      "Epoch 3:  60%|█████▉    | 307/512 [00:58<00:39,  5.21it/s, v_num=11]Training loss: 1.7330130338668823\n",
      "Epoch 3:  60%|██████    | 308/512 [00:59<00:39,  5.21it/s, v_num=11]Training loss: 1.58967125415802\n",
      "Epoch 3:  60%|██████    | 309/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.4635498523712158\n",
      "Epoch 3:  61%|██████    | 310/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.427817940711975\n",
      "Epoch 3:  61%|██████    | 311/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.3971686363220215\n",
      "Epoch 3:  61%|██████    | 312/512 [00:59<00:38,  5.21it/s, v_num=11]Training loss: 1.5926295518875122\n",
      "Epoch 3:  61%|██████    | 313/512 [01:00<00:38,  5.21it/s, v_num=11]Training loss: 2.2542102336883545\n",
      "Epoch 3:  61%|██████▏   | 314/512 [01:00<00:38,  5.21it/s, v_num=11]Training loss: 1.258702278137207\n",
      "Epoch 3:  62%|██████▏   | 315/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 2.3237125873565674\n",
      "Epoch 3:  62%|██████▏   | 316/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 1.7879568338394165\n",
      "Epoch 3:  62%|██████▏   | 317/512 [01:00<00:37,  5.21it/s, v_num=11]Training loss: 1.0452786684036255\n",
      "Epoch 3:  62%|██████▏   | 318/512 [01:01<00:37,  5.21it/s, v_num=11]Training loss: 1.2383623123168945\n",
      "Epoch 3:  62%|██████▏   | 319/512 [01:01<00:37,  5.21it/s, v_num=11]Training loss: 1.396830677986145\n",
      "Epoch 3:  62%|██████▎   | 320/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.5815314054489136\n",
      "Epoch 3:  63%|██████▎   | 321/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.6962970495224\n",
      "Epoch 3:  63%|██████▎   | 322/512 [01:01<00:36,  5.21it/s, v_num=11]Training loss: 1.6182901859283447\n",
      "Epoch 3:  63%|██████▎   | 323/512 [01:02<00:36,  5.21it/s, v_num=11]Training loss: 1.8536508083343506\n",
      "Epoch 3:  63%|██████▎   | 324/512 [01:02<00:36,  5.21it/s, v_num=11]Training loss: 0.9060053825378418\n",
      "Epoch 3:  63%|██████▎   | 325/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.6709492206573486\n",
      "Epoch 3:  64%|██████▎   | 326/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.4675579071044922\n",
      "Epoch 3:  64%|██████▍   | 327/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 1.4777498245239258\n",
      "Epoch 3:  64%|██████▍   | 328/512 [01:02<00:35,  5.21it/s, v_num=11]Training loss: 0.9497480392456055\n",
      "Epoch 3:  64%|██████▍   | 329/512 [01:03<00:35,  5.21it/s, v_num=11]Training loss: 1.114364743232727\n",
      "Epoch 3:  64%|██████▍   | 330/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.6910039186477661\n",
      "Epoch 3:  65%|██████▍   | 331/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 2.021714210510254\n",
      "Epoch 3:  65%|██████▍   | 332/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.6252375841140747\n",
      "Epoch 3:  65%|██████▌   | 333/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.3385963439941406\n",
      "Epoch 3:  65%|██████▌   | 334/512 [01:04<00:34,  5.21it/s, v_num=11]Training loss: 1.8650931119918823\n",
      "Epoch 3:  65%|██████▌   | 335/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.4421743154525757\n",
      "Epoch 3:  66%|██████▌   | 336/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.3638215065002441\n",
      "Epoch 3:  66%|██████▌   | 337/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 0.9080983400344849\n",
      "Epoch 3:  66%|██████▌   | 338/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.121111512184143\n",
      "Epoch 3:  66%|██████▌   | 339/512 [01:05<00:33,  5.21it/s, v_num=11]Training loss: 1.7747502326965332\n",
      "Epoch 3:  66%|██████▋   | 340/512 [01:05<00:33,  5.21it/s, v_num=11]Training loss: 1.3892146348953247\n",
      "Epoch 3:  67%|██████▋   | 341/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 2.300142526626587\n",
      "Epoch 3:  67%|██████▋   | 342/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.6958465576171875\n",
      "Epoch 3:  67%|██████▋   | 343/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.8455506563186646\n",
      "Epoch 3:  67%|██████▋   | 344/512 [01:06<00:32,  5.21it/s, v_num=11]Training loss: 1.3980611562728882\n",
      "Epoch 3:  67%|██████▋   | 345/512 [01:06<00:32,  5.21it/s, v_num=11]Training loss: 1.9303843975067139\n",
      "Epoch 3:  68%|██████▊   | 346/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.1758402585983276\n",
      "Epoch 3:  68%|██████▊   | 347/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.3020179271697998\n",
      "Epoch 3:  68%|██████▊   | 348/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.69218111038208\n",
      "Epoch 3:  68%|██████▊   | 349/512 [01:07<00:31,  5.21it/s, v_num=11]Training loss: 1.7536165714263916\n",
      "Epoch 3:  68%|██████▊   | 350/512 [01:07<00:31,  5.21it/s, v_num=11]Training loss: 1.3264867067337036\n",
      "Epoch 3:  69%|██████▊   | 351/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.4256856441497803\n",
      "Epoch 3:  69%|██████▉   | 352/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.3932766914367676\n",
      "Epoch 3:  69%|██████▉   | 353/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.6658122539520264\n",
      "Epoch 3:  69%|██████▉   | 354/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.3999658823013306\n",
      "Epoch 3:  69%|██████▉   | 355/512 [01:08<00:30,  5.21it/s, v_num=11]Training loss: 1.450751543045044\n",
      "Epoch 3:  70%|██████▉   | 356/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 0.9612663984298706\n",
      "Epoch 3:  70%|██████▉   | 357/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.196460247039795\n",
      "Epoch 3:  70%|██████▉   | 358/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.4655648469924927\n",
      "Epoch 3:  70%|███████   | 359/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 2.6252923011779785\n",
      "Epoch 3:  70%|███████   | 360/512 [01:09<00:29,  5.21it/s, v_num=11]Training loss: 1.1889314651489258\n",
      "Epoch 3:  71%|███████   | 361/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.385798692703247\n",
      "Epoch 3:  71%|███████   | 362/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.3453810214996338\n",
      "Epoch 3:  71%|███████   | 363/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.4036180973052979\n",
      "Epoch 3:  71%|███████   | 364/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.5152642726898193\n",
      "Epoch 3:  71%|███████▏  | 365/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 1.5401438474655151\n",
      "Epoch 3:  71%|███████▏  | 366/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 1.3104050159454346\n",
      "Epoch 3:  72%|███████▏  | 367/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.4091057777404785\n",
      "Epoch 3:  72%|███████▏  | 368/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.5996726751327515\n",
      "Epoch 3:  72%|███████▏  | 369/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.428472876548767\n",
      "Epoch 3:  72%|███████▏  | 370/512 [01:11<00:27,  5.21it/s, v_num=11]Training loss: 1.5277056694030762\n",
      "Epoch 3:  72%|███████▏  | 371/512 [01:11<00:27,  5.21it/s, v_num=11]Training loss: 1.1158353090286255\n",
      "Epoch 3:  73%|███████▎  | 372/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.6938343048095703\n",
      "Epoch 3:  73%|███████▎  | 373/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.4265471696853638\n",
      "Epoch 3:  73%|███████▎  | 374/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.6470115184783936\n",
      "Epoch 3:  73%|███████▎  | 375/512 [01:12<00:26,  5.21it/s, v_num=11]Training loss: 1.788295030593872\n",
      "Epoch 3:  73%|███████▎  | 376/512 [01:12<00:26,  5.21it/s, v_num=11]Training loss: 1.9389150142669678\n",
      "Epoch 3:  74%|███████▎  | 377/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.248207926750183\n",
      "Epoch 3:  74%|███████▍  | 378/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.5307517051696777\n",
      "Epoch 3:  74%|███████▍  | 379/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.4313403367996216\n",
      "Epoch 3:  74%|███████▍  | 380/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.4063910245895386\n",
      "Epoch 3:  74%|███████▍  | 381/512 [01:13<00:25,  5.21it/s, v_num=11]Training loss: 1.497153401374817\n",
      "Epoch 3:  75%|███████▍  | 382/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.1322681903839111\n",
      "Epoch 3:  75%|███████▍  | 383/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.5357086658477783\n",
      "Epoch 3:  75%|███████▌  | 384/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 2.6470320224761963\n",
      "Epoch 3:  75%|███████▌  | 385/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.4123027324676514\n",
      "Epoch 3:  75%|███████▌  | 386/512 [01:14<00:24,  5.21it/s, v_num=11]Training loss: 1.7148973941802979\n",
      "Epoch 3:  76%|███████▌  | 387/512 [01:14<00:24,  5.21it/s, v_num=11]Training loss: 1.038320541381836\n",
      "Epoch 3:  76%|███████▌  | 388/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.4505023956298828\n",
      "Epoch 3:  76%|███████▌  | 389/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.21234130859375\n",
      "Epoch 3:  76%|███████▌  | 390/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.5359342098236084\n",
      "Epoch 3:  76%|███████▋  | 391/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 1.4227712154388428\n",
      "Epoch 3:  77%|███████▋  | 392/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 1.6320812702178955\n",
      "Epoch 3:  77%|███████▋  | 393/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.924225091934204\n",
      "Epoch 3:  77%|███████▋  | 394/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.9038975238800049\n",
      "Epoch 3:  77%|███████▋  | 395/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.553636074066162\n",
      "Epoch 3:  77%|███████▋  | 396/512 [01:16<00:22,  5.21it/s, v_num=11]Training loss: 1.6505476236343384\n",
      "Epoch 3:  78%|███████▊  | 397/512 [01:16<00:22,  5.21it/s, v_num=11]Training loss: 1.994248628616333\n",
      "Epoch 3:  78%|███████▊  | 398/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.0500905513763428\n",
      "Epoch 3:  78%|███████▊  | 399/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.595963954925537\n",
      "Epoch 3:  78%|███████▊  | 400/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 2.160731077194214\n",
      "Epoch 3:  78%|███████▊  | 401/512 [01:17<00:21,  5.21it/s, v_num=11]Training loss: 1.6918258666992188\n",
      "Epoch 3:  79%|███████▊  | 402/512 [01:17<00:21,  5.21it/s, v_num=11]Training loss: 1.7596876621246338\n",
      "Epoch 3:  79%|███████▊  | 403/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.3316999673843384\n",
      "Epoch 3:  79%|███████▉  | 404/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.3292171955108643\n",
      "Epoch 3:  79%|███████▉  | 405/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.4583148956298828\n",
      "Epoch 3:  79%|███████▉  | 406/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 2.425133466720581\n",
      "Epoch 3:  79%|███████▉  | 407/512 [01:18<00:20,  5.21it/s, v_num=11]Training loss: 1.3597633838653564\n",
      "Epoch 3:  80%|███████▉  | 408/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.2510114908218384\n",
      "Epoch 3:  80%|███████▉  | 409/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.3410048484802246\n",
      "Epoch 3:  80%|████████  | 410/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.2545807361602783\n",
      "Epoch 3:  80%|████████  | 411/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.641242504119873\n",
      "Epoch 3:  80%|████████  | 412/512 [01:19<00:19,  5.21it/s, v_num=11]Training loss: 1.7918040752410889\n",
      "Epoch 3:  81%|████████  | 413/512 [01:19<00:19,  5.21it/s, v_num=11]Training loss: 1.6915932893753052\n",
      "Epoch 3:  81%|████████  | 414/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.7514190673828125\n",
      "Epoch 3:  81%|████████  | 415/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.2833940982818604\n",
      "Epoch 3:  81%|████████▏ | 416/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.520918369293213\n",
      "Epoch 3:  81%|████████▏ | 417/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 1.3165056705474854\n",
      "Epoch 3:  82%|████████▏ | 418/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 1.312713384628296\n",
      "Epoch 3:  82%|████████▏ | 419/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.5098172426223755\n",
      "Epoch 3:  82%|████████▏ | 420/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.6704964637756348\n",
      "Epoch 3:  82%|████████▏ | 421/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.0772329568862915\n",
      "Epoch 3:  82%|████████▏ | 422/512 [01:21<00:17,  5.21it/s, v_num=11]Training loss: 1.4663903713226318\n",
      "Epoch 3:  83%|████████▎ | 423/512 [01:21<00:17,  5.21it/s, v_num=11]Training loss: 0.947723388671875\n",
      "Epoch 3:  83%|████████▎ | 424/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.8571016788482666\n",
      "Epoch 3:  83%|████████▎ | 425/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.0093564987182617\n",
      "Epoch 3:  83%|████████▎ | 426/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.5647857189178467\n",
      "Epoch 3:  83%|████████▎ | 427/512 [01:22<00:16,  5.21it/s, v_num=11]Training loss: 1.5011489391326904\n",
      "Epoch 3:  84%|████████▎ | 428/512 [01:22<00:16,  5.21it/s, v_num=11]Training loss: 1.7752364873886108\n",
      "Epoch 3:  84%|████████▍ | 429/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.4209320545196533\n",
      "Epoch 3:  84%|████████▍ | 430/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.3904657363891602\n",
      "Epoch 3:  84%|████████▍ | 431/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.6687471866607666\n",
      "Epoch 3:  84%|████████▍ | 432/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.4138665199279785\n",
      "Epoch 3:  85%|████████▍ | 433/512 [01:23<00:15,  5.21it/s, v_num=11]Training loss: 1.8297679424285889\n",
      "Epoch 3:  85%|████████▍ | 434/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.6896294355392456\n",
      "Epoch 3:  85%|████████▍ | 435/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.524782657623291\n",
      "Epoch 3:  85%|████████▌ | 436/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.4082541465759277\n",
      "Epoch 3:  85%|████████▌ | 437/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.4909921884536743\n",
      "Epoch 3:  86%|████████▌ | 438/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 1.7655308246612549\n",
      "Epoch 3:  86%|████████▌ | 439/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 1.859740138053894\n",
      "Epoch 3:  86%|████████▌ | 440/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.193842887878418\n",
      "Epoch 3:  86%|████████▌ | 441/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.458931565284729\n",
      "Epoch 3:  86%|████████▋ | 442/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.2710239887237549\n",
      "Epoch 3:  87%|████████▋ | 443/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.0587437152862549\n",
      "Epoch 3:  87%|████████▋ | 444/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.7214338779449463\n",
      "Epoch 3:  87%|████████▋ | 445/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.1606318950653076\n",
      "Epoch 3:  87%|████████▋ | 446/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 0.9351674914360046\n",
      "Epoch 3:  87%|████████▋ | 447/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.3104337453842163\n",
      "Epoch 3:  88%|████████▊ | 448/512 [01:26<00:12,  5.21it/s, v_num=11]Training loss: 1.3379813432693481\n",
      "Epoch 3:  88%|████████▊ | 449/512 [01:26<00:12,  5.21it/s, v_num=11]Training loss: 1.3531135320663452\n",
      "Epoch 3:  88%|████████▊ | 450/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.3192843198776245\n",
      "Epoch 3:  88%|████████▊ | 451/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.5775538682937622\n",
      "Epoch 3:  88%|████████▊ | 452/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.254734754562378\n",
      "Epoch 3:  88%|████████▊ | 453/512 [01:27<00:11,  5.21it/s, v_num=11]Training loss: 1.8629639148712158\n",
      "Epoch 3:  89%|████████▊ | 454/512 [01:27<00:11,  5.21it/s, v_num=11]Training loss: 1.926660418510437\n",
      "Epoch 3:  89%|████████▉ | 455/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.4063793420791626\n",
      "Epoch 3:  89%|████████▉ | 456/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.6431212425231934\n",
      "Epoch 3:  89%|████████▉ | 457/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.8328731060028076\n",
      "Epoch 3:  89%|████████▉ | 458/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.866375207901001\n",
      "Epoch 3:  90%|████████▉ | 459/512 [01:28<00:10,  5.21it/s, v_num=11]Training loss: 1.3072640895843506\n",
      "Epoch 3:  90%|████████▉ | 460/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.675290822982788\n",
      "Epoch 3:  90%|█████████ | 461/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.9336392879486084\n",
      "Epoch 3:  90%|█████████ | 462/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.5038187503814697\n",
      "Epoch 3:  90%|█████████ | 463/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.2559305429458618\n",
      "Epoch 3:  91%|█████████ | 464/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 1.7020268440246582\n",
      "Epoch 3:  91%|█████████ | 465/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 1.6261200904846191\n",
      "Epoch 3:  91%|█████████ | 466/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.6071116924285889\n",
      "Epoch 3:  91%|█████████ | 467/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.2023425102233887\n",
      "Epoch 3:  91%|█████████▏| 468/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.360355019569397\n",
      "Epoch 3:  92%|█████████▏| 469/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.2934931516647339\n",
      "Epoch 3:  92%|█████████▏| 470/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.603592872619629\n",
      "Epoch 3:  92%|█████████▏| 471/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.4916630983352661\n",
      "Epoch 3:  92%|█████████▏| 472/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.9320117235183716\n",
      "Epoch 3:  92%|█████████▏| 473/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.5156943798065186\n",
      "Epoch 3:  93%|█████████▎| 474/512 [01:31<00:07,  5.21it/s, v_num=11]Training loss: 2.032866954803467\n",
      "Epoch 3:  93%|█████████▎| 475/512 [01:31<00:07,  5.21it/s, v_num=11]Training loss: 0.9179115891456604\n",
      "Epoch 3:  93%|█████████▎| 476/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.2580536603927612\n",
      "Epoch 3:  93%|█████████▎| 477/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.3958884477615356\n",
      "Epoch 3:  93%|█████████▎| 478/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.4360170364379883\n",
      "Epoch 3:  94%|█████████▎| 479/512 [01:32<00:06,  5.21it/s, v_num=11]Training loss: 2.011974573135376\n",
      "Epoch 3:  94%|█████████▍| 480/512 [01:32<00:06,  5.21it/s, v_num=11]Training loss: 2.253657817840576\n",
      "Epoch 3:  94%|█████████▍| 481/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.388594150543213\n",
      "Epoch 3:  94%|█████████▍| 482/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.5889291763305664\n",
      "Epoch 3:  94%|█████████▍| 483/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.4087733030319214\n",
      "Epoch 3:  95%|█████████▍| 484/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.2280144691467285\n",
      "Epoch 3:  95%|█████████▍| 485/512 [01:33<00:05,  5.21it/s, v_num=11]Training loss: 1.4678680896759033\n",
      "Epoch 3:  95%|█████████▍| 486/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 2.0183253288269043\n",
      "Epoch 3:  95%|█████████▌| 487/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.0979282855987549\n",
      "Epoch 3:  95%|█████████▌| 488/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.6119017601013184\n",
      "Epoch 3:  96%|█████████▌| 489/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.4048376083374023\n",
      "Epoch 3:  96%|█████████▌| 490/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 1.880807638168335\n",
      "Epoch 3:  96%|█████████▌| 491/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 0.9217420816421509\n",
      "Epoch 3:  96%|█████████▌| 492/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.7314693927764893\n",
      "Epoch 3:  96%|█████████▋| 493/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.8402658700942993\n",
      "Epoch 3:  96%|█████████▋| 494/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.1127732992172241\n",
      "Epoch 3:  97%|█████████▋| 495/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.005586862564087\n",
      "Epoch 3:  97%|█████████▋| 496/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.8176243305206299\n",
      "Epoch 3:  97%|█████████▋| 497/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.2429791688919067\n",
      "Epoch 3:  97%|█████████▋| 498/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.5829768180847168\n",
      "Epoch 3:  97%|█████████▋| 499/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 0.9091178178787231\n",
      "Epoch 3:  98%|█████████▊| 500/512 [01:36<00:02,  5.21it/s, v_num=11]Training loss: 1.6110328435897827\n",
      "Epoch 3:  98%|█████████▊| 501/512 [01:36<00:02,  5.21it/s, v_num=11]Training loss: 2.307473659515381\n",
      "Epoch 3:  98%|█████████▊| 502/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.4473555088043213\n",
      "Epoch 3:  98%|█████████▊| 503/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.6966743469238281\n",
      "Epoch 3:  98%|█████████▊| 504/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.353434443473816\n",
      "Epoch 3:  99%|█████████▊| 505/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.4774876832962036\n",
      "Epoch 3:  99%|█████████▉| 506/512 [01:37<00:01,  5.21it/s, v_num=11]Training loss: 3.158496379852295\n",
      "Epoch 3:  99%|█████████▉| 507/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.4074400663375854\n",
      "Epoch 3:  99%|█████████▉| 508/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.0007050037384033\n",
      "Epoch 3:  99%|█████████▉| 509/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.4353752136230469\n",
      "Epoch 3: 100%|█████████▉| 510/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 40.05900192260742\n",
      "Epoch 3: 100%|█████████▉| 511/512 [01:38<00:00,  5.21it/s, v_num=11]Training loss: 2.008486032485962\n",
      "Epoch 3: 100%|██████████| 512/512 [01:38<00:00,  5.21it/s, v_num=11]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 2.9706902503967285\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:02, 49.45it/s]\u001b[AValidation loss: 1.5620994567871094\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:02, 36.21it/s]\u001b[AValidation loss: 1.8633671998977661\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:03, 33.31it/s]\u001b[AValidation loss: 1.4510372877120972\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:03, 33.18it/s]\u001b[AValidation loss: 1.286297082901001\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:03, 33.52it/s]\u001b[AValidation loss: 2.020932197570801\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:03, 33.48it/s]\u001b[AValidation loss: 2.3419270515441895\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:03, 33.70it/s]\u001b[AValidation loss: 1.4829614162445068\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:03, 33.62it/s]\u001b[AValidation loss: 2.4529731273651123\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:03, 33.58it/s]\u001b[AValidation loss: 1.515099048614502\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:02, 33.71it/s]\u001b[AValidation loss: 1.554840326309204\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:02, 33.17it/s]\u001b[AValidation loss: 1.4912183284759521\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:02, 32.74it/s]\u001b[AValidation loss: 1.6656219959259033\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:02, 32.38it/s]\u001b[AValidation loss: 1.4027700424194336\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:02, 32.08it/s]\u001b[AValidation loss: 1.636051893234253\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:02, 32.13it/s]\u001b[AValidation loss: 1.3237920999526978\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:02, 32.28it/s]\u001b[AValidation loss: 1.6124582290649414\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:02, 32.34it/s]\u001b[AValidation loss: 1.520995855331421\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:02, 32.45it/s]\u001b[AValidation loss: 1.7319447994232178\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:02, 32.49it/s]\u001b[AValidation loss: 1.9933874607086182\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:02, 32.52it/s]\u001b[AValidation loss: 1.8919737339019775\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:02, 32.54it/s]\u001b[AValidation loss: 1.5011062622070312\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:02, 32.56it/s]\u001b[AValidation loss: 1.6984753608703613\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:00<00:02, 32.60it/s]\u001b[AValidation loss: 1.959816336631775\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:00<00:02, 32.63it/s]\u001b[AValidation loss: 1.4919493198394775\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:00<00:02, 32.65it/s]\u001b[AValidation loss: 1.9391746520996094\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:00<00:02, 32.67it/s]\u001b[AValidation loss: 1.3006129264831543\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:00<00:02, 32.70it/s]\u001b[AValidation loss: 2.0186047554016113\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:00<00:02, 32.72it/s]\u001b[AValidation loss: 2.05816650390625\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:00<00:02, 32.73it/s]\u001b[AValidation loss: 2.6380767822265625\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:00<00:02, 32.76it/s]\u001b[AValidation loss: 1.208125352859497\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:00<00:02, 32.78it/s]\u001b[AValidation loss: 1.8319668769836426\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:00<00:02, 32.79it/s]\u001b[AValidation loss: 2.2493934631347656\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:02, 32.82it/s]\u001b[AValidation loss: 9.670475959777832\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:02, 32.79it/s]\u001b[AValidation loss: 1.686093807220459\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:02, 32.81it/s]\u001b[AValidation loss: 1.0933988094329834\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:02, 32.82it/s]\u001b[AValidation loss: 2.122713565826416\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:02, 32.84it/s]\u001b[AValidation loss: 1.7048505544662476\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:02, 32.85it/s]\u001b[AValidation loss: 1.1385972499847412\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:02, 32.86it/s]\u001b[AValidation loss: 1.980851650238037\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:02, 32.87it/s]\u001b[AValidation loss: 1.6166398525238037\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:02, 32.89it/s]\u001b[AValidation loss: 1.6169791221618652\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:02, 32.90it/s]\u001b[AValidation loss: 1.3106911182403564\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 32.91it/s]\u001b[AValidation loss: 2.483025074005127\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 32.92it/s]\u001b[AValidation loss: 2.127723455429077\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:01<00:01, 32.93it/s]\u001b[AValidation loss: 1.7051563262939453\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:01<00:01, 32.94it/s]\u001b[AValidation loss: 1.7927814722061157\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:01<00:01, 32.95it/s]\u001b[AValidation loss: 2.2443723678588867\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:01<00:01, 32.96it/s]\u001b[AValidation loss: 2.9408621788024902\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:01<00:01, 32.97it/s]\u001b[AValidation loss: 1.6546244621276855\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:01<00:01, 32.98it/s]\u001b[AValidation loss: 1.3822702169418335\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:01<00:01, 32.99it/s]\u001b[AValidation loss: 1.3079286813735962\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:01<00:01, 33.00it/s]\u001b[AValidation loss: 1.6443347930908203\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:01<00:01, 33.01it/s]\u001b[AValidation loss: 1.9160435199737549\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:01<00:01, 33.02it/s]\u001b[AValidation loss: 1.66488778591156\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:01<00:01, 33.03it/s]\u001b[AValidation loss: 1.917977213859558\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:01<00:01, 33.03it/s]\u001b[AValidation loss: 2.1133649349212646\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:01<00:01, 33.04it/s]\u001b[AValidation loss: 1.4796991348266602\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:01<00:01, 33.05it/s]\u001b[AValidation loss: 1.490917444229126\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:01<00:01, 33.06it/s]\u001b[AValidation loss: 1.797588586807251\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:01<00:01, 33.06it/s]\u001b[AValidation loss: 1.552790641784668\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:01<00:01, 33.06it/s]\u001b[AValidation loss: 1.9145326614379883\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:01<00:01, 33.07it/s]\u001b[AValidation loss: 1.474159836769104\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:01<00:01, 33.07it/s]\u001b[AValidation loss: 1.6667917966842651\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:01<00:01, 33.08it/s]\u001b[AValidation loss: 1.4142534732818604\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:01<00:01, 33.09it/s]\u001b[AValidation loss: 2.603727102279663\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:01<00:01, 33.09it/s]\u001b[AValidation loss: 1.2404530048370361\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:02<00:01, 33.10it/s]\u001b[AValidation loss: 1.5080108642578125\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:02<00:01, 33.10it/s]\u001b[AValidation loss: 2.4133248329162598\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:02<00:01, 33.11it/s]\u001b[AValidation loss: 1.794692039489746\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:02<00:01, 33.11it/s]\u001b[AValidation loss: 1.48822021484375\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:02<00:01, 33.12it/s]\u001b[AValidation loss: 1.9657948017120361\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:02<00:01, 33.13it/s]\u001b[AValidation loss: 2.0762925148010254\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:02<00:01, 33.13it/s]\u001b[AValidation loss: 2.019918918609619\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:02<00:01, 33.14it/s]\u001b[AValidation loss: 2.465669631958008\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:02<00:01, 33.14it/s]\u001b[AValidation loss: 2.292198657989502\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:02<00:01, 33.15it/s]\u001b[AValidation loss: 1.7766103744506836\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:02<00:00, 33.15it/s]\u001b[AValidation loss: 1.8830641508102417\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:02<00:00, 33.16it/s]\u001b[AValidation loss: 1.6601061820983887\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:02<00:00, 33.16it/s]\u001b[AValidation loss: 1.6177836656570435\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.6075470447540283\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.706087589263916\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:02<00:00, 33.17it/s]\u001b[AValidation loss: 1.4621591567993164\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.8706523180007935\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.9126390218734741\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:02<00:00, 33.18it/s]\u001b[AValidation loss: 1.7964277267456055\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:02<00:00, 33.19it/s]\u001b[AValidation loss: 1.7009479999542236\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:02<00:00, 33.19it/s]\u001b[AValidation loss: 1.8685572147369385\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 1.6507480144500732\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:02<00:00, 33.20it/s]\u001b[AValidation loss: 2.9375979900360107\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 1.7320306301116943\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 1.9890320301055908\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:02<00:00, 33.21it/s]\u001b[AValidation loss: 3.709998607635498\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:02<00:00, 33.22it/s]\u001b[AValidation loss: 1.6433237791061401\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:02<00:00, 33.22it/s]\u001b[AValidation loss: 2.019155502319336\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:02<00:00, 33.22it/s]\u001b[AValidation loss: 1.2193431854248047\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:02<00:00, 33.23it/s]\u001b[AValidation loss: 1.4516944885253906\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:02<00:00, 33.23it/s]\u001b[AValidation loss: 1.7887579202651978\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:02<00:00, 33.24it/s]\u001b[AValidation loss: 1.5836316347122192\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:02<00:00, 33.24it/s]\u001b[AValidation loss: 1.343880295753479\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:03<00:00, 33.24it/s]\u001b[AValidation loss: 2.248256206512451\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:03<00:00, 33.25it/s]\u001b[AValidation loss: 1.5464303493499756\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:03<00:00, 33.25it/s]\u001b[AValidation loss: 1.2398722171783447\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:03<00:00, 33.25it/s]\u001b[AValidation loss: 1.9372729063034058\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:03<00:00, 33.26it/s]\u001b[AValidation loss: 1.7469849586486816\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:03<00:00, 33.26it/s]\u001b[AValidation loss: 2.085272789001465\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:03<00:00, 33.26it/s]\u001b[AValidation loss: 1.8545355796813965\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:03<00:00, 33.27it/s]\u001b[AValidation loss: 1.5811057090759277\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:03<00:00, 33.27it/s]\u001b[AValidation loss: 2.014840602874756\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:03<00:00, 33.27it/s]\u001b[AValidation loss: 2.794410228729248\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:03<00:00, 33.33it/s]\u001b[A\n",
      "Epoch 4:   0%|          | 0/512 [00:00<?, ?it/s, v_num=11]                \u001b[ATraining loss: 1.5506844520568848\n",
      "Epoch 4:   0%|          | 1/512 [00:00<00:21, 23.48it/s, v_num=11]Training loss: 1.575289249420166\n",
      "Epoch 4:   0%|          | 2/512 [00:00<00:59,  8.53it/s, v_num=11]Training loss: 1.4208344221115112\n",
      "Epoch 4:   1%|          | 3/512 [00:00<01:12,  7.03it/s, v_num=11]Training loss: 1.4413557052612305\n",
      "Epoch 4:   1%|          | 4/512 [00:00<01:18,  6.47it/s, v_num=11]Training loss: 3.099595785140991\n",
      "Epoch 4:   1%|          | 5/512 [00:00<01:22,  6.17it/s, v_num=11]Training loss: 1.663865566253662\n",
      "Epoch 4:   1%|          | 6/512 [00:01<01:24,  5.98it/s, v_num=11]Training loss: 1.4872586727142334\n",
      "Epoch 4:   1%|▏         | 7/512 [00:01<01:26,  5.86it/s, v_num=11]Training loss: 1.8290605545043945\n",
      "Epoch 4:   2%|▏         | 8/512 [00:01<01:27,  5.77it/s, v_num=11]Training loss: 0.9587744474411011\n",
      "Epoch 4:   2%|▏         | 9/512 [00:01<01:28,  5.70it/s, v_num=11]Training loss: 1.5513418912887573\n",
      "Epoch 4:   2%|▏         | 10/512 [00:01<01:28,  5.65it/s, v_num=11]Training loss: 1.7473151683807373\n",
      "Epoch 4:   2%|▏         | 11/512 [00:01<01:29,  5.60it/s, v_num=11]Training loss: 1.0328470468521118\n",
      "Epoch 4:   2%|▏         | 12/512 [00:02<01:29,  5.57it/s, v_num=11]Training loss: 1.7221754789352417\n",
      "Epoch 4:   3%|▎         | 13/512 [00:02<01:30,  5.54it/s, v_num=11]Training loss: 1.4302539825439453\n",
      "Epoch 4:   3%|▎         | 14/512 [00:02<01:30,  5.51it/s, v_num=11]Training loss: 1.4285486936569214\n",
      "Epoch 4:   3%|▎         | 15/512 [00:02<01:30,  5.49it/s, v_num=11]Training loss: 1.346711277961731\n",
      "Epoch 4:   3%|▎         | 16/512 [00:02<01:30,  5.47it/s, v_num=11]Training loss: 1.0924742221832275\n",
      "Epoch 4:   3%|▎         | 17/512 [00:03<01:30,  5.45it/s, v_num=11]Training loss: 1.6014862060546875\n",
      "Epoch 4:   4%|▎         | 18/512 [00:03<01:30,  5.44it/s, v_num=11]Training loss: 1.7713041305541992\n",
      "Epoch 4:   4%|▎         | 19/512 [00:03<01:30,  5.43it/s, v_num=11]Training loss: 1.6665130853652954\n",
      "Epoch 4:   4%|▍         | 20/512 [00:03<01:30,  5.42it/s, v_num=11]Training loss: 1.195388674736023\n",
      "Epoch 4:   4%|▍         | 21/512 [00:03<01:30,  5.41it/s, v_num=11]Training loss: 1.6691834926605225\n",
      "Epoch 4:   4%|▍         | 22/512 [00:04<01:30,  5.40it/s, v_num=11]Training loss: 1.311274766921997\n",
      "Epoch 4:   4%|▍         | 23/512 [00:04<01:30,  5.39it/s, v_num=11]Training loss: 1.6485594511032104\n",
      "Epoch 4:   5%|▍         | 24/512 [00:04<01:30,  5.38it/s, v_num=11]Training loss: 1.3164639472961426\n",
      "Epoch 4:   5%|▍         | 25/512 [00:04<01:30,  5.37it/s, v_num=11]Training loss: 1.0642120838165283\n",
      "Epoch 4:   5%|▌         | 26/512 [00:04<01:30,  5.37it/s, v_num=11]Training loss: 1.3767621517181396\n",
      "Epoch 4:   5%|▌         | 27/512 [00:05<01:30,  5.36it/s, v_num=11]Training loss: 1.3312097787857056\n",
      "Epoch 4:   5%|▌         | 28/512 [00:05<01:30,  5.35it/s, v_num=11]Training loss: 1.7944363355636597\n",
      "Epoch 4:   6%|▌         | 29/512 [00:05<01:30,  5.35it/s, v_num=11]Training loss: 1.2595160007476807\n",
      "Epoch 4:   6%|▌         | 30/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.690284013748169\n",
      "Epoch 4:   6%|▌         | 31/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.113827109336853\n",
      "Epoch 4:   6%|▋         | 32/512 [00:05<01:29,  5.34it/s, v_num=11]Training loss: 1.4089163541793823\n",
      "Epoch 4:   6%|▋         | 33/512 [00:06<01:29,  5.33it/s, v_num=11]Training loss: 1.8640750646591187\n",
      "Epoch 4:   7%|▋         | 34/512 [00:06<01:29,  5.33it/s, v_num=11]Training loss: 1.2010869979858398\n",
      "Epoch 4:   7%|▋         | 35/512 [00:06<01:29,  5.32it/s, v_num=11]Training loss: 1.18274986743927\n",
      "Epoch 4:   7%|▋         | 36/512 [00:06<01:29,  5.32it/s, v_num=11]Training loss: 1.880126714706421\n",
      "Epoch 4:   7%|▋         | 37/512 [00:06<01:29,  5.32it/s, v_num=11]Training loss: 1.6805696487426758\n",
      "Epoch 4:   7%|▋         | 38/512 [00:07<01:29,  5.31it/s, v_num=11]Training loss: 1.269216537475586\n",
      "Epoch 4:   8%|▊         | 39/512 [00:07<01:29,  5.31it/s, v_num=11]Training loss: 0.8979406356811523\n",
      "Epoch 4:   8%|▊         | 40/512 [00:07<01:28,  5.31it/s, v_num=11]Training loss: 1.8123219013214111\n",
      "Epoch 4:   8%|▊         | 41/512 [00:07<01:28,  5.31it/s, v_num=11]Training loss: 1.6215463876724243\n",
      "Epoch 4:   8%|▊         | 42/512 [00:07<01:28,  5.30it/s, v_num=11]Training loss: 1.699925422668457\n",
      "Epoch 4:   8%|▊         | 43/512 [00:08<01:28,  5.30it/s, v_num=11]Training loss: 1.4011268615722656\n",
      "Epoch 4:   9%|▊         | 44/512 [00:08<01:28,  5.30it/s, v_num=11]Training loss: 1.6069422960281372\n",
      "Epoch 4:   9%|▉         | 45/512 [00:08<01:28,  5.30it/s, v_num=11]Training loss: 1.5231143236160278\n",
      "Epoch 4:   9%|▉         | 46/512 [00:08<01:28,  5.30it/s, v_num=11]Training loss: 1.0616281032562256\n",
      "Epoch 4:   9%|▉         | 47/512 [00:08<01:27,  5.29it/s, v_num=11]Training loss: 0.9078383445739746\n",
      "Epoch 4:   9%|▉         | 48/512 [00:09<01:27,  5.29it/s, v_num=11]Training loss: 1.4678637981414795\n",
      "Epoch 4:  10%|▉         | 49/512 [00:09<01:27,  5.29it/s, v_num=11]Training loss: 1.308416724205017\n",
      "Epoch 4:  10%|▉         | 50/512 [00:09<01:27,  5.29it/s, v_num=11]Training loss: 1.663480281829834\n",
      "Epoch 4:  10%|▉         | 51/512 [00:09<01:27,  5.29it/s, v_num=11]Training loss: 1.620423674583435\n",
      "Epoch 4:  10%|█         | 52/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.504253625869751\n",
      "Epoch 4:  10%|█         | 53/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.829565405845642\n",
      "Epoch 4:  11%|█         | 54/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.2736806869506836\n",
      "Epoch 4:  11%|█         | 55/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.8495815992355347\n",
      "Epoch 4:  11%|█         | 56/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.0976800918579102\n",
      "Epoch 4:  11%|█         | 57/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.8171448707580566\n",
      "Epoch 4:  11%|█▏        | 58/512 [00:10<01:26,  5.28it/s, v_num=11]Training loss: 1.9958211183547974\n",
      "Epoch 4:  12%|█▏        | 59/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.517038106918335\n",
      "Epoch 4:  12%|█▏        | 60/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.55790376663208\n",
      "Epoch 4:  12%|█▏        | 61/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.7586206197738647\n",
      "Epoch 4:  12%|█▏        | 62/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.6810983419418335\n",
      "Epoch 4:  12%|█▏        | 63/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.5449707508087158\n",
      "Epoch 4:  12%|█▎        | 64/512 [00:12<01:25,  5.27it/s, v_num=11]Training loss: 1.2983993291854858\n",
      "Epoch 4:  13%|█▎        | 65/512 [00:12<01:24,  5.27it/s, v_num=11]Training loss: 1.6863590478897095\n",
      "Epoch 4:  13%|█▎        | 66/512 [00:12<01:24,  5.27it/s, v_num=11]Training loss: 1.414336919784546\n",
      "Epoch 4:  13%|█▎        | 67/512 [00:12<01:24,  5.27it/s, v_num=11]Training loss: 0.9462425708770752\n",
      "Epoch 4:  13%|█▎        | 68/512 [00:12<01:24,  5.27it/s, v_num=11]Training loss: 1.5868521928787231\n",
      "Epoch 4:  13%|█▎        | 69/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 1.5587245225906372\n",
      "Epoch 4:  14%|█▎        | 70/512 [00:13<01:23,  5.26it/s, v_num=11]Training loss: 1.7404729127883911\n",
      "Epoch 4:  14%|█▍        | 71/512 [00:13<01:23,  5.26it/s, v_num=11]Training loss: 1.2586463689804077\n",
      "Epoch 4:  14%|█▍        | 72/512 [00:13<01:23,  5.26it/s, v_num=11]Training loss: 1.688213586807251\n",
      "Epoch 4:  14%|█▍        | 73/512 [00:13<01:23,  5.26it/s, v_num=11]Training loss: 1.5230225324630737\n",
      "Epoch 4:  14%|█▍        | 74/512 [00:14<01:23,  5.26it/s, v_num=11]Training loss: 1.0465890169143677\n",
      "Epoch 4:  15%|█▍        | 75/512 [00:14<01:23,  5.26it/s, v_num=11]Training loss: 1.5275431871414185\n",
      "Epoch 4:  15%|█▍        | 76/512 [00:14<01:22,  5.26it/s, v_num=11]Training loss: 1.6307436227798462\n",
      "Epoch 4:  15%|█▌        | 77/512 [00:14<01:22,  5.26it/s, v_num=11]Training loss: 1.5235366821289062\n",
      "Epoch 4:  15%|█▌        | 78/512 [00:14<01:22,  5.26it/s, v_num=11]Training loss: 1.6194140911102295\n",
      "Epoch 4:  15%|█▌        | 79/512 [00:15<01:22,  5.26it/s, v_num=11]Training loss: 2.031468152999878\n",
      "Epoch 4:  16%|█▌        | 80/512 [00:15<01:22,  5.26it/s, v_num=11]Training loss: 1.218412160873413\n",
      "Epoch 4:  16%|█▌        | 81/512 [00:15<01:22,  5.26it/s, v_num=11]Training loss: 1.0294638872146606\n",
      "Epoch 4:  16%|█▌        | 82/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.5023260116577148\n",
      "Epoch 4:  16%|█▌        | 83/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 2.351525068283081\n",
      "Epoch 4:  16%|█▋        | 84/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 2.2013144493103027\n",
      "Epoch 4:  17%|█▋        | 85/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.3367451429367065\n",
      "Epoch 4:  17%|█▋        | 86/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.1026557683944702\n",
      "Epoch 4:  17%|█▋        | 87/512 [00:16<01:20,  5.25it/s, v_num=11]Training loss: 2.113734245300293\n",
      "Epoch 4:  17%|█▋        | 88/512 [00:16<01:20,  5.25it/s, v_num=11]Training loss: 2.2180566787719727\n",
      "Epoch 4:  17%|█▋        | 89/512 [00:16<01:20,  5.25it/s, v_num=11]Training loss: 1.2856111526489258\n",
      "Epoch 4:  18%|█▊        | 90/512 [00:17<01:20,  5.25it/s, v_num=11]Training loss: 2.1902735233306885\n",
      "Epoch 4:  18%|█▊        | 91/512 [00:17<01:20,  5.25it/s, v_num=11]Training loss: 1.2664778232574463\n",
      "Epoch 4:  18%|█▊        | 92/512 [00:17<01:20,  5.25it/s, v_num=11]Training loss: 1.6383116245269775\n",
      "Epoch 4:  18%|█▊        | 93/512 [00:17<01:19,  5.25it/s, v_num=11]Training loss: 1.1568490266799927\n",
      "Epoch 4:  18%|█▊        | 94/512 [00:17<01:19,  5.25it/s, v_num=11]Training loss: 1.0000091791152954\n",
      "Epoch 4:  19%|█▊        | 95/512 [00:18<01:19,  5.25it/s, v_num=11]Training loss: 1.7419579029083252\n",
      "Epoch 4:  19%|█▉        | 96/512 [00:18<01:19,  5.25it/s, v_num=11]Training loss: 1.4039489030838013\n",
      "Epoch 4:  19%|█▉        | 97/512 [00:18<01:19,  5.25it/s, v_num=11]Training loss: 1.3617095947265625\n",
      "Epoch 4:  19%|█▉        | 98/512 [00:18<01:18,  5.25it/s, v_num=11]Training loss: 2.6293816566467285\n",
      "Epoch 4:  19%|█▉        | 99/512 [00:18<01:18,  5.25it/s, v_num=11]Training loss: 0.972859263420105\n",
      "Epoch 4:  20%|█▉        | 100/512 [00:19<01:18,  5.25it/s, v_num=11]Training loss: 1.325137734413147\n",
      "Epoch 4:  20%|█▉        | 101/512 [00:19<01:18,  5.25it/s, v_num=11]Training loss: 1.3825178146362305\n",
      "Epoch 4:  20%|█▉        | 102/512 [00:19<01:18,  5.25it/s, v_num=11]Training loss: 2.043656349182129\n",
      "Epoch 4:  20%|██        | 103/512 [00:19<01:17,  5.24it/s, v_num=11]Training loss: 1.5045170783996582\n",
      "Epoch 4:  20%|██        | 104/512 [00:19<01:17,  5.24it/s, v_num=11]Training loss: 1.3483604192733765\n",
      "Epoch 4:  21%|██        | 105/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 2.2584290504455566\n",
      "Epoch 4:  21%|██        | 106/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.128798484802246\n",
      "Epoch 4:  21%|██        | 107/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.562812328338623\n",
      "Epoch 4:  21%|██        | 108/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.8648432493209839\n",
      "Epoch 4:  21%|██▏       | 109/512 [00:20<01:16,  5.24it/s, v_num=11]Training loss: 1.2276246547698975\n",
      "Epoch 4:  21%|██▏       | 110/512 [00:20<01:16,  5.24it/s, v_num=11]Training loss: 2.1153950691223145\n",
      "Epoch 4:  22%|██▏       | 111/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.058377981185913\n",
      "Epoch 4:  22%|██▏       | 112/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.649903655052185\n",
      "Epoch 4:  22%|██▏       | 113/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.8854330778121948\n",
      "Epoch 4:  22%|██▏       | 114/512 [00:21<01:15,  5.24it/s, v_num=11]Training loss: 1.3528852462768555\n",
      "Epoch 4:  22%|██▏       | 115/512 [00:21<01:15,  5.24it/s, v_num=11]Training loss: 1.2683535814285278\n",
      "Epoch 4:  23%|██▎       | 116/512 [00:22<01:15,  5.24it/s, v_num=11]Training loss: 1.1915740966796875\n",
      "Epoch 4:  23%|██▎       | 117/512 [00:22<01:15,  5.24it/s, v_num=11]Training loss: 1.162672996520996\n",
      "Epoch 4:  23%|██▎       | 118/512 [00:22<01:15,  5.24it/s, v_num=11]Training loss: 1.0545878410339355\n",
      "Epoch 4:  23%|██▎       | 119/512 [00:22<01:15,  5.24it/s, v_num=11]Training loss: 1.6028422117233276\n",
      "Epoch 4:  23%|██▎       | 120/512 [00:22<01:14,  5.24it/s, v_num=11]Training loss: 1.2190574407577515\n",
      "Epoch 4:  24%|██▎       | 121/512 [00:23<01:14,  5.24it/s, v_num=11]Training loss: 1.1588327884674072\n",
      "Epoch 4:  24%|██▍       | 122/512 [00:23<01:14,  5.24it/s, v_num=11]Training loss: 0.969897985458374\n",
      "Epoch 4:  24%|██▍       | 123/512 [00:23<01:14,  5.24it/s, v_num=11]Training loss: 1.7418832778930664\n",
      "Epoch 4:  24%|██▍       | 124/512 [00:23<01:14,  5.24it/s, v_num=11]Training loss: 2.1561219692230225\n",
      "Epoch 4:  24%|██▍       | 125/512 [00:23<01:13,  5.24it/s, v_num=11]Training loss: 1.4212079048156738\n",
      "Epoch 4:  25%|██▍       | 126/512 [00:24<01:13,  5.24it/s, v_num=11]Training loss: 1.253821849822998\n",
      "Epoch 4:  25%|██▍       | 127/512 [00:24<01:13,  5.24it/s, v_num=11]Training loss: 1.2336618900299072\n",
      "Epoch 4:  25%|██▌       | 128/512 [00:24<01:13,  5.24it/s, v_num=11]Training loss: 1.0047376155853271\n",
      "Epoch 4:  25%|██▌       | 129/512 [00:24<01:13,  5.24it/s, v_num=11]Training loss: 2.184670925140381\n",
      "Epoch 4:  25%|██▌       | 130/512 [00:24<01:12,  5.24it/s, v_num=11]Training loss: 1.3164031505584717\n",
      "Epoch 4:  26%|██▌       | 131/512 [00:25<01:12,  5.24it/s, v_num=11]Training loss: 1.8768187761306763\n",
      "Epoch 4:  26%|██▌       | 132/512 [00:25<01:12,  5.24it/s, v_num=11]Training loss: 1.8397905826568604\n",
      "Epoch 4:  26%|██▌       | 133/512 [00:25<01:12,  5.24it/s, v_num=11]Training loss: 1.8763184547424316\n",
      "Epoch 4:  26%|██▌       | 134/512 [00:25<01:12,  5.24it/s, v_num=11]Training loss: 0.9861114621162415\n",
      "Epoch 4:  26%|██▋       | 135/512 [00:25<01:12,  5.24it/s, v_num=11]Training loss: 2.304811954498291\n",
      "Epoch 4:  27%|██▋       | 136/512 [00:25<01:11,  5.23it/s, v_num=11]Training loss: 1.541968822479248\n",
      "Epoch 4:  27%|██▋       | 137/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.1874648332595825\n",
      "Epoch 4:  27%|██▋       | 138/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.1425743103027344\n",
      "Epoch 4:  27%|██▋       | 139/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.202507495880127\n",
      "Epoch 4:  27%|██▋       | 140/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.615058183670044\n",
      "Epoch 4:  28%|██▊       | 141/512 [00:26<01:10,  5.23it/s, v_num=11]Training loss: 1.1001017093658447\n",
      "Epoch 4:  28%|██▊       | 142/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.9201000928878784\n",
      "Epoch 4:  28%|██▊       | 143/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.2786022424697876\n",
      "Epoch 4:  28%|██▊       | 144/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.1689300537109375\n",
      "Epoch 4:  28%|██▊       | 145/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.917130708694458\n",
      "Epoch 4:  29%|██▊       | 146/512 [00:27<01:09,  5.23it/s, v_num=11]Training loss: 1.610783338546753\n",
      "Epoch 4:  29%|██▊       | 147/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 2.1185691356658936\n",
      "Epoch 4:  29%|██▉       | 148/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.5739436149597168\n",
      "Epoch 4:  29%|██▉       | 149/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.8768666982650757\n",
      "Epoch 4:  29%|██▉       | 150/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.5391607284545898\n",
      "Epoch 4:  29%|██▉       | 151/512 [00:28<01:08,  5.23it/s, v_num=11]Training loss: 1.3497235774993896\n",
      "Epoch 4:  30%|██▉       | 152/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 0.9777384996414185\n",
      "Epoch 4:  30%|██▉       | 153/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.3951225280761719\n",
      "Epoch 4:  30%|███       | 154/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.0582542419433594\n",
      "Epoch 4:  30%|███       | 155/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.2702140808105469\n",
      "Epoch 4:  30%|███       | 156/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.4405097961425781\n",
      "Epoch 4:  31%|███       | 157/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.386138916015625\n",
      "Epoch 4:  31%|███       | 158/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 0.9828048348426819\n",
      "Epoch 4:  31%|███       | 159/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.466888666152954\n",
      "Epoch 4:  31%|███▏      | 160/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 0.9445978403091431\n",
      "Epoch 4:  31%|███▏      | 161/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.6959750652313232\n",
      "Epoch 4:  32%|███▏      | 162/512 [00:30<01:06,  5.23it/s, v_num=11]Training loss: 1.3050868511199951\n",
      "Epoch 4:  32%|███▏      | 163/512 [00:31<01:06,  5.23it/s, v_num=11]Training loss: 1.8615554571151733\n",
      "Epoch 4:  32%|███▏      | 164/512 [00:31<01:06,  5.23it/s, v_num=11]Training loss: 1.4090533256530762\n",
      "Epoch 4:  32%|███▏      | 165/512 [00:31<01:06,  5.23it/s, v_num=11]Training loss: 1.3886291980743408\n",
      "Epoch 4:  32%|███▏      | 166/512 [00:31<01:06,  5.23it/s, v_num=11]Training loss: 1.2900959253311157\n",
      "Epoch 4:  33%|███▎      | 167/512 [00:31<01:05,  5.23it/s, v_num=11]Training loss: 1.5722692012786865\n",
      "Epoch 4:  33%|███▎      | 168/512 [00:32<01:05,  5.23it/s, v_num=11]Training loss: 1.5542149543762207\n",
      "Epoch 4:  33%|███▎      | 169/512 [00:32<01:05,  5.23it/s, v_num=11]Training loss: 1.0630245208740234\n",
      "Epoch 4:  33%|███▎      | 170/512 [00:32<01:05,  5.23it/s, v_num=11]Training loss: 1.8422350883483887\n",
      "Epoch 4:  33%|███▎      | 171/512 [00:32<01:05,  5.23it/s, v_num=11]Training loss: 1.737743616104126\n",
      "Epoch 4:  34%|███▎      | 172/512 [00:32<01:05,  5.23it/s, v_num=11]Training loss: 1.2910248041152954\n",
      "Epoch 4:  34%|███▍      | 173/512 [00:33<01:04,  5.23it/s, v_num=11]Training loss: 1.618949055671692\n",
      "Epoch 4:  34%|███▍      | 174/512 [00:33<01:04,  5.23it/s, v_num=11]Training loss: 1.2802094221115112\n",
      "Epoch 4:  34%|███▍      | 175/512 [00:33<01:04,  5.23it/s, v_num=11]Training loss: 1.7741386890411377\n",
      "Epoch 4:  34%|███▍      | 176/512 [00:33<01:04,  5.23it/s, v_num=11]Training loss: 1.0235612392425537\n",
      "Epoch 4:  35%|███▍      | 177/512 [00:33<01:04,  5.23it/s, v_num=11]Training loss: 1.222046971321106\n",
      "Epoch 4:  35%|███▍      | 178/512 [00:34<01:03,  5.23it/s, v_num=11]Training loss: 0.9985753893852234\n",
      "Epoch 4:  35%|███▍      | 179/512 [00:34<01:03,  5.23it/s, v_num=11]Training loss: 1.063073992729187\n",
      "Epoch 4:  35%|███▌      | 180/512 [00:34<01:03,  5.23it/s, v_num=11]Training loss: 1.0785998106002808\n",
      "Epoch 4:  35%|███▌      | 181/512 [00:34<01:03,  5.23it/s, v_num=11]Training loss: 1.576108455657959\n",
      "Epoch 4:  36%|███▌      | 182/512 [00:34<01:03,  5.23it/s, v_num=11]Training loss: 0.9590833187103271\n",
      "Epoch 4:  36%|███▌      | 183/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.382347822189331\n",
      "Epoch 4:  36%|███▌      | 184/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.7217886447906494\n",
      "Epoch 4:  36%|███▌      | 185/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.4954307079315186\n",
      "Epoch 4:  36%|███▋      | 186/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.103637933731079\n",
      "Epoch 4:  37%|███▋      | 187/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.661576747894287\n",
      "Epoch 4:  37%|███▋      | 188/512 [00:35<01:02,  5.23it/s, v_num=11]Training loss: 1.941053867340088\n",
      "Epoch 4:  37%|███▋      | 189/512 [00:36<01:01,  5.23it/s, v_num=11]Training loss: 1.4000376462936401\n",
      "Epoch 4:  37%|███▋      | 190/512 [00:36<01:01,  5.23it/s, v_num=11]Training loss: 0.8771529197692871\n",
      "Epoch 4:  37%|███▋      | 191/512 [00:36<01:01,  5.23it/s, v_num=11]Training loss: 1.0509989261627197\n",
      "Epoch 4:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.5586833953857422\n",
      "Epoch 4:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.0347967147827148\n",
      "Epoch 4:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.3977060317993164\n",
      "Epoch 4:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.3795934915542603\n",
      "Epoch 4:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.7429012060165405\n",
      "Epoch 4:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.575674057006836\n",
      "Epoch 4:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.4386472702026367\n",
      "Epoch 4:  39%|███▉      | 199/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.6322654485702515\n",
      "Epoch 4:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.4429094791412354\n",
      "Epoch 4:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.243175745010376\n",
      "Epoch 4:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.7236087322235107\n",
      "Epoch 4:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 2.1994009017944336\n",
      "Epoch 4:  40%|███▉      | 204/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.2732398509979248\n",
      "Epoch 4:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.6788690090179443\n",
      "Epoch 4:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.7999039888381958\n",
      "Epoch 4:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 0.9182977676391602\n",
      "Epoch 4:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.0025758743286133\n",
      "Epoch 4:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 1.3070781230926514\n",
      "Epoch 4:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 3.981095790863037\n",
      "Epoch 4:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.361167311668396\n",
      "Epoch 4:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.6906070709228516\n",
      "Epoch 4:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 2.119168758392334\n",
      "Epoch 4:  42%|████▏     | 214/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.1021910905838013\n",
      "Epoch 4:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4700016975402832\n",
      "Epoch 4:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.9466431140899658\n",
      "Epoch 4:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.109454870223999\n",
      "Epoch 4:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.7203598022460938\n",
      "Epoch 4:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.2246290445327759\n",
      "Epoch 4:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.621180772781372\n",
      "Epoch 4:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.2398438453674316\n",
      "Epoch 4:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.666326642036438\n",
      "Epoch 4:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 2.422356128692627\n",
      "Epoch 4:  44%|████▍     | 224/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.5054012537002563\n",
      "Epoch 4:  44%|████▍     | 225/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.5233051776885986\n",
      "Epoch 4:  44%|████▍     | 226/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.2038648128509521\n",
      "Epoch 4:  44%|████▍     | 227/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.197430968284607\n",
      "Epoch 4:  45%|████▍     | 228/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 0.8813487887382507\n",
      "Epoch 4:  45%|████▍     | 229/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.8192108869552612\n",
      "Epoch 4:  45%|████▍     | 230/512 [00:44<00:54,  5.22it/s, v_num=11]Training loss: 2.3469481468200684\n",
      "Epoch 4:  45%|████▌     | 231/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 0.9761020541191101\n",
      "Epoch 4:  45%|████▌     | 232/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.2975126504898071\n",
      "Epoch 4:  46%|████▌     | 233/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 2.901543617248535\n",
      "Epoch 4:  46%|████▌     | 234/512 [00:44<00:53,  5.22it/s, v_num=11]Training loss: 1.8819515705108643\n",
      "Epoch 4:  46%|████▌     | 235/512 [00:45<00:53,  5.22it/s, v_num=11]Training loss: 1.7282960414886475\n",
      "Epoch 4:  46%|████▌     | 236/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.3155040740966797\n",
      "Epoch 4:  46%|████▋     | 237/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.9964466094970703\n",
      "Epoch 4:  46%|████▋     | 238/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.5263326168060303\n",
      "Epoch 4:  47%|████▋     | 239/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.076341152191162\n",
      "Epoch 4:  47%|████▋     | 240/512 [00:45<00:52,  5.22it/s, v_num=11]Training loss: 1.8050283193588257\n",
      "Epoch 4:  47%|████▋     | 241/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.3847386837005615\n",
      "Epoch 4:  47%|████▋     | 242/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.3410017490386963\n",
      "Epoch 4:  47%|████▋     | 243/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.183658242225647\n",
      "Epoch 4:  48%|████▊     | 244/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.2445018291473389\n",
      "Epoch 4:  48%|████▊     | 245/512 [00:46<00:51,  5.22it/s, v_num=11]Training loss: 1.1010775566101074\n",
      "Epoch 4:  48%|████▊     | 246/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.2432805299758911\n",
      "Epoch 4:  48%|████▊     | 247/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.462186574935913\n",
      "Epoch 4:  48%|████▊     | 248/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.326870083808899\n",
      "Epoch 4:  49%|████▊     | 249/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.566730260848999\n",
      "Epoch 4:  49%|████▉     | 250/512 [00:47<00:50,  5.22it/s, v_num=11]Training loss: 1.343282699584961\n",
      "Epoch 4:  49%|████▉     | 251/512 [00:48<00:50,  5.22it/s, v_num=11]Training loss: 1.1477649211883545\n",
      "Epoch 4:  49%|████▉     | 252/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.2936503887176514\n",
      "Epoch 4:  49%|████▉     | 253/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.357507586479187\n",
      "Epoch 4:  50%|████▉     | 254/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 1.1126524209976196\n",
      "Epoch 4:  50%|████▉     | 255/512 [00:48<00:49,  5.22it/s, v_num=11]Training loss: 0.9928580522537231\n",
      "Epoch 4:  50%|█████     | 256/512 [00:49<00:49,  5.22it/s, v_num=11]Training loss: 1.2349759340286255\n",
      "Epoch 4:  50%|█████     | 257/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.5125105381011963\n",
      "Epoch 4:  50%|█████     | 258/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 1.5575944185256958\n",
      "Epoch 4:  51%|█████     | 259/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 2.077697515487671\n",
      "Epoch 4:  51%|█████     | 260/512 [00:49<00:48,  5.22it/s, v_num=11]Training loss: 2.240882396697998\n",
      "Epoch 4:  51%|█████     | 261/512 [00:50<00:48,  5.22it/s, v_num=11]Training loss: 1.1863396167755127\n",
      "Epoch 4:  51%|█████     | 262/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 2.2486727237701416\n",
      "Epoch 4:  51%|█████▏    | 263/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 0.9570209980010986\n",
      "Epoch 4:  52%|█████▏    | 264/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.8999955654144287\n",
      "Epoch 4:  52%|█████▏    | 265/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.4405677318572998\n",
      "Epoch 4:  52%|█████▏    | 266/512 [00:50<00:47,  5.22it/s, v_num=11]Training loss: 1.2054808139801025\n",
      "Epoch 4:  52%|█████▏    | 267/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.9524290561676025\n",
      "Epoch 4:  52%|█████▏    | 268/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.2114677429199219\n",
      "Epoch 4:  53%|█████▎    | 269/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.2879209518432617\n",
      "Epoch 4:  53%|█████▎    | 270/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.5806262493133545\n",
      "Epoch 4:  53%|█████▎    | 271/512 [00:51<00:46,  5.22it/s, v_num=11]Training loss: 1.590369701385498\n",
      "Epoch 4:  53%|█████▎    | 272/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.3661746978759766\n",
      "Epoch 4:  53%|█████▎    | 273/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.431668758392334\n",
      "Epoch 4:  54%|█████▎    | 274/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.330282211303711\n",
      "Epoch 4:  54%|█████▎    | 275/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 1.4661372900009155\n",
      "Epoch 4:  54%|█████▍    | 276/512 [00:52<00:45,  5.22it/s, v_num=11]Training loss: 0.9578239321708679\n",
      "Epoch 4:  54%|█████▍    | 277/512 [00:53<00:45,  5.22it/s, v_num=11]Training loss: 1.9163949489593506\n",
      "Epoch 4:  54%|█████▍    | 278/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.932700514793396\n",
      "Epoch 4:  54%|█████▍    | 279/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.3047393560409546\n",
      "Epoch 4:  55%|█████▍    | 280/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 0.9624249935150146\n",
      "Epoch 4:  55%|█████▍    | 281/512 [00:53<00:44,  5.22it/s, v_num=11]Training loss: 1.5778443813323975\n",
      "Epoch 4:  55%|█████▌    | 282/512 [00:54<00:44,  5.22it/s, v_num=11]Training loss: 1.0802638530731201\n",
      "Epoch 4:  55%|█████▌    | 283/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.5549813508987427\n",
      "Epoch 4:  55%|█████▌    | 284/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.5450276136398315\n",
      "Epoch 4:  56%|█████▌    | 285/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 1.260413646697998\n",
      "Epoch 4:  56%|█████▌    | 286/512 [00:54<00:43,  5.22it/s, v_num=11]Training loss: 0.9901348948478699\n",
      "Epoch 4:  56%|█████▌    | 287/512 [00:55<00:43,  5.22it/s, v_num=11]Training loss: 2.1125757694244385\n",
      "Epoch 4:  56%|█████▋    | 288/512 [00:55<00:42,  5.22it/s, v_num=11]Training loss: 1.2979769706726074\n",
      "Epoch 4:  56%|█████▋    | 289/512 [00:55<00:42,  5.22it/s, v_num=11]Training loss: 1.188746690750122\n",
      "Epoch 4:  57%|█████▋    | 290/512 [00:55<00:42,  5.22it/s, v_num=11]Training loss: 1.2148022651672363\n",
      "Epoch 4:  57%|█████▋    | 291/512 [00:55<00:42,  5.22it/s, v_num=11]Training loss: 1.6832355260849\n",
      "Epoch 4:  57%|█████▋    | 292/512 [00:55<00:42,  5.22it/s, v_num=11]Training loss: 1.316787838935852\n",
      "Epoch 4:  57%|█████▋    | 293/512 [00:56<00:41,  5.22it/s, v_num=11]Training loss: 1.1452851295471191\n",
      "Epoch 4:  57%|█████▋    | 294/512 [00:56<00:41,  5.22it/s, v_num=11]Training loss: 1.3446943759918213\n",
      "Epoch 4:  58%|█████▊    | 295/512 [00:56<00:41,  5.22it/s, v_num=11]Training loss: 1.6321455240249634\n",
      "Epoch 4:  58%|█████▊    | 296/512 [00:56<00:41,  5.22it/s, v_num=11]Training loss: 1.3953357934951782\n",
      "Epoch 4:  58%|█████▊    | 297/512 [00:56<00:41,  5.22it/s, v_num=11]Training loss: 1.7608283758163452\n",
      "Epoch 4:  58%|█████▊    | 298/512 [00:57<00:41,  5.22it/s, v_num=11]Training loss: 1.292170524597168\n",
      "Epoch 4:  58%|█████▊    | 299/512 [00:57<00:40,  5.22it/s, v_num=11]Training loss: 1.4868462085723877\n",
      "Epoch 4:  59%|█████▊    | 300/512 [00:57<00:40,  5.22it/s, v_num=11]Training loss: 1.120408296585083\n",
      "Epoch 4:  59%|█████▉    | 301/512 [00:57<00:40,  5.22it/s, v_num=11]Training loss: 1.5008907318115234\n",
      "Epoch 4:  59%|█████▉    | 302/512 [00:57<00:40,  5.22it/s, v_num=11]Training loss: 1.4270968437194824\n",
      "Epoch 4:  59%|█████▉    | 303/512 [00:58<00:40,  5.22it/s, v_num=11]Training loss: 1.3363714218139648\n",
      "Epoch 4:  59%|█████▉    | 304/512 [00:58<00:39,  5.22it/s, v_num=11]Training loss: 2.4954590797424316\n",
      "Epoch 4:  60%|█████▉    | 305/512 [00:58<00:39,  5.22it/s, v_num=11]Training loss: 1.5616588592529297\n",
      "Epoch 4:  60%|█████▉    | 306/512 [00:58<00:39,  5.22it/s, v_num=11]Training loss: 0.9662207961082458\n",
      "Epoch 4:  60%|█████▉    | 307/512 [00:58<00:39,  5.22it/s, v_num=11]Training loss: 1.3660709857940674\n",
      "Epoch 4:  60%|██████    | 308/512 [00:59<00:39,  5.22it/s, v_num=11]Training loss: 1.7708170413970947\n",
      "Epoch 4:  60%|██████    | 309/512 [00:59<00:38,  5.22it/s, v_num=11]Training loss: 1.9334594011306763\n",
      "Epoch 4:  61%|██████    | 310/512 [00:59<00:38,  5.22it/s, v_num=11]Training loss: 1.546640396118164\n",
      "Epoch 4:  61%|██████    | 311/512 [00:59<00:38,  5.22it/s, v_num=11]Training loss: 1.8764513731002808\n",
      "Epoch 4:  61%|██████    | 312/512 [00:59<00:38,  5.22it/s, v_num=11]Training loss: 1.3779892921447754\n",
      "Epoch 4:  61%|██████    | 313/512 [01:00<00:38,  5.22it/s, v_num=11]Training loss: 1.4258232116699219\n",
      "Epoch 4:  61%|██████▏   | 314/512 [01:00<00:37,  5.22it/s, v_num=11]Training loss: 1.4056971073150635\n",
      "Epoch 4:  62%|██████▏   | 315/512 [01:00<00:37,  5.22it/s, v_num=11]Training loss: 1.5701994895935059\n",
      "Epoch 4:  62%|██████▏   | 316/512 [01:00<00:37,  5.22it/s, v_num=11]Training loss: 1.8614428043365479\n",
      "Epoch 4:  62%|██████▏   | 317/512 [01:00<00:37,  5.22it/s, v_num=11]Training loss: 1.3191293478012085\n",
      "Epoch 4:  62%|██████▏   | 318/512 [01:00<00:37,  5.22it/s, v_num=11]Training loss: 1.2613825798034668\n",
      "Epoch 4:  62%|██████▏   | 319/512 [01:01<00:37,  5.22it/s, v_num=11]Training loss: 1.714019536972046\n",
      "Epoch 4:  62%|██████▎   | 320/512 [01:01<00:36,  5.22it/s, v_num=11]Training loss: 1.8548905849456787\n",
      "Epoch 4:  63%|██████▎   | 321/512 [01:01<00:36,  5.22it/s, v_num=11]Training loss: 0.9319185018539429\n",
      "Epoch 4:  63%|██████▎   | 322/512 [01:01<00:36,  5.22it/s, v_num=11]Training loss: 1.4390190839767456\n",
      "Epoch 4:  63%|██████▎   | 323/512 [01:01<00:36,  5.22it/s, v_num=11]Training loss: 1.443642258644104\n",
      "Epoch 4:  63%|██████▎   | 324/512 [01:02<00:36,  5.22it/s, v_num=11]Training loss: 1.4270049333572388\n",
      "Epoch 4:  63%|██████▎   | 325/512 [01:02<00:35,  5.22it/s, v_num=11]Training loss: 1.4606351852416992\n",
      "Epoch 4:  64%|██████▎   | 326/512 [01:02<00:35,  5.22it/s, v_num=11]Training loss: 1.897408366203308\n",
      "Epoch 4:  64%|██████▍   | 327/512 [01:02<00:35,  5.22it/s, v_num=11]Training loss: 1.5443761348724365\n",
      "Epoch 4:  64%|██████▍   | 328/512 [01:02<00:35,  5.22it/s, v_num=11]Training loss: 1.5070185661315918\n",
      "Epoch 4:  64%|██████▍   | 329/512 [01:03<00:35,  5.22it/s, v_num=11]Training loss: 1.594386339187622\n",
      "Epoch 4:  64%|██████▍   | 330/512 [01:03<00:34,  5.22it/s, v_num=11]Training loss: 1.3574926853179932\n",
      "Epoch 4:  65%|██████▍   | 331/512 [01:03<00:34,  5.22it/s, v_num=11]Training loss: 1.4057412147521973\n",
      "Epoch 4:  65%|██████▍   | 332/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.4803407192230225\n",
      "Epoch 4:  65%|██████▌   | 333/512 [01:03<00:34,  5.21it/s, v_num=11]Training loss: 1.5972479581832886\n",
      "Epoch 4:  65%|██████▌   | 334/512 [01:04<00:34,  5.21it/s, v_num=11]Training loss: 1.3645691871643066\n",
      "Epoch 4:  65%|██████▌   | 335/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.5190027952194214\n",
      "Epoch 4:  66%|██████▌   | 336/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.040182113647461\n",
      "Epoch 4:  66%|██████▌   | 337/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 2.075166940689087\n",
      "Epoch 4:  66%|██████▌   | 338/512 [01:04<00:33,  5.21it/s, v_num=11]Training loss: 1.6907808780670166\n",
      "Epoch 4:  66%|██████▌   | 339/512 [01:05<00:33,  5.21it/s, v_num=11]Training loss: 1.226284146308899\n",
      "Epoch 4:  66%|██████▋   | 340/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.5322474241256714\n",
      "Epoch 4:  67%|██████▋   | 341/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.5074137449264526\n",
      "Epoch 4:  67%|██████▋   | 342/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.3820230960845947\n",
      "Epoch 4:  67%|██████▋   | 343/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 2.256730318069458\n",
      "Epoch 4:  67%|██████▋   | 344/512 [01:05<00:32,  5.21it/s, v_num=11]Training loss: 1.4627902507781982\n",
      "Epoch 4:  67%|██████▋   | 345/512 [01:06<00:32,  5.21it/s, v_num=11]Training loss: 2.120112895965576\n",
      "Epoch 4:  68%|██████▊   | 346/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.5149779319763184\n",
      "Epoch 4:  68%|██████▊   | 347/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.1515148878097534\n",
      "Epoch 4:  68%|██████▊   | 348/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.148146390914917\n",
      "Epoch 4:  68%|██████▊   | 349/512 [01:06<00:31,  5.21it/s, v_num=11]Training loss: 1.1673314571380615\n",
      "Epoch 4:  68%|██████▊   | 350/512 [01:07<00:31,  5.21it/s, v_num=11]Training loss: 1.8474769592285156\n",
      "Epoch 4:  69%|██████▊   | 351/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.3506965637207031\n",
      "Epoch 4:  69%|██████▉   | 352/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.7053173780441284\n",
      "Epoch 4:  69%|██████▉   | 353/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.092003345489502\n",
      "Epoch 4:  69%|██████▉   | 354/512 [01:07<00:30,  5.21it/s, v_num=11]Training loss: 1.1175787448883057\n",
      "Epoch 4:  69%|██████▉   | 355/512 [01:08<00:30,  5.21it/s, v_num=11]Training loss: 1.8329813480377197\n",
      "Epoch 4:  70%|██████▉   | 356/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.1143468618392944\n",
      "Epoch 4:  70%|██████▉   | 357/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.52647066116333\n",
      "Epoch 4:  70%|██████▉   | 358/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.5370547771453857\n",
      "Epoch 4:  70%|███████   | 359/512 [01:08<00:29,  5.21it/s, v_num=11]Training loss: 1.2890630960464478\n",
      "Epoch 4:  70%|███████   | 360/512 [01:09<00:29,  5.21it/s, v_num=11]Training loss: 2.9732608795166016\n",
      "Epoch 4:  71%|███████   | 361/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.1906782388687134\n",
      "Epoch 4:  71%|███████   | 362/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.7721742391586304\n",
      "Epoch 4:  71%|███████   | 363/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.287581443786621\n",
      "Epoch 4:  71%|███████   | 364/512 [01:09<00:28,  5.21it/s, v_num=11]Training loss: 1.976252555847168\n",
      "Epoch 4:  71%|███████▏  | 365/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 1.0096358060836792\n",
      "Epoch 4:  71%|███████▏  | 366/512 [01:10<00:28,  5.21it/s, v_num=11]Training loss: 3.1001458168029785\n",
      "Epoch 4:  72%|███████▏  | 367/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.3669826984405518\n",
      "Epoch 4:  72%|███████▏  | 368/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.0661579370498657\n",
      "Epoch 4:  72%|███████▏  | 369/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.3338496685028076\n",
      "Epoch 4:  72%|███████▏  | 370/512 [01:10<00:27,  5.21it/s, v_num=11]Training loss: 1.1640208959579468\n",
      "Epoch 4:  72%|███████▏  | 371/512 [01:11<00:27,  5.21it/s, v_num=11]Training loss: 1.4051620960235596\n",
      "Epoch 4:  73%|███████▎  | 372/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.245485544204712\n",
      "Epoch 4:  73%|███████▎  | 373/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.4337472915649414\n",
      "Epoch 4:  73%|███████▎  | 374/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.49234938621521\n",
      "Epoch 4:  73%|███████▎  | 375/512 [01:11<00:26,  5.21it/s, v_num=11]Training loss: 1.6696155071258545\n",
      "Epoch 4:  73%|███████▎  | 376/512 [01:12<00:26,  5.21it/s, v_num=11]Training loss: 1.3884763717651367\n",
      "Epoch 4:  74%|███████▎  | 377/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.190530776977539\n",
      "Epoch 4:  74%|███████▍  | 378/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.1881471872329712\n",
      "Epoch 4:  74%|███████▍  | 379/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 39.08110427856445\n",
      "Epoch 4:  74%|███████▍  | 380/512 [01:12<00:25,  5.21it/s, v_num=11]Training loss: 1.9075500965118408\n",
      "Epoch 4:  74%|███████▍  | 381/512 [01:13<00:25,  5.21it/s, v_num=11]Training loss: 1.7432736158370972\n",
      "Epoch 4:  75%|███████▍  | 382/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.2828564643859863\n",
      "Epoch 4:  75%|███████▍  | 383/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.2804826498031616\n",
      "Epoch 4:  75%|███████▌  | 384/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.7868900299072266\n",
      "Epoch 4:  75%|███████▌  | 385/512 [01:13<00:24,  5.21it/s, v_num=11]Training loss: 1.8380143642425537\n",
      "Epoch 4:  75%|███████▌  | 386/512 [01:14<00:24,  5.21it/s, v_num=11]Training loss: 1.293839454650879\n",
      "Epoch 4:  76%|███████▌  | 387/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.7611284255981445\n",
      "Epoch 4:  76%|███████▌  | 388/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.2540028095245361\n",
      "Epoch 4:  76%|███████▌  | 389/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.5226819515228271\n",
      "Epoch 4:  76%|███████▌  | 390/512 [01:14<00:23,  5.21it/s, v_num=11]Training loss: 1.9688160419464111\n",
      "Epoch 4:  76%|███████▋  | 391/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 1.5475424528121948\n",
      "Epoch 4:  77%|███████▋  | 392/512 [01:15<00:23,  5.21it/s, v_num=11]Training loss: 2.1464595794677734\n",
      "Epoch 4:  77%|███████▋  | 393/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 2.1521215438842773\n",
      "Epoch 4:  77%|███████▋  | 394/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.3393793106079102\n",
      "Epoch 4:  77%|███████▋  | 395/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.7829668521881104\n",
      "Epoch 4:  77%|███████▋  | 396/512 [01:15<00:22,  5.21it/s, v_num=11]Training loss: 1.2789465188980103\n",
      "Epoch 4:  78%|███████▊  | 397/512 [01:16<00:22,  5.21it/s, v_num=11]Training loss: 1.8261538743972778\n",
      "Epoch 4:  78%|███████▊  | 398/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.2708241939544678\n",
      "Epoch 4:  78%|███████▊  | 399/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.4262402057647705\n",
      "Epoch 4:  78%|███████▊  | 400/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.2545945644378662\n",
      "Epoch 4:  78%|███████▊  | 401/512 [01:16<00:21,  5.21it/s, v_num=11]Training loss: 1.0650278329849243\n",
      "Epoch 4:  79%|███████▊  | 402/512 [01:17<00:21,  5.21it/s, v_num=11]Training loss: 1.2793855667114258\n",
      "Epoch 4:  79%|███████▊  | 403/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 2.229175329208374\n",
      "Epoch 4:  79%|███████▉  | 404/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.2342069149017334\n",
      "Epoch 4:  79%|███████▉  | 405/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 2.208138942718506\n",
      "Epoch 4:  79%|███████▉  | 406/512 [01:17<00:20,  5.21it/s, v_num=11]Training loss: 1.565114974975586\n",
      "Epoch 4:  79%|███████▉  | 407/512 [01:18<00:20,  5.21it/s, v_num=11]Training loss: 1.2323343753814697\n",
      "Epoch 4:  80%|███████▉  | 408/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.0894334316253662\n",
      "Epoch 4:  80%|███████▉  | 409/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.9939225912094116\n",
      "Epoch 4:  80%|████████  | 410/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.6291958093643188\n",
      "Epoch 4:  80%|████████  | 411/512 [01:18<00:19,  5.21it/s, v_num=11]Training loss: 1.0350496768951416\n",
      "Epoch 4:  80%|████████  | 412/512 [01:19<00:19,  5.21it/s, v_num=11]Training loss: 36.89836502075195\n",
      "Epoch 4:  81%|████████  | 413/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.7050222158432007\n",
      "Epoch 4:  81%|████████  | 414/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.7601966857910156\n",
      "Epoch 4:  81%|████████  | 415/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.7273271083831787\n",
      "Epoch 4:  81%|████████▏ | 416/512 [01:19<00:18,  5.21it/s, v_num=11]Training loss: 1.0653960704803467\n",
      "Epoch 4:  81%|████████▏ | 417/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 1.2324371337890625\n",
      "Epoch 4:  82%|████████▏ | 418/512 [01:20<00:18,  5.21it/s, v_num=11]Training loss: 0.8292201161384583\n",
      "Epoch 4:  82%|████████▏ | 419/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.6377785205841064\n",
      "Epoch 4:  82%|████████▏ | 420/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.7219908237457275\n",
      "Epoch 4:  82%|████████▏ | 421/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 2.1788015365600586\n",
      "Epoch 4:  82%|████████▏ | 422/512 [01:20<00:17,  5.21it/s, v_num=11]Training loss: 1.6877549886703491\n",
      "Epoch 4:  83%|████████▎ | 423/512 [01:21<00:17,  5.21it/s, v_num=11]Training loss: 1.6351758241653442\n",
      "Epoch 4:  83%|████████▎ | 424/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.4508917331695557\n",
      "Epoch 4:  83%|████████▎ | 425/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.0230841636657715\n",
      "Epoch 4:  83%|████████▎ | 426/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.4596165418624878\n",
      "Epoch 4:  83%|████████▎ | 427/512 [01:21<00:16,  5.21it/s, v_num=11]Training loss: 1.6078294515609741\n",
      "Epoch 4:  84%|████████▎ | 428/512 [01:22<00:16,  5.21it/s, v_num=11]Training loss: 1.1444392204284668\n",
      "Epoch 4:  84%|████████▍ | 429/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.0309395790100098\n",
      "Epoch 4:  84%|████████▍ | 430/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.2653074264526367\n",
      "Epoch 4:  84%|████████▍ | 431/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.3760007619857788\n",
      "Epoch 4:  84%|████████▍ | 432/512 [01:22<00:15,  5.21it/s, v_num=11]Training loss: 1.8122912645339966\n",
      "Epoch 4:  85%|████████▍ | 433/512 [01:23<00:15,  5.21it/s, v_num=11]Training loss: 1.4304887056350708\n",
      "Epoch 4:  85%|████████▍ | 434/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.2991187572479248\n",
      "Epoch 4:  85%|████████▍ | 435/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 1.7428945302963257\n",
      "Epoch 4:  85%|████████▌ | 436/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 0.9941972494125366\n",
      "Epoch 4:  85%|████████▌ | 437/512 [01:23<00:14,  5.21it/s, v_num=11]Training loss: 2.40568208694458\n",
      "Epoch 4:  86%|████████▌ | 438/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 0.8231512904167175\n",
      "Epoch 4:  86%|████████▌ | 439/512 [01:24<00:14,  5.21it/s, v_num=11]Training loss: 1.6759216785430908\n",
      "Epoch 4:  86%|████████▌ | 440/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.5759272575378418\n",
      "Epoch 4:  86%|████████▌ | 441/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 7.932986259460449\n",
      "Epoch 4:  86%|████████▋ | 442/512 [01:24<00:13,  5.21it/s, v_num=11]Training loss: 1.2398970127105713\n",
      "Epoch 4:  87%|████████▋ | 443/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.6538089513778687\n",
      "Epoch 4:  87%|████████▋ | 444/512 [01:25<00:13,  5.21it/s, v_num=11]Training loss: 1.4343068599700928\n",
      "Epoch 4:  87%|████████▋ | 445/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.3948779106140137\n",
      "Epoch 4:  87%|████████▋ | 446/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.4116686582565308\n",
      "Epoch 4:  87%|████████▋ | 447/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.4450918436050415\n",
      "Epoch 4:  88%|████████▊ | 448/512 [01:25<00:12,  5.21it/s, v_num=11]Training loss: 1.5019710063934326\n",
      "Epoch 4:  88%|████████▊ | 449/512 [01:26<00:12,  5.21it/s, v_num=11]Training loss: 1.3263771533966064\n",
      "Epoch 4:  88%|████████▊ | 450/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.007073163986206\n",
      "Epoch 4:  88%|████████▊ | 451/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 2.07023024559021\n",
      "Epoch 4:  88%|████████▊ | 452/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.1733025312423706\n",
      "Epoch 4:  88%|████████▊ | 453/512 [01:26<00:11,  5.21it/s, v_num=11]Training loss: 1.3366190195083618\n",
      "Epoch 4:  89%|████████▊ | 454/512 [01:27<00:11,  5.21it/s, v_num=11]Training loss: 1.331666350364685\n",
      "Epoch 4:  89%|████████▉ | 455/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.5604358911514282\n",
      "Epoch 4:  89%|████████▉ | 456/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.7843708992004395\n",
      "Epoch 4:  89%|████████▉ | 457/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.5435940027236938\n",
      "Epoch 4:  89%|████████▉ | 458/512 [01:27<00:10,  5.21it/s, v_num=11]Training loss: 1.1540298461914062\n",
      "Epoch 4:  90%|████████▉ | 459/512 [01:28<00:10,  5.21it/s, v_num=11]Training loss: 1.8453316688537598\n",
      "Epoch 4:  90%|████████▉ | 460/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.5110033750534058\n",
      "Epoch 4:  90%|█████████ | 461/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 2.4892265796661377\n",
      "Epoch 4:  90%|█████████ | 462/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 0.9206592440605164\n",
      "Epoch 4:  90%|█████████ | 463/512 [01:28<00:09,  5.21it/s, v_num=11]Training loss: 1.4417271614074707\n",
      "Epoch 4:  91%|█████████ | 464/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 0.9441996216773987\n",
      "Epoch 4:  91%|█████████ | 465/512 [01:29<00:09,  5.21it/s, v_num=11]Training loss: 2.002622604370117\n",
      "Epoch 4:  91%|█████████ | 466/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.7626441717147827\n",
      "Epoch 4:  91%|█████████ | 467/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.4476168155670166\n",
      "Epoch 4:  91%|█████████▏| 468/512 [01:29<00:08,  5.21it/s, v_num=11]Training loss: 1.320805311203003\n",
      "Epoch 4:  92%|█████████▏| 469/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.396506667137146\n",
      "Epoch 4:  92%|█████████▏| 470/512 [01:30<00:08,  5.21it/s, v_num=11]Training loss: 1.6193236112594604\n",
      "Epoch 4:  92%|█████████▏| 471/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.3451833724975586\n",
      "Epoch 4:  92%|█████████▏| 472/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.1601381301879883\n",
      "Epoch 4:  92%|█████████▏| 473/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.4847893714904785\n",
      "Epoch 4:  93%|█████████▎| 474/512 [01:30<00:07,  5.21it/s, v_num=11]Training loss: 1.6179536581039429\n",
      "Epoch 4:  93%|█████████▎| 475/512 [01:31<00:07,  5.21it/s, v_num=11]Training loss: 1.456118106842041\n",
      "Epoch 4:  93%|█████████▎| 476/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.2287516593933105\n",
      "Epoch 4:  93%|█████████▎| 477/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.7239130735397339\n",
      "Epoch 4:  93%|█████████▎| 478/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.0793832540512085\n",
      "Epoch 4:  94%|█████████▎| 479/512 [01:31<00:06,  5.21it/s, v_num=11]Training loss: 1.2934287786483765\n",
      "Epoch 4:  94%|█████████▍| 480/512 [01:32<00:06,  5.21it/s, v_num=11]Training loss: 2.0530354976654053\n",
      "Epoch 4:  94%|█████████▍| 481/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.434737205505371\n",
      "Epoch 4:  94%|█████████▍| 482/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.1474695205688477\n",
      "Epoch 4:  94%|█████████▍| 483/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.848225712776184\n",
      "Epoch 4:  95%|█████████▍| 484/512 [01:32<00:05,  5.21it/s, v_num=11]Training loss: 1.3665589094161987\n",
      "Epoch 4:  95%|█████████▍| 485/512 [01:33<00:05,  5.21it/s, v_num=11]Training loss: 2.0395867824554443\n",
      "Epoch 4:  95%|█████████▍| 486/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 2.0692269802093506\n",
      "Epoch 4:  95%|█████████▌| 487/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.6892460584640503\n",
      "Epoch 4:  95%|█████████▌| 488/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.0208604335784912\n",
      "Epoch 4:  96%|█████████▌| 489/512 [01:33<00:04,  5.21it/s, v_num=11]Training loss: 1.1920795440673828\n",
      "Epoch 4:  96%|█████████▌| 490/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 1.4212322235107422\n",
      "Epoch 4:  96%|█████████▌| 491/512 [01:34<00:04,  5.21it/s, v_num=11]Training loss: 2.159740686416626\n",
      "Epoch 4:  96%|█████████▌| 492/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.3674813508987427\n",
      "Epoch 4:  96%|█████████▋| 493/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.5555890798568726\n",
      "Epoch 4:  96%|█████████▋| 494/512 [01:34<00:03,  5.21it/s, v_num=11]Training loss: 1.3930437564849854\n",
      "Epoch 4:  97%|█████████▋| 495/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.3865424394607544\n",
      "Epoch 4:  97%|█████████▋| 496/512 [01:35<00:03,  5.21it/s, v_num=11]Training loss: 1.1278431415557861\n",
      "Epoch 4:  97%|█████████▋| 497/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.8094024658203125\n",
      "Epoch 4:  97%|█████████▋| 498/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.271787166595459\n",
      "Epoch 4:  97%|█████████▋| 499/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.2528272867202759\n",
      "Epoch 4:  98%|█████████▊| 500/512 [01:35<00:02,  5.21it/s, v_num=11]Training loss: 1.5592461824417114\n",
      "Epoch 4:  98%|█████████▊| 501/512 [01:36<00:02,  5.21it/s, v_num=11]Training loss: 1.8820158243179321\n",
      "Epoch 4:  98%|█████████▊| 502/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.3766144514083862\n",
      "Epoch 4:  98%|█████████▊| 503/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 0.9656171798706055\n",
      "Epoch 4:  98%|█████████▊| 504/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.1018787622451782\n",
      "Epoch 4:  99%|█████████▊| 505/512 [01:36<00:01,  5.21it/s, v_num=11]Training loss: 1.4606672525405884\n",
      "Epoch 4:  99%|█████████▉| 506/512 [01:37<00:01,  5.21it/s, v_num=11]Training loss: 1.372949481010437\n",
      "Epoch 4:  99%|█████████▉| 507/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 2.3553104400634766\n",
      "Epoch 4:  99%|█████████▉| 508/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.287920355796814\n",
      "Epoch 4:  99%|█████████▉| 509/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.2083344459533691\n",
      "Epoch 4: 100%|█████████▉| 510/512 [01:37<00:00,  5.21it/s, v_num=11]Training loss: 1.6950472593307495\n",
      "Epoch 4: 100%|█████████▉| 511/512 [01:38<00:00,  5.21it/s, v_num=11]Training loss: 2.197636365890503\n",
      "Epoch 4: 100%|██████████| 512/512 [01:38<00:00,  5.21it/s, v_num=11]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 2.822610378265381\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:02, 39.94it/s]\u001b[AValidation loss: 1.4815256595611572\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:03, 32.46it/s]\u001b[AValidation loss: 1.7121343612670898\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:03, 30.51it/s]\u001b[AValidation loss: 1.4328348636627197\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:03, 29.54it/s]\u001b[AValidation loss: 1.288880705833435\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:03, 28.98it/s]\u001b[AValidation loss: 1.9621036052703857\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:03, 28.60it/s]\u001b[AValidation loss: 2.098135232925415\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:03, 28.87it/s]\u001b[AValidation loss: 1.4337117671966553\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:03, 29.31it/s]\u001b[AValidation loss: 2.1223056316375732\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:03, 29.51it/s]\u001b[AValidation loss: 1.4447803497314453\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:03, 29.71it/s]\u001b[AValidation loss: 1.4376466274261475\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:03, 29.85it/s]\u001b[AValidation loss: 1.488696813583374\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:03, 30.08it/s]\u001b[AValidation loss: 1.6614930629730225\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:03, 30.15it/s]\u001b[AValidation loss: 1.398261547088623\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:03, 30.22it/s]\u001b[AValidation loss: 1.4940237998962402\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:03, 30.29it/s]\u001b[AValidation loss: 1.2610336542129517\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:03, 30.35it/s]\u001b[AValidation loss: 1.6013492345809937\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:03, 30.40it/s]\u001b[AValidation loss: 1.4671673774719238\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:03, 30.45it/s]\u001b[AValidation loss: 1.5517256259918213\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:02, 30.50it/s]\u001b[AValidation loss: 1.8225882053375244\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:02, 30.48it/s]\u001b[AValidation loss: 1.7847414016723633\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:02, 30.35it/s]\u001b[AValidation loss: 1.445317268371582\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:02, 30.37it/s]\u001b[AValidation loss: 1.695799469947815\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:00<00:02, 30.47it/s]\u001b[AValidation loss: 1.7701425552368164\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:00<00:02, 30.50it/s]\u001b[AValidation loss: 1.4381601810455322\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:00<00:02, 30.59it/s]\u001b[AValidation loss: 1.8505101203918457\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:00<00:02, 30.62it/s]\u001b[AValidation loss: 1.3167657852172852\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:00<00:02, 30.69it/s]\u001b[AValidation loss: 1.985132098197937\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:00<00:02, 30.72it/s]\u001b[AValidation loss: 1.9720771312713623\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:00<00:02, 30.79it/s]\u001b[AValidation loss: 2.322481870651245\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:00<00:02, 30.80it/s]\u001b[AValidation loss: 1.1787512302398682\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:02, 30.84it/s]\u001b[AValidation loss: 1.7060439586639404\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:02, 30.85it/s]\u001b[AValidation loss: 2.0791573524475098\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:02, 30.89it/s]\u001b[AValidation loss: 9.845436096191406\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:02, 30.90it/s]\u001b[AValidation loss: 1.5744447708129883\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:02, 30.94it/s]\u001b[AValidation loss: 1.074747085571289\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:02, 30.95it/s]\u001b[AValidation loss: 1.900272250175476\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:02, 30.99it/s]\u001b[AValidation loss: 1.637812614440918\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:02, 31.00it/s]\u001b[AValidation loss: 1.1257820129394531\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:02, 31.04it/s]\u001b[AValidation loss: 1.796345829963684\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:02, 31.03it/s]\u001b[AValidation loss: 1.6208947896957397\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:02, 31.06it/s]\u001b[AValidation loss: 1.5627682209014893\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:02, 31.06it/s]\u001b[AValidation loss: 1.2786242961883545\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 31.09it/s]\u001b[AValidation loss: 2.2815608978271484\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 31.09it/s]\u001b[AValidation loss: 1.850414514541626\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:01<00:02, 31.12it/s]\u001b[AValidation loss: 1.5623650550842285\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:01<00:02, 31.13it/s]\u001b[AValidation loss: 1.7057199478149414\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:01<00:02, 31.15it/s]\u001b[AValidation loss: 2.0722341537475586\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:01<00:01, 31.16it/s]\u001b[AValidation loss: 2.772927761077881\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:01<00:01, 31.18it/s]\u001b[AValidation loss: 1.592661738395691\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:01<00:01, 31.19it/s]\u001b[AValidation loss: 1.4137636423110962\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:01<00:01, 31.21it/s]\u001b[AValidation loss: 1.298596739768982\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:01<00:01, 31.22it/s]\u001b[AValidation loss: 1.5655028820037842\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:01<00:01, 31.24it/s]\u001b[AValidation loss: 1.8371760845184326\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:01<00:01, 31.24it/s]\u001b[AValidation loss: 1.5812537670135498\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:01<00:01, 31.26it/s]\u001b[AValidation loss: 1.8722654581069946\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:01<00:01, 31.22it/s]\u001b[AValidation loss: 1.9728195667266846\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:01<00:01, 31.24it/s]\u001b[AValidation loss: 1.4787704944610596\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:01<00:01, 31.25it/s]\u001b[AValidation loss: 1.508741021156311\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:01<00:01, 31.26it/s]\u001b[AValidation loss: 1.7084394693374634\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:01<00:01, 31.27it/s]\u001b[AValidation loss: 1.520796537399292\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:01<00:01, 31.29it/s]\u001b[AValidation loss: 1.857377290725708\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:01<00:01, 31.29it/s]\u001b[AValidation loss: 1.4553771018981934\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:01, 31.31it/s]\u001b[AValidation loss: 1.6850882768630981\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:01, 31.31it/s]\u001b[AValidation loss: 1.4081320762634277\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:01, 31.33it/s]\u001b[AValidation loss: 2.48905611038208\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:02<00:01, 31.33it/s]\u001b[AValidation loss: 1.192688226699829\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:02<00:01, 31.34it/s]\u001b[AValidation loss: 1.520437479019165\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:02<00:01, 31.35it/s]\u001b[AValidation loss: 2.2839972972869873\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:02<00:01, 31.36it/s]\u001b[AValidation loss: 1.684621810913086\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:02<00:01, 31.37it/s]\u001b[AValidation loss: 1.3302849531173706\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:02<00:01, 31.38it/s]\u001b[AValidation loss: 1.85619056224823\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:02<00:01, 31.38it/s]\u001b[AValidation loss: 1.984991431236267\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:02<00:01, 31.40it/s]\u001b[AValidation loss: 1.914603352546692\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:02<00:01, 31.40it/s]\u001b[AValidation loss: 2.4567134380340576\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:02<00:01, 31.41it/s]\u001b[AValidation loss: 2.1565237045288086\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:02<00:01, 31.40it/s]\u001b[AValidation loss: 1.6952756643295288\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:02<00:01, 31.41it/s]\u001b[AValidation loss: 1.7980446815490723\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:02<00:01, 31.41it/s]\u001b[AValidation loss: 1.6154115200042725\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:02<00:00, 31.42it/s]\u001b[AValidation loss: 1.5698356628417969\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:02<00:00, 31.42it/s]\u001b[AValidation loss: 1.5116379261016846\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:02<00:00, 31.42it/s]\u001b[AValidation loss: 1.7213513851165771\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:02<00:00, 31.42it/s]\u001b[AValidation loss: 1.364793300628662\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:02<00:00, 31.43it/s]\u001b[AValidation loss: 1.8014037609100342\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:02<00:00, 31.43it/s]\u001b[AValidation loss: 1.778775691986084\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:02<00:00, 31.44it/s]\u001b[AValidation loss: 1.6589586734771729\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:02<00:00, 31.44it/s]\u001b[AValidation loss: 1.6149773597717285\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:02<00:00, 31.45it/s]\u001b[AValidation loss: 1.813576579093933\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:02<00:00, 31.45it/s]\u001b[AValidation loss: 1.5990325212478638\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:02<00:00, 31.46it/s]\u001b[AValidation loss: 2.8444085121154785\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:02<00:00, 31.46it/s]\u001b[AValidation loss: 1.6544089317321777\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:02<00:00, 31.47it/s]\u001b[AValidation loss: 1.756354570388794\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:02<00:00, 31.47it/s]\u001b[AValidation loss: 3.3639140129089355\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:02<00:00, 31.48it/s]\u001b[AValidation loss: 1.5446845293045044\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:02<00:00, 31.48it/s]\u001b[AValidation loss: 1.9514405727386475\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:03<00:00, 31.49it/s]\u001b[AValidation loss: 1.2611098289489746\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:03<00:00, 31.49it/s]\u001b[AValidation loss: 1.4627416133880615\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:03<00:00, 31.50it/s]\u001b[AValidation loss: 1.763070821762085\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:03<00:00, 31.50it/s]\u001b[AValidation loss: 1.564574122428894\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:03<00:00, 31.51it/s]\u001b[AValidation loss: 1.2990225553512573\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:03<00:00, 31.51it/s]\u001b[AValidation loss: 2.101577043533325\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:03<00:00, 31.51it/s]\u001b[AValidation loss: 1.5274534225463867\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:03<00:00, 31.51it/s]\u001b[AValidation loss: 1.21946382522583\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 1.844498634338379\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 1.6804451942443848\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 2.0759096145629883\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 1.7665228843688965\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 1.5626587867736816\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 1.885933518409729\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:03<00:00, 31.52it/s]\u001b[AValidation loss: 2.703371524810791\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:03<00:00, 31.57it/s]\u001b[A\n",
      "Epoch 5:   0%|          | 0/512 [00:00<?, ?it/s, v_num=11]                \u001b[ATraining loss: 1.9692449569702148\n",
      "Epoch 5:   0%|          | 1/512 [00:00<00:24, 20.55it/s, v_num=11]Training loss: 1.396451473236084\n",
      "Epoch 5:   0%|          | 2/512 [00:00<01:01,  8.32it/s, v_num=11]Training loss: 1.0623862743377686\n",
      "Epoch 5:   1%|          | 3/512 [00:00<01:13,  6.94it/s, v_num=11]Training loss: 1.2138155698776245\n",
      "Epoch 5:   1%|          | 4/512 [00:00<01:19,  6.40it/s, v_num=11]Training loss: 1.5110690593719482\n",
      "Epoch 5:   1%|          | 5/512 [00:00<01:22,  6.12it/s, v_num=11]Training loss: 1.7310776710510254\n",
      "Epoch 5:   1%|          | 6/512 [00:01<01:25,  5.95it/s, v_num=11]Training loss: 1.2301466464996338\n",
      "Epoch 5:   1%|▏         | 7/512 [00:01<01:26,  5.82it/s, v_num=11]Training loss: 1.0239685773849487\n",
      "Epoch 5:   2%|▏         | 8/512 [00:01<01:27,  5.74it/s, v_num=11]Training loss: 1.4261599779129028\n",
      "Epoch 5:   2%|▏         | 9/512 [00:01<01:28,  5.67it/s, v_num=11]Training loss: 1.0858819484710693\n",
      "Epoch 5:   2%|▏         | 10/512 [00:01<01:29,  5.62it/s, v_num=11]Training loss: 1.5963106155395508\n",
      "Epoch 5:   2%|▏         | 11/512 [00:01<01:29,  5.58it/s, v_num=11]Training loss: 1.3118300437927246\n",
      "Epoch 5:   2%|▏         | 12/512 [00:02<01:30,  5.55it/s, v_num=11]Training loss: 0.9531137943267822\n",
      "Epoch 5:   3%|▎         | 13/512 [00:02<01:30,  5.52it/s, v_num=11]Training loss: 1.5378204584121704\n",
      "Epoch 5:   3%|▎         | 14/512 [00:02<01:30,  5.50it/s, v_num=11]Training loss: 1.0613491535186768\n",
      "Epoch 5:   3%|▎         | 15/512 [00:02<01:30,  5.48it/s, v_num=11]Training loss: 1.7596261501312256\n",
      "Epoch 5:   3%|▎         | 16/512 [00:02<01:30,  5.46it/s, v_num=11]Training loss: 1.4377696514129639\n",
      "Epoch 5:   3%|▎         | 17/512 [00:03<01:30,  5.44it/s, v_num=11]Training loss: 1.3404085636138916\n",
      "Epoch 5:   4%|▎         | 18/512 [00:03<01:30,  5.43it/s, v_num=11]Training loss: 1.1955173015594482\n",
      "Epoch 5:   4%|▎         | 19/512 [00:03<01:30,  5.42it/s, v_num=11]Training loss: 1.7583367824554443\n",
      "Epoch 5:   4%|▍         | 20/512 [00:03<01:30,  5.41it/s, v_num=11]Training loss: 1.2583587169647217\n",
      "Epoch 5:   4%|▍         | 21/512 [00:03<01:31,  5.40it/s, v_num=11]Training loss: 1.6696794033050537\n",
      "Epoch 5:   4%|▍         | 22/512 [00:04<01:30,  5.39it/s, v_num=11]Training loss: 0.7620550394058228\n",
      "Epoch 5:   4%|▍         | 23/512 [00:04<01:30,  5.38it/s, v_num=11]Training loss: 1.1398845911026\n",
      "Epoch 5:   5%|▍         | 24/512 [00:04<01:30,  5.37it/s, v_num=11]Training loss: 1.5662205219268799\n",
      "Epoch 5:   5%|▍         | 25/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 1.067349910736084\n",
      "Epoch 5:   5%|▌         | 26/512 [00:04<01:30,  5.36it/s, v_num=11]Training loss: 1.2889587879180908\n",
      "Epoch 5:   5%|▌         | 27/512 [00:05<01:30,  5.35it/s, v_num=11]Training loss: 1.1989006996154785\n",
      "Epoch 5:   5%|▌         | 28/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.4555175304412842\n",
      "Epoch 5:   6%|▌         | 29/512 [00:05<01:30,  5.34it/s, v_num=11]Training loss: 1.7501046657562256\n",
      "Epoch 5:   6%|▌         | 30/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 1.0622495412826538\n",
      "Epoch 5:   6%|▌         | 31/512 [00:05<01:30,  5.33it/s, v_num=11]Training loss: 2.9482007026672363\n",
      "Epoch 5:   6%|▋         | 32/512 [00:06<01:30,  5.33it/s, v_num=11]Training loss: 1.2861562967300415\n",
      "Epoch 5:   6%|▋         | 33/512 [00:06<01:30,  5.32it/s, v_num=11]Training loss: 1.2767188549041748\n",
      "Epoch 5:   7%|▋         | 34/512 [00:06<01:29,  5.32it/s, v_num=11]Training loss: 0.8330169916152954\n",
      "Epoch 5:   7%|▋         | 35/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.790968656539917\n",
      "Epoch 5:   7%|▋         | 36/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.2551045417785645\n",
      "Epoch 5:   7%|▋         | 37/512 [00:06<01:29,  5.31it/s, v_num=11]Training loss: 1.6584101915359497\n",
      "Epoch 5:   7%|▋         | 38/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.1959946155548096\n",
      "Epoch 5:   8%|▊         | 39/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.6605634689331055\n",
      "Epoch 5:   8%|▊         | 40/512 [00:07<01:29,  5.30it/s, v_num=11]Training loss: 1.002733588218689\n",
      "Epoch 5:   8%|▊         | 41/512 [00:07<01:28,  5.30it/s, v_num=11]Training loss: 1.804264783859253\n",
      "Epoch 5:   8%|▊         | 42/512 [00:07<01:28,  5.29it/s, v_num=11]Training loss: 1.521147608757019\n",
      "Epoch 5:   8%|▊         | 43/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.2897076606750488\n",
      "Epoch 5:   9%|▊         | 44/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.3473211526870728\n",
      "Epoch 5:   9%|▉         | 45/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.3054139614105225\n",
      "Epoch 5:   9%|▉         | 46/512 [00:08<01:28,  5.29it/s, v_num=11]Training loss: 1.3649438619613647\n",
      "Epoch 5:   9%|▉         | 47/512 [00:08<01:28,  5.28it/s, v_num=11]Training loss: 2.5129895210266113\n",
      "Epoch 5:   9%|▉         | 48/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.416945457458496\n",
      "Epoch 5:  10%|▉         | 49/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.297936201095581\n",
      "Epoch 5:  10%|▉         | 50/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.2651890516281128\n",
      "Epoch 5:  10%|▉         | 51/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.6718486547470093\n",
      "Epoch 5:  10%|█         | 52/512 [00:09<01:27,  5.28it/s, v_num=11]Training loss: 1.0098942518234253\n",
      "Epoch 5:  10%|█         | 53/512 [00:10<01:27,  5.27it/s, v_num=11]Training loss: 1.1256309747695923\n",
      "Epoch 5:  11%|█         | 54/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.1664390563964844\n",
      "Epoch 5:  11%|█         | 55/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.189786434173584\n",
      "Epoch 5:  11%|█         | 56/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.4593127965927124\n",
      "Epoch 5:  11%|█         | 57/512 [00:10<01:26,  5.27it/s, v_num=11]Training loss: 1.3686379194259644\n",
      "Epoch 5:  11%|█▏        | 58/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 2.5815606117248535\n",
      "Epoch 5:  12%|█▏        | 59/512 [00:11<01:26,  5.27it/s, v_num=11]Training loss: 1.479247808456421\n",
      "Epoch 5:  12%|█▏        | 60/512 [00:11<01:25,  5.27it/s, v_num=11]Training loss: 1.4491562843322754\n",
      "Epoch 5:  12%|█▏        | 61/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.2697880268096924\n",
      "Epoch 5:  12%|█▏        | 62/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.4449923038482666\n",
      "Epoch 5:  12%|█▏        | 63/512 [00:11<01:25,  5.26it/s, v_num=11]Training loss: 1.494052767753601\n",
      "Epoch 5:  12%|█▎        | 64/512 [00:12<01:25,  5.26it/s, v_num=11]Training loss: 1.4069570302963257\n",
      "Epoch 5:  13%|█▎        | 65/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.0965973138809204\n",
      "Epoch 5:  13%|█▎        | 66/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.7738710641860962\n",
      "Epoch 5:  13%|█▎        | 67/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.3319475650787354\n",
      "Epoch 5:  13%|█▎        | 68/512 [00:12<01:24,  5.26it/s, v_num=11]Training loss: 1.4203320741653442\n",
      "Epoch 5:  13%|█▎        | 69/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 1.2143577337265015\n",
      "Epoch 5:  14%|█▎        | 70/512 [00:13<01:24,  5.26it/s, v_num=11]Training loss: 1.3440337181091309\n",
      "Epoch 5:  14%|█▍        | 71/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 0.9017904996871948\n",
      "Epoch 5:  14%|█▍        | 72/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.1031782627105713\n",
      "Epoch 5:  14%|█▍        | 73/512 [00:13<01:23,  5.25it/s, v_num=11]Training loss: 1.3038954734802246\n",
      "Epoch 5:  14%|█▍        | 74/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.3307089805603027\n",
      "Epoch 5:  15%|█▍        | 75/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.9456316232681274\n",
      "Epoch 5:  15%|█▍        | 76/512 [00:14<01:23,  5.25it/s, v_num=11]Training loss: 1.5681710243225098\n",
      "Epoch 5:  15%|█▌        | 77/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.0544933080673218\n",
      "Epoch 5:  15%|█▌        | 78/512 [00:14<01:22,  5.25it/s, v_num=11]Training loss: 1.4419797658920288\n",
      "Epoch 5:  15%|█▌        | 79/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.8568285703659058\n",
      "Epoch 5:  16%|█▌        | 80/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.6301597356796265\n",
      "Epoch 5:  16%|█▌        | 81/512 [00:15<01:22,  5.25it/s, v_num=11]Training loss: 1.2484089136123657\n",
      "Epoch 5:  16%|█▌        | 82/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 2.3296964168548584\n",
      "Epoch 5:  16%|█▌        | 83/512 [00:15<01:21,  5.25it/s, v_num=11]Training loss: 1.0060755014419556\n",
      "Epoch 5:  16%|█▋        | 84/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.2025508880615234\n",
      "Epoch 5:  17%|█▋        | 85/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.2306761741638184\n",
      "Epoch 5:  17%|█▋        | 86/512 [00:16<01:21,  5.25it/s, v_num=11]Training loss: 1.9279415607452393\n",
      "Epoch 5:  17%|█▋        | 87/512 [00:16<01:21,  5.24it/s, v_num=11]Training loss: 1.3699604272842407\n",
      "Epoch 5:  17%|█▋        | 88/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 1.6902058124542236\n",
      "Epoch 5:  17%|█▋        | 89/512 [00:16<01:20,  5.24it/s, v_num=11]Training loss: 2.042602062225342\n",
      "Epoch 5:  18%|█▊        | 90/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.405961275100708\n",
      "Epoch 5:  18%|█▊        | 91/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.5440317392349243\n",
      "Epoch 5:  18%|█▊        | 92/512 [00:17<01:20,  5.24it/s, v_num=11]Training loss: 1.1812238693237305\n",
      "Epoch 5:  18%|█▊        | 93/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 2.1669185161590576\n",
      "Epoch 5:  18%|█▊        | 94/512 [00:17<01:19,  5.24it/s, v_num=11]Training loss: 1.2899105548858643\n",
      "Epoch 5:  19%|█▊        | 95/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.1494395732879639\n",
      "Epoch 5:  19%|█▉        | 96/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 4.325427532196045\n",
      "Epoch 5:  19%|█▉        | 97/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.1957927942276\n",
      "Epoch 5:  19%|█▉        | 98/512 [00:18<01:19,  5.24it/s, v_num=11]Training loss: 1.3777728080749512\n",
      "Epoch 5:  19%|█▉        | 99/512 [00:18<01:18,  5.24it/s, v_num=11]Training loss: 1.4556405544281006\n",
      "Epoch 5:  20%|█▉        | 100/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.8552526235580444\n",
      "Epoch 5:  20%|█▉        | 101/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.6175062656402588\n",
      "Epoch 5:  20%|█▉        | 102/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.5724214315414429\n",
      "Epoch 5:  20%|██        | 103/512 [00:19<01:18,  5.24it/s, v_num=11]Training loss: 1.5572283267974854\n",
      "Epoch 5:  20%|██        | 104/512 [00:19<01:17,  5.24it/s, v_num=11]Training loss: 1.434960126876831\n",
      "Epoch 5:  21%|██        | 105/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.0504964590072632\n",
      "Epoch 5:  21%|██        | 106/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.5213737487792969\n",
      "Epoch 5:  21%|██        | 107/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.011971116065979\n",
      "Epoch 5:  21%|██        | 108/512 [00:20<01:17,  5.24it/s, v_num=11]Training loss: 1.1596907377243042\n",
      "Epoch 5:  21%|██▏       | 109/512 [00:20<01:16,  5.24it/s, v_num=11]Training loss: 1.9394550323486328\n",
      "Epoch 5:  21%|██▏       | 110/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.1303678750991821\n",
      "Epoch 5:  22%|██▏       | 111/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.2272484302520752\n",
      "Epoch 5:  22%|██▏       | 112/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.3067519664764404\n",
      "Epoch 5:  22%|██▏       | 113/512 [00:21<01:16,  5.24it/s, v_num=11]Training loss: 1.4069632291793823\n",
      "Epoch 5:  22%|██▏       | 114/512 [00:21<01:16,  5.23it/s, v_num=11]Training loss: 1.029619812965393\n",
      "Epoch 5:  22%|██▏       | 115/512 [00:21<01:15,  5.23it/s, v_num=11]Training loss: 1.9346762895584106\n",
      "Epoch 5:  23%|██▎       | 116/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.9643617868423462\n",
      "Epoch 5:  23%|██▎       | 117/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.380042314529419\n",
      "Epoch 5:  23%|██▎       | 118/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.3168271780014038\n",
      "Epoch 5:  23%|██▎       | 119/512 [00:22<01:15,  5.23it/s, v_num=11]Training loss: 1.1973507404327393\n",
      "Epoch 5:  23%|██▎       | 120/512 [00:22<01:14,  5.23it/s, v_num=11]Training loss: 0.9576213359832764\n",
      "Epoch 5:  24%|██▎       | 121/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.5952085256576538\n",
      "Epoch 5:  24%|██▍       | 122/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.1766268014907837\n",
      "Epoch 5:  24%|██▍       | 123/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.4519513845443726\n",
      "Epoch 5:  24%|██▍       | 124/512 [00:23<01:14,  5.23it/s, v_num=11]Training loss: 1.2713732719421387\n",
      "Epoch 5:  24%|██▍       | 125/512 [00:23<01:13,  5.23it/s, v_num=11]Training loss: 1.0174572467803955\n",
      "Epoch 5:  25%|██▍       | 126/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.4530588388442993\n",
      "Epoch 5:  25%|██▍       | 127/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.0916595458984375\n",
      "Epoch 5:  25%|██▌       | 128/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.674456238746643\n",
      "Epoch 5:  25%|██▌       | 129/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.3357698917388916\n",
      "Epoch 5:  25%|██▌       | 130/512 [00:24<01:13,  5.23it/s, v_num=11]Training loss: 1.6224030256271362\n",
      "Epoch 5:  26%|██▌       | 131/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 0.9853953123092651\n",
      "Epoch 5:  26%|██▌       | 132/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 2.4292564392089844\n",
      "Epoch 5:  26%|██▌       | 133/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.4000986814498901\n",
      "Epoch 5:  26%|██▌       | 134/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 1.5955928564071655\n",
      "Epoch 5:  26%|██▋       | 135/512 [00:25<01:12,  5.23it/s, v_num=11]Training loss: 2.348374843597412\n",
      "Epoch 5:  27%|██▋       | 136/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.7781260013580322\n",
      "Epoch 5:  27%|██▋       | 137/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 2.0604629516601562\n",
      "Epoch 5:  27%|██▋       | 138/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.1996676921844482\n",
      "Epoch 5:  27%|██▋       | 139/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.214232325553894\n",
      "Epoch 5:  27%|██▋       | 140/512 [00:26<01:11,  5.23it/s, v_num=11]Training loss: 1.6466495990753174\n",
      "Epoch 5:  28%|██▊       | 141/512 [00:26<01:10,  5.23it/s, v_num=11]Training loss: 1.6086108684539795\n",
      "Epoch 5:  28%|██▊       | 142/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.5353000164031982\n",
      "Epoch 5:  28%|██▊       | 143/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.3563406467437744\n",
      "Epoch 5:  28%|██▊       | 144/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.5106511116027832\n",
      "Epoch 5:  28%|██▊       | 145/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.3912153244018555\n",
      "Epoch 5:  29%|██▊       | 146/512 [00:27<01:10,  5.23it/s, v_num=11]Training loss: 1.2437713146209717\n",
      "Epoch 5:  29%|██▊       | 147/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.3357584476470947\n",
      "Epoch 5:  29%|██▉       | 148/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.7872213125228882\n",
      "Epoch 5:  29%|██▉       | 149/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.3689241409301758\n",
      "Epoch 5:  29%|██▉       | 150/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.5756889581680298\n",
      "Epoch 5:  29%|██▉       | 151/512 [00:28<01:09,  5.23it/s, v_num=11]Training loss: 1.1441112756729126\n",
      "Epoch 5:  30%|██▉       | 152/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.5743263959884644\n",
      "Epoch 5:  30%|██▉       | 153/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.633117437362671\n",
      "Epoch 5:  30%|███       | 154/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 0.9695011973381042\n",
      "Epoch 5:  30%|███       | 155/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.0254385471343994\n",
      "Epoch 5:  30%|███       | 156/512 [00:29<01:08,  5.23it/s, v_num=11]Training loss: 1.7142984867095947\n",
      "Epoch 5:  31%|███       | 157/512 [00:30<01:07,  5.23it/s, v_num=11]Training loss: 1.2178263664245605\n",
      "Epoch 5:  31%|███       | 158/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.4569807052612305\n",
      "Epoch 5:  31%|███       | 159/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.9203095436096191\n",
      "Epoch 5:  31%|███▏      | 160/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 2.079848527908325\n",
      "Epoch 5:  31%|███▏      | 161/512 [00:30<01:07,  5.22it/s, v_num=11]Training loss: 1.356900930404663\n",
      "Epoch 5:  32%|███▏      | 162/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.2081514596939087\n",
      "Epoch 5:  32%|███▏      | 163/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.6925185918807983\n",
      "Epoch 5:  32%|███▏      | 164/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.521471619606018\n",
      "Epoch 5:  32%|███▏      | 165/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.596360683441162\n",
      "Epoch 5:  32%|███▏      | 166/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 1.0436646938323975\n",
      "Epoch 5:  33%|███▎      | 167/512 [00:31<01:06,  5.22it/s, v_num=11]Training loss: 0.8932151198387146\n",
      "Epoch 5:  33%|███▎      | 168/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 0.9470208287239075\n",
      "Epoch 5:  33%|███▎      | 169/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.2899724245071411\n",
      "Epoch 5:  33%|███▎      | 170/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.9783835411071777\n",
      "Epoch 5:  33%|███▎      | 171/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.4059655666351318\n",
      "Epoch 5:  34%|███▎      | 172/512 [00:32<01:05,  5.22it/s, v_num=11]Training loss: 1.6263072490692139\n",
      "Epoch 5:  34%|███▍      | 173/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.357710838317871\n",
      "Epoch 5:  34%|███▍      | 174/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.31538987159729\n",
      "Epoch 5:  34%|███▍      | 175/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.9190163612365723\n",
      "Epoch 5:  34%|███▍      | 176/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 2.1383652687072754\n",
      "Epoch 5:  35%|███▍      | 177/512 [00:33<01:04,  5.22it/s, v_num=11]Training loss: 1.3587865829467773\n",
      "Epoch 5:  35%|███▍      | 178/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 0.7574059963226318\n",
      "Epoch 5:  35%|███▍      | 179/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 0.9128300547599792\n",
      "Epoch 5:  35%|███▌      | 180/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 2.774662494659424\n",
      "Epoch 5:  35%|███▌      | 181/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.2800177335739136\n",
      "Epoch 5:  36%|███▌      | 182/512 [00:34<01:03,  5.22it/s, v_num=11]Training loss: 1.5373731851577759\n",
      "Epoch 5:  36%|███▌      | 183/512 [00:35<01:03,  5.22it/s, v_num=11]Training loss: 1.6199063062667847\n",
      "Epoch 5:  36%|███▌      | 184/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 0.9722965359687805\n",
      "Epoch 5:  36%|███▌      | 185/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.3744251728057861\n",
      "Epoch 5:  36%|███▋      | 186/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.7971093654632568\n",
      "Epoch 5:  37%|███▋      | 187/512 [00:35<01:02,  5.22it/s, v_num=11]Training loss: 1.2871973514556885\n",
      "Epoch 5:  37%|███▋      | 188/512 [00:36<01:02,  5.22it/s, v_num=11]Training loss: 1.5854909420013428\n",
      "Epoch 5:  37%|███▋      | 189/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.196178913116455\n",
      "Epoch 5:  37%|███▋      | 190/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.8516855239868164\n",
      "Epoch 5:  37%|███▋      | 191/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 0.8409931063652039\n",
      "Epoch 5:  38%|███▊      | 192/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 2.520660877227783\n",
      "Epoch 5:  38%|███▊      | 193/512 [00:36<01:01,  5.22it/s, v_num=11]Training loss: 1.1607528924942017\n",
      "Epoch 5:  38%|███▊      | 194/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.9946789741516113\n",
      "Epoch 5:  38%|███▊      | 195/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.5300769805908203\n",
      "Epoch 5:  38%|███▊      | 196/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.0579965114593506\n",
      "Epoch 5:  38%|███▊      | 197/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.5293484926223755\n",
      "Epoch 5:  39%|███▊      | 198/512 [00:37<01:00,  5.22it/s, v_num=11]Training loss: 1.076674222946167\n",
      "Epoch 5:  39%|███▉      | 199/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.2444862127304077\n",
      "Epoch 5:  39%|███▉      | 200/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.0579476356506348\n",
      "Epoch 5:  39%|███▉      | 201/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.5322202444076538\n",
      "Epoch 5:  39%|███▉      | 202/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.5194369554519653\n",
      "Epoch 5:  40%|███▉      | 203/512 [00:38<00:59,  5.22it/s, v_num=11]Training loss: 1.5653793811798096\n",
      "Epoch 5:  40%|███▉      | 204/512 [00:39<00:59,  5.22it/s, v_num=11]Training loss: 1.1565454006195068\n",
      "Epoch 5:  40%|████      | 205/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.020634412765503\n",
      "Epoch 5:  40%|████      | 206/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.5823854207992554\n",
      "Epoch 5:  40%|████      | 207/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 1.8815138339996338\n",
      "Epoch 5:  41%|████      | 208/512 [00:39<00:58,  5.22it/s, v_num=11]Training loss: 2.171459913253784\n",
      "Epoch 5:  41%|████      | 209/512 [00:40<00:58,  5.22it/s, v_num=11]Training loss: 1.452735424041748\n",
      "Epoch 5:  41%|████      | 210/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.201110601425171\n",
      "Epoch 5:  41%|████      | 211/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.5302224159240723\n",
      "Epoch 5:  41%|████▏     | 212/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.9132916927337646\n",
      "Epoch 5:  42%|████▏     | 213/512 [00:40<00:57,  5.22it/s, v_num=11]Training loss: 1.3947982788085938\n",
      "Epoch 5:  42%|████▏     | 214/512 [00:41<00:57,  5.22it/s, v_num=11]Training loss: 0.9808366298675537\n",
      "Epoch 5:  42%|████▏     | 215/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.6853848695755005\n",
      "Epoch 5:  42%|████▏     | 216/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.4248828887939453\n",
      "Epoch 5:  42%|████▏     | 217/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.5371520519256592\n",
      "Epoch 5:  43%|████▎     | 218/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.609697699546814\n",
      "Epoch 5:  43%|████▎     | 219/512 [00:41<00:56,  5.22it/s, v_num=11]Training loss: 1.9136779308319092\n",
      "Epoch 5:  43%|████▎     | 220/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 0.9970705509185791\n",
      "Epoch 5:  43%|████▎     | 221/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.976645827293396\n",
      "Epoch 5:  43%|████▎     | 222/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.7049163579940796\n",
      "Epoch 5:  44%|████▎     | 223/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 1.0583382844924927\n",
      "Epoch 5:  44%|████▍     | 224/512 [00:42<00:55,  5.22it/s, v_num=11]Training loss: 0.9507865309715271\n",
      "Epoch 5:  44%|████▍     | 225/512 [00:43<00:55,  5.22it/s, v_num=11]Training loss: 1.1739790439605713\n",
      "Epoch 5:  44%|████▍     | 226/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.9461064338684082\n",
      "Epoch 5:  44%|████▍     | 227/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.1312936544418335\n",
      "Epoch 5:  45%|████▍     | 228/512 [00:43<00:54,  5.22it/s, v_num=11]Training loss: 1.361030101776123\n",
      "Epoch 5:  45%|████▍     | 229/512 [00:43<00:54,  5.21it/s, v_num=11]Training loss: 1.7308679819107056\n",
      "Epoch 5:  45%|████▍     | 230/512 [00:44<00:54,  5.21it/s, v_num=11]Training loss: 1.4279091358184814\n",
      "Epoch 5:  45%|████▌     | 231/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 2.4327986240386963\n",
      "Epoch 5:  45%|████▌     | 232/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 1.374059796333313\n",
      "Epoch 5:  46%|████▌     | 233/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 1.1726878881454468\n",
      "Epoch 5:  46%|████▌     | 234/512 [00:44<00:53,  5.21it/s, v_num=11]Training loss: 1.6735756397247314\n",
      "Epoch 5:  46%|████▌     | 235/512 [00:45<00:53,  5.21it/s, v_num=11]Training loss: 1.581790804862976\n",
      "Epoch 5:  46%|████▌     | 236/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.4862775802612305\n",
      "Epoch 5:  46%|████▋     | 237/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.118118166923523\n",
      "Epoch 5:  46%|████▋     | 238/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.5082573890686035\n",
      "Epoch 5:  47%|████▋     | 239/512 [00:45<00:52,  5.21it/s, v_num=11]Training loss: 1.1737048625946045\n",
      "Epoch 5:  47%|████▋     | 240/512 [00:46<00:52,  5.21it/s, v_num=11]Training loss: 1.6477720737457275\n",
      "Epoch 5:  47%|████▋     | 241/512 [00:46<00:51,  5.21it/s, v_num=11]Training loss: 1.5569956302642822\n",
      "Epoch 5:  47%|████▋     | 242/512 [00:46<00:51,  5.21it/s, v_num=11]Training loss: 1.035688877105713\n",
      "Epoch 5:  47%|████▋     | 243/512 [00:46<00:51,  5.21it/s, v_num=11]Training loss: 1.4366724491119385\n",
      "Epoch 5:  48%|████▊     | 244/512 [00:46<00:51,  5.21it/s, v_num=11]Training loss: 2.009857654571533\n",
      "Epoch 5:  48%|████▊     | 245/512 [00:46<00:51,  5.21it/s, v_num=11]Training loss: 1.168748140335083\n",
      "Epoch 5:  48%|████▊     | 246/512 [00:47<00:51,  5.21it/s, v_num=11]Training loss: 1.5437530279159546\n",
      "Epoch 5:  48%|████▊     | 247/512 [00:47<00:50,  5.21it/s, v_num=11]Training loss: 1.2547045946121216\n",
      "Epoch 5:  48%|████▊     | 248/512 [00:47<00:50,  5.21it/s, v_num=11]Training loss: 0.9174351692199707\n",
      "Epoch 5:  49%|████▊     | 249/512 [00:47<00:50,  5.21it/s, v_num=11]Training loss: 1.493497610092163\n",
      "Epoch 5:  49%|████▉     | 250/512 [00:47<00:50,  5.21it/s, v_num=11]Training loss: 1.2081148624420166\n",
      "Epoch 5:  49%|████▉     | 251/512 [00:48<00:50,  5.21it/s, v_num=11]Training loss: 1.4545769691467285\n",
      "Epoch 5:  49%|████▉     | 252/512 [00:48<00:49,  5.21it/s, v_num=11]Training loss: 1.0508846044540405\n",
      "Epoch 5:  49%|████▉     | 253/512 [00:48<00:49,  5.21it/s, v_num=11]Training loss: 2.1699490547180176\n",
      "Epoch 5:  50%|████▉     | 254/512 [00:48<00:49,  5.21it/s, v_num=11]Training loss: 1.7875893115997314\n",
      "Epoch 5:  50%|████▉     | 255/512 [00:48<00:49,  5.21it/s, v_num=11]Training loss: 1.7118282318115234\n",
      "Epoch 5:  50%|█████     | 256/512 [00:49<00:49,  5.21it/s, v_num=11]Training loss: 1.4931237697601318\n",
      "Epoch 5:  50%|█████     | 257/512 [00:49<00:48,  5.21it/s, v_num=11]Training loss: 1.322657585144043\n",
      "Epoch 5:  50%|█████     | 258/512 [00:49<00:48,  5.21it/s, v_num=11]Training loss: 1.194831132888794\n",
      "Epoch 5:  51%|█████     | 259/512 [00:49<00:48,  5.21it/s, v_num=11]Training loss: 1.183585524559021\n",
      "Epoch 5:  51%|█████     | 260/512 [00:49<00:48,  5.21it/s, v_num=11]Training loss: 1.7204302549362183\n",
      "Epoch 5:  51%|█████     | 261/512 [00:50<00:48,  5.21it/s, v_num=11]Training loss: 1.693273901939392\n",
      "Epoch 5:  51%|█████     | 262/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.2418533563613892\n",
      "Epoch 5:  51%|█████▏    | 263/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.7303171157836914\n",
      "Epoch 5:  52%|█████▏    | 264/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.307053804397583\n",
      "Epoch 5:  52%|█████▏    | 265/512 [00:50<00:47,  5.21it/s, v_num=11]Training loss: 1.0442278385162354\n",
      "Epoch 5:  52%|█████▏    | 266/512 [00:51<00:47,  5.21it/s, v_num=11]Training loss: 1.6468771696090698\n",
      "Epoch 5:  52%|█████▏    | 267/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.9115309715270996\n",
      "Epoch 5:  52%|█████▏    | 268/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.713708758354187\n",
      "Epoch 5:  53%|█████▎    | 269/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.0961322784423828\n",
      "Epoch 5:  53%|█████▎    | 270/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 0.8790971040725708\n",
      "Epoch 5:  53%|█████▎    | 271/512 [00:51<00:46,  5.21it/s, v_num=11]Training loss: 1.1797693967819214\n",
      "Epoch 5:  53%|█████▎    | 272/512 [00:52<00:46,  5.21it/s, v_num=11]Training loss: 1.0542430877685547\n",
      "Epoch 5:  53%|█████▎    | 273/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 0.7765004634857178\n",
      "Epoch 5:  54%|█████▎    | 274/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 2.152282476425171\n",
      "Epoch 5:  54%|█████▎    | 275/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.4596350193023682\n",
      "Epoch 5:  54%|█████▍    | 276/512 [00:52<00:45,  5.21it/s, v_num=11]Training loss: 1.1832232475280762\n",
      "Epoch 5:  54%|█████▍    | 277/512 [00:53<00:45,  5.21it/s, v_num=11]Training loss: 1.5336799621582031\n",
      "Epoch 5:  54%|█████▍    | 278/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.7330243587493896\n",
      "Epoch 5:  54%|█████▍    | 279/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.3645086288452148\n",
      "Epoch 5:  55%|█████▍    | 280/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 1.5471256971359253\n",
      "Epoch 5:  55%|█████▍    | 281/512 [00:53<00:44,  5.21it/s, v_num=11]Training loss: 2.0366241931915283\n",
      "Epoch 5:  55%|█████▌    | 282/512 [00:54<00:44,  5.21it/s, v_num=11]Training loss: 1.7710726261138916\n",
      "Epoch 5:  55%|█████▌    | 283/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 2.0130770206451416\n",
      "Epoch 5:  55%|█████▌    | 284/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 3.107659101486206\n",
      "Epoch 5:  56%|█████▌    | 285/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 1.3146553039550781\n",
      "Epoch 5:  56%|█████▌    | 286/512 [00:54<00:43,  5.21it/s, v_num=11]Training loss: 1.190565586090088\n",
      "Epoch 5:  56%|█████▌    | 287/512 [00:55<00:43,  5.21it/s, v_num=11]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Initialize the VAE Lightning model\n",
    "input_dim = X_train_tensor.shape[1]  # The number of input features\n",
    "latent_dim = 100  # Latent dimension size, can be tuned\n",
    "model = VAE_Lightning(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=1000, lr=1e-6)\n",
    "\n",
    "# Training\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gradient_clip_val=0.5,  # Clip gradients to avoid explosion\n",
    "    callbacks=[checkpoint_callback],\n",
    "    precision=32,\n",
    "    accelerator='gpu',          # Use 'gpu' or 'cpu'\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',  # Use 1 GPU or CPU ('auto' will pick the appropriate one)\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8c82f-6050-40f5-a048-725a3a1b641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431f599-80a3-41d2-94f0-d91882af4a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb373265-2411-49c0-ab8e-b739615c35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "checkpoint_path = \"lightning_logs/version_11/checkpoints/epoch=9-step=5120.ckpt\"\n",
    "\n",
    "loaded_model = VAE_Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=1000,\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151a777-f314-4cc2-8456-aa01e7b220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_embeddings(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x,y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Replace NaNs with zero or another neutral value for forward pass\n",
    "            x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "            \n",
    "            z, _, _ = model.forward(x_filled)\n",
    "            embeddings.append(z)\n",
    "            labels.append(y)\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = get_latent_embeddings(loaded_model, train_loader)\n",
    "val_embeddings, val_labels = get_latent_embeddings(loaded_model, val_loader)\n",
    "test_embeddings, test_labels = get_latent_embeddings(loaded_model, test_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7fbab96-e2f0-44ee-bf0c-63e74b8659dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8185, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "595a6bf4-15d5-4bf3-a6c9-2f113fc5db7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.0 or less. Got NumPy 2.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/umap/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/umap/umap_.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/numba/__init__.py:59\u001b[0m\n\u001b[1;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/numba/__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.0 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.1."
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Combine train and validation embeddings\n",
    "combined_embeddings = torch.cat([train_embeddings, val_embeddings], dim=0).cpu().numpy()\n",
    "combined_labels = torch.cat([train_labels, val_labels], dim=0).cpu().numpy()\n",
    "\n",
    "# Fit UMAP on the combined embeddings\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "combined_umap = umap_model.fit_transform(combined_embeddings)\n",
    "\n",
    "# Plot UMAP embeddings (train + validation)\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(combined_umap[:, 0], combined_umap[:, 1], c=combined_labels, cmap='Spectral', s=10, alpha=0.8)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP Projection of Train + Validation Embeddings')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf32d338-23d8-4867-82b3-0ea305b55c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.0 or less. Got NumPy 2.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/umap/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/umap/umap_.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/numba/__init__.py:59\u001b[0m\n\u001b[1;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m/fast/AG_Ohler/ekarimi/miniforge/envs/meth/lib/python3.10/site-packages/numba/__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.0 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.1."
     ]
    }
   ],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9933dc-318e-4ba3-838f-87836ef4e237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meth]",
   "language": "python",
   "name": "conda-env-meth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
