{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd1e0ee-137a-4e88-8826-f1aa74aa83f5",
   "metadata": {},
   "source": [
    "# Train models on subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4e60-85f2-4786-b816-cd2ba384f459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25597bcb-917e-4df6-bc83-6e282da50a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Disables fast auto-tuning\n",
    "\n",
    "\n",
    "# Time tracking decorator\n",
    "def time_tracker(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting {func.__name__}...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "@time_tracker\n",
    "def sample_data(numerical_data_filtered, labels_encoded, split_size, random_state=42):\n",
    "    if split_size == 'shuffled_10000':\n",
    "        return generate_shuffled_data(numerical_data_filtered, 10000, random_state)\n",
    "    else:\n",
    "        size = int(split_size)\n",
    "        if size < len(numerical_data_filtered):\n",
    "            print(f\"Splitting data with size: {size}\")\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, train_size=size, random_state=random_state)\n",
    "            for train_idx, _ in splitter.split(numerical_data_filtered, labels_encoded):\n",
    "                return numerical_data_filtered.iloc[train_idx], [labels_encoded[i] for i in train_idx]\n",
    "        else:\n",
    "            return numerical_data_filtered, labels_encoded\n",
    "\n",
    "\n",
    "@time_tracker\n",
    "def train_val_test_split(data, labels, random_state=42):\n",
    "    print(\"Splitting data into training, validation, and test sets...\")\n",
    "    \n",
    "    # Convert labels list to pandas Series to use value_counts()\n",
    "    import pandas as pd\n",
    "    labels_series = pd.Series(labels, index=data.index)\n",
    "    \n",
    "    # Filter labels to exclude classes with fewer than 20 samples\n",
    "    label_counts = labels_series.value_counts()\n",
    "    valid_labels = label_counts[label_counts > 20].index\n",
    "\n",
    "    # Select only data and labels corresponding to classes with more than 20 samples\n",
    "    valid_indices = labels_series.isin(valid_labels)\n",
    "    data_filtered = data.loc[valid_indices]\n",
    "    labels_filtered = labels_series.loc[valid_indices]\n",
    "\n",
    "    # Perform the split with stratification on filtered data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "        data_filtered, labels_filtered, test_size=0.3, random_state=random_state, stratify=labels_filtered\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_remaining, y_remaining, test_size=0.4, random_state=random_state, stratify=y_remaining\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "@time_tracker\n",
    "def scale_and_save_data(train_val_test_splits, output_path):\n",
    "    scaler = StandardScaler()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_splits\n",
    "    print(f\"Scaling and saving data...\")\n",
    "    \n",
    "    # Scaling the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Convert labels to numpy arrays before converting to tensors\n",
    "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Creating TensorDataset objects\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    dataset_path = f'{output_path}/{split_data_value}/'\n",
    "    directory_path = Path(dataset_path)\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the datasets\n",
    "    torch.save(train_dataset.tensors, f'{dataset_path}train_dataset_tensors.pt')\n",
    "    torch.save(val_dataset.tensors, f'{dataset_path}val_dataset_tensors.pt')\n",
    "    torch.save(test_dataset.tensors, f'{dataset_path}test_dataset_tensors.pt')\n",
    "    print(f\"Data saved successfully.\")\n",
    "\n",
    "\n",
    "@time_tracker\n",
    "def generate_shuffled_data(numerical_data_filtered, size, random_state=42):\n",
    "    print(\"Generating shuffled data for null hypothesis...\")\n",
    "    flattened_values = numerical_data_filtered.values.flatten()\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(flattened_values)\n",
    "    shuffled_matrix = flattened_values.reshape(numerical_data_filtered.shape)\n",
    "    shuffled_data = pd.DataFrame(shuffled_matrix, columns=numerical_data_filtered.columns).sample(n=size, random_state=random_state).reset_index(drop=True)\n",
    "    shuffled_labels = [0] * size  # Use dummy labels to represent null hypothesis\n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb662172-7b96-4e16-81b6-67bb22e0bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09325c9d-4686-4cf0-be29-c012fa031b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = ['5000', '10000', '20000', '37067', 'shuffled_10000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42ceb7a-386e-45e9-b4ac-97ed51a518be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable inside the script\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ad77-9006-41ec-9ff8-7b1a6d0753ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b485a-57b5-4d4f-af6b-7c5168286ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fbd7193-a7fe-4e11-92c0-dec7c94a9fc9",
   "metadata": {},
   "source": [
    "# Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1344de36-d29d-432d-b4cb-8f4e31882834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims=[2048,1024,512], dropout_rate=0.2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = self.build_layers(input_dim, hidden_dims, dropout_rate)\n",
    "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)  # for mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)  # for log variance\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_hidden_dims = hidden_dims[::-1]\n",
    "        self.decoder_layers =self.build_layers(latent_dim, decoder_hidden_dims, dropout_rate)\n",
    "        self.fc_output = nn.Linear(hidden_dims[0], input_dim)\n",
    "        # self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        # self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def build_layers(self, input_dim, hidden_dims, dropout_rate):\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = h_dim\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder_layers(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Check if logvar has NaN or Inf values\n",
    "        if torch.isnan(logvar).any() or torch.isinf(logvar).any():\n",
    "            print(f\"NaN or Inf detected in logvar: logvar={logvar}\")\n",
    "        \n",
    "        # Clamp logvar to prevent extreme values\n",
    "        logvar = torch.clamp(logvar, min=-5, max=5)\n",
    "        \n",
    "        # Calculate std from logvar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Check if std has NaN or Inf values\n",
    "        if torch.isnan(std).any() or torch.isinf(std).any():\n",
    "            print(f\"NaN or Inf detected in std computation: std={std}\")\n",
    "        \n",
    "        # Sample from the latent space\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Check if z has NaN or Inf values\n",
    "        if torch.isnan(z).any() or torch.isinf(z).any():\n",
    "            print(f\"NaN or Inf detected in z computation: z={z}\")\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_layers(z)\n",
    "        # h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc_output(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Method to get the latent embedding (the `z` vector) for an input.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)  # this is the embedding\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62bd86a-8aff-492d-9745-fe7071e3446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Lightning(pl.LightningModule):\n",
    "    def __init__(self, input_dim=485577, latent_dim=128, hidden_dims=[2048, 1024, 512], dropout_rate=0.2, lr=1e-6):\n",
    "        super(VAE_Lightning, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()  # Save hyperparameters for checkpointing\n",
    "\n",
    "        self.model = VAE(input_dim, latent_dim, hidden_dims, dropout_rate)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.model.encode(x)\n",
    "        z = self.model.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        return self.model.get_latent_embedding(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)  # mask where values are not NaN\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Validation loss: {loss.item()}\")\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "  \n",
    "\n",
    "    def _vae_loss(self, original_x, x_hat, mu, logvar, mask):\n",
    "        # Apply mask to ignore NaN values in the loss calculation\n",
    "        recon_loss = F.mse_loss(x_hat[mask], original_x[mask], reduction='mean')\n",
    "    \n",
    "        # Scale the KL divergence to balance the losses\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss / original_x.shape[0]  # Normalize by batch size or apply weighting\n",
    "    \n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d2c96b-8ddb-4e73-a3c2-7743bf5e4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last training epoch from the logs\n",
    "        train_loss = trainer.callback_metrics.get('train_loss')\n",
    "        if train_loss is not None:\n",
    "            self.train_losses.append(train_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last validation epoch from the logs\n",
    "        val_loss = trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "def replace_nan_with_mean(x):\n",
    "    # Calculate the column-wise mean, ignoring NaNs\n",
    "    col_mean = torch.nanmean(x, dim=0)\n",
    "    \n",
    "    # Find where NaN values are located\n",
    "    nan_mask = torch.isnan(x)\n",
    "    \n",
    "    # Replace NaNs with the corresponding column means\n",
    "    x[nan_mask] = torch.take(col_mean, nan_mask.nonzero()[:, 1])\n",
    "    \n",
    "    # Check if there are still NaN or Inf values\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in the input data after imputation!\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b9194-7cc9-4121-8b43-649794d892c3",
   "metadata": {},
   "source": [
    "## Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32624f45-a42b-40e1-a495-8eda0f11cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd2ceef-19cf-48e1-81d8-ee67dd3c2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509808f-8e5e-4a7a-a9c4-acd96c9e87c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d648b65-faac-49ce-a8c2-ad4970ca0785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "<timed exec>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "<timed exec>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 87.6 M | train\n",
      "---------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.335   Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Validation loss: 3.0101165771484375\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  3.23it/s]Validation loss: 2.665041923522949\n",
      "Epoch 0:   0%|          | 0/212 [00:00<?, ?it/s]                           Training loss: 1.233135461807251\n",
      "Epoch 0:   0%|          | 1/212 [00:00<00:29,  7.11it/s, v_num=0]Training loss: 1.4413628578186035\n",
      "Epoch 0:   1%|          | 2/212 [00:00<00:17, 12.31it/s, v_num=0]Training loss: 1.4879541397094727\n",
      "Epoch 0:   1%|▏         | 3/212 [00:00<00:13, 15.82it/s, v_num=0]Training loss: 1.920178771018982\n",
      "Epoch 0:   2%|▏         | 4/212 [00:00<00:11, 18.49it/s, v_num=0]Training loss: 1.6109426021575928\n",
      "Epoch 0:   2%|▏         | 5/212 [00:00<00:10, 20.58it/s, v_num=0]Training loss: 1.510411262512207\n",
      "Epoch 0:   3%|▎         | 6/212 [00:00<00:09, 22.23it/s, v_num=0]Training loss: 2.2189767360687256\n",
      "Epoch 0:   3%|▎         | 7/212 [00:00<00:08, 23.60it/s, v_num=0]Training loss: 1.9329006671905518\n",
      "Epoch 0:   4%|▍         | 8/212 [00:00<00:08, 24.74it/s, v_num=0]Training loss: 1.4837136268615723\n",
      "Epoch 0:   4%|▍         | 9/212 [00:00<00:07, 25.71it/s, v_num=0]Training loss: 1.9231730699539185\n",
      "Epoch 0:   5%|▍         | 10/212 [00:00<00:07, 26.54it/s, v_num=0]Training loss: 1.4885838031768799\n",
      "Epoch 0:   5%|▌         | 11/212 [00:00<00:07, 27.26it/s, v_num=0]Training loss: 1.8000224828720093\n",
      "Epoch 0:   6%|▌         | 12/212 [00:00<00:07, 27.89it/s, v_num=0]Training loss: 1.6319172382354736\n",
      "Epoch 0:   6%|▌         | 13/212 [00:00<00:06, 28.44it/s, v_num=0]Training loss: 1.8944364786148071\n",
      "Epoch 0:   7%|▋         | 14/212 [00:00<00:06, 28.92it/s, v_num=0]Training loss: 2.248117208480835\n",
      "Epoch 0:   7%|▋         | 15/212 [00:00<00:06, 29.37it/s, v_num=0]Training loss: 1.4434034824371338\n",
      "Epoch 0:   8%|▊         | 16/212 [00:00<00:06, 29.77it/s, v_num=0]Training loss: 2.0001020431518555\n",
      "Epoch 0:   8%|▊         | 17/212 [00:00<00:06, 30.14it/s, v_num=0]Training loss: 1.573000431060791\n",
      "Epoch 0:   8%|▊         | 18/212 [00:00<00:06, 30.47it/s, v_num=0]Training loss: 1.3130881786346436\n",
      "Epoch 0:   9%|▉         | 19/212 [00:00<00:06, 30.77it/s, v_num=0]Training loss: 1.7632997035980225\n",
      "Epoch 0:   9%|▉         | 20/212 [00:00<00:06, 31.06it/s, v_num=0]Training loss: 1.8318188190460205\n",
      "Epoch 0:  10%|▉         | 21/212 [00:00<00:06, 31.32it/s, v_num=0]Training loss: 1.655663013458252\n",
      "Epoch 0:  10%|█         | 22/212 [00:00<00:06, 31.55it/s, v_num=0]Training loss: 1.6387028694152832\n",
      "Epoch 0:  11%|█         | 23/212 [00:00<00:05, 31.77it/s, v_num=0]Training loss: 1.6204683780670166\n",
      "Epoch 0:  11%|█▏        | 24/212 [00:00<00:05, 31.95it/s, v_num=0]Training loss: 1.1809206008911133\n",
      "Epoch 0:  12%|█▏        | 25/212 [00:00<00:05, 32.15it/s, v_num=0]Training loss: 5.484248161315918\n",
      "Epoch 0:  12%|█▏        | 26/212 [00:00<00:05, 32.32it/s, v_num=0]Training loss: 1.8580849170684814\n",
      "Epoch 0:  13%|█▎        | 27/212 [00:00<00:05, 32.49it/s, v_num=0]Training loss: 1.8254339694976807\n",
      "Epoch 0:  13%|█▎        | 28/212 [00:00<00:05, 32.65it/s, v_num=0]Training loss: 2.350409507751465\n",
      "Epoch 0:  14%|█▎        | 29/212 [00:00<00:05, 32.79it/s, v_num=0]Training loss: 1.2308608293533325\n",
      "Epoch 0:  14%|█▍        | 30/212 [00:00<00:05, 32.93it/s, v_num=0]Training loss: 1.116121768951416\n",
      "Epoch 0:  15%|█▍        | 31/212 [00:00<00:05, 33.06it/s, v_num=0]Training loss: 1.1997767686843872\n",
      "Epoch 0:  15%|█▌        | 32/212 [00:00<00:05, 33.18it/s, v_num=0]Training loss: 1.5447942018508911\n",
      "Epoch 0:  16%|█▌        | 33/212 [00:00<00:05, 33.30it/s, v_num=0]Training loss: 1.9494943618774414\n",
      "Epoch 0:  16%|█▌        | 34/212 [00:01<00:05, 33.40it/s, v_num=0]Training loss: 1.094761610031128\n",
      "Epoch 0:  17%|█▋        | 35/212 [00:01<00:05, 33.47it/s, v_num=0]Training loss: 1.2122430801391602\n",
      "Epoch 0:  17%|█▋        | 36/212 [00:01<00:05, 33.54it/s, v_num=0]Training loss: 1.2633743286132812\n",
      "Epoch 0:  17%|█▋        | 37/212 [00:01<00:05, 33.60it/s, v_num=0]Training loss: 1.4617772102355957\n",
      "Epoch 0:  18%|█▊        | 38/212 [00:01<00:05, 33.67it/s, v_num=0]Training loss: 2.1767351627349854\n",
      "Epoch 0:  18%|█▊        | 39/212 [00:01<00:05, 33.73it/s, v_num=0]Training loss: 1.3561278581619263\n",
      "Epoch 0:  19%|█▉        | 40/212 [00:01<00:05, 33.76it/s, v_num=0]Training loss: 1.067307949066162\n",
      "Epoch 0:  19%|█▉        | 41/212 [00:01<00:05, 33.82it/s, v_num=0]Training loss: 1.949723482131958\n",
      "Epoch 0:  20%|█▉        | 42/212 [00:01<00:05, 33.87it/s, v_num=0]Training loss: 1.1147574186325073\n",
      "Epoch 0:  20%|██        | 43/212 [00:01<00:04, 33.95it/s, v_num=0]Training loss: 1.3572927713394165\n",
      "Epoch 0:  21%|██        | 44/212 [00:01<00:04, 34.04it/s, v_num=0]Training loss: 1.71841561794281\n",
      "Epoch 0:  21%|██        | 45/212 [00:01<00:04, 34.11it/s, v_num=0]Training loss: 1.6716699600219727\n",
      "Epoch 0:  22%|██▏       | 46/212 [00:01<00:04, 34.17it/s, v_num=0]Training loss: 1.073227882385254\n",
      "Epoch 0:  22%|██▏       | 47/212 [00:01<00:04, 34.24it/s, v_num=0]Training loss: 1.0476999282836914\n",
      "Epoch 0:  23%|██▎       | 48/212 [00:01<00:04, 34.30it/s, v_num=0]Training loss: 1.3647828102111816\n",
      "Epoch 0:  23%|██▎       | 49/212 [00:01<00:04, 34.30it/s, v_num=0]Training loss: 1.166170597076416\n",
      "Epoch 0:  24%|██▎       | 50/212 [00:01<00:04, 34.36it/s, v_num=0]Training loss: 1.223700761795044\n",
      "Epoch 0:  24%|██▍       | 51/212 [00:01<00:04, 34.40it/s, v_num=0]Training loss: 1.075364351272583\n",
      "Epoch 0:  25%|██▍       | 52/212 [00:01<00:04, 34.43it/s, v_num=0]Training loss: 2.9956705570220947\n",
      "Epoch 0:  25%|██▌       | 53/212 [00:01<00:04, 34.41it/s, v_num=0]Training loss: 2.146827220916748\n",
      "Epoch 0:  25%|██▌       | 54/212 [00:01<00:04, 34.47it/s, v_num=0]Training loss: 2.0111918449401855\n",
      "Epoch 0:  26%|██▌       | 55/212 [00:01<00:04, 34.47it/s, v_num=0]Training loss: 1.6102714538574219\n",
      "Epoch 0:  26%|██▋       | 56/212 [00:01<00:04, 34.53it/s, v_num=0]Training loss: 1.8441897630691528\n",
      "Epoch 0:  27%|██▋       | 57/212 [00:01<00:04, 34.50it/s, v_num=0]Training loss: 1.6775251626968384\n",
      "Epoch 0:  27%|██▋       | 58/212 [00:01<00:04, 34.53it/s, v_num=0]Training loss: 1.8165346384048462\n",
      "Epoch 0:  28%|██▊       | 59/212 [00:01<00:04, 34.57it/s, v_num=0]Training loss: 2.083695888519287\n",
      "Epoch 0:  28%|██▊       | 60/212 [00:01<00:04, 34.60it/s, v_num=0]Training loss: 1.6158266067504883\n",
      "Epoch 0:  29%|██▉       | 61/212 [00:01<00:04, 34.63it/s, v_num=0]Training loss: 1.2786355018615723\n",
      "Epoch 0:  29%|██▉       | 62/212 [00:01<00:04, 34.67it/s, v_num=0]Training loss: 1.098069190979004\n",
      "Epoch 0:  30%|██▉       | 63/212 [00:01<00:04, 34.72it/s, v_num=0]Training loss: 1.2861988544464111\n",
      "Epoch 0:  30%|███       | 64/212 [00:01<00:04, 34.77it/s, v_num=0]Training loss: 1.519624948501587\n",
      "Epoch 0:  31%|███       | 65/212 [00:01<00:04, 34.81it/s, v_num=0]Training loss: 1.4893267154693604\n",
      "Epoch 0:  31%|███       | 66/212 [00:01<00:04, 34.85it/s, v_num=0]Training loss: 1.1200435161590576\n",
      "Epoch 0:  32%|███▏      | 67/212 [00:01<00:04, 34.88it/s, v_num=0]Training loss: 1.7062914371490479\n",
      "Epoch 0:  32%|███▏      | 68/212 [00:01<00:04, 34.92it/s, v_num=0]Training loss: 0.9867122173309326\n",
      "Epoch 0:  33%|███▎      | 69/212 [00:01<00:04, 34.95it/s, v_num=0]Training loss: 1.490204095840454\n",
      "Epoch 0:  33%|███▎      | 70/212 [00:02<00:04, 34.99it/s, v_num=0]Training loss: 0.8642137050628662\n",
      "Epoch 0:  33%|███▎      | 71/212 [00:02<00:04, 35.02it/s, v_num=0]Training loss: 1.6376862525939941\n",
      "Epoch 0:  34%|███▍      | 72/212 [00:02<00:03, 35.05it/s, v_num=0]Training loss: 1.1444432735443115\n",
      "Epoch 0:  34%|███▍      | 73/212 [00:02<00:03, 35.08it/s, v_num=0]Training loss: 1.3084635734558105\n",
      "Epoch 0:  35%|███▍      | 74/212 [00:02<00:03, 35.09it/s, v_num=0]Training loss: 1.4343326091766357\n",
      "Epoch 0:  35%|███▌      | 75/212 [00:02<00:03, 35.10it/s, v_num=0]Training loss: 1.715450644493103\n",
      "Epoch 0:  36%|███▌      | 76/212 [00:02<00:03, 35.11it/s, v_num=0]Training loss: 1.3908717632293701\n",
      "Epoch 0:  36%|███▋      | 77/212 [00:02<00:03, 35.13it/s, v_num=0]Training loss: 1.4878040552139282\n",
      "Epoch 0:  37%|███▋      | 78/212 [00:02<00:03, 35.14it/s, v_num=0]Training loss: 1.7242814302444458\n",
      "Epoch 0:  37%|███▋      | 79/212 [00:02<00:03, 35.14it/s, v_num=0]Training loss: 1.608537197113037\n",
      "Epoch 0:  38%|███▊      | 80/212 [00:02<00:03, 35.16it/s, v_num=0]Training loss: 2.2891488075256348\n",
      "Epoch 0:  38%|███▊      | 81/212 [00:02<00:03, 35.17it/s, v_num=0]Training loss: 1.7422999143600464\n",
      "Epoch 0:  39%|███▊      | 82/212 [00:02<00:03, 35.20it/s, v_num=0]Training loss: 14.108163833618164\n",
      "Epoch 0:  39%|███▉      | 83/212 [00:02<00:03, 35.23it/s, v_num=0]Training loss: 1.143557071685791\n",
      "Epoch 0:  40%|███▉      | 84/212 [00:02<00:03, 35.26it/s, v_num=0]Training loss: 1.1247341632843018\n",
      "Epoch 0:  40%|████      | 85/212 [00:02<00:03, 35.28it/s, v_num=0]Training loss: 1.9672691822052002\n",
      "Epoch 0:  41%|████      | 86/212 [00:02<00:03, 35.30it/s, v_num=0]Training loss: 1.8432214260101318\n",
      "Epoch 0:  41%|████      | 87/212 [00:02<00:03, 35.33it/s, v_num=0]Training loss: 1.181247591972351\n",
      "Epoch 0:  42%|████▏     | 88/212 [00:02<00:03, 35.35it/s, v_num=0]Training loss: 1.5506125688552856\n",
      "Epoch 0:  42%|████▏     | 89/212 [00:02<00:03, 35.37it/s, v_num=0]Training loss: 1.3233366012573242\n",
      "Epoch 0:  42%|████▏     | 90/212 [00:02<00:03, 35.39it/s, v_num=0]Training loss: 1.1824760437011719\n",
      "Epoch 0:  43%|████▎     | 91/212 [00:02<00:03, 35.41it/s, v_num=0]Training loss: 0.9945319890975952\n",
      "Epoch 0:  43%|████▎     | 92/212 [00:02<00:03, 35.43it/s, v_num=0]Training loss: 1.7867668867111206\n",
      "Epoch 0:  44%|████▍     | 93/212 [00:02<00:03, 35.45it/s, v_num=0]Training loss: 1.4664214849472046\n",
      "Epoch 0:  44%|████▍     | 94/212 [00:02<00:03, 35.47it/s, v_num=0]Training loss: 1.2995214462280273\n",
      "Epoch 0:  45%|████▍     | 95/212 [00:02<00:03, 35.49it/s, v_num=0]Training loss: 0.7741572856903076\n",
      "Epoch 0:  45%|████▌     | 96/212 [00:02<00:03, 35.51it/s, v_num=0]Training loss: 1.852468729019165\n",
      "Epoch 0:  46%|████▌     | 97/212 [00:02<00:03, 35.53it/s, v_num=0]Training loss: 1.1327903270721436\n",
      "Epoch 0:  46%|████▌     | 98/212 [00:02<00:03, 35.55it/s, v_num=0]Training loss: 2.6726772785186768\n",
      "Epoch 0:  47%|████▋     | 99/212 [00:02<00:03, 35.57it/s, v_num=0]Training loss: 1.781725287437439\n",
      "Epoch 0:  47%|████▋     | 100/212 [00:02<00:03, 35.59it/s, v_num=0]Training loss: 1.6956632137298584\n",
      "Epoch 0:  48%|████▊     | 101/212 [00:02<00:03, 35.61it/s, v_num=0]Training loss: 1.2441800832748413\n",
      "Epoch 0:  48%|████▊     | 102/212 [00:02<00:03, 35.62it/s, v_num=0]Training loss: 1.3871387243270874\n",
      "Epoch 0:  49%|████▊     | 103/212 [00:02<00:03, 35.64it/s, v_num=0]Training loss: 1.275971531867981\n",
      "Epoch 0:  49%|████▉     | 104/212 [00:02<00:03, 35.66it/s, v_num=0]Training loss: 1.4687927961349487\n",
      "Epoch 0:  50%|████▉     | 105/212 [00:02<00:02, 35.68it/s, v_num=0]Training loss: 1.6522667407989502\n",
      "Epoch 0:  50%|█████     | 106/212 [00:02<00:02, 35.69it/s, v_num=0]Training loss: 1.3327360153198242\n",
      "Epoch 0:  50%|█████     | 107/212 [00:02<00:02, 35.70it/s, v_num=0]Training loss: 2.845503568649292\n",
      "Epoch 0:  51%|█████     | 108/212 [00:03<00:02, 35.71it/s, v_num=0]Training loss: 1.3444836139678955\n",
      "Epoch 0:  51%|█████▏    | 109/212 [00:03<00:02, 35.73it/s, v_num=0]Training loss: 1.4416141510009766\n",
      "Epoch 0:  52%|█████▏    | 110/212 [00:03<00:02, 35.74it/s, v_num=0]Training loss: 1.6920485496520996\n",
      "Epoch 0:  52%|█████▏    | 111/212 [00:03<00:02, 35.76it/s, v_num=0]Training loss: 1.3872445821762085\n",
      "Epoch 0:  53%|█████▎    | 112/212 [00:03<00:02, 35.77it/s, v_num=0]Training loss: 2.6036524772644043\n",
      "Epoch 0:  53%|█████▎    | 113/212 [00:03<00:02, 35.78it/s, v_num=0]Training loss: 1.2687649726867676\n",
      "Epoch 0:  54%|█████▍    | 114/212 [00:03<00:02, 35.78it/s, v_num=0]Training loss: 1.7879830598831177\n",
      "Epoch 0:  54%|█████▍    | 115/212 [00:03<00:02, 35.78it/s, v_num=0]Training loss: 1.035282850265503\n",
      "Epoch 0:  55%|█████▍    | 116/212 [00:03<00:02, 35.79it/s, v_num=0]Training loss: 0.991928219795227\n",
      "Epoch 0:  55%|█████▌    | 117/212 [00:03<00:02, 35.79it/s, v_num=0]Training loss: 1.5793710947036743\n",
      "Epoch 0:  56%|█████▌    | 118/212 [00:03<00:02, 35.79it/s, v_num=0]Training loss: 1.87344491481781\n",
      "Epoch 0:  56%|█████▌    | 119/212 [00:03<00:02, 35.79it/s, v_num=0]Training loss: 1.2600101232528687\n",
      "Epoch 0:  57%|█████▋    | 120/212 [00:03<00:02, 35.79it/s, v_num=0]Training loss: 1.184523582458496\n",
      "Epoch 0:  57%|█████▋    | 121/212 [00:03<00:02, 35.80it/s, v_num=0]Training loss: 2.749540090560913\n",
      "Epoch 0:  58%|█████▊    | 122/212 [00:03<00:02, 35.82it/s, v_num=0]Training loss: 2.1975834369659424\n",
      "Epoch 0:  58%|█████▊    | 123/212 [00:03<00:02, 35.83it/s, v_num=0]Training loss: 2.4280428886413574\n",
      "Epoch 0:  58%|█████▊    | 124/212 [00:03<00:02, 35.85it/s, v_num=0]Training loss: 1.3697420358657837\n",
      "Epoch 0:  59%|█████▉    | 125/212 [00:03<00:02, 35.86it/s, v_num=0]Training loss: 1.4871561527252197\n",
      "Epoch 0:  59%|█████▉    | 126/212 [00:03<00:02, 35.87it/s, v_num=0]Training loss: 1.371415376663208\n",
      "Epoch 0:  60%|█████▉    | 127/212 [00:03<00:02, 35.88it/s, v_num=0]Training loss: 1.762845754623413\n",
      "Epoch 0:  60%|██████    | 128/212 [00:03<00:02, 35.89it/s, v_num=0]Training loss: 2.2056989669799805\n",
      "Epoch 0:  61%|██████    | 129/212 [00:03<00:02, 35.91it/s, v_num=0]Training loss: 4.272589206695557\n",
      "Epoch 0:  61%|██████▏   | 130/212 [00:03<00:02, 35.92it/s, v_num=0]Training loss: 2.1248254776000977\n",
      "Epoch 0:  62%|██████▏   | 131/212 [00:03<00:02, 35.93it/s, v_num=0]Training loss: 1.211951732635498\n",
      "Epoch 0:  62%|██████▏   | 132/212 [00:03<00:02, 35.94it/s, v_num=0]Training loss: 2.13785982131958\n",
      "Epoch 0:  63%|██████▎   | 133/212 [00:03<00:02, 35.95it/s, v_num=0]Training loss: 1.6335413455963135\n",
      "Epoch 0:  63%|██████▎   | 134/212 [00:03<00:02, 35.96it/s, v_num=0]Training loss: 1.2685014009475708\n",
      "Epoch 0:  64%|██████▎   | 135/212 [00:03<00:02, 35.97it/s, v_num=0]Training loss: 1.2193071842193604\n",
      "Epoch 0:  64%|██████▍   | 136/212 [00:03<00:02, 35.98it/s, v_num=0]Training loss: 1.0475233793258667\n",
      "Epoch 0:  65%|██████▍   | 137/212 [00:03<00:02, 35.99it/s, v_num=0]Training loss: 1.542586326599121\n",
      "Epoch 0:  65%|██████▌   | 138/212 [00:03<00:02, 36.00it/s, v_num=0]Training loss: 1.3283421993255615\n",
      "Epoch 0:  66%|██████▌   | 139/212 [00:03<00:02, 36.01it/s, v_num=0]Training loss: 1.7351341247558594\n",
      "Epoch 0:  66%|██████▌   | 140/212 [00:03<00:01, 36.02it/s, v_num=0]Training loss: 1.7032018899917603\n",
      "Epoch 0:  67%|██████▋   | 141/212 [00:03<00:01, 36.03it/s, v_num=0]Training loss: 1.0665168762207031\n",
      "Epoch 0:  67%|██████▋   | 142/212 [00:03<00:01, 36.04it/s, v_num=0]Training loss: 0.9010822772979736\n",
      "Epoch 0:  67%|██████▋   | 143/212 [00:03<00:01, 36.05it/s, v_num=0]Training loss: 1.623822808265686\n",
      "Epoch 0:  68%|██████▊   | 144/212 [00:03<00:01, 36.06it/s, v_num=0]Training loss: 1.3481323719024658\n",
      "Epoch 0:  68%|██████▊   | 145/212 [00:04<00:01, 36.07it/s, v_num=0]Training loss: 1.0616284608840942\n",
      "Epoch 0:  69%|██████▉   | 146/212 [00:04<00:01, 36.08it/s, v_num=0]Training loss: 2.082111120223999\n",
      "Epoch 0:  69%|██████▉   | 147/212 [00:04<00:01, 36.09it/s, v_num=0]Training loss: 1.2816293239593506\n",
      "Epoch 0:  70%|██████▉   | 148/212 [00:04<00:01, 36.10it/s, v_num=0]Training loss: 1.4417766332626343\n",
      "Epoch 0:  70%|███████   | 149/212 [00:04<00:01, 36.11it/s, v_num=0]Training loss: 1.5708128213882446\n",
      "Epoch 0:  71%|███████   | 150/212 [00:04<00:01, 36.12it/s, v_num=0]Training loss: 1.2547420263290405\n",
      "Epoch 0:  71%|███████   | 151/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 1.7327632904052734\n",
      "Epoch 0:  72%|███████▏  | 152/212 [00:04<00:01, 36.14it/s, v_num=0]Training loss: 1.2069334983825684\n",
      "Epoch 0:  72%|███████▏  | 153/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 2.245008945465088\n",
      "Epoch 0:  73%|███████▎  | 154/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 1.8550901412963867\n",
      "Epoch 0:  73%|███████▎  | 155/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 1.2107172012329102\n",
      "Epoch 0:  74%|███████▎  | 156/212 [00:04<00:01, 36.14it/s, v_num=0]Training loss: 1.609243631362915\n",
      "Epoch 0:  74%|███████▍  | 157/212 [00:04<00:01, 36.14it/s, v_num=0]Training loss: 1.6963218450546265\n",
      "Epoch 0:  75%|███████▍  | 158/212 [00:04<00:01, 36.14it/s, v_num=0]Training loss: 1.4669749736785889\n",
      "Epoch 0:  75%|███████▌  | 159/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 1.1625380516052246\n",
      "Epoch 0:  75%|███████▌  | 160/212 [00:04<00:01, 36.13it/s, v_num=0]Training loss: 1.619093656539917\n",
      "Epoch 0:  76%|███████▌  | 161/212 [00:04<00:01, 36.14it/s, v_num=0]Training loss: 1.2564959526062012\n",
      "Epoch 0:  76%|███████▋  | 162/212 [00:04<00:01, 36.15it/s, v_num=0]Training loss: 0.9574665427207947\n",
      "Epoch 0:  77%|███████▋  | 163/212 [00:04<00:01, 36.16it/s, v_num=0]Training loss: 1.0588759183883667\n",
      "Epoch 0:  77%|███████▋  | 164/212 [00:04<00:01, 36.17it/s, v_num=0]Training loss: 1.0877461433410645\n",
      "Epoch 0:  78%|███████▊  | 165/212 [00:04<00:01, 36.17it/s, v_num=0]Training loss: 1.0193628072738647\n",
      "Epoch 0:  78%|███████▊  | 166/212 [00:04<00:01, 36.18it/s, v_num=0]Training loss: 1.2155646085739136\n",
      "Epoch 0:  79%|███████▉  | 167/212 [00:04<00:01, 36.19it/s, v_num=0]Training loss: 1.2778396606445312\n",
      "Epoch 0:  79%|███████▉  | 168/212 [00:04<00:01, 36.20it/s, v_num=0]Training loss: 0.7727267742156982\n",
      "Epoch 0:  80%|███████▉  | 169/212 [00:04<00:01, 36.20it/s, v_num=0]Training loss: 1.7732574939727783\n",
      "Epoch 0:  80%|████████  | 170/212 [00:04<00:01, 36.21it/s, v_num=0]Training loss: 11.741044998168945\n",
      "Epoch 0:  81%|████████  | 171/212 [00:04<00:01, 36.22it/s, v_num=0]Training loss: 1.5607502460479736\n",
      "Epoch 0:  81%|████████  | 172/212 [00:04<00:01, 36.22it/s, v_num=0]Training loss: 2.0035529136657715\n",
      "Epoch 0:  82%|████████▏ | 173/212 [00:04<00:01, 36.23it/s, v_num=0]Training loss: 0.8900190591812134\n",
      "Epoch 0:  82%|████████▏ | 174/212 [00:04<00:01, 36.24it/s, v_num=0]Training loss: 1.9647449254989624\n",
      "Epoch 0:  83%|████████▎ | 175/212 [00:04<00:01, 36.24it/s, v_num=0]Training loss: 1.5788092613220215\n",
      "Epoch 0:  83%|████████▎ | 176/212 [00:04<00:00, 36.25it/s, v_num=0]Training loss: 2.8043644428253174\n",
      "Epoch 0:  83%|████████▎ | 177/212 [00:04<00:00, 36.26it/s, v_num=0]Training loss: 1.5440032482147217\n",
      "Epoch 0:  84%|████████▍ | 178/212 [00:04<00:00, 36.26it/s, v_num=0]Training loss: 1.4943037033081055\n",
      "Epoch 0:  84%|████████▍ | 179/212 [00:04<00:00, 36.27it/s, v_num=0]Training loss: 1.4868115186691284\n",
      "Epoch 0:  85%|████████▍ | 180/212 [00:04<00:00, 36.28it/s, v_num=0]Training loss: 0.9739652276039124\n",
      "Epoch 0:  85%|████████▌ | 181/212 [00:04<00:00, 36.28it/s, v_num=0]Training loss: 0.8629321455955505\n",
      "Epoch 0:  86%|████████▌ | 182/212 [00:05<00:00, 36.29it/s, v_num=0]Training loss: 0.9457282423973083\n",
      "Epoch 0:  86%|████████▋ | 183/212 [00:05<00:00, 36.30it/s, v_num=0]Training loss: 1.4660108089447021\n",
      "Epoch 0:  87%|████████▋ | 184/212 [00:05<00:00, 36.30it/s, v_num=0]Training loss: 1.67000150680542\n",
      "Epoch 0:  87%|████████▋ | 185/212 [00:05<00:00, 36.31it/s, v_num=0]Training loss: 1.1638524532318115\n",
      "Epoch 0:  88%|████████▊ | 186/212 [00:05<00:00, 36.32it/s, v_num=0]Training loss: 1.8459519147872925\n",
      "Epoch 0:  88%|████████▊ | 187/212 [00:05<00:00, 36.32it/s, v_num=0]Training loss: 1.3846861124038696\n",
      "Epoch 0:  89%|████████▊ | 188/212 [00:05<00:00, 36.33it/s, v_num=0]Training loss: 2.241208791732788\n",
      "Epoch 0:  89%|████████▉ | 189/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 1.6865501403808594\n",
      "Epoch 0:  90%|████████▉ | 190/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 1.2105259895324707\n",
      "Epoch 0:  90%|█████████ | 191/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.2335495948791504\n",
      "Epoch 0:  91%|█████████ | 192/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.4084583520889282\n",
      "Epoch 0:  91%|█████████ | 193/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.597691535949707\n",
      "Epoch 0:  92%|█████████▏| 194/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.292980432510376\n",
      "Epoch 0:  92%|█████████▏| 195/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 1.496685266494751\n",
      "Epoch 0:  92%|█████████▏| 196/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 1.4055513143539429\n",
      "Epoch 0:  93%|█████████▎| 197/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 2.022942066192627\n",
      "Epoch 0:  93%|█████████▎| 198/212 [00:05<00:00, 36.33it/s, v_num=0]Training loss: 1.0002179145812988\n",
      "Epoch 0:  94%|█████████▍| 199/212 [00:05<00:00, 36.33it/s, v_num=0]Training loss: 1.4870145320892334\n",
      "Epoch 0:  94%|█████████▍| 200/212 [00:05<00:00, 36.33it/s, v_num=0]Training loss: 1.9369323253631592\n",
      "Epoch 0:  95%|█████████▍| 201/212 [00:05<00:00, 36.34it/s, v_num=0]Training loss: 1.7313411235809326\n",
      "Epoch 0:  95%|█████████▌| 202/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.1438912153244019\n",
      "Epoch 0:  96%|█████████▌| 203/212 [00:05<00:00, 36.35it/s, v_num=0]Training loss: 1.2862999439239502\n",
      "Epoch 0:  96%|█████████▌| 204/212 [00:05<00:00, 36.36it/s, v_num=0]Training loss: 1.4394837617874146\n",
      "Epoch 0:  97%|█████████▋| 205/212 [00:05<00:00, 36.36it/s, v_num=0]Training loss: 1.4316530227661133\n",
      "Epoch 0:  97%|█████████▋| 206/212 [00:05<00:00, 36.37it/s, v_num=0]Training loss: 1.1191895008087158\n",
      "Epoch 0:  98%|█████████▊| 207/212 [00:05<00:00, 36.37it/s, v_num=0]Training loss: 1.5374820232391357\n",
      "Epoch 0:  98%|█████████▊| 208/212 [00:05<00:00, 36.38it/s, v_num=0]Training loss: 1.0728703737258911\n",
      "Epoch 0:  99%|█████████▊| 209/212 [00:05<00:00, 36.38it/s, v_num=0]Training loss: 0.8923208713531494\n",
      "Epoch 0:  99%|█████████▉| 210/212 [00:05<00:00, 36.39it/s, v_num=0]Training loss: 1.5452463626861572\n",
      "Epoch 0: 100%|█████████▉| 211/212 [00:05<00:00, 36.39it/s, v_num=0]Training loss: 1.441107988357544\n",
      "Epoch 0: 100%|██████████| 212/212 [00:05<00:00, 36.38it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.4685044288635254\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 81.14it/s]\u001b[AValidation loss: 2.2059576511383057\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 88.78it/s]\u001b[AValidation loss: 1.2159359455108643\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 89.15it/s]\u001b[AValidation loss: 0.9168336391448975\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 91.23it/s]\u001b[AValidation loss: 0.931543231010437\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 93.53it/s]\u001b[AValidation loss: 1.1120555400848389\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 95.23it/s]\u001b[AValidation loss: 1.4699345827102661\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 96.34it/s]\u001b[AValidation loss: 2.517786741256714\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 97.41it/s]\u001b[AValidation loss: 0.8229758739471436\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 98.16it/s]\u001b[AValidation loss: 0.9426970481872559\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 98.81it/s]\u001b[AValidation loss: 0.9724435806274414\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 99.37it/s]\u001b[AValidation loss: 1.2979742288589478\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 97.73it/s]\u001b[AValidation loss: 1.266014814376831\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 97.23it/s]\u001b[AValidation loss: 1.2069151401519775\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 97.34it/s]\u001b[AValidation loss: 1.0021814107894897\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 97.79it/s]\u001b[AValidation loss: 1.3749943971633911\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 97.99it/s]\u001b[AValidation loss: 1.2682522535324097\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 98.40it/s]\u001b[AValidation loss: 1.8244004249572754\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 98.73it/s]\u001b[AValidation loss: 0.970653772354126\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 98.99it/s]\u001b[AValidation loss: 1.4733256101608276\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 98.87it/s]\u001b[AValidation loss: 1.330705165863037\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 99.12it/s]\u001b[AValidation loss: 1.3671411275863647\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 99.30it/s]\u001b[AValidation loss: 1.3482372760772705\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 99.54it/s]\u001b[AValidation loss: 1.202474594116211\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 99.71it/s]\u001b[AValidation loss: 1.663856863975525\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 99.83it/s]\u001b[AValidation loss: 0.9994215965270996\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 99.99it/s]\u001b[AValidation loss: 1.2283146381378174\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 100.09it/s]\u001b[AValidation loss: 1.2507957220077515\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 100.14it/s]\u001b[AValidation loss: 1.2523113489151\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 100.27it/s]\u001b[AValidation loss: 1.504989504814148\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 100.37it/s]\u001b[AValidation loss: 1.1125304698944092\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 100.51it/s]\u001b[AValidation loss: 1.2066389322280884\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 100.63it/s]\u001b[AValidation loss: 1.1836649179458618\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 100.73it/s]\u001b[AValidation loss: 1.0800909996032715\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 100.84it/s]\u001b[AValidation loss: 2.2060303688049316\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 100.89it/s]\u001b[AValidation loss: 0.8090701103210449\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 100.81it/s]\u001b[AValidation loss: 1.7896215915679932\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 100.87it/s]\u001b[AValidation loss: 0.8568350076675415\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 100.92it/s]\u001b[AValidation loss: 1.3092161417007446\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 100.97it/s]\u001b[AValidation loss: 1.203238606452942\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 100.89it/s]\u001b[AValidation loss: 1.072256326675415\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 100.95it/s]\u001b[AValidation loss: 0.9547459483146667\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 101.01it/s]\u001b[AValidation loss: 1.0615147352218628\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 101.05it/s]\u001b[AValidation loss: 1.5421801805496216\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 101.12it/s]\u001b[AValidation loss: 1.7135231494903564\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 101.17it/s]\u001b[AValidation loss: 0.9394508600234985\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 101.21it/s]\u001b[AValidation loss: 1.2163968086242676\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 101.27it/s]\u001b[AValidation loss: 1.206510066986084\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 101.30it/s]\u001b[AValidation loss: 1.3476839065551758\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 101.34it/s]\u001b[AValidation loss: 1.6621012687683105\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 101.36it/s]\u001b[AValidation loss: 1.503905177116394\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 101.39it/s]\u001b[AValidation loss: 1.3323787450790405\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 101.38it/s]\u001b[AValidation loss: 1.6743104457855225\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 101.28it/s]\u001b[AValidation loss: 1.3221713304519653\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 101.15it/s]\u001b[AValidation loss: 2.4288570880889893\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 101.04it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]                \u001b[ATraining loss: 1.1545199155807495\n",
      "Epoch 1:   0%|          | 1/212 [00:00<00:04, 44.85it/s, v_num=0]Training loss: 1.7533464431762695\n",
      "Epoch 1:   1%|          | 2/212 [00:00<00:05, 40.89it/s, v_num=0]Training loss: 1.3831565380096436\n",
      "Epoch 1:   1%|▏         | 3/212 [00:00<00:05, 39.71it/s, v_num=0]Training loss: 0.9112370610237122\n",
      "Epoch 1:   2%|▏         | 4/212 [00:00<00:05, 39.15it/s, v_num=0]Training loss: 1.2647672891616821\n",
      "Epoch 1:   2%|▏         | 5/212 [00:00<00:05, 38.81it/s, v_num=0]Training loss: 2.7055277824401855\n",
      "Epoch 1:   3%|▎         | 6/212 [00:00<00:05, 38.61it/s, v_num=0]Training loss: 1.9579702615737915\n",
      "Epoch 1:   3%|▎         | 7/212 [00:00<00:05, 38.46it/s, v_num=0]Training loss: 1.1573188304901123\n",
      "Epoch 1:   4%|▍         | 8/212 [00:00<00:05, 38.07it/s, v_num=0]Training loss: 3.8850767612457275\n",
      "Epoch 1:   4%|▍         | 9/212 [00:00<00:05, 37.82it/s, v_num=0]Training loss: 0.9879122972488403\n",
      "Epoch 1:   5%|▍         | 10/212 [00:00<00:05, 37.65it/s, v_num=0]Training loss: 1.3245441913604736\n",
      "Epoch 1:   5%|▌         | 11/212 [00:00<00:05, 37.51it/s, v_num=0]Training loss: 1.2984435558319092\n",
      "Epoch 1:   6%|▌         | 12/212 [00:00<00:05, 37.39it/s, v_num=0]Training loss: 1.2401891946792603\n",
      "Epoch 1:   6%|▌         | 13/212 [00:00<00:05, 37.22it/s, v_num=0]Training loss: 0.9942031502723694\n",
      "Epoch 1:   7%|▋         | 14/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.545994758605957\n",
      "Epoch 1:   7%|▋         | 15/212 [00:00<00:05, 37.08it/s, v_num=0]Training loss: 1.2700508832931519\n",
      "Epoch 1:   8%|▊         | 16/212 [00:00<00:05, 37.02it/s, v_num=0]Training loss: 1.005725622177124\n",
      "Epoch 1:   8%|▊         | 17/212 [00:00<00:05, 37.06it/s, v_num=0]Training loss: 1.004297137260437\n",
      "Epoch 1:   8%|▊         | 18/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.8523200750350952\n",
      "Epoch 1:   9%|▉         | 19/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.7839146852493286\n",
      "Epoch 1:   9%|▉         | 20/212 [00:00<00:05, 37.16it/s, v_num=0]Training loss: 0.9772765636444092\n",
      "Epoch 1:  10%|▉         | 21/212 [00:00<00:05, 37.17it/s, v_num=0]Training loss: 1.795250654220581\n",
      "Epoch 1:  10%|█         | 22/212 [00:00<00:05, 37.18it/s, v_num=0]Training loss: 1.2776259183883667\n",
      "Epoch 1:  11%|█         | 23/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 1.3371951580047607\n",
      "Epoch 1:  11%|█▏        | 24/212 [00:00<00:05, 37.20it/s, v_num=0]Training loss: 1.4035688638687134\n",
      "Epoch 1:  12%|█▏        | 25/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 1.1890727281570435\n",
      "Epoch 1:  12%|█▏        | 26/212 [00:00<00:04, 37.21it/s, v_num=0]Training loss: 1.4144951105117798\n",
      "Epoch 1:  13%|█▎        | 27/212 [00:00<00:04, 37.20it/s, v_num=0]Training loss: 0.6453545093536377\n",
      "Epoch 1:  13%|█▎        | 28/212 [00:00<00:04, 37.22it/s, v_num=0]Training loss: 1.5236531496047974\n",
      "Epoch 1:  14%|█▎        | 29/212 [00:00<00:04, 37.23it/s, v_num=0]Training loss: 1.5086982250213623\n",
      "Epoch 1:  14%|█▍        | 30/212 [00:00<00:04, 37.24it/s, v_num=0]Training loss: 1.6319520473480225\n",
      "Epoch 1:  15%|█▍        | 31/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 1.3798799514770508\n",
      "Epoch 1:  15%|█▌        | 32/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.0743749141693115\n",
      "Epoch 1:  16%|█▌        | 33/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.3861465454101562\n",
      "Epoch 1:  16%|█▌        | 34/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 1.0171369314193726\n",
      "Epoch 1:  17%|█▋        | 35/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 1.0401508808135986\n",
      "Epoch 1:  17%|█▋        | 36/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 2.0341036319732666\n",
      "Epoch 1:  17%|█▋        | 37/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 2.0865068435668945\n",
      "Epoch 1:  18%|█▊        | 38/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.9675858020782471\n",
      "Epoch 1:  18%|█▊        | 39/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 0.9069480299949646\n",
      "Epoch 1:  19%|█▉        | 40/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.4010437726974487\n",
      "Epoch 1:  19%|█▉        | 41/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.3688559532165527\n",
      "Epoch 1:  20%|█▉        | 42/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.2149074077606201\n",
      "Epoch 1:  20%|██        | 43/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.1498358249664307\n",
      "Epoch 1:  21%|██        | 44/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.2478997707366943\n",
      "Epoch 1:  21%|██        | 45/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.8469594717025757\n",
      "Epoch 1:  22%|██▏       | 46/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.4494743347167969\n",
      "Epoch 1:  22%|██▏       | 47/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.4035301208496094\n",
      "Epoch 1:  23%|██▎       | 48/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.704804539680481\n",
      "Epoch 1:  23%|██▎       | 49/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.9330053329467773\n",
      "Epoch 1:  24%|██▎       | 50/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.3942980766296387\n",
      "Epoch 1:  24%|██▍       | 51/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.120836853981018\n",
      "Epoch 1:  25%|██▍       | 52/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.5993016958236694\n",
      "Epoch 1:  25%|██▌       | 53/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.8497114181518555\n",
      "Epoch 1:  25%|██▌       | 54/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.684387445449829\n",
      "Epoch 1:  26%|██▌       | 55/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 1.773071050643921\n",
      "Epoch 1:  26%|██▋       | 56/212 [00:01<00:04, 37.12it/s, v_num=0]Training loss: 3.9365315437316895\n",
      "Epoch 1:  27%|██▋       | 57/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.5384163856506348\n",
      "Epoch 1:  27%|██▋       | 58/212 [00:01<00:04, 37.14it/s, v_num=0]Training loss: 1.2703489065170288\n",
      "Epoch 1:  28%|██▊       | 59/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 1.3305796384811401\n",
      "Epoch 1:  28%|██▊       | 60/212 [00:01<00:04, 37.15it/s, v_num=0]Training loss: 0.7736465334892273\n",
      "Epoch 1:  29%|██▉       | 61/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.8608702421188354\n",
      "Epoch 1:  29%|██▉       | 62/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.0439260005950928\n",
      "Epoch 1:  30%|██▉       | 63/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 1.5781939029693604\n",
      "Epoch 1:  30%|███       | 64/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 1.5342793464660645\n",
      "Epoch 1:  31%|███       | 65/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 0.9614989161491394\n",
      "Epoch 1:  31%|███       | 66/212 [00:01<00:03, 37.17it/s, v_num=0]Training loss: 1.4410783052444458\n",
      "Epoch 1:  32%|███▏      | 67/212 [00:01<00:03, 37.18it/s, v_num=0]Training loss: 1.2206499576568604\n",
      "Epoch 1:  32%|███▏      | 68/212 [00:01<00:03, 37.18it/s, v_num=0]Training loss: 0.8342791795730591\n",
      "Epoch 1:  33%|███▎      | 69/212 [00:01<00:03, 37.19it/s, v_num=0]Training loss: 1.469076156616211\n",
      "Epoch 1:  33%|███▎      | 70/212 [00:01<00:03, 37.19it/s, v_num=0]Training loss: 1.4289849996566772\n",
      "Epoch 1:  33%|███▎      | 71/212 [00:01<00:03, 37.20it/s, v_num=0]Training loss: 1.2283775806427002\n",
      "Epoch 1:  34%|███▍      | 72/212 [00:01<00:03, 37.19it/s, v_num=0]Training loss: 1.810802698135376\n",
      "Epoch 1:  34%|███▍      | 73/212 [00:01<00:03, 37.20it/s, v_num=0]Training loss: 1.2064515352249146\n",
      "Epoch 1:  35%|███▍      | 74/212 [00:01<00:03, 37.20it/s, v_num=0]Training loss: 1.3946595191955566\n",
      "Epoch 1:  35%|███▌      | 75/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 1.6096760034561157\n",
      "Epoch 1:  36%|███▌      | 76/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.9337698221206665\n",
      "Epoch 1:  36%|███▋      | 77/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 1.4744521379470825\n",
      "Epoch 1:  37%|███▋      | 78/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.109256625175476\n",
      "Epoch 1:  37%|███▋      | 79/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.3794242143630981\n",
      "Epoch 1:  38%|███▊      | 80/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.1205447912216187\n",
      "Epoch 1:  38%|███▊      | 81/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.8289846181869507\n",
      "Epoch 1:  39%|███▊      | 82/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 2.2727537155151367\n",
      "Epoch 1:  39%|███▉      | 83/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.2505004405975342\n",
      "Epoch 1:  40%|███▉      | 84/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.5685333013534546\n",
      "Epoch 1:  40%|████      | 85/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9620494842529297\n",
      "Epoch 1:  41%|████      | 86/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.7819379568099976\n",
      "Epoch 1:  41%|████      | 87/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.971197247505188\n",
      "Epoch 1:  42%|████▏     | 88/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.3983559608459473\n",
      "Epoch 1:  42%|████▏     | 89/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.0555634498596191\n",
      "Epoch 1:  42%|████▏     | 90/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 1.5257333517074585\n",
      "Epoch 1:  43%|████▎     | 91/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.0127692222595215\n",
      "Epoch 1:  43%|████▎     | 92/212 [00:02<00:03, 37.18it/s, v_num=0]Training loss: 1.1191010475158691\n",
      "Epoch 1:  44%|████▍     | 93/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.3190314769744873\n",
      "Epoch 1:  44%|████▍     | 94/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.5090668201446533\n",
      "Epoch 1:  45%|████▍     | 95/212 [00:02<00:03, 37.14it/s, v_num=0]Training loss: 1.5651077032089233\n",
      "Epoch 1:  45%|████▌     | 96/212 [00:02<00:03, 37.15it/s, v_num=0]Training loss: 1.1126270294189453\n",
      "Epoch 1:  46%|████▌     | 97/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.2719922065734863\n",
      "Epoch 1:  46%|████▌     | 98/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 1.7623894214630127\n",
      "Epoch 1:  47%|████▋     | 99/212 [00:02<00:03, 37.16it/s, v_num=0]Training loss: 2.3139209747314453\n",
      "Epoch 1:  47%|████▋     | 100/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 1.120133876800537\n",
      "Epoch 1:  48%|████▊     | 101/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 1.1516385078430176\n",
      "Epoch 1:  48%|████▊     | 102/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 1.1617090702056885\n",
      "Epoch 1:  49%|████▊     | 103/212 [00:02<00:02, 37.17it/s, v_num=0]Training loss: 1.6877857446670532\n",
      "Epoch 1:  49%|████▉     | 104/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.0808511972427368\n",
      "Epoch 1:  50%|████▉     | 105/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.025264024734497\n",
      "Epoch 1:  50%|█████     | 106/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.1209315061569214\n",
      "Epoch 1:  50%|█████     | 107/212 [00:02<00:02, 37.18it/s, v_num=0]Training loss: 1.650784969329834\n",
      "Epoch 1:  51%|█████     | 108/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 2.3685224056243896\n",
      "Epoch 1:  51%|█████▏    | 109/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 1.3825922012329102\n",
      "Epoch 1:  52%|█████▏    | 110/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 10.947958946228027\n",
      "Epoch 1:  52%|█████▏    | 111/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 1.7960846424102783\n",
      "Epoch 1:  53%|█████▎    | 112/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 0.8197438716888428\n",
      "Epoch 1:  53%|█████▎    | 113/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.5377986431121826\n",
      "Epoch 1:  54%|█████▍    | 114/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.504333257675171\n",
      "Epoch 1:  54%|█████▍    | 115/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.4801127910614014\n",
      "Epoch 1:  55%|█████▍    | 116/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.5185617208480835\n",
      "Epoch 1:  55%|█████▌    | 117/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.3712232112884521\n",
      "Epoch 1:  56%|█████▌    | 118/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.208399772644043\n",
      "Epoch 1:  56%|█████▌    | 119/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.0172038078308105\n",
      "Epoch 1:  57%|█████▋    | 120/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.4196031093597412\n",
      "Epoch 1:  57%|█████▋    | 121/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 2.0432703495025635\n",
      "Epoch 1:  58%|█████▊    | 122/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 2.1467108726501465\n",
      "Epoch 1:  58%|█████▊    | 123/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 2.1432762145996094\n",
      "Epoch 1:  58%|█████▊    | 124/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.8024113178253174\n",
      "Epoch 1:  59%|█████▉    | 125/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.8158848285675049\n",
      "Epoch 1:  59%|█████▉    | 126/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.4970351457595825\n",
      "Epoch 1:  60%|█████▉    | 127/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.4563149213790894\n",
      "Epoch 1:  60%|██████    | 128/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.9919178485870361\n",
      "Epoch 1:  61%|██████    | 129/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.6183100938796997\n",
      "Epoch 1:  61%|██████▏   | 130/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.2560245990753174\n",
      "Epoch 1:  62%|██████▏   | 131/212 [00:03<00:02, 37.18it/s, v_num=0]Training loss: 1.3558526039123535\n",
      "Epoch 1:  62%|██████▏   | 132/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 1.0795114040374756\n",
      "Epoch 1:  63%|██████▎   | 133/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 1.057361125946045\n",
      "Epoch 1:  63%|██████▎   | 134/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 1.243822693824768\n",
      "Epoch 1:  64%|██████▎   | 135/212 [00:03<00:02, 37.14it/s, v_num=0]Training loss: 0.9725974202156067\n",
      "Epoch 1:  64%|██████▍   | 136/212 [00:03<00:02, 37.14it/s, v_num=0]Training loss: 1.0454124212265015\n",
      "Epoch 1:  65%|██████▍   | 137/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 0.7785192728042603\n",
      "Epoch 1:  65%|██████▌   | 138/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 1.0019960403442383\n",
      "Epoch 1:  66%|██████▌   | 139/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 0.8508005142211914\n",
      "Epoch 1:  66%|██████▌   | 140/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.3195455074310303\n",
      "Epoch 1:  67%|██████▋   | 141/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 9.947202682495117\n",
      "Epoch 1:  67%|██████▋   | 142/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.0824456214904785\n",
      "Epoch 1:  67%|██████▋   | 143/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 0.8631409406661987\n",
      "Epoch 1:  68%|██████▊   | 144/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.0677692890167236\n",
      "Epoch 1:  68%|██████▊   | 145/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.596278429031372\n",
      "Epoch 1:  69%|██████▉   | 146/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.354150652885437\n",
      "Epoch 1:  69%|██████▉   | 147/212 [00:03<00:01, 37.17it/s, v_num=0]Training loss: 1.1889210939407349\n",
      "Epoch 1:  70%|██████▉   | 148/212 [00:03<00:01, 37.17it/s, v_num=0]Training loss: 0.9885622262954712\n",
      "Epoch 1:  70%|███████   | 149/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.3115222454071045\n",
      "Epoch 1:  71%|███████   | 150/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.0063059329986572\n",
      "Epoch 1:  71%|███████   | 151/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.0528455972671509\n",
      "Epoch 1:  72%|███████▏  | 152/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 0.9345411658287048\n",
      "Epoch 1:  72%|███████▏  | 153/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.520749807357788\n",
      "Epoch 1:  73%|███████▎  | 154/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.12509286403656\n",
      "Epoch 1:  73%|███████▎  | 155/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.2860491275787354\n",
      "Epoch 1:  74%|███████▎  | 156/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.9063570499420166\n",
      "Epoch 1:  74%|███████▍  | 157/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.8057142496109009\n",
      "Epoch 1:  75%|███████▍  | 158/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.1104092597961426\n",
      "Epoch 1:  75%|███████▌  | 159/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 1.104136347770691\n",
      "Epoch 1:  75%|███████▌  | 160/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.8445363640785217\n",
      "Epoch 1:  76%|███████▌  | 161/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.2754194736480713\n",
      "Epoch 1:  76%|███████▋  | 162/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.9694161415100098\n",
      "Epoch 1:  77%|███████▋  | 163/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.2134885787963867\n",
      "Epoch 1:  77%|███████▋  | 164/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.048475742340088\n",
      "Epoch 1:  78%|███████▊  | 165/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 0.8961797952651978\n",
      "Epoch 1:  78%|███████▊  | 166/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.225644826889038\n",
      "Epoch 1:  79%|███████▉  | 167/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.062246561050415\n",
      "Epoch 1:  79%|███████▉  | 168/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.7090433835983276\n",
      "Epoch 1:  80%|███████▉  | 169/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 1.1113481521606445\n",
      "Epoch 1:  80%|████████  | 170/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 1.2644169330596924\n",
      "Epoch 1:  81%|████████  | 171/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 1.4490195512771606\n",
      "Epoch 1:  81%|████████  | 172/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 1.0388370752334595\n",
      "Epoch 1:  82%|████████▏ | 173/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.508321762084961\n",
      "Epoch 1:  82%|████████▏ | 174/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.2654017210006714\n",
      "Epoch 1:  83%|████████▎ | 175/212 [00:04<00:00, 37.13it/s, v_num=0]Training loss: 1.1186938285827637\n",
      "Epoch 1:  83%|████████▎ | 176/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.4515711069107056\n",
      "Epoch 1:  83%|████████▎ | 177/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 2.3530964851379395\n",
      "Epoch 1:  84%|████████▍ | 178/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.3121789693832397\n",
      "Epoch 1:  84%|████████▍ | 179/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.1240694522857666\n",
      "Epoch 1:  85%|████████▍ | 180/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.3023637533187866\n",
      "Epoch 1:  85%|████████▌ | 181/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.5197603702545166\n",
      "Epoch 1:  86%|████████▌ | 182/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.2347133159637451\n",
      "Epoch 1:  86%|████████▋ | 183/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.0271714925765991\n",
      "Epoch 1:  87%|████████▋ | 184/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 3.2056331634521484\n",
      "Epoch 1:  87%|████████▋ | 185/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.4735863208770752\n",
      "Epoch 1:  88%|████████▊ | 186/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.1877975463867188\n",
      "Epoch 1:  88%|████████▊ | 187/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.3889787197113037\n",
      "Epoch 1:  89%|████████▊ | 188/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.5592097043991089\n",
      "Epoch 1:  89%|████████▉ | 189/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 0.9737346172332764\n",
      "Epoch 1:  90%|████████▉ | 190/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 0.8906614780426025\n",
      "Epoch 1:  90%|█████████ | 191/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.001811146736145\n",
      "Epoch 1:  91%|█████████ | 192/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.1584646701812744\n",
      "Epoch 1:  91%|█████████ | 193/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.1639610528945923\n",
      "Epoch 1:  92%|█████████▏| 194/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.0530939102172852\n",
      "Epoch 1:  92%|█████████▏| 195/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.5073055028915405\n",
      "Epoch 1:  92%|█████████▏| 196/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.330138087272644\n",
      "Epoch 1:  93%|█████████▎| 197/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.5894383192062378\n",
      "Epoch 1:  93%|█████████▎| 198/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.7420812845230103\n",
      "Epoch 1:  94%|█████████▍| 199/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.423370122909546\n",
      "Epoch 1:  94%|█████████▍| 200/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 2.7063443660736084\n",
      "Epoch 1:  95%|█████████▍| 201/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 1.3620017766952515\n",
      "Epoch 1:  95%|█████████▌| 202/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 1.0619741678237915\n",
      "Epoch 1:  96%|█████████▌| 203/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 1.0276437997817993\n",
      "Epoch 1:  96%|█████████▌| 204/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 2.1190266609191895\n",
      "Epoch 1:  97%|█████████▋| 205/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.8631141185760498\n",
      "Epoch 1:  97%|█████████▋| 206/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.8739111423492432\n",
      "Epoch 1:  98%|█████████▊| 207/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 1.9059712886810303\n",
      "Epoch 1:  98%|█████████▊| 208/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.051034927368164\n",
      "Epoch 1:  99%|█████████▊| 209/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 0.8749833106994629\n",
      "Epoch 1:  99%|█████████▉| 210/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.3316185474395752\n",
      "Epoch 1: 100%|█████████▉| 211/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 0.800148606300354\n",
      "Epoch 1: 100%|██████████| 212/212 [00:05<00:00, 37.15it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.3669278621673584\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 90.80it/s]\u001b[AValidation loss: 2.1432955265045166\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.83it/s]\u001b[AValidation loss: 1.1750714778900146\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 90.89it/s]\u001b[AValidation loss: 0.8775457143783569\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 92.54it/s]\u001b[AValidation loss: 0.8904790878295898\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 93.96it/s]\u001b[AValidation loss: 1.068639874458313\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 95.14it/s]\u001b[AValidation loss: 1.4247487783432007\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 94.63it/s]\u001b[AValidation loss: 2.4292075634002686\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 95.62it/s]\u001b[AValidation loss: 0.7842376232147217\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 96.61it/s]\u001b[AValidation loss: 0.9006727933883667\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 97.29it/s]\u001b[AValidation loss: 0.9289933443069458\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 97.90it/s]\u001b[AValidation loss: 1.2474737167358398\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 98.41it/s]\u001b[AValidation loss: 1.2116185426712036\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 98.72it/s]\u001b[AValidation loss: 1.1578912734985352\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 98.97it/s]\u001b[AValidation loss: 0.9588086009025574\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 99.25it/s]\u001b[AValidation loss: 1.3282335996627808\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 99.47it/s]\u001b[AValidation loss: 1.2204549312591553\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 99.74it/s]\u001b[AValidation loss: 1.7664761543273926\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 99.93it/s]\u001b[AValidation loss: 0.9273086190223694\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 100.11it/s]\u001b[AValidation loss: 1.4213733673095703\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 100.06it/s]\u001b[AValidation loss: 1.2823351621627808\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 100.17it/s]\u001b[AValidation loss: 1.3196114301681519\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 100.29it/s]\u001b[AValidation loss: 1.3010072708129883\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 100.40it/s]\u001b[AValidation loss: 1.157528281211853\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 100.41it/s]\u001b[AValidation loss: 1.6117985248565674\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 100.50it/s]\u001b[AValidation loss: 0.9567908644676208\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 100.57it/s]\u001b[AValidation loss: 1.1853129863739014\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 100.66it/s]\u001b[AValidation loss: 1.2119776010513306\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 100.71it/s]\u001b[AValidation loss: 1.2061080932617188\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 100.77it/s]\u001b[AValidation loss: 1.449784278869629\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 100.80it/s]\u001b[AValidation loss: 1.0715922117233276\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 100.81it/s]\u001b[AValidation loss: 1.1487482786178589\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 100.75it/s]\u001b[AValidation loss: 1.1363306045532227\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 100.78it/s]\u001b[AValidation loss: 1.0348701477050781\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 100.82it/s]\u001b[AValidation loss: 2.1333742141723633\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 100.84it/s]\u001b[AValidation loss: 0.7721307873725891\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 100.86it/s]\u001b[AValidation loss: 1.7222014665603638\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 100.79it/s]\u001b[AValidation loss: 0.8237683773040771\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 100.86it/s]\u001b[AValidation loss: 1.2631603479385376\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 100.92it/s]\u001b[AValidation loss: 1.1550874710083008\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 100.96it/s]\u001b[AValidation loss: 1.0288225412368774\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 101.04it/s]\u001b[AValidation loss: 0.9143980145454407\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 101.00it/s]\u001b[AValidation loss: 1.0185425281524658\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 101.07it/s]\u001b[AValidation loss: 1.4874446392059326\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 101.12it/s]\u001b[AValidation loss: 1.6498578786849976\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 101.17it/s]\u001b[AValidation loss: 0.8955371379852295\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 101.22it/s]\u001b[AValidation loss: 1.17276930809021\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 101.28it/s]\u001b[AValidation loss: 1.1608227491378784\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 101.33it/s]\u001b[AValidation loss: 1.297448992729187\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 101.37it/s]\u001b[AValidation loss: 1.5995066165924072\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 101.41it/s]\u001b[AValidation loss: 1.452290654182434\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 101.46it/s]\u001b[AValidation loss: 1.2780009508132935\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 101.50it/s]\u001b[AValidation loss: 1.6076123714447021\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 101.54it/s]\u001b[AValidation loss: 1.2720680236816406\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 101.56it/s]\u001b[AValidation loss: 2.3458447456359863\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 101.73it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]                \u001b[ATraining loss: 2.532712459564209\n",
      "Epoch 2:   0%|          | 1/212 [00:00<00:04, 44.17it/s, v_num=0]Training loss: 1.1813163757324219\n",
      "Epoch 2:   1%|          | 2/212 [00:00<00:05, 40.60it/s, v_num=0]Training loss: 1.069854736328125\n",
      "Epoch 2:   1%|▏         | 3/212 [00:00<00:05, 39.53it/s, v_num=0]Training loss: 0.9573714733123779\n",
      "Epoch 2:   2%|▏         | 4/212 [00:00<00:05, 38.99it/s, v_num=0]Training loss: 1.5521125793457031\n",
      "Epoch 2:   2%|▏         | 5/212 [00:00<00:05, 38.71it/s, v_num=0]Training loss: 10.471504211425781\n",
      "Epoch 2:   3%|▎         | 6/212 [00:00<00:05, 38.53it/s, v_num=0]Training loss: 1.343423843383789\n",
      "Epoch 2:   3%|▎         | 7/212 [00:00<00:05, 38.36it/s, v_num=0]Training loss: 1.2866153717041016\n",
      "Epoch 2:   4%|▍         | 8/212 [00:00<00:05, 38.14it/s, v_num=0]Training loss: 1.5227779150009155\n",
      "Epoch 2:   4%|▍         | 9/212 [00:00<00:05, 38.07it/s, v_num=0]Training loss: 0.9196474552154541\n",
      "Epoch 2:   5%|▍         | 10/212 [00:00<00:05, 38.00it/s, v_num=0]Training loss: 1.0190088748931885\n",
      "Epoch 2:   5%|▌         | 11/212 [00:00<00:05, 37.96it/s, v_num=0]Training loss: 0.775590181350708\n",
      "Epoch 2:   6%|▌         | 12/212 [00:00<00:05, 37.93it/s, v_num=0]Training loss: 1.267570972442627\n",
      "Epoch 2:   6%|▌         | 13/212 [00:00<00:05, 37.90it/s, v_num=0]Training loss: 1.0358264446258545\n",
      "Epoch 2:   7%|▋         | 14/212 [00:00<00:05, 37.88it/s, v_num=0]Training loss: 1.3069934844970703\n",
      "Epoch 2:   7%|▋         | 15/212 [00:00<00:05, 37.86it/s, v_num=0]Training loss: 1.1910910606384277\n",
      "Epoch 2:   8%|▊         | 16/212 [00:00<00:05, 37.84it/s, v_num=0]Training loss: 1.4022464752197266\n",
      "Epoch 2:   8%|▊         | 17/212 [00:00<00:05, 37.82it/s, v_num=0]Training loss: 1.243951439857483\n",
      "Epoch 2:   8%|▊         | 18/212 [00:00<00:05, 37.80it/s, v_num=0]Training loss: 1.5404802560806274\n",
      "Epoch 2:   9%|▉         | 19/212 [00:00<00:05, 37.78it/s, v_num=0]Training loss: 1.2152830362319946\n",
      "Epoch 2:   9%|▉         | 20/212 [00:00<00:05, 37.78it/s, v_num=0]Training loss: 3.506024122238159\n",
      "Epoch 2:  10%|▉         | 21/212 [00:00<00:05, 37.68it/s, v_num=0]Training loss: 1.3937642574310303\n",
      "Epoch 2:  10%|█         | 22/212 [00:00<00:05, 37.62it/s, v_num=0]Training loss: 1.390988826751709\n",
      "Epoch 2:  11%|█         | 23/212 [00:00<00:05, 37.55it/s, v_num=0]Training loss: 1.8768391609191895\n",
      "Epoch 2:  11%|█▏        | 24/212 [00:00<00:05, 37.49it/s, v_num=0]Training loss: 2.493424892425537\n",
      "Epoch 2:  12%|█▏        | 25/212 [00:00<00:04, 37.44it/s, v_num=0]Training loss: 0.930122971534729\n",
      "Epoch 2:  12%|█▏        | 26/212 [00:00<00:04, 37.39it/s, v_num=0]Training loss: 1.4622247219085693\n",
      "Epoch 2:  13%|█▎        | 27/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.1244882345199585\n",
      "Epoch 2:  13%|█▎        | 28/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.035164713859558\n",
      "Epoch 2:  14%|█▎        | 29/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 1.4757322072982788\n",
      "Epoch 2:  14%|█▍        | 30/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 1.1839239597320557\n",
      "Epoch 2:  15%|█▍        | 31/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 1.5637619495391846\n",
      "Epoch 2:  15%|█▌        | 32/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 1.3307198286056519\n",
      "Epoch 2:  16%|█▌        | 33/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 0.9986559748649597\n",
      "Epoch 2:  16%|█▌        | 34/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 0.97402024269104\n",
      "Epoch 2:  17%|█▋        | 35/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.3296349048614502\n",
      "Epoch 2:  17%|█▋        | 36/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 0.9976638555526733\n",
      "Epoch 2:  17%|█▋        | 37/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 1.0469504594802856\n",
      "Epoch 2:  18%|█▊        | 38/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.9912303686141968\n",
      "Epoch 2:  18%|█▊        | 39/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.8469821810722351\n",
      "Epoch 2:  19%|█▉        | 40/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.3341606855392456\n",
      "Epoch 2:  19%|█▉        | 41/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 2.3340353965759277\n",
      "Epoch 2:  20%|█▉        | 42/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.9613913297653198\n",
      "Epoch 2:  20%|██        | 43/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.1449344158172607\n",
      "Epoch 2:  21%|██        | 44/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.1704542636871338\n",
      "Epoch 2:  21%|██        | 45/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.314745306968689\n",
      "Epoch 2:  22%|██▏       | 46/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.8237088918685913\n",
      "Epoch 2:  22%|██▏       | 47/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.9416931867599487\n",
      "Epoch 2:  23%|██▎       | 48/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.4951238632202148\n",
      "Epoch 2:  23%|██▎       | 49/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.42181396484375\n",
      "Epoch 2:  24%|██▎       | 50/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.1608563661575317\n",
      "Epoch 2:  24%|██▍       | 51/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.0503106117248535\n",
      "Epoch 2:  25%|██▍       | 52/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.3529102802276611\n",
      "Epoch 2:  25%|██▌       | 53/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.0611212253570557\n",
      "Epoch 2:  25%|██▌       | 54/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.9754918217658997\n",
      "Epoch 2:  26%|██▌       | 55/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.542248249053955\n",
      "Epoch 2:  26%|██▋       | 56/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.228294849395752\n",
      "Epoch 2:  27%|██▋       | 57/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.2892504930496216\n",
      "Epoch 2:  27%|██▋       | 58/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 2.04066801071167\n",
      "Epoch 2:  28%|██▊       | 59/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.5252925157546997\n",
      "Epoch 2:  28%|██▊       | 60/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.9545207619667053\n",
      "Epoch 2:  29%|██▉       | 61/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.4503552913665771\n",
      "Epoch 2:  29%|██▉       | 62/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 4.380720138549805\n",
      "Epoch 2:  30%|██▉       | 63/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 1.8536317348480225\n",
      "Epoch 2:  30%|███       | 64/212 [00:01<00:03, 37.29it/s, v_num=0]Training loss: 1.3917858600616455\n",
      "Epoch 2:  31%|███       | 65/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 1.4071077108383179\n",
      "Epoch 2:  31%|███       | 66/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.1048815250396729\n",
      "Epoch 2:  32%|███▏      | 67/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.1033837795257568\n",
      "Epoch 2:  32%|███▏      | 68/212 [00:01<00:03, 37.21it/s, v_num=0]Training loss: 1.5883320569992065\n",
      "Epoch 2:  33%|███▎      | 69/212 [00:01<00:03, 37.21it/s, v_num=0]Training loss: 1.0780630111694336\n",
      "Epoch 2:  33%|███▎      | 70/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.4110356569290161\n",
      "Epoch 2:  33%|███▎      | 71/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.9046319127082825\n",
      "Epoch 2:  34%|███▍      | 72/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.8428822755813599\n",
      "Epoch 2:  34%|███▍      | 73/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 0.9135535955429077\n",
      "Epoch 2:  35%|███▍      | 74/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.1309151649475098\n",
      "Epoch 2:  35%|███▌      | 75/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.8092098236083984\n",
      "Epoch 2:  36%|███▌      | 76/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.6855974197387695\n",
      "Epoch 2:  36%|███▋      | 77/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9748530983924866\n",
      "Epoch 2:  37%|███▋      | 78/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.1876351833343506\n",
      "Epoch 2:  37%|███▋      | 79/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9349423050880432\n",
      "Epoch 2:  38%|███▊      | 80/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.2023879289627075\n",
      "Epoch 2:  38%|███▊      | 81/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.2950143814086914\n",
      "Epoch 2:  39%|███▊      | 82/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.2944421768188477\n",
      "Epoch 2:  39%|███▉      | 83/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9764039516448975\n",
      "Epoch 2:  40%|███▉      | 84/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.7383791208267212\n",
      "Epoch 2:  40%|████      | 85/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9461631774902344\n",
      "Epoch 2:  41%|████      | 86/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.497352123260498\n",
      "Epoch 2:  41%|████      | 87/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.0280790328979492\n",
      "Epoch 2:  42%|████▏     | 88/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.5136303901672363\n",
      "Epoch 2:  42%|████▏     | 89/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.1158349514007568\n",
      "Epoch 2:  42%|████▏     | 90/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 2.356637716293335\n",
      "Epoch 2:  43%|████▎     | 91/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.542773962020874\n",
      "Epoch 2:  43%|████▎     | 92/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.6885389089584351\n",
      "Epoch 2:  44%|████▍     | 93/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.7944769263267517\n",
      "Epoch 2:  44%|████▍     | 94/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.0034571886062622\n",
      "Epoch 2:  45%|████▍     | 95/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.7358649969100952\n",
      "Epoch 2:  45%|████▌     | 96/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.1341402530670166\n",
      "Epoch 2:  46%|████▌     | 97/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.480272889137268\n",
      "Epoch 2:  46%|████▌     | 98/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.9224223494529724\n",
      "Epoch 2:  47%|████▋     | 99/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.4278409481048584\n",
      "Epoch 2:  47%|████▋     | 100/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.111997365951538\n",
      "Epoch 2:  48%|████▊     | 101/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.1530592441558838\n",
      "Epoch 2:  48%|████▊     | 102/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.5400409698486328\n",
      "Epoch 2:  49%|████▊     | 103/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.3346763849258423\n",
      "Epoch 2:  49%|████▉     | 104/212 [00:02<00:02, 37.26it/s, v_num=0]Training loss: 9.352526664733887\n",
      "Epoch 2:  50%|████▉     | 105/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.758138120174408\n",
      "Epoch 2:  50%|█████     | 106/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.8014412522315979\n",
      "Epoch 2:  50%|█████     | 107/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.3901830911636353\n",
      "Epoch 2:  51%|█████     | 108/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 1.0876725912094116\n",
      "Epoch 2:  51%|█████▏    | 109/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 1.3386836051940918\n",
      "Epoch 2:  52%|█████▏    | 110/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.585202693939209\n",
      "Epoch 2:  52%|█████▏    | 111/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.4353400468826294\n",
      "Epoch 2:  53%|█████▎    | 112/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.546730875968933\n",
      "Epoch 2:  53%|█████▎    | 113/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.0815556049346924\n",
      "Epoch 2:  54%|█████▍    | 114/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.2472466230392456\n",
      "Epoch 2:  54%|█████▍    | 115/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.072744369506836\n",
      "Epoch 2:  55%|█████▍    | 116/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.6326572895050049\n",
      "Epoch 2:  55%|█████▌    | 117/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.3272640705108643\n",
      "Epoch 2:  56%|█████▌    | 118/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1900206804275513\n",
      "Epoch 2:  56%|█████▌    | 119/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1590228080749512\n",
      "Epoch 2:  57%|█████▋    | 120/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.5966657400131226\n",
      "Epoch 2:  57%|█████▋    | 121/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.6868586540222168\n",
      "Epoch 2:  58%|█████▊    | 122/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.153546929359436\n",
      "Epoch 2:  58%|█████▊    | 123/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.341933012008667\n",
      "Epoch 2:  58%|█████▊    | 124/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.6218671798706055\n",
      "Epoch 2:  59%|█████▉    | 125/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 2.130051612854004\n",
      "Epoch 2:  59%|█████▉    | 126/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.7963231801986694\n",
      "Epoch 2:  60%|█████▉    | 127/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9117566347122192\n",
      "Epoch 2:  60%|██████    | 128/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9257934093475342\n",
      "Epoch 2:  61%|██████    | 129/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.30781888961792\n",
      "Epoch 2:  61%|██████▏   | 130/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0593758821487427\n",
      "Epoch 2:  62%|██████▏   | 131/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0522022247314453\n",
      "Epoch 2:  62%|██████▏   | 132/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.5486949682235718\n",
      "Epoch 2:  63%|██████▎   | 133/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8844362497329712\n",
      "Epoch 2:  63%|██████▎   | 134/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.1834118366241455\n",
      "Epoch 2:  64%|██████▎   | 135/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0463132858276367\n",
      "Epoch 2:  64%|██████▍   | 136/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.1344170570373535\n",
      "Epoch 2:  65%|██████▍   | 137/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.9152127504348755\n",
      "Epoch 2:  65%|██████▌   | 138/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 0.7718577980995178\n",
      "Epoch 2:  66%|██████▌   | 139/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 1.9629428386688232\n",
      "Epoch 2:  66%|██████▌   | 140/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 1.1289212703704834\n",
      "Epoch 2:  67%|██████▋   | 141/212 [00:03<00:01, 37.26it/s, v_num=0]Training loss: 0.8059021830558777\n",
      "Epoch 2:  67%|██████▋   | 142/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.8670042753219604\n",
      "Epoch 2:  67%|██████▋   | 143/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.8410772085189819\n",
      "Epoch 2:  68%|██████▊   | 144/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 1.0915840864181519\n",
      "Epoch 2:  68%|██████▊   | 145/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 2.997886896133423\n",
      "Epoch 2:  69%|██████▉   | 146/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 1.1056941747665405\n",
      "Epoch 2:  69%|██████▉   | 147/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.1543229818344116\n",
      "Epoch 2:  70%|██████▉   | 148/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.253432035446167\n",
      "Epoch 2:  70%|███████   | 149/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.1793614625930786\n",
      "Epoch 2:  71%|███████   | 150/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0606411695480347\n",
      "Epoch 2:  71%|███████   | 151/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.3501195907592773\n",
      "Epoch 2:  72%|███████▏  | 152/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0172086954116821\n",
      "Epoch 2:  72%|███████▏  | 153/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.9465864300727844\n",
      "Epoch 2:  73%|███████▎  | 154/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0188229084014893\n",
      "Epoch 2:  73%|███████▎  | 155/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.7118924856185913\n",
      "Epoch 2:  74%|███████▎  | 156/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.1079195737838745\n",
      "Epoch 2:  74%|███████▍  | 157/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.633533239364624\n",
      "Epoch 2:  75%|███████▍  | 158/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.023618459701538\n",
      "Epoch 2:  75%|███████▌  | 159/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.9604982137680054\n",
      "Epoch 2:  75%|███████▌  | 160/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.6003609895706177\n",
      "Epoch 2:  76%|███████▌  | 161/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.3763254880905151\n",
      "Epoch 2:  76%|███████▋  | 162/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.0098991394042969\n",
      "Epoch 2:  77%|███████▋  | 163/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.8131165504455566\n",
      "Epoch 2:  77%|███████▋  | 164/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.3174662590026855\n",
      "Epoch 2:  78%|███████▊  | 165/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8081801533699036\n",
      "Epoch 2:  78%|███████▊  | 166/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.023425817489624\n",
      "Epoch 2:  79%|███████▉  | 167/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.9870867729187012\n",
      "Epoch 2:  79%|███████▉  | 168/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8829063177108765\n",
      "Epoch 2:  80%|███████▉  | 169/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.3901216983795166\n",
      "Epoch 2:  80%|████████  | 170/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.6475343704223633\n",
      "Epoch 2:  81%|████████  | 171/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8338133692741394\n",
      "Epoch 2:  81%|████████  | 172/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.1091294288635254\n",
      "Epoch 2:  82%|████████▏ | 173/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9254798889160156\n",
      "Epoch 2:  82%|████████▏ | 174/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.4329583644866943\n",
      "Epoch 2:  83%|████████▎ | 175/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.0404890775680542\n",
      "Epoch 2:  83%|████████▎ | 176/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.0648736953735352\n",
      "Epoch 2:  83%|████████▎ | 177/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.8866969347000122\n",
      "Epoch 2:  84%|████████▍ | 178/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.2442169189453125\n",
      "Epoch 2:  84%|████████▍ | 179/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 2.3354909420013428\n",
      "Epoch 2:  85%|████████▍ | 180/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.9855588674545288\n",
      "Epoch 2:  85%|████████▌ | 181/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.9749745726585388\n",
      "Epoch 2:  86%|████████▌ | 182/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 1.777501106262207\n",
      "Epoch 2:  86%|████████▋ | 183/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.1291872262954712\n",
      "Epoch 2:  87%|████████▋ | 184/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.404055118560791\n",
      "Epoch 2:  87%|████████▋ | 185/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.8753134608268738\n",
      "Epoch 2:  88%|████████▊ | 186/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 1.1684900522232056\n",
      "Epoch 2:  88%|████████▊ | 187/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.3037272691726685\n",
      "Epoch 2:  89%|████████▊ | 188/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.0251998901367188\n",
      "Epoch 2:  89%|████████▉ | 189/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 2.07832932472229\n",
      "Epoch 2:  90%|████████▉ | 190/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.4028677940368652\n",
      "Epoch 2:  90%|█████████ | 191/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.0476438999176025\n",
      "Epoch 2:  91%|█████████ | 192/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.4699523448944092\n",
      "Epoch 2:  91%|█████████ | 193/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9423801898956299\n",
      "Epoch 2:  92%|█████████▏| 194/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.137622594833374\n",
      "Epoch 2:  92%|█████████▏| 195/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.0855563879013062\n",
      "Epoch 2:  92%|█████████▏| 196/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.0938724279403687\n",
      "Epoch 2:  93%|█████████▎| 197/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9574631452560425\n",
      "Epoch 2:  93%|█████████▎| 198/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.7986611127853394\n",
      "Epoch 2:  94%|█████████▍| 199/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.1051788330078125\n",
      "Epoch 2:  94%|█████████▍| 200/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.5118412971496582\n",
      "Epoch 2:  95%|█████████▍| 201/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.88254314661026\n",
      "Epoch 2:  95%|█████████▌| 202/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8431664109230042\n",
      "Epoch 2:  96%|█████████▌| 203/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.3333699703216553\n",
      "Epoch 2:  96%|█████████▌| 204/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.9354405403137207\n",
      "Epoch 2:  97%|█████████▋| 205/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.8642231225967407\n",
      "Epoch 2:  97%|█████████▋| 206/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.413520097732544\n",
      "Epoch 2:  98%|█████████▊| 207/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.5964548587799072\n",
      "Epoch 2:  98%|█████████▊| 208/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.7999656200408936\n",
      "Epoch 2:  99%|█████████▊| 209/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.804454505443573\n",
      "Epoch 2:  99%|█████████▉| 210/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.9505674839019775\n",
      "Epoch 2: 100%|█████████▉| 211/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.4470635652542114\n",
      "Epoch 2: 100%|██████████| 212/212 [00:05<00:00, 37.20it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.3121912479400635\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 81.86it/s]\u001b[AValidation loss: 2.0992960929870605\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.56it/s]\u001b[AValidation loss: 1.1364561319351196\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 94.76it/s]\u001b[AValidation loss: 0.8446049094200134\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 96.14it/s]\u001b[AValidation loss: 0.8564379215240479\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 96.89it/s]\u001b[AValidation loss: 1.0333683490753174\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 98.12it/s]\u001b[AValidation loss: 1.3885856866836548\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 98.99it/s]\u001b[AValidation loss: 2.377591133117676\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 98.85it/s]\u001b[AValidation loss: 0.7535597681999207\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 99.41it/s]\u001b[AValidation loss: 0.8691994547843933\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 99.98it/s]\u001b[AValidation loss: 0.8938342928886414\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 100.42it/s]\u001b[AValidation loss: 1.2104899883270264\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 100.78it/s]\u001b[AValidation loss: 1.1762067079544067\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 101.05it/s]\u001b[AValidation loss: 1.1197181940078735\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 101.36it/s]\u001b[AValidation loss: 0.9270011782646179\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 101.63it/s]\u001b[AValidation loss: 1.2908344268798828\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 101.85it/s]\u001b[AValidation loss: 1.185376763343811\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 102.03it/s]\u001b[AValidation loss: 1.7245159149169922\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.09it/s]\u001b[AValidation loss: 0.8945779204368591\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 101.46it/s]\u001b[AValidation loss: 1.3855924606323242\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 101.21it/s]\u001b[AValidation loss: 1.2465463876724243\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 100.84it/s]\u001b[AValidation loss: 1.2831610441207886\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 100.58it/s]\u001b[AValidation loss: 1.2640098333358765\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 100.41it/s]\u001b[AValidation loss: 1.121160864830017\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 100.24it/s]\u001b[AValidation loss: 1.5722254514694214\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 100.09it/s]\u001b[AValidation loss: 0.9216959476470947\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 99.92it/s] \u001b[AValidation loss: 1.1479511260986328\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 99.33it/s]\u001b[AValidation loss: 1.178372859954834\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 98.71it/s]\u001b[AValidation loss: 1.1704908609390259\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 98.66it/s]\u001b[AValidation loss: 1.414011001586914\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 98.55it/s]\u001b[AValidation loss: 1.0357297658920288\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 98.44it/s]\u001b[AValidation loss: 1.1138553619384766\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 98.34it/s]\u001b[AValidation loss: 1.1002925634384155\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 98.27it/s]\u001b[AValidation loss: 0.9979069232940674\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 98.21it/s]\u001b[AValidation loss: 2.085861921310425\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 98.14it/s]\u001b[AValidation loss: 0.7404505014419556\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 97.87it/s]\u001b[AValidation loss: 1.6822996139526367\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 97.57it/s]\u001b[AValidation loss: 0.7934777736663818\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 97.37it/s]\u001b[AValidation loss: 1.2247494459152222\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 97.34it/s]\u001b[AValidation loss: 1.1178109645843506\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 97.39it/s]\u001b[AValidation loss: 0.9932215809822083\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 97.35it/s]\u001b[AValidation loss: 0.8816579580307007\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 97.29it/s]\u001b[AValidation loss: 0.9825664758682251\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 97.27it/s]\u001b[AValidation loss: 1.4522929191589355\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 97.25it/s]\u001b[AValidation loss: 1.6090519428253174\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 97.24it/s]\u001b[AValidation loss: 0.863491415977478\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 97.39it/s]\u001b[AValidation loss: 1.1369998455047607\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 97.54it/s]\u001b[AValidation loss: 1.1255412101745605\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 97.64it/s]\u001b[AValidation loss: 1.2643486261367798\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 97.73it/s]\u001b[AValidation loss: 1.5585558414459229\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 97.82it/s]\u001b[AValidation loss: 1.413895845413208\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 97.92it/s]\u001b[AValidation loss: 1.2417293787002563\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 97.95it/s]\u001b[AValidation loss: 1.5636470317840576\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 98.04it/s]\u001b[AValidation loss: 1.2345449924468994\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 98.13it/s]\u001b[AValidation loss: 2.299783945083618\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 98.35it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.3064664602279663\n",
      "Epoch 3:   0%|          | 1/212 [00:00<00:04, 43.56it/s, v_num=0]Training loss: 0.8338660001754761\n",
      "Epoch 3:   1%|          | 2/212 [00:00<00:05, 39.53it/s, v_num=0]Training loss: 1.2745656967163086\n",
      "Epoch 3:   1%|▏         | 3/212 [00:00<00:05, 38.53it/s, v_num=0]Training loss: 1.7704200744628906\n",
      "Epoch 3:   2%|▏         | 4/212 [00:00<00:05, 38.26it/s, v_num=0]Training loss: 9.630342483520508\n",
      "Epoch 3:   2%|▏         | 5/212 [00:00<00:05, 38.13it/s, v_num=0]Training loss: 1.5923142433166504\n",
      "Epoch 3:   3%|▎         | 6/212 [00:00<00:05, 38.02it/s, v_num=0]Training loss: 1.1138886213302612\n",
      "Epoch 3:   3%|▎         | 7/212 [00:00<00:05, 37.94it/s, v_num=0]Training loss: 1.1204354763031006\n",
      "Epoch 3:   4%|▍         | 8/212 [00:00<00:05, 37.89it/s, v_num=0]Training loss: 1.0594309568405151\n",
      "Epoch 3:   4%|▍         | 9/212 [00:00<00:05, 37.85it/s, v_num=0]Training loss: 1.0494049787521362\n",
      "Epoch 3:   5%|▍         | 10/212 [00:00<00:05, 37.80it/s, v_num=0]Training loss: 1.235795259475708\n",
      "Epoch 3:   5%|▌         | 11/212 [00:00<00:05, 37.77it/s, v_num=0]Training loss: 1.3712551593780518\n",
      "Epoch 3:   6%|▌         | 12/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 1.7653225660324097\n",
      "Epoch 3:   6%|▌         | 13/212 [00:00<00:05, 37.68it/s, v_num=0]Training loss: 1.7767229080200195\n",
      "Epoch 3:   7%|▋         | 14/212 [00:00<00:05, 37.63it/s, v_num=0]Training loss: 0.8753650188446045\n",
      "Epoch 3:   7%|▋         | 15/212 [00:00<00:05, 37.62it/s, v_num=0]Training loss: 1.205589771270752\n",
      "Epoch 3:   8%|▊         | 16/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 0.7405109405517578\n",
      "Epoch 3:   8%|▊         | 17/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 1.317884087562561\n",
      "Epoch 3:   8%|▊         | 18/212 [00:00<00:05, 37.60it/s, v_num=0]Training loss: 0.9827236533164978\n",
      "Epoch 3:   9%|▉         | 19/212 [00:00<00:05, 37.60it/s, v_num=0]Training loss: 1.0587090253829956\n",
      "Epoch 3:   9%|▉         | 20/212 [00:00<00:05, 37.59it/s, v_num=0]Training loss: 3.6815550327301025\n",
      "Epoch 3:  10%|▉         | 21/212 [00:00<00:05, 37.59it/s, v_num=0]Training loss: 1.0161515474319458\n",
      "Epoch 3:  10%|█         | 22/212 [00:00<00:05, 37.58it/s, v_num=0]Training loss: 0.9175838828086853\n",
      "Epoch 3:  11%|█         | 23/212 [00:00<00:05, 37.58it/s, v_num=0]Training loss: 1.074573278427124\n",
      "Epoch 3:  11%|█▏        | 24/212 [00:00<00:05, 37.58it/s, v_num=0]Training loss: 1.3506187200546265\n",
      "Epoch 3:  12%|█▏        | 25/212 [00:00<00:04, 37.58it/s, v_num=0]Training loss: 0.6975980997085571\n",
      "Epoch 3:  12%|█▏        | 26/212 [00:00<00:04, 37.58it/s, v_num=0]Training loss: 1.7313847541809082\n",
      "Epoch 3:  13%|█▎        | 27/212 [00:00<00:04, 37.57it/s, v_num=0]Training loss: 1.2950152158737183\n",
      "Epoch 3:  13%|█▎        | 28/212 [00:00<00:04, 37.57it/s, v_num=0]Training loss: 1.0115838050842285\n",
      "Epoch 3:  14%|█▎        | 29/212 [00:00<00:04, 37.57it/s, v_num=0]Training loss: 1.4832310676574707\n",
      "Epoch 3:  14%|█▍        | 30/212 [00:00<00:04, 37.57it/s, v_num=0]Training loss: 1.0166622400283813\n",
      "Epoch 3:  15%|█▍        | 31/212 [00:00<00:04, 37.56it/s, v_num=0]Training loss: 0.9478706121444702\n",
      "Epoch 3:  15%|█▌        | 32/212 [00:00<00:04, 37.56it/s, v_num=0]Training loss: 0.8846370577812195\n",
      "Epoch 3:  16%|█▌        | 33/212 [00:00<00:04, 37.56it/s, v_num=0]Training loss: 1.3098232746124268\n",
      "Epoch 3:  16%|█▌        | 34/212 [00:00<00:04, 37.56it/s, v_num=0]Training loss: 1.720270037651062\n",
      "Epoch 3:  17%|█▋        | 35/212 [00:00<00:04, 37.49it/s, v_num=0]Training loss: 1.2458219528198242\n",
      "Epoch 3:  17%|█▋        | 36/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 0.9479108452796936\n",
      "Epoch 3:  17%|█▋        | 37/212 [00:00<00:04, 37.42it/s, v_num=0]Training loss: 0.9691458940505981\n",
      "Epoch 3:  18%|█▊        | 38/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.8998821377754211\n",
      "Epoch 3:  18%|█▊        | 39/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.1934576034545898\n",
      "Epoch 3:  19%|█▉        | 40/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.160007119178772\n",
      "Epoch 3:  19%|█▉        | 41/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 0.8981016278266907\n",
      "Epoch 3:  20%|█▉        | 42/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.9341759085655212\n",
      "Epoch 3:  20%|██        | 43/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.9190775752067566\n",
      "Epoch 3:  21%|██        | 44/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 0.9695958495140076\n",
      "Epoch 3:  21%|██        | 45/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 9.678387641906738\n",
      "Epoch 3:  22%|██▏       | 46/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 0.7867140769958496\n",
      "Epoch 3:  22%|██▏       | 47/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.9190841913223267\n",
      "Epoch 3:  23%|██▎       | 48/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 1.2155331373214722\n",
      "Epoch 3:  23%|██▎       | 49/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.1556421518325806\n",
      "Epoch 3:  24%|██▎       | 50/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 0.9694231152534485\n",
      "Epoch 3:  24%|██▍       | 51/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.064863681793213\n",
      "Epoch 3:  25%|██▍       | 52/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.7434393167495728\n",
      "Epoch 3:  25%|██▌       | 53/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.7344619035720825\n",
      "Epoch 3:  25%|██▌       | 54/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.148653507232666\n",
      "Epoch 3:  26%|██▌       | 55/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.074965238571167\n",
      "Epoch 3:  26%|██▋       | 56/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.7238346338272095\n",
      "Epoch 3:  27%|██▋       | 57/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.4736638069152832\n",
      "Epoch 3:  27%|██▋       | 58/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.473851203918457\n",
      "Epoch 3:  28%|██▊       | 59/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.309151291847229\n",
      "Epoch 3:  28%|██▊       | 60/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.229193925857544\n",
      "Epoch 3:  29%|██▉       | 61/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.3306142091751099\n",
      "Epoch 3:  29%|██▉       | 62/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.205781102180481\n",
      "Epoch 3:  30%|██▉       | 63/212 [00:01<00:03, 37.32it/s, v_num=0]Training loss: 1.1164873838424683\n",
      "Epoch 3:  30%|███       | 64/212 [00:01<00:03, 37.32it/s, v_num=0]Training loss: 1.8480417728424072\n",
      "Epoch 3:  31%|███       | 65/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 1.383558988571167\n",
      "Epoch 3:  31%|███       | 66/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 1.0607240200042725\n",
      "Epoch 3:  32%|███▏      | 67/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 1.0741467475891113\n",
      "Epoch 3:  32%|███▏      | 68/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 1.0438511371612549\n",
      "Epoch 3:  33%|███▎      | 69/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 1.1488769054412842\n",
      "Epoch 3:  33%|███▎      | 70/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 1.6272536516189575\n",
      "Epoch 3:  33%|███▎      | 71/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 1.147312879562378\n",
      "Epoch 3:  34%|███▍      | 72/212 [00:01<00:03, 37.35it/s, v_num=0]Training loss: 1.4190298318862915\n",
      "Epoch 3:  34%|███▍      | 73/212 [00:01<00:03, 37.35it/s, v_num=0]Training loss: 1.7453171014785767\n",
      "Epoch 3:  35%|███▍      | 74/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 0.8680351972579956\n",
      "Epoch 3:  35%|███▌      | 75/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.50661301612854\n",
      "Epoch 3:  36%|███▌      | 76/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.285351037979126\n",
      "Epoch 3:  36%|███▋      | 77/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.979743480682373\n",
      "Epoch 3:  37%|███▋      | 78/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.6846615076065063\n",
      "Epoch 3:  37%|███▋      | 79/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.035530686378479\n",
      "Epoch 3:  38%|███▊      | 80/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.379621148109436\n",
      "Epoch 3:  38%|███▊      | 81/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.3353151082992554\n",
      "Epoch 3:  39%|███▊      | 82/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.8509437441825867\n",
      "Epoch 3:  39%|███▉      | 83/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.2608951330184937\n",
      "Epoch 3:  40%|███▉      | 84/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.1385265588760376\n",
      "Epoch 3:  40%|████      | 85/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.6895999908447266\n",
      "Epoch 3:  41%|████      | 86/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.6842750310897827\n",
      "Epoch 3:  41%|████      | 87/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.2579610347747803\n",
      "Epoch 3:  42%|████▏     | 88/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9150530099868774\n",
      "Epoch 3:  42%|████▏     | 89/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.564009666442871\n",
      "Epoch 3:  42%|████▏     | 90/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.2072092294692993\n",
      "Epoch 3:  43%|████▎     | 91/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.2412666082382202\n",
      "Epoch 3:  43%|████▎     | 92/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.7286465167999268\n",
      "Epoch 3:  44%|████▍     | 93/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9328829050064087\n",
      "Epoch 3:  44%|████▍     | 94/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.4391145706176758\n",
      "Epoch 3:  45%|████▍     | 95/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.9214074611663818\n",
      "Epoch 3:  45%|████▌     | 96/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.0718472003936768\n",
      "Epoch 3:  46%|████▌     | 97/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 2.7829644680023193\n",
      "Epoch 3:  46%|████▌     | 98/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.2999731302261353\n",
      "Epoch 3:  47%|████▋     | 99/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.8771789073944092\n",
      "Epoch 3:  47%|████▋     | 100/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.1477766036987305\n",
      "Epoch 3:  48%|████▊     | 101/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.062362790107727\n",
      "Epoch 3:  48%|████▊     | 102/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.62662672996521\n",
      "Epoch 3:  49%|████▊     | 103/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 2.4785523414611816\n",
      "Epoch 3:  49%|████▉     | 104/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.0775480270385742\n",
      "Epoch 3:  50%|████▉     | 105/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 0.8995717763900757\n",
      "Epoch 3:  50%|█████     | 106/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.1423205137252808\n",
      "Epoch 3:  50%|█████     | 107/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.0560431480407715\n",
      "Epoch 3:  51%|█████     | 108/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.1639454364776611\n",
      "Epoch 3:  51%|█████▏    | 109/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 1.4410006999969482\n",
      "Epoch 3:  52%|█████▏    | 110/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 1.2343555688858032\n",
      "Epoch 3:  52%|█████▏    | 111/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 2.0195484161376953\n",
      "Epoch 3:  53%|█████▎    | 112/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.4110562801361084\n",
      "Epoch 3:  53%|█████▎    | 113/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.161649465560913\n",
      "Epoch 3:  54%|█████▍    | 114/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.553144931793213\n",
      "Epoch 3:  54%|█████▍    | 115/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.0578185319900513\n",
      "Epoch 3:  55%|█████▍    | 116/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.3145277500152588\n",
      "Epoch 3:  55%|█████▌    | 117/212 [00:03<00:02, 37.18it/s, v_num=0]Training loss: 1.0105105638504028\n",
      "Epoch 3:  56%|█████▌    | 118/212 [00:03<00:02, 37.16it/s, v_num=0]Training loss: 1.4375345706939697\n",
      "Epoch 3:  56%|█████▌    | 119/212 [00:03<00:02, 37.15it/s, v_num=0]Training loss: 1.1518198251724243\n",
      "Epoch 3:  57%|█████▋    | 120/212 [00:03<00:02, 37.13it/s, v_num=0]Training loss: 0.8650367856025696\n",
      "Epoch 3:  57%|█████▋    | 121/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.8779346942901611\n",
      "Epoch 3:  58%|█████▊    | 122/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.3271057605743408\n",
      "Epoch 3:  58%|█████▊    | 123/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.0361695289611816\n",
      "Epoch 3:  58%|█████▊    | 124/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 3.929811716079712\n",
      "Epoch 3:  59%|█████▉    | 125/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.974639892578125\n",
      "Epoch 3:  59%|█████▉    | 126/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 0.9802414774894714\n",
      "Epoch 3:  60%|█████▉    | 127/212 [00:03<00:02, 37.09it/s, v_num=0]Training loss: 1.5354514122009277\n",
      "Epoch 3:  60%|██████    | 128/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.259821891784668\n",
      "Epoch 3:  61%|██████    | 129/212 [00:03<00:02, 37.10it/s, v_num=0]Training loss: 1.3042839765548706\n",
      "Epoch 3:  61%|██████▏   | 130/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 0.9620488286018372\n",
      "Epoch 3:  62%|██████▏   | 131/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.2665035724639893\n",
      "Epoch 3:  62%|██████▏   | 132/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 1.149322271347046\n",
      "Epoch 3:  63%|██████▎   | 133/212 [00:03<00:02, 37.11it/s, v_num=0]Training loss: 0.8665851950645447\n",
      "Epoch 3:  63%|██████▎   | 134/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.1508746147155762\n",
      "Epoch 3:  64%|██████▎   | 135/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.3467031717300415\n",
      "Epoch 3:  64%|██████▍   | 136/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.298259973526001\n",
      "Epoch 3:  65%|██████▍   | 137/212 [00:03<00:02, 37.12it/s, v_num=0]Training loss: 1.0829861164093018\n",
      "Epoch 3:  65%|██████▌   | 138/212 [00:03<00:01, 37.13it/s, v_num=0]Training loss: 1.2695703506469727\n",
      "Epoch 3:  66%|██████▌   | 139/212 [00:03<00:01, 37.13it/s, v_num=0]Training loss: 1.8825246095657349\n",
      "Epoch 3:  66%|██████▌   | 140/212 [00:03<00:01, 37.13it/s, v_num=0]Training loss: 0.8178570866584778\n",
      "Epoch 3:  67%|██████▋   | 141/212 [00:03<00:01, 37.14it/s, v_num=0]Training loss: 1.3715864419937134\n",
      "Epoch 3:  67%|██████▋   | 142/212 [00:03<00:01, 37.14it/s, v_num=0]Training loss: 1.0636619329452515\n",
      "Epoch 3:  67%|██████▋   | 143/212 [00:03<00:01, 37.14it/s, v_num=0]Training loss: 1.188989520072937\n",
      "Epoch 3:  68%|██████▊   | 144/212 [00:03<00:01, 37.14it/s, v_num=0]Training loss: 0.9534844756126404\n",
      "Epoch 3:  68%|██████▊   | 145/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 0.9499136805534363\n",
      "Epoch 3:  69%|██████▉   | 146/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 0.7944809198379517\n",
      "Epoch 3:  69%|██████▉   | 147/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 1.2084221839904785\n",
      "Epoch 3:  70%|██████▉   | 148/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 0.9055656790733337\n",
      "Epoch 3:  70%|███████   | 149/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 1.236810326576233\n",
      "Epoch 3:  71%|███████   | 150/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 0.822337806224823\n",
      "Epoch 3:  71%|███████   | 151/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 2.579587697982788\n",
      "Epoch 3:  72%|███████▏  | 152/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.9188268184661865\n",
      "Epoch 3:  72%|███████▏  | 153/212 [00:04<00:01, 37.16it/s, v_num=0]Training loss: 1.8817744255065918\n",
      "Epoch 3:  73%|███████▎  | 154/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 1.0711230039596558\n",
      "Epoch 3:  73%|███████▎  | 155/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 0.9177451133728027\n",
      "Epoch 3:  74%|███████▎  | 156/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 0.6940780282020569\n",
      "Epoch 3:  74%|███████▍  | 157/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 0.7528950572013855\n",
      "Epoch 3:  75%|███████▍  | 158/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.9574927091598511\n",
      "Epoch 3:  75%|███████▌  | 159/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.228480577468872\n",
      "Epoch 3:  75%|███████▌  | 160/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 1.0767269134521484\n",
      "Epoch 3:  76%|███████▌  | 161/212 [00:04<00:01, 37.11it/s, v_num=0]Training loss: 1.6345096826553345\n",
      "Epoch 3:  76%|███████▋  | 162/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 0.9981034994125366\n",
      "Epoch 3:  77%|███████▋  | 163/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 1.0132136344909668\n",
      "Epoch 3:  77%|███████▋  | 164/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 1.2681607007980347\n",
      "Epoch 3:  78%|███████▊  | 165/212 [00:04<00:01, 37.12it/s, v_num=0]Training loss: 2.3090643882751465\n",
      "Epoch 3:  78%|███████▊  | 166/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.4547199010849\n",
      "Epoch 3:  79%|███████▉  | 167/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.9796837568283081\n",
      "Epoch 3:  79%|███████▉  | 168/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.7406391501426697\n",
      "Epoch 3:  80%|███████▉  | 169/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.060397982597351\n",
      "Epoch 3:  80%|████████  | 170/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 1.1099395751953125\n",
      "Epoch 3:  81%|████████  | 171/212 [00:04<00:01, 37.13it/s, v_num=0]Training loss: 0.9807825088500977\n",
      "Epoch 3:  81%|████████  | 172/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 1.3747388124465942\n",
      "Epoch 3:  82%|████████▏ | 173/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 0.961787223815918\n",
      "Epoch 3:  82%|████████▏ | 174/212 [00:04<00:01, 37.14it/s, v_num=0]Training loss: 0.9953517913818359\n",
      "Epoch 3:  83%|████████▎ | 175/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 1.4629172086715698\n",
      "Epoch 3:  83%|████████▎ | 176/212 [00:04<00:00, 37.14it/s, v_num=0]Training loss: 0.9002822041511536\n",
      "Epoch 3:  83%|████████▎ | 177/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.2102843523025513\n",
      "Epoch 3:  84%|████████▍ | 178/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 0.9231308102607727\n",
      "Epoch 3:  84%|████████▍ | 179/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 0.9528189897537231\n",
      "Epoch 3:  85%|████████▍ | 180/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.7412556409835815\n",
      "Epoch 3:  85%|████████▌ | 181/212 [00:04<00:00, 37.15it/s, v_num=0]Training loss: 1.3782565593719482\n",
      "Epoch 3:  86%|████████▌ | 182/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 0.9224296808242798\n",
      "Epoch 3:  86%|████████▋ | 183/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 1.9223871231079102\n",
      "Epoch 3:  87%|████████▋ | 184/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 1.0860501527786255\n",
      "Epoch 3:  87%|████████▋ | 185/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 0.8216390013694763\n",
      "Epoch 3:  88%|████████▊ | 186/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.1201797723770142\n",
      "Epoch 3:  88%|████████▊ | 187/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 0.9213426113128662\n",
      "Epoch 3:  89%|████████▊ | 188/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.9774746894836426\n",
      "Epoch 3:  89%|████████▉ | 189/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.9493439197540283\n",
      "Epoch 3:  90%|████████▉ | 190/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 1.3285588026046753\n",
      "Epoch 3:  90%|█████████ | 191/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 0.9055638313293457\n",
      "Epoch 3:  91%|█████████ | 192/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.8963008522987366\n",
      "Epoch 3:  91%|█████████ | 193/212 [00:05<00:00, 37.17it/s, v_num=0]Training loss: 0.8886884450912476\n",
      "Epoch 3:  92%|█████████▏| 194/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 0.8454175591468811\n",
      "Epoch 3:  92%|█████████▏| 195/212 [00:05<00:00, 37.16it/s, v_num=0]Training loss: 1.2418557405471802\n",
      "Epoch 3:  92%|█████████▏| 196/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.1076164245605469\n",
      "Epoch 3:  93%|█████████▎| 197/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.3143402338027954\n",
      "Epoch 3:  93%|█████████▎| 198/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 1.3172006607055664\n",
      "Epoch 3:  94%|█████████▍| 199/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 1.151711106300354\n",
      "Epoch 3:  94%|█████████▍| 200/212 [00:05<00:00, 37.13it/s, v_num=0]Training loss: 1.0389535427093506\n",
      "Epoch 3:  95%|█████████▍| 201/212 [00:05<00:00, 37.13it/s, v_num=0]Training loss: 0.8712022304534912\n",
      "Epoch 3:  95%|█████████▌| 202/212 [00:05<00:00, 37.13it/s, v_num=0]Training loss: 1.1748601198196411\n",
      "Epoch 3:  96%|█████████▌| 203/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 0.910161554813385\n",
      "Epoch 3:  96%|█████████▌| 204/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 1.4688405990600586\n",
      "Epoch 3:  97%|█████████▋| 205/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 1.1151593923568726\n",
      "Epoch 3:  97%|█████████▋| 206/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 1.4612531661987305\n",
      "Epoch 3:  98%|█████████▊| 207/212 [00:05<00:00, 37.14it/s, v_num=0]Training loss: 0.7632682919502258\n",
      "Epoch 3:  98%|█████████▊| 208/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.5881708860397339\n",
      "Epoch 3:  99%|█████████▊| 209/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 0.938410758972168\n",
      "Epoch 3:  99%|█████████▉| 210/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 1.1359279155731201\n",
      "Epoch 3: 100%|█████████▉| 211/212 [00:05<00:00, 37.15it/s, v_num=0]Training loss: 0.9843603372573853\n",
      "Epoch 3: 100%|██████████| 212/212 [00:05<00:00, 37.15it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.255544662475586\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 85.42it/s]\u001b[AValidation loss: 2.039066791534424\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 93.57it/s]\u001b[AValidation loss: 1.0797851085662842\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 96.66it/s]\u001b[AValidation loss: 0.7968933582305908\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 98.20it/s]\u001b[AValidation loss: 0.8143318891525269\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 99.42it/s]\u001b[AValidation loss: 0.9867854118347168\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 98.81it/s]\u001b[AValidation loss: 1.3382536172866821\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 99.66it/s]\u001b[AValidation loss: 2.3185930252075195\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 100.23it/s]\u001b[AValidation loss: 0.7127094268798828\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 100.65it/s]\u001b[AValidation loss: 0.8267072439193726\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 101.12it/s]\u001b[AValidation loss: 0.8481061458587646\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 101.45it/s]\u001b[AValidation loss: 1.1597808599472046\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 101.64it/s]\u001b[AValidation loss: 1.1356518268585205\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 101.93it/s]\u001b[AValidation loss: 1.072356104850769\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 102.07it/s]\u001b[AValidation loss: 0.8829830288887024\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 102.26it/s]\u001b[AValidation loss: 1.2397518157958984\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 102.41it/s]\u001b[AValidation loss: 1.1404261589050293\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 102.56it/s]\u001b[AValidation loss: 1.6683034896850586\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.54it/s]\u001b[AValidation loss: 0.8509801030158997\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 102.66it/s]\u001b[AValidation loss: 1.3407468795776367\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 102.46it/s]\u001b[AValidation loss: 1.2014678716659546\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 102.30it/s]\u001b[AValidation loss: 1.2335387468338013\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 102.31it/s]\u001b[AValidation loss: 1.2157070636749268\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 102.34it/s]\u001b[AValidation loss: 1.0707733631134033\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 102.42it/s]\u001b[AValidation loss: 1.5165406465530396\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 102.46it/s]\u001b[AValidation loss: 0.8724638223648071\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 102.26it/s]\u001b[AValidation loss: 1.0933164358139038\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 102.34it/s]\u001b[AValidation loss: 1.1239619255065918\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 102.40it/s]\u001b[AValidation loss: 1.1184614896774292\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 102.47it/s]\u001b[AValidation loss: 1.3710076808929443\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 102.52it/s]\u001b[AValidation loss: 0.984851598739624\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.55it/s]\u001b[AValidation loss: 1.0729947090148926\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.63it/s]\u001b[AValidation loss: 1.0526307821273804\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 102.65it/s]\u001b[AValidation loss: 0.9477509260177612\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 102.70it/s]\u001b[AValidation loss: 2.018678665161133\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 102.72it/s]\u001b[AValidation loss: 0.6961157321929932\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.77it/s]\u001b[AValidation loss: 1.633715271949768\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.73it/s]\u001b[AValidation loss: 0.7497320175170898\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.78it/s]\u001b[AValidation loss: 1.1695622205734253\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.81it/s]\u001b[AValidation loss: 1.0700817108154297\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.85it/s]\u001b[AValidation loss: 0.942091166973114\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.70it/s]\u001b[AValidation loss: 0.8391784429550171\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.74it/s]\u001b[AValidation loss: 0.9290012121200562\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 102.76it/s]\u001b[AValidation loss: 1.4034807682037354\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 102.78it/s]\u001b[AValidation loss: 1.55219566822052\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 102.81it/s]\u001b[AValidation loss: 0.8189614415168762\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 102.84it/s]\u001b[AValidation loss: 1.0856289863586426\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 102.71it/s]\u001b[AValidation loss: 1.0717967748641968\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 102.74it/s]\u001b[AValidation loss: 1.2219303846359253\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 102.77it/s]\u001b[AValidation loss: 1.502977967262268\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 102.80it/s]\u001b[AValidation loss: 1.3587472438812256\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 102.81it/s]\u001b[AValidation loss: 1.1969633102416992\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 102.82it/s]\u001b[AValidation loss: 1.5095562934875488\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 102.84it/s]\u001b[AValidation loss: 1.1844499111175537\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 102.78it/s]\u001b[AValidation loss: 2.2499098777770996\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 102.83it/s]\u001b[A\n",
      "Epoch 4:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]                \u001b[ATraining loss: 1.200206995010376\n",
      "Epoch 4:   0%|          | 1/212 [00:00<00:04, 45.23it/s, v_num=0]Training loss: 1.0892921686172485\n",
      "Epoch 4:   1%|          | 2/212 [00:00<00:05, 41.06it/s, v_num=0]Training loss: 3.854736328125\n",
      "Epoch 4:   1%|▏         | 3/212 [00:00<00:05, 39.83it/s, v_num=0]Training loss: 1.2651147842407227\n",
      "Epoch 4:   2%|▏         | 4/212 [00:00<00:05, 39.25it/s, v_num=0]Training loss: 1.2169002294540405\n",
      "Epoch 4:   2%|▏         | 5/212 [00:00<00:05, 38.91it/s, v_num=0]Training loss: 1.4668192863464355\n",
      "Epoch 4:   3%|▎         | 6/212 [00:00<00:05, 38.68it/s, v_num=0]Training loss: 2.5833208560943604\n",
      "Epoch 4:   3%|▎         | 7/212 [00:00<00:05, 38.52it/s, v_num=0]Training loss: 1.014945149421692\n",
      "Epoch 4:   4%|▍         | 8/212 [00:00<00:05, 38.13it/s, v_num=0]Training loss: 0.9068603515625\n",
      "Epoch 4:   4%|▍         | 9/212 [00:00<00:05, 37.89it/s, v_num=0]Training loss: 1.2585666179656982\n",
      "Epoch 4:   5%|▍         | 10/212 [00:00<00:05, 37.70it/s, v_num=0]Training loss: 1.7848849296569824\n",
      "Epoch 4:   5%|▌         | 11/212 [00:00<00:05, 37.56it/s, v_num=0]Training loss: 0.9105108976364136\n",
      "Epoch 4:   6%|▌         | 12/212 [00:00<00:05, 37.44it/s, v_num=0]Training loss: 1.583975076675415\n",
      "Epoch 4:   6%|▌         | 13/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 1.1159008741378784\n",
      "Epoch 4:   7%|▋         | 14/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 1.4029566049575806\n",
      "Epoch 4:   7%|▋         | 15/212 [00:00<00:05, 37.12it/s, v_num=0]Training loss: 0.8457167148590088\n",
      "Epoch 4:   8%|▊         | 16/212 [00:00<00:05, 37.09it/s, v_num=0]Training loss: 0.9900256395339966\n",
      "Epoch 4:   8%|▊         | 17/212 [00:00<00:05, 37.17it/s, v_num=0]Training loss: 1.3629169464111328\n",
      "Epoch 4:   8%|▊         | 18/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 0.9364790320396423\n",
      "Epoch 4:   9%|▉         | 19/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 0.8870655298233032\n",
      "Epoch 4:   9%|▉         | 20/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 0.7930911779403687\n",
      "Epoch 4:  10%|▉         | 21/212 [00:00<00:05, 37.23it/s, v_num=0]Training loss: 1.248658299446106\n",
      "Epoch 4:  10%|█         | 22/212 [00:00<00:05, 37.23it/s, v_num=0]Training loss: 1.2873104810714722\n",
      "Epoch 4:  11%|█         | 23/212 [00:00<00:05, 37.24it/s, v_num=0]Training loss: 1.4944465160369873\n",
      "Epoch 4:  11%|█▏        | 24/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 1.0412734746932983\n",
      "Epoch 4:  12%|█▏        | 25/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 1.0554453134536743\n",
      "Epoch 4:  12%|█▏        | 26/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 1.1117697954177856\n",
      "Epoch 4:  13%|█▎        | 27/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 0.825049877166748\n",
      "Epoch 4:  13%|█▎        | 28/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.0081583261489868\n",
      "Epoch 4:  14%|█▎        | 29/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 1.068118691444397\n",
      "Epoch 4:  14%|█▍        | 30/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 2.0850155353546143\n",
      "Epoch 4:  15%|█▍        | 31/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 1.0187259912490845\n",
      "Epoch 4:  15%|█▌        | 32/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 1.0404353141784668\n",
      "Epoch 4:  16%|█▌        | 33/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.854741096496582\n",
      "Epoch 4:  16%|█▌        | 34/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 1.0089805126190186\n",
      "Epoch 4:  17%|█▋        | 35/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 0.7173865437507629\n",
      "Epoch 4:  17%|█▋        | 36/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.2827452421188354\n",
      "Epoch 4:  17%|█▋        | 37/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 1.5986496210098267\n",
      "Epoch 4:  18%|█▊        | 38/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.675755262374878\n",
      "Epoch 4:  18%|█▊        | 39/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.9943259358406067\n",
      "Epoch 4:  19%|█▉        | 40/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.1699621677398682\n",
      "Epoch 4:  19%|█▉        | 41/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.7422814965248108\n",
      "Epoch 4:  20%|█▉        | 42/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.242371678352356\n",
      "Epoch 4:  20%|██        | 43/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.4462674856185913\n",
      "Epoch 4:  21%|██        | 44/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.9121558666229248\n",
      "Epoch 4:  21%|██        | 45/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.0128610134124756\n",
      "Epoch 4:  22%|██▏       | 46/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.9811510443687439\n",
      "Epoch 4:  22%|██▏       | 47/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.3722187280654907\n",
      "Epoch 4:  23%|██▎       | 48/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.7870068550109863\n",
      "Epoch 4:  23%|██▎       | 49/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.228808045387268\n",
      "Epoch 4:  24%|██▎       | 50/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 0.8617820143699646\n",
      "Epoch 4:  24%|██▍       | 51/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.9589012861251831\n",
      "Epoch 4:  25%|██▍       | 52/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.1711671352386475\n",
      "Epoch 4:  25%|██▌       | 53/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.1558244228363037\n",
      "Epoch 4:  25%|██▌       | 54/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 1.1645082235336304\n",
      "Epoch 4:  26%|██▌       | 55/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.9879499077796936\n",
      "Epoch 4:  26%|██▋       | 56/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 2.5265004634857178\n",
      "Epoch 4:  27%|██▋       | 57/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.9926360249519348\n",
      "Epoch 4:  27%|██▋       | 58/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.9213405251502991\n",
      "Epoch 4:  28%|██▊       | 59/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.4779353141784668\n",
      "Epoch 4:  28%|██▊       | 60/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.9444738030433655\n",
      "Epoch 4:  29%|██▉       | 61/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.0871208906173706\n",
      "Epoch 4:  29%|██▉       | 62/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.0278887748718262\n",
      "Epoch 4:  30%|██▉       | 63/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.4195048809051514\n",
      "Epoch 4:  30%|███       | 64/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.9536964297294617\n",
      "Epoch 4:  31%|███       | 65/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.9560055136680603\n",
      "Epoch 4:  31%|███       | 66/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 1.5375711917877197\n",
      "Epoch 4:  32%|███▏      | 67/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.8612627983093262\n",
      "Epoch 4:  32%|███▏      | 68/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.2805808782577515\n",
      "Epoch 4:  33%|███▎      | 69/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.8553451895713806\n",
      "Epoch 4:  33%|███▎      | 70/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.858143925666809\n",
      "Epoch 4:  33%|███▎      | 71/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.2021187543869019\n",
      "Epoch 4:  34%|███▍      | 72/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.1750521659851074\n",
      "Epoch 4:  34%|███▍      | 73/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.2054190635681152\n",
      "Epoch 4:  35%|███▍      | 74/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.8236017227172852\n",
      "Epoch 4:  35%|███▌      | 75/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.4325847625732422\n",
      "Epoch 4:  36%|███▌      | 76/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.7211787104606628\n",
      "Epoch 4:  36%|███▋      | 77/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.3114185333251953\n",
      "Epoch 4:  37%|███▋      | 78/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.0161417722702026\n",
      "Epoch 4:  37%|███▋      | 79/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9201430082321167\n",
      "Epoch 4:  38%|███▊      | 80/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.1111819744110107\n",
      "Epoch 4:  38%|███▊      | 81/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.8471929430961609\n",
      "Epoch 4:  39%|███▊      | 82/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.7767189145088196\n",
      "Epoch 4:  39%|███▉      | 83/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9955955743789673\n",
      "Epoch 4:  40%|███▉      | 84/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9570568203926086\n",
      "Epoch 4:  40%|████      | 85/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.1818585395812988\n",
      "Epoch 4:  41%|████      | 86/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.6573689579963684\n",
      "Epoch 4:  41%|████      | 87/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.098892331123352\n",
      "Epoch 4:  42%|████▏     | 88/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 2.231247901916504\n",
      "Epoch 4:  42%|████▏     | 89/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.8728267550468445\n",
      "Epoch 4:  42%|████▏     | 90/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.155831217765808\n",
      "Epoch 4:  43%|████▎     | 91/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.5465285778045654\n",
      "Epoch 4:  43%|████▎     | 92/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.9912721514701843\n",
      "Epoch 4:  44%|████▍     | 93/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.264277458190918\n",
      "Epoch 4:  44%|████▍     | 94/212 [00:02<00:03, 37.18it/s, v_num=0]Training loss: 1.4423940181732178\n",
      "Epoch 4:  45%|████▍     | 95/212 [00:02<00:03, 37.17it/s, v_num=0]Training loss: 0.7663134336471558\n",
      "Epoch 4:  45%|████▌     | 96/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.6901910305023193\n",
      "Epoch 4:  46%|████▌     | 97/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.3550727367401123\n",
      "Epoch 4:  46%|████▌     | 98/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.6618412733078003\n",
      "Epoch 4:  47%|████▋     | 99/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.1320780515670776\n",
      "Epoch 4:  47%|████▋     | 100/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.4515516757965088\n",
      "Epoch 4:  48%|████▊     | 101/212 [00:02<00:02, 37.20it/s, v_num=0]Training loss: 1.3296873569488525\n",
      "Epoch 4:  48%|████▊     | 102/212 [00:02<00:02, 37.20it/s, v_num=0]Training loss: 0.7640647292137146\n",
      "Epoch 4:  49%|████▊     | 103/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.8346109390258789\n",
      "Epoch 4:  49%|████▉     | 104/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 4.169017791748047\n",
      "Epoch 4:  50%|████▉     | 105/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.704826831817627\n",
      "Epoch 4:  50%|█████     | 106/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 1.52491295337677\n",
      "Epoch 4:  50%|█████     | 107/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.9649664759635925\n",
      "Epoch 4:  51%|█████     | 108/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.562551498413086\n",
      "Epoch 4:  51%|█████▏    | 109/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.2251827716827393\n",
      "Epoch 4:  52%|█████▏    | 110/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 0.9375380277633667\n",
      "Epoch 4:  52%|█████▏    | 111/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.5701359510421753\n",
      "Epoch 4:  53%|█████▎    | 112/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.4414230585098267\n",
      "Epoch 4:  53%|█████▎    | 113/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1900584697723389\n",
      "Epoch 4:  54%|█████▍    | 114/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9440633058547974\n",
      "Epoch 4:  54%|█████▍    | 115/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.3063464164733887\n",
      "Epoch 4:  55%|█████▍    | 116/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.0256233215332031\n",
      "Epoch 4:  55%|█████▌    | 117/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1996411085128784\n",
      "Epoch 4:  56%|█████▌    | 118/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.558530569076538\n",
      "Epoch 4:  56%|█████▌    | 119/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9729540944099426\n",
      "Epoch 4:  57%|█████▋    | 120/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1497379541397095\n",
      "Epoch 4:  57%|█████▋    | 121/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.4812428951263428\n",
      "Epoch 4:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.2981858253479004\n",
      "Epoch 4:  58%|█████▊    | 123/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9395155310630798\n",
      "Epoch 4:  58%|█████▊    | 124/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7462998628616333\n",
      "Epoch 4:  59%|█████▉    | 125/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8370716571807861\n",
      "Epoch 4:  59%|█████▉    | 126/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.296613335609436\n",
      "Epoch 4:  60%|█████▉    | 127/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 18.363510131835938\n",
      "Epoch 4:  60%|██████    | 128/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.9329863786697388\n",
      "Epoch 4:  61%|██████    | 129/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.8305810689926147\n",
      "Epoch 4:  61%|██████▏   | 130/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.2410874366760254\n",
      "Epoch 4:  62%|██████▏   | 131/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.8852882385253906\n",
      "Epoch 4:  62%|██████▏   | 132/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.9791816473007202\n",
      "Epoch 4:  63%|██████▎   | 133/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.4847092628479004\n",
      "Epoch 4:  63%|██████▎   | 134/212 [00:03<00:02, 37.18it/s, v_num=0]Training loss: 1.4949337244033813\n",
      "Epoch 4:  64%|██████▎   | 135/212 [00:03<00:02, 37.18it/s, v_num=0]Training loss: 1.6632165908813477\n",
      "Epoch 4:  64%|██████▍   | 136/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.4278486967086792\n",
      "Epoch 4:  65%|██████▍   | 137/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.0019851922988892\n",
      "Epoch 4:  65%|██████▌   | 138/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.0875792503356934\n",
      "Epoch 4:  66%|██████▌   | 139/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.9328606724739075\n",
      "Epoch 4:  66%|██████▌   | 140/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.2577046155929565\n",
      "Epoch 4:  67%|██████▋   | 141/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.2592644691467285\n",
      "Epoch 4:  67%|██████▋   | 142/212 [00:03<00:01, 37.15it/s, v_num=0]Training loss: 0.7025424242019653\n",
      "Epoch 4:  67%|██████▋   | 143/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 0.8908783197402954\n",
      "Epoch 4:  68%|██████▊   | 144/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.0861589908599854\n",
      "Epoch 4:  68%|██████▊   | 145/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.0903129577636719\n",
      "Epoch 4:  69%|██████▉   | 146/212 [00:03<00:01, 37.16it/s, v_num=0]Training loss: 1.2740110158920288\n",
      "Epoch 4:  69%|██████▉   | 147/212 [00:03<00:01, 37.17it/s, v_num=0]Training loss: 1.000557541847229\n",
      "Epoch 4:  70%|██████▉   | 148/212 [00:03<00:01, 37.17it/s, v_num=0]Training loss: 1.1473749876022339\n",
      "Epoch 4:  70%|███████   | 149/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.9972578287124634\n",
      "Epoch 4:  71%|███████   | 150/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.0979195833206177\n",
      "Epoch 4:  71%|███████   | 151/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.6033188104629517\n",
      "Epoch 4:  72%|███████▏  | 152/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.2645211219787598\n",
      "Epoch 4:  72%|███████▏  | 153/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 0.5832430720329285\n",
      "Epoch 4:  73%|███████▎  | 154/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 0.8011579513549805\n",
      "Epoch 4:  73%|███████▎  | 155/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.0916662216186523\n",
      "Epoch 4:  74%|███████▎  | 156/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 0.9029626250267029\n",
      "Epoch 4:  74%|███████▍  | 157/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.093482494354248\n",
      "Epoch 4:  75%|███████▍  | 158/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.1925300359725952\n",
      "Epoch 4:  75%|███████▌  | 159/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 3.533391237258911\n",
      "Epoch 4:  75%|███████▌  | 160/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.1525254249572754\n",
      "Epoch 4:  76%|███████▌  | 161/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.2953013181686401\n",
      "Epoch 4:  76%|███████▋  | 162/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.6830099821090698\n",
      "Epoch 4:  77%|███████▋  | 163/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.8684384226799011\n",
      "Epoch 4:  77%|███████▋  | 164/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.7192259430885315\n",
      "Epoch 4:  78%|███████▊  | 165/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.2326066493988037\n",
      "Epoch 4:  78%|███████▊  | 166/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.1603267192840576\n",
      "Epoch 4:  79%|███████▉  | 167/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.9832679033279419\n",
      "Epoch 4:  79%|███████▉  | 168/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 1.2506664991378784\n",
      "Epoch 4:  80%|███████▉  | 169/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 1.0394272804260254\n",
      "Epoch 4:  80%|████████  | 170/212 [00:04<00:01, 37.18it/s, v_num=0]Training loss: 0.9103225469589233\n",
      "Epoch 4:  81%|████████  | 171/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 0.9571328163146973\n",
      "Epoch 4:  81%|████████  | 172/212 [00:04<00:01, 37.17it/s, v_num=0]Training loss: 1.3863904476165771\n",
      "Epoch 4:  82%|████████▏ | 173/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 0.6168501377105713\n",
      "Epoch 4:  82%|████████▏ | 174/212 [00:04<00:01, 37.15it/s, v_num=0]Training loss: 1.1270687580108643\n",
      "Epoch 4:  83%|████████▎ | 175/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 0.8770377039909363\n",
      "Epoch 4:  83%|████████▎ | 176/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 0.5848565697669983\n",
      "Epoch 4:  83%|████████▎ | 177/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 1.217839002609253\n",
      "Epoch 4:  84%|████████▍ | 178/212 [00:04<00:00, 37.16it/s, v_num=0]Training loss: 1.0526303052902222\n",
      "Epoch 4:  84%|████████▍ | 179/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 1.3649799823760986\n",
      "Epoch 4:  85%|████████▍ | 180/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 0.7651623487472534\n",
      "Epoch 4:  85%|████████▌ | 181/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 0.9057964086532593\n",
      "Epoch 4:  86%|████████▌ | 182/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 0.9236563444137573\n",
      "Epoch 4:  86%|████████▋ | 183/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 0.7730021476745605\n",
      "Epoch 4:  87%|████████▋ | 184/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 1.3010443449020386\n",
      "Epoch 4:  87%|████████▋ | 185/212 [00:04<00:00, 37.17it/s, v_num=0]Training loss: 0.63243168592453\n",
      "Epoch 4:  88%|████████▊ | 186/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.9060893058776855\n",
      "Epoch 4:  88%|████████▊ | 187/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 1.2548614740371704\n",
      "Epoch 4:  89%|████████▊ | 188/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.899175226688385\n",
      "Epoch 4:  89%|████████▉ | 189/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.8956884741783142\n",
      "Epoch 4:  90%|████████▉ | 190/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.944625735282898\n",
      "Epoch 4:  90%|█████████ | 191/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 0.9555914402008057\n",
      "Epoch 4:  91%|█████████ | 192/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.299553632736206\n",
      "Epoch 4:  91%|█████████ | 193/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 0.8666344285011292\n",
      "Epoch 4:  92%|█████████▏| 194/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.0871232748031616\n",
      "Epoch 4:  92%|█████████▏| 195/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.5337207317352295\n",
      "Epoch 4:  92%|█████████▏| 196/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7061938643455505\n",
      "Epoch 4:  93%|█████████▎| 197/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.8095278143882751\n",
      "Epoch 4:  93%|█████████▎| 198/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.883629560470581\n",
      "Epoch 4:  94%|█████████▍| 199/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.3228330612182617\n",
      "Epoch 4:  94%|█████████▍| 200/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.8765497207641602\n",
      "Epoch 4:  95%|█████████▍| 201/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.1273870468139648\n",
      "Epoch 4:  95%|█████████▌| 202/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.3037649393081665\n",
      "Epoch 4:  96%|█████████▌| 203/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.7690052390098572\n",
      "Epoch 4:  96%|█████████▌| 204/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.008345603942871\n",
      "Epoch 4:  97%|█████████▋| 205/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.993159830570221\n",
      "Epoch 4:  97%|█████████▋| 206/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.986091136932373\n",
      "Epoch 4:  98%|█████████▊| 207/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.6280875205993652\n",
      "Epoch 4:  98%|█████████▊| 208/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.9948287606239319\n",
      "Epoch 4:  99%|█████████▊| 209/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 1.2591252326965332\n",
      "Epoch 4:  99%|█████████▉| 210/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 0.8165348768234253\n",
      "Epoch 4: 100%|█████████▉| 211/212 [00:05<00:00, 37.18it/s, v_num=0]Training loss: 0.8376275300979614\n",
      "Epoch 4: 100%|██████████| 212/212 [00:05<00:00, 37.17it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.191967725753784\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 89.87it/s]\u001b[AValidation loss: 1.9592798948287964\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 92.20it/s]\u001b[AValidation loss: 1.0008022785186768\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 93.28it/s]\u001b[AValidation loss: 0.7386205792427063\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 94.21it/s]\u001b[AValidation loss: 0.7574231624603271\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 95.47it/s]\u001b[AValidation loss: 0.9275925755500793\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 96.63it/s]\u001b[AValidation loss: 1.2667953968048096\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 97.41it/s]\u001b[AValidation loss: 2.2393109798431396\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 98.20it/s]\u001b[AValidation loss: 0.6595953106880188\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 98.80it/s]\u001b[AValidation loss: 0.77043616771698\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 99.30it/s]\u001b[AValidation loss: 0.7848753929138184\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 99.19it/s]\u001b[AValidation loss: 1.096805214881897\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 99.65it/s]\u001b[AValidation loss: 1.082804799079895\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 99.78it/s]\u001b[AValidation loss: 1.0078108310699463\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 99.98it/s]\u001b[AValidation loss: 0.8306821584701538\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 100.20it/s]\u001b[AValidation loss: 1.164719820022583\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 100.38it/s]\u001b[AValidation loss: 1.0827414989471436\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 100.52it/s]\u001b[AValidation loss: 1.5968875885009766\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 100.67it/s]\u001b[AValidation loss: 0.7938784956932068\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 100.81it/s]\u001b[AValidation loss: 1.282247543334961\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 100.54it/s]\u001b[AValidation loss: 1.1416523456573486\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 100.61it/s]\u001b[AValidation loss: 1.1740328073501587\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 100.73it/s]\u001b[AValidation loss: 1.1536616086959839\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 100.84it/s]\u001b[AValidation loss: 1.0070759057998657\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 100.95it/s]\u001b[AValidation loss: 1.447035312652588\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 101.04it/s]\u001b[AValidation loss: 0.8085000514984131\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.09it/s]\u001b[AValidation loss: 1.0188387632369995\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.12it/s]\u001b[AValidation loss: 1.0530028343200684\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 101.23it/s]\u001b[AValidation loss: 1.0495986938476562\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 101.29it/s]\u001b[AValidation loss: 1.311561942100525\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 101.37it/s]\u001b[AValidation loss: 0.9138704538345337\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 101.39it/s]\u001b[AValidation loss: 1.0236024856567383\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 101.39it/s]\u001b[AValidation loss: 0.9912688136100769\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 101.44it/s]\u001b[AValidation loss: 0.883337676525116\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 101.46it/s]\u001b[AValidation loss: 1.9383779764175415\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 101.48it/s]\u001b[AValidation loss: 0.6376304626464844\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 101.53it/s]\u001b[AValidation loss: 1.5779491662979126\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 101.56it/s]\u001b[AValidation loss: 0.692814290523529\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 101.60it/s]\u001b[AValidation loss: 1.0925110578536987\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 101.64it/s]\u001b[AValidation loss: 1.0099666118621826\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 101.70it/s]\u001b[AValidation loss: 0.8768267035484314\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 101.74it/s]\u001b[AValidation loss: 0.7814825177192688\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 101.80it/s]\u001b[AValidation loss: 0.8606611490249634\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 101.85it/s]\u001b[AValidation loss: 1.3467597961425781\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 101.88it/s]\u001b[AValidation loss: 1.4812595844268799\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 101.90it/s]\u001b[AValidation loss: 0.7694644331932068\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 101.97it/s]\u001b[AValidation loss: 1.0189366340637207\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 102.01it/s]\u001b[AValidation loss: 1.001775860786438\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 102.00it/s]\u001b[AValidation loss: 1.1706819534301758\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 102.03it/s]\u001b[AValidation loss: 1.4337570667266846\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 102.07it/s]\u001b[AValidation loss: 1.2909858226776123\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 102.10it/s]\u001b[AValidation loss: 1.1442943811416626\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 102.13it/s]\u001b[AValidation loss: 1.4353748559951782\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 102.17it/s]\u001b[AValidation loss: 1.115617275238037\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 102.21it/s]\u001b[AValidation loss: 2.191410541534424\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 102.38it/s]\u001b[A\n",
      "Epoch 5:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]                \u001b[ATraining loss: 1.1609975099563599\n",
      "Epoch 5:   0%|          | 1/212 [00:00<00:04, 43.66it/s, v_num=0]Training loss: 1.0701048374176025\n",
      "Epoch 5:   1%|          | 2/212 [00:00<00:05, 40.21it/s, v_num=0]Training loss: 0.9888550639152527\n",
      "Epoch 5:   1%|▏         | 3/212 [00:00<00:05, 39.31it/s, v_num=0]Training loss: 0.7563501000404358\n",
      "Epoch 5:   2%|▏         | 4/212 [00:00<00:05, 38.84it/s, v_num=0]Training loss: 1.002662181854248\n",
      "Epoch 5:   2%|▏         | 5/212 [00:00<00:05, 38.59it/s, v_num=0]Training loss: 0.8381854295730591\n",
      "Epoch 5:   3%|▎         | 6/212 [00:00<00:05, 38.44it/s, v_num=0]Training loss: 1.3563439846038818\n",
      "Epoch 5:   3%|▎         | 7/212 [00:00<00:05, 38.30it/s, v_num=0]Training loss: 0.7909923791885376\n",
      "Epoch 5:   4%|▍         | 8/212 [00:00<00:05, 38.08it/s, v_num=0]Training loss: 1.0957963466644287\n",
      "Epoch 5:   4%|▍         | 9/212 [00:00<00:05, 38.03it/s, v_num=0]Training loss: 1.0360791683197021\n",
      "Epoch 5:   5%|▍         | 10/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 1.0200815200805664\n",
      "Epoch 5:   5%|▌         | 11/212 [00:00<00:05, 37.94it/s, v_num=0]Training loss: 1.3583898544311523\n",
      "Epoch 5:   6%|▌         | 12/212 [00:00<00:05, 37.90it/s, v_num=0]Training loss: 0.9915987253189087\n",
      "Epoch 5:   6%|▌         | 13/212 [00:00<00:05, 37.88it/s, v_num=0]Training loss: 1.1024008989334106\n",
      "Epoch 5:   7%|▋         | 14/212 [00:00<00:05, 37.85it/s, v_num=0]Training loss: 0.877066969871521\n",
      "Epoch 5:   7%|▋         | 15/212 [00:00<00:05, 37.83it/s, v_num=0]Training loss: 1.0085461139678955\n",
      "Epoch 5:   8%|▊         | 16/212 [00:00<00:05, 37.81it/s, v_num=0]Training loss: 0.7458745241165161\n",
      "Epoch 5:   8%|▊         | 17/212 [00:00<00:05, 37.79it/s, v_num=0]Training loss: 10.332735061645508\n",
      "Epoch 5:   8%|▊         | 18/212 [00:00<00:05, 37.78it/s, v_num=0]Training loss: 0.8240143060684204\n",
      "Epoch 5:   9%|▉         | 19/212 [00:00<00:05, 37.77it/s, v_num=0]Training loss: 0.892577588558197\n",
      "Epoch 5:   9%|▉         | 20/212 [00:00<00:05, 37.75it/s, v_num=0]Training loss: 0.7020838856697083\n",
      "Epoch 5:  10%|▉         | 21/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 1.898769736289978\n",
      "Epoch 5:  10%|█         | 22/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 1.4291646480560303\n",
      "Epoch 5:  11%|█         | 23/212 [00:00<00:05, 37.63it/s, v_num=0]Training loss: 1.0525623559951782\n",
      "Epoch 5:  11%|█▏        | 24/212 [00:00<00:05, 37.56it/s, v_num=0]Training loss: 0.6567145586013794\n",
      "Epoch 5:  12%|█▏        | 25/212 [00:00<00:04, 37.52it/s, v_num=0]Training loss: 1.2110826969146729\n",
      "Epoch 5:  12%|█▏        | 26/212 [00:00<00:04, 37.47it/s, v_num=0]Training loss: 1.0111843347549438\n",
      "Epoch 5:  13%|█▎        | 27/212 [00:00<00:04, 37.41it/s, v_num=0]Training loss: 1.2212820053100586\n",
      "Epoch 5:  13%|█▎        | 28/212 [00:00<00:04, 37.38it/s, v_num=0]Training loss: 3.804014205932617\n",
      "Epoch 5:  14%|█▎        | 29/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.3196977376937866\n",
      "Epoch 5:  14%|█▍        | 30/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 0.9121321439743042\n",
      "Epoch 5:  15%|█▍        | 31/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 1.498780369758606\n",
      "Epoch 5:  15%|█▌        | 32/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.9609584212303162\n",
      "Epoch 5:  16%|█▌        | 33/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 2.2697434425354004\n",
      "Epoch 5:  16%|█▌        | 34/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 2.0169451236724854\n",
      "Epoch 5:  17%|█▋        | 35/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 0.8402401804924011\n",
      "Epoch 5:  17%|█▋        | 36/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.171679139137268\n",
      "Epoch 5:  17%|█▋        | 37/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 2.3689746856689453\n",
      "Epoch 5:  18%|█▊        | 38/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.449419379234314\n",
      "Epoch 5:  18%|█▊        | 39/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.3364293575286865\n",
      "Epoch 5:  19%|█▉        | 40/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.940937340259552\n",
      "Epoch 5:  19%|█▉        | 41/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.3689556121826172\n",
      "Epoch 5:  20%|█▉        | 42/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.753684401512146\n",
      "Epoch 5:  20%|██        | 43/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.3959189653396606\n",
      "Epoch 5:  21%|██        | 44/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.3320482969284058\n",
      "Epoch 5:  21%|██        | 45/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.179226040840149\n",
      "Epoch 5:  22%|██▏       | 46/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.4024739265441895\n",
      "Epoch 5:  22%|██▏       | 47/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.139987826347351\n",
      "Epoch 5:  23%|██▎       | 48/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.13588285446167\n",
      "Epoch 5:  23%|██▎       | 49/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.761151134967804\n",
      "Epoch 5:  24%|██▎       | 50/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.8764702081680298\n",
      "Epoch 5:  24%|██▍       | 51/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.7138558626174927\n",
      "Epoch 5:  25%|██▍       | 52/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.9052053689956665\n",
      "Epoch 5:  25%|██▌       | 53/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.3149532079696655\n",
      "Epoch 5:  25%|██▌       | 54/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 2.2122976779937744\n",
      "Epoch 5:  26%|██▌       | 55/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.9642210006713867\n",
      "Epoch 5:  26%|██▋       | 56/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.845885694026947\n",
      "Epoch 5:  27%|██▋       | 57/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.6487296223640442\n",
      "Epoch 5:  27%|██▋       | 58/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.2409156560897827\n",
      "Epoch 5:  28%|██▊       | 59/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 2.078857898712158\n",
      "Epoch 5:  28%|██▊       | 60/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.5430867671966553\n",
      "Epoch 5:  29%|██▉       | 61/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.8675949573516846\n",
      "Epoch 5:  29%|██▉       | 62/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.1317205429077148\n",
      "Epoch 5:  30%|██▉       | 63/212 [00:01<00:03, 37.36it/s, v_num=0]Training loss: 1.2759298086166382\n",
      "Epoch 5:  30%|███       | 64/212 [00:01<00:03, 37.34it/s, v_num=0]Training loss: 1.0518122911453247\n",
      "Epoch 5:  31%|███       | 65/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 0.8218629360198975\n",
      "Epoch 5:  31%|███       | 66/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 0.8952271938323975\n",
      "Epoch 5:  32%|███▏      | 67/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 1.134303331375122\n",
      "Epoch 5:  32%|███▏      | 68/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 0.8626556992530823\n",
      "Epoch 5:  33%|███▎      | 69/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.224115252494812\n",
      "Epoch 5:  33%|███▎      | 70/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.014614224433899\n",
      "Epoch 5:  33%|███▎      | 71/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 1.4361259937286377\n",
      "Epoch 5:  34%|███▍      | 72/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.09977126121521\n",
      "Epoch 5:  34%|███▍      | 73/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.9570461511611938\n",
      "Epoch 5:  35%|███▍      | 74/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.2271240949630737\n",
      "Epoch 5:  35%|███▌      | 75/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.2548696994781494\n",
      "Epoch 5:  36%|███▌      | 76/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9208247661590576\n",
      "Epoch 5:  36%|███▋      | 77/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.3784106969833374\n",
      "Epoch 5:  37%|███▋      | 78/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.0349596738815308\n",
      "Epoch 5:  37%|███▋      | 79/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.8726757764816284\n",
      "Epoch 5:  38%|███▊      | 80/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9635398387908936\n",
      "Epoch 5:  38%|███▊      | 81/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7555423378944397\n",
      "Epoch 5:  39%|███▊      | 82/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.774926483631134\n",
      "Epoch 5:  39%|███▉      | 83/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.2321698665618896\n",
      "Epoch 5:  40%|███▉      | 84/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.194462776184082\n",
      "Epoch 5:  40%|████      | 85/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9176310896873474\n",
      "Epoch 5:  41%|████      | 86/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.0117048025131226\n",
      "Epoch 5:  41%|████      | 87/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9108121991157532\n",
      "Epoch 5:  42%|████▏     | 88/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9288150072097778\n",
      "Epoch 5:  42%|████▏     | 89/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.4364198446273804\n",
      "Epoch 5:  42%|████▏     | 90/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.8568711280822754\n",
      "Epoch 5:  43%|████▎     | 91/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.1043362617492676\n",
      "Epoch 5:  43%|████▎     | 92/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.3552663326263428\n",
      "Epoch 5:  44%|████▍     | 93/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.7769365310668945\n",
      "Epoch 5:  44%|████▍     | 94/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.0127054452896118\n",
      "Epoch 5:  45%|████▍     | 95/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.8698395490646362\n",
      "Epoch 5:  45%|████▌     | 96/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.1844104528427124\n",
      "Epoch 5:  46%|████▌     | 97/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.016000747680664\n",
      "Epoch 5:  46%|████▌     | 98/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.7158363461494446\n",
      "Epoch 5:  47%|████▋     | 99/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.358682632446289\n",
      "Epoch 5:  47%|████▋     | 100/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.8986051082611084\n",
      "Epoch 5:  48%|████▊     | 101/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 1.9277681112289429\n",
      "Epoch 5:  48%|████▊     | 102/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.9885347485542297\n",
      "Epoch 5:  49%|████▊     | 103/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 1.0434073209762573\n",
      "Epoch 5:  49%|████▉     | 104/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 1.1124801635742188\n",
      "Epoch 5:  50%|████▉     | 105/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 0.706246018409729\n",
      "Epoch 5:  50%|█████     | 106/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.1331825256347656\n",
      "Epoch 5:  50%|█████     | 107/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.0909360647201538\n",
      "Epoch 5:  51%|█████     | 108/212 [00:02<00:02, 37.26it/s, v_num=0]Training loss: 0.8784949779510498\n",
      "Epoch 5:  51%|█████▏    | 109/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.0700106620788574\n",
      "Epoch 5:  52%|█████▏    | 110/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.9317472577095032\n",
      "Epoch 5:  52%|█████▏    | 111/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.0348423719406128\n",
      "Epoch 5:  53%|█████▎    | 112/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1435691118240356\n",
      "Epoch 5:  53%|█████▎    | 113/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1510229110717773\n",
      "Epoch 5:  54%|█████▍    | 114/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6597995758056641\n",
      "Epoch 5:  54%|█████▍    | 115/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.4877595901489258\n",
      "Epoch 5:  55%|█████▍    | 116/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.2152173519134521\n",
      "Epoch 5:  55%|█████▌    | 117/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0336281061172485\n",
      "Epoch 5:  56%|█████▌    | 118/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.2810070514678955\n",
      "Epoch 5:  56%|█████▌    | 119/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7904520034790039\n",
      "Epoch 5:  57%|█████▋    | 120/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0527571439743042\n",
      "Epoch 5:  57%|█████▋    | 121/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.5783091187477112\n",
      "Epoch 5:  58%|█████▊    | 122/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7891317009925842\n",
      "Epoch 5:  58%|█████▊    | 123/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.674328088760376\n",
      "Epoch 5:  58%|█████▊    | 124/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.3413028717041016\n",
      "Epoch 5:  59%|█████▉    | 125/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.8126958012580872\n",
      "Epoch 5:  59%|█████▉    | 126/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.8236404061317444\n",
      "Epoch 5:  60%|█████▉    | 127/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 2.709214687347412\n",
      "Epoch 5:  60%|██████    | 128/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.0319410562515259\n",
      "Epoch 5:  61%|██████    | 129/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.5776406526565552\n",
      "Epoch 5:  61%|██████▏   | 130/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.3175355195999146\n",
      "Epoch 5:  62%|██████▏   | 131/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.5369716286659241\n",
      "Epoch 5:  62%|██████▏   | 132/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.045112133026123\n",
      "Epoch 5:  63%|██████▎   | 133/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 0.7684547901153564\n",
      "Epoch 5:  63%|██████▎   | 134/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.4911060333251953\n",
      "Epoch 5:  64%|██████▎   | 135/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 0.9320144057273865\n",
      "Epoch 5:  64%|██████▍   | 136/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.0476555824279785\n",
      "Epoch 5:  65%|██████▍   | 137/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.1533247232437134\n",
      "Epoch 5:  65%|██████▌   | 138/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 0.9687182903289795\n",
      "Epoch 5:  66%|██████▌   | 139/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 1.2220776081085205\n",
      "Epoch 5:  66%|██████▌   | 140/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 0.8272144794464111\n",
      "Epoch 5:  67%|██████▋   | 141/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 0.9163143634796143\n",
      "Epoch 5:  67%|██████▋   | 142/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 1.1474323272705078\n",
      "Epoch 5:  67%|██████▋   | 143/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.8371984958648682\n",
      "Epoch 5:  68%|██████▊   | 144/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 1.1371320486068726\n",
      "Epoch 5:  68%|██████▊   | 145/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 1.2243143320083618\n",
      "Epoch 5:  69%|██████▉   | 146/212 [00:03<00:01, 37.26it/s, v_num=0]Training loss: 1.1879147291183472\n",
      "Epoch 5:  69%|██████▉   | 147/212 [00:03<00:01, 37.26it/s, v_num=0]Training loss: 0.9254719614982605\n",
      "Epoch 5:  70%|██████▉   | 148/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 0.8299610018730164\n",
      "Epoch 5:  70%|███████   | 149/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.6457757949829102\n",
      "Epoch 5:  71%|███████   | 150/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.4295406341552734\n",
      "Epoch 5:  71%|███████   | 151/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2587494850158691\n",
      "Epoch 5:  72%|███████▏  | 152/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2643545866012573\n",
      "Epoch 5:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7307901978492737\n",
      "Epoch 5:  73%|███████▎  | 154/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.194593071937561\n",
      "Epoch 5:  73%|███████▎  | 155/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9795585870742798\n",
      "Epoch 5:  74%|███████▎  | 156/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7937412858009338\n",
      "Epoch 5:  74%|███████▍  | 157/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.185445785522461\n",
      "Epoch 5:  75%|███████▍  | 158/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.5195726156234741\n",
      "Epoch 5:  75%|███████▌  | 159/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.0078599452972412\n",
      "Epoch 5:  75%|███████▌  | 160/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.0770225524902344\n",
      "Epoch 5:  76%|███████▌  | 161/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.6721078157424927\n",
      "Epoch 5:  76%|███████▋  | 162/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.3442059755325317\n",
      "Epoch 5:  77%|███████▋  | 163/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.931850016117096\n",
      "Epoch 5:  77%|███████▋  | 164/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.9670798778533936\n",
      "Epoch 5:  78%|███████▊  | 165/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.0895081758499146\n",
      "Epoch 5:  78%|███████▊  | 166/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.2686280012130737\n",
      "Epoch 5:  79%|███████▉  | 167/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.1298655271530151\n",
      "Epoch 5:  79%|███████▉  | 168/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.234750747680664\n",
      "Epoch 5:  80%|███████▉  | 169/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8421446681022644\n",
      "Epoch 5:  80%|████████  | 170/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.0166025161743164\n",
      "Epoch 5:  81%|████████  | 171/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.7977455258369446\n",
      "Epoch 5:  81%|████████  | 172/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 1.3009690046310425\n",
      "Epoch 5:  82%|████████▏ | 173/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.885722279548645\n",
      "Epoch 5:  82%|████████▏ | 174/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.4671786427497864\n",
      "Epoch 5:  83%|████████▎ | 175/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.922248363494873\n",
      "Epoch 5:  83%|████████▎ | 176/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.9337356686592102\n",
      "Epoch 5:  83%|████████▎ | 177/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 1.2967641353607178\n",
      "Epoch 5:  84%|████████▍ | 178/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 1.1394290924072266\n",
      "Epoch 5:  84%|████████▍ | 179/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 1.260019302368164\n",
      "Epoch 5:  85%|████████▍ | 180/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 0.7852964401245117\n",
      "Epoch 5:  85%|████████▌ | 181/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 0.8556246161460876\n",
      "Epoch 5:  86%|████████▌ | 182/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.6758543848991394\n",
      "Epoch 5:  86%|████████▋ | 183/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.9656257629394531\n",
      "Epoch 5:  87%|████████▋ | 184/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 3.421903371810913\n",
      "Epoch 5:  87%|████████▋ | 185/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.629047155380249\n",
      "Epoch 5:  88%|████████▊ | 186/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 0.9855719804763794\n",
      "Epoch 5:  88%|████████▊ | 187/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 2.6172497272491455\n",
      "Epoch 5:  89%|████████▊ | 188/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 9.844034194946289\n",
      "Epoch 5:  89%|████████▉ | 189/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8127956390380859\n",
      "Epoch 5:  90%|████████▉ | 190/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.6216893792152405\n",
      "Epoch 5:  90%|█████████ | 191/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.734254777431488\n",
      "Epoch 5:  91%|█████████ | 192/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8878348469734192\n",
      "Epoch 5:  91%|█████████ | 193/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.938088595867157\n",
      "Epoch 5:  92%|█████████▏| 194/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.698663055896759\n",
      "Epoch 5:  92%|█████████▏| 195/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.1250278949737549\n",
      "Epoch 5:  92%|█████████▏| 196/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.6049079895019531\n",
      "Epoch 5:  93%|█████████▎| 197/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.2938705682754517\n",
      "Epoch 5:  93%|█████████▎| 198/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.0169011354446411\n",
      "Epoch 5:  94%|█████████▍| 199/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.6268703937530518\n",
      "Epoch 5:  94%|█████████▍| 200/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.1457189321517944\n",
      "Epoch 5:  95%|█████████▍| 201/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9318903684616089\n",
      "Epoch 5:  95%|█████████▌| 202/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.7202453017234802\n",
      "Epoch 5:  96%|█████████▌| 203/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.7481741905212402\n",
      "Epoch 5:  96%|█████████▌| 204/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.039497971534729\n",
      "Epoch 5:  97%|█████████▋| 205/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9092816710472107\n",
      "Epoch 5:  97%|█████████▋| 206/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9062440991401672\n",
      "Epoch 5:  98%|█████████▊| 207/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.8911259174346924\n",
      "Epoch 5:  98%|█████████▊| 208/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.024289846420288\n",
      "Epoch 5:  99%|█████████▊| 209/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.8402584791183472\n",
      "Epoch 5:  99%|█████████▉| 210/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.7827333211898804\n",
      "Epoch 5: 100%|█████████▉| 211/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.1204094886779785\n",
      "Epoch 5: 100%|██████████| 212/212 [00:05<00:00, 37.26it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.1228339672088623\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 79.78it/s]\u001b[AValidation loss: 1.8775092363357544\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.68it/s]\u001b[AValidation loss: 0.9276627898216248\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 94.75it/s]\u001b[AValidation loss: 0.6822933554649353\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 96.92it/s]\u001b[AValidation loss: 0.7087057828903198\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 98.53it/s]\u001b[AValidation loss: 0.869620680809021\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 99.59it/s]\u001b[AValidation loss: 1.2063509225845337\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 100.42it/s]\u001b[AValidation loss: 2.16452693939209\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 101.04it/s]\u001b[AValidation loss: 0.6113815903663635\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 100.69it/s]\u001b[AValidation loss: 0.7229619026184082\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 101.14it/s]\u001b[AValidation loss: 0.7266588807106018\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 101.49it/s]\u001b[AValidation loss: 1.0354514122009277\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 101.76it/s]\u001b[AValidation loss: 1.0364145040512085\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 102.02it/s]\u001b[AValidation loss: 0.948754072189331\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 102.17it/s]\u001b[AValidation loss: 0.7831274271011353\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 102.36it/s]\u001b[AValidation loss: 1.0974301099777222\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 102.57it/s]\u001b[AValidation loss: 1.0334120988845825\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 102.71it/s]\u001b[AValidation loss: 1.5293446779251099\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.87it/s]\u001b[AValidation loss: 0.7381840944290161\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 103.00it/s]\u001b[AValidation loss: 1.2221719026565552\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 103.10it/s]\u001b[AValidation loss: 1.0855697393417358\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 103.12it/s]\u001b[AValidation loss: 1.11307954788208\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 102.88it/s]\u001b[AValidation loss: 1.0922152996063232\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 102.60it/s]\u001b[AValidation loss: 0.9370176792144775\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 102.28it/s]\u001b[AValidation loss: 1.3713481426239014\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 102.04it/s]\u001b[AValidation loss: 0.7492182850837708\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.72it/s]\u001b[AValidation loss: 0.9451703429222107\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.50it/s]\u001b[AValidation loss: 0.9817785620689392\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 101.33it/s]\u001b[AValidation loss: 0.9872969388961792\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 101.11it/s]\u001b[AValidation loss: 1.2572300434112549\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 100.91it/s]\u001b[AValidation loss: 0.8503063917160034\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 100.73it/s]\u001b[AValidation loss: 0.981311559677124\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 100.37it/s]\u001b[AValidation loss: 0.9313079714775085\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 100.02it/s]\u001b[AValidation loss: 0.8172933459281921\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 99.86it/s] \u001b[AValidation loss: 1.8463863134384155\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 99.71it/s]\u001b[AValidation loss: 0.5829959511756897\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 99.60it/s]\u001b[AValidation loss: 1.5292418003082275\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 99.48it/s]\u001b[AValidation loss: 0.6436491012573242\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 99.39it/s]\u001b[AValidation loss: 1.0211769342422485\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 99.31it/s]\u001b[AValidation loss: 0.9521801471710205\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 99.23it/s]\u001b[AValidation loss: 0.8123841285705566\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 98.94it/s]\u001b[AValidation loss: 0.7256625890731812\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 98.68it/s]\u001b[AValidation loss: 0.7936251759529114\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 98.46it/s]\u001b[AValidation loss: 1.2831003665924072\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 98.26it/s]\u001b[AValidation loss: 1.4081264734268188\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 98.26it/s]\u001b[AValidation loss: 0.7151954174041748\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 98.20it/s]\u001b[AValidation loss: 0.9583979249000549\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 98.15it/s]\u001b[AValidation loss: 0.9380099773406982\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 98.09it/s]\u001b[AValidation loss: 1.119064211845398\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 98.05it/s]\u001b[AValidation loss: 1.3676023483276367\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 98.04it/s]\u001b[AValidation loss: 1.225233793258667\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 98.02it/s]\u001b[AValidation loss: 1.0907657146453857\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 98.02it/s]\u001b[AValidation loss: 1.373207449913025\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 98.13it/s]\u001b[AValidation loss: 1.0540355443954468\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 98.21it/s]\u001b[AValidation loss: 2.142655611038208\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 98.44it/s]\u001b[A\n",
      "Epoch 6:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.9534025192260742\n",
      "Epoch 6:   0%|          | 1/212 [00:00<00:08, 25.68it/s, v_num=0]Training loss: 1.1540391445159912\n",
      "Epoch 6:   1%|          | 2/212 [00:00<00:07, 26.92it/s, v_num=0]Training loss: 0.8492335081100464\n",
      "Epoch 6:   1%|▏         | 3/212 [00:00<00:07, 27.10it/s, v_num=0]Training loss: 0.8529399633407593\n",
      "Epoch 6:   2%|▏         | 4/212 [00:00<00:07, 27.36it/s, v_num=0]Training loss: 1.431397795677185\n",
      "Epoch 6:   2%|▏         | 5/212 [00:00<00:07, 27.53it/s, v_num=0]Training loss: 1.0528062582015991\n",
      "Epoch 6:   3%|▎         | 6/212 [00:00<00:07, 27.31it/s, v_num=0]Training loss: 0.9186140894889832\n",
      "Epoch 6:   3%|▎         | 7/212 [00:00<00:07, 27.25it/s, v_num=0]Training loss: 1.3344414234161377\n",
      "Epoch 6:   4%|▍         | 8/212 [00:00<00:07, 27.33it/s, v_num=0]Training loss: 0.6746624708175659\n",
      "Epoch 6:   4%|▍         | 9/212 [00:00<00:07, 27.47it/s, v_num=0]Training loss: 1.077816128730774\n",
      "Epoch 6:   5%|▍         | 10/212 [00:00<00:07, 27.92it/s, v_num=0]Training loss: 1.1975970268249512\n",
      "Epoch 6:   5%|▌         | 11/212 [00:00<00:07, 28.09it/s, v_num=0]Training loss: 1.3895071744918823\n",
      "Epoch 6:   6%|▌         | 12/212 [00:00<00:07, 28.15it/s, v_num=0]Training loss: 1.1339513063430786\n",
      "Epoch 6:   6%|▌         | 13/212 [00:00<00:07, 28.23it/s, v_num=0]Training loss: 1.3187378644943237\n",
      "Epoch 6:   7%|▋         | 14/212 [00:00<00:07, 28.11it/s, v_num=0]Training loss: 0.7304919958114624\n",
      "Epoch 6:   7%|▋         | 15/212 [00:00<00:06, 28.31it/s, v_num=0]Training loss: 1.1689890623092651\n",
      "Epoch 6:   8%|▊         | 16/212 [00:00<00:06, 28.44it/s, v_num=0]Training loss: 0.7893835306167603\n",
      "Epoch 6:   8%|▊         | 17/212 [00:00<00:06, 28.43it/s, v_num=0]Training loss: 3.9977030754089355\n",
      "Epoch 6:   8%|▊         | 18/212 [00:00<00:06, 28.46it/s, v_num=0]Training loss: 0.5241624116897583\n",
      "Epoch 6:   9%|▉         | 19/212 [00:00<00:06, 28.52it/s, v_num=0]Training loss: 0.9928627014160156\n",
      "Epoch 6:   9%|▉         | 20/212 [00:00<00:06, 28.89it/s, v_num=0]Training loss: 0.8557921648025513\n",
      "Epoch 6:  10%|▉         | 21/212 [00:00<00:06, 29.18it/s, v_num=0]Training loss: 0.9175824522972107\n",
      "Epoch 6:  10%|█         | 22/212 [00:00<00:06, 29.48it/s, v_num=0]Training loss: 1.2749412059783936\n",
      "Epoch 6:  11%|█         | 23/212 [00:00<00:06, 29.76it/s, v_num=0]Training loss: 1.1764824390411377\n",
      "Epoch 6:  11%|█▏        | 24/212 [00:00<00:06, 29.99it/s, v_num=0]Training loss: 0.9299543499946594\n",
      "Epoch 6:  12%|█▏        | 25/212 [00:00<00:06, 30.24it/s, v_num=0]Training loss: 1.173326015472412\n",
      "Epoch 6:  12%|█▏        | 26/212 [00:00<00:06, 30.40it/s, v_num=0]Training loss: 0.7993980050086975\n",
      "Epoch 6:  13%|█▎        | 27/212 [00:00<00:06, 30.61it/s, v_num=0]Training loss: 2.2443370819091797\n",
      "Epoch 6:  13%|█▎        | 28/212 [00:00<00:05, 30.78it/s, v_num=0]Training loss: 1.0152581930160522\n",
      "Epoch 6:  14%|█▎        | 29/212 [00:00<00:05, 30.94it/s, v_num=0]Training loss: 0.793236494064331\n",
      "Epoch 6:  14%|█▍        | 30/212 [00:00<00:05, 31.09it/s, v_num=0]Training loss: 1.370671272277832\n",
      "Epoch 6:  15%|█▍        | 31/212 [00:00<00:05, 31.23it/s, v_num=0]Training loss: 1.0380945205688477\n",
      "Epoch 6:  15%|█▌        | 32/212 [00:01<00:05, 31.35it/s, v_num=0]Training loss: 0.9025927782058716\n",
      "Epoch 6:  16%|█▌        | 33/212 [00:01<00:05, 31.47it/s, v_num=0]Training loss: 0.8963961601257324\n",
      "Epoch 6:  16%|█▌        | 34/212 [00:01<00:05, 31.61it/s, v_num=0]Training loss: 0.801939845085144\n",
      "Epoch 6:  17%|█▋        | 35/212 [00:01<00:05, 31.77it/s, v_num=0]Training loss: 1.405224323272705\n",
      "Epoch 6:  17%|█▋        | 36/212 [00:01<00:05, 31.91it/s, v_num=0]Training loss: 1.2578270435333252\n",
      "Epoch 6:  17%|█▋        | 37/212 [00:01<00:05, 32.04it/s, v_num=0]Training loss: 0.6168341040611267\n",
      "Epoch 6:  18%|█▊        | 38/212 [00:01<00:05, 32.16it/s, v_num=0]Training loss: 1.2819775342941284\n",
      "Epoch 6:  18%|█▊        | 39/212 [00:01<00:05, 32.28it/s, v_num=0]Training loss: 0.6899515986442566\n",
      "Epoch 6:  19%|█▉        | 40/212 [00:01<00:05, 32.39it/s, v_num=0]Training loss: 0.8555155992507935\n",
      "Epoch 6:  19%|█▉        | 41/212 [00:01<00:05, 32.50it/s, v_num=0]Training loss: 0.799201250076294\n",
      "Epoch 6:  20%|█▉        | 42/212 [00:01<00:05, 32.60it/s, v_num=0]Training loss: 1.4184904098510742\n",
      "Epoch 6:  20%|██        | 43/212 [00:01<00:05, 32.70it/s, v_num=0]Training loss: 0.7852007746696472\n",
      "Epoch 6:  21%|██        | 44/212 [00:01<00:05, 32.79it/s, v_num=0]Training loss: 0.8393261432647705\n",
      "Epoch 6:  21%|██        | 45/212 [00:01<00:05, 32.88it/s, v_num=0]Training loss: 0.7635137438774109\n",
      "Epoch 6:  22%|██▏       | 46/212 [00:01<00:05, 32.97it/s, v_num=0]Training loss: 0.7228107452392578\n",
      "Epoch 6:  22%|██▏       | 47/212 [00:01<00:04, 33.05it/s, v_num=0]Training loss: 0.8659892678260803\n",
      "Epoch 6:  23%|██▎       | 48/212 [00:01<00:04, 33.14it/s, v_num=0]Training loss: 1.428926706314087\n",
      "Epoch 6:  23%|██▎       | 49/212 [00:01<00:04, 33.22it/s, v_num=0]Training loss: 0.8369579315185547\n",
      "Epoch 6:  24%|██▎       | 50/212 [00:01<00:04, 33.29it/s, v_num=0]Training loss: 0.7224538922309875\n",
      "Epoch 6:  24%|██▍       | 51/212 [00:01<00:04, 33.37it/s, v_num=0]Training loss: 1.0926153659820557\n",
      "Epoch 6:  25%|██▍       | 52/212 [00:01<00:04, 33.44it/s, v_num=0]Training loss: 0.8318504095077515\n",
      "Epoch 6:  25%|██▌       | 53/212 [00:01<00:04, 33.51it/s, v_num=0]Training loss: 0.9402477741241455\n",
      "Epoch 6:  25%|██▌       | 54/212 [00:01<00:04, 33.57it/s, v_num=0]Training loss: 1.1555137634277344\n",
      "Epoch 6:  26%|██▌       | 55/212 [00:01<00:04, 33.64it/s, v_num=0]Training loss: 0.6041392087936401\n",
      "Epoch 6:  26%|██▋       | 56/212 [00:01<00:04, 33.70it/s, v_num=0]Training loss: 0.6781433820724487\n",
      "Epoch 6:  27%|██▋       | 57/212 [00:01<00:04, 33.76it/s, v_num=0]Training loss: 0.6394379734992981\n",
      "Epoch 6:  27%|██▋       | 58/212 [00:01<00:04, 33.82it/s, v_num=0]Training loss: 0.6838251948356628\n",
      "Epoch 6:  28%|██▊       | 59/212 [00:01<00:04, 33.88it/s, v_num=0]Training loss: 0.8693689703941345\n",
      "Epoch 6:  28%|██▊       | 60/212 [00:01<00:04, 33.94it/s, v_num=0]Training loss: 1.38284170627594\n",
      "Epoch 6:  29%|██▉       | 61/212 [00:01<00:04, 33.99it/s, v_num=0]Training loss: 1.4637186527252197\n",
      "Epoch 6:  29%|██▉       | 62/212 [00:01<00:04, 34.04it/s, v_num=0]Training loss: 0.8604570627212524\n",
      "Epoch 6:  30%|██▉       | 63/212 [00:01<00:04, 34.10it/s, v_num=0]Training loss: 0.9497804045677185\n",
      "Epoch 6:  30%|███       | 64/212 [00:01<00:04, 34.15it/s, v_num=0]Training loss: 0.917679488658905\n",
      "Epoch 6:  31%|███       | 65/212 [00:01<00:04, 34.19it/s, v_num=0]Training loss: 1.423958659172058\n",
      "Epoch 6:  31%|███       | 66/212 [00:01<00:04, 34.21it/s, v_num=0]Training loss: 0.8536301851272583\n",
      "Epoch 6:  32%|███▏      | 67/212 [00:01<00:04, 34.24it/s, v_num=0]Training loss: 1.6003323793411255\n",
      "Epoch 6:  32%|███▏      | 68/212 [00:01<00:04, 34.27it/s, v_num=0]Training loss: 0.800422191619873\n",
      "Epoch 6:  33%|███▎      | 69/212 [00:02<00:04, 34.30it/s, v_num=0]Training loss: 0.9717317819595337\n",
      "Epoch 6:  33%|███▎      | 70/212 [00:02<00:04, 34.33it/s, v_num=0]Training loss: 0.892545223236084\n",
      "Epoch 6:  33%|███▎      | 71/212 [00:02<00:04, 34.35it/s, v_num=0]Training loss: 0.8999327421188354\n",
      "Epoch 6:  34%|███▍      | 72/212 [00:02<00:04, 34.37it/s, v_num=0]Training loss: 0.9282330274581909\n",
      "Epoch 6:  34%|███▍      | 73/212 [00:02<00:04, 34.39it/s, v_num=0]Training loss: 1.1175552606582642\n",
      "Epoch 6:  35%|███▍      | 74/212 [00:02<00:04, 34.44it/s, v_num=0]Training loss: 0.9120526313781738\n",
      "Epoch 6:  35%|███▌      | 75/212 [00:02<00:03, 34.47it/s, v_num=0]Training loss: 1.3067874908447266\n",
      "Epoch 6:  36%|███▌      | 76/212 [00:02<00:03, 34.51it/s, v_num=0]Training loss: 0.7072416543960571\n",
      "Epoch 6:  36%|███▋      | 77/212 [00:02<00:03, 34.54it/s, v_num=0]Training loss: 1.2215903997421265\n",
      "Epoch 6:  37%|███▋      | 78/212 [00:02<00:03, 34.58it/s, v_num=0]Training loss: 1.8249001502990723\n",
      "Epoch 6:  37%|███▋      | 79/212 [00:02<00:03, 34.61it/s, v_num=0]Training loss: 0.8440158367156982\n",
      "Epoch 6:  38%|███▊      | 80/212 [00:02<00:03, 34.64it/s, v_num=0]Training loss: 1.420225977897644\n",
      "Epoch 6:  38%|███▊      | 81/212 [00:02<00:03, 34.68it/s, v_num=0]Training loss: 0.7849909663200378\n",
      "Epoch 6:  39%|███▊      | 82/212 [00:02<00:03, 34.71it/s, v_num=0]Training loss: 0.6573063135147095\n",
      "Epoch 6:  39%|███▉      | 83/212 [00:02<00:03, 34.74it/s, v_num=0]Training loss: 9.758673667907715\n",
      "Epoch 6:  40%|███▉      | 84/212 [00:02<00:03, 34.77it/s, v_num=0]Training loss: 1.0834909677505493\n",
      "Epoch 6:  40%|████      | 85/212 [00:02<00:03, 34.79it/s, v_num=0]Training loss: 1.0499718189239502\n",
      "Epoch 6:  41%|████      | 86/212 [00:02<00:03, 34.82it/s, v_num=0]Training loss: 1.1867239475250244\n",
      "Epoch 6:  41%|████      | 87/212 [00:02<00:03, 34.85it/s, v_num=0]Training loss: 0.9509956240653992\n",
      "Epoch 6:  42%|████▏     | 88/212 [00:02<00:03, 34.88it/s, v_num=0]Training loss: 0.9644593596458435\n",
      "Epoch 6:  42%|████▏     | 89/212 [00:02<00:03, 34.91it/s, v_num=0]Training loss: 0.8081542253494263\n",
      "Epoch 6:  42%|████▏     | 90/212 [00:02<00:03, 34.94it/s, v_num=0]Training loss: 1.2134796380996704\n",
      "Epoch 6:  43%|████▎     | 91/212 [00:02<00:03, 34.96it/s, v_num=0]Training loss: 0.9002857804298401\n",
      "Epoch 6:  43%|████▎     | 92/212 [00:02<00:03, 34.99it/s, v_num=0]Training loss: 0.5890278816223145\n",
      "Epoch 6:  44%|████▍     | 93/212 [00:02<00:03, 35.02it/s, v_num=0]Training loss: 1.0770490169525146\n",
      "Epoch 6:  44%|████▍     | 94/212 [00:02<00:03, 35.04it/s, v_num=0]Training loss: 0.8970245122909546\n",
      "Epoch 6:  45%|████▍     | 95/212 [00:02<00:03, 35.07it/s, v_num=0]Training loss: 0.8108576536178589\n",
      "Epoch 6:  45%|████▌     | 96/212 [00:02<00:03, 35.09it/s, v_num=0]Training loss: 1.0156044960021973\n",
      "Epoch 6:  46%|████▌     | 97/212 [00:02<00:03, 35.11it/s, v_num=0]Training loss: 1.1816776990890503\n",
      "Epoch 6:  46%|████▌     | 98/212 [00:02<00:03, 35.14it/s, v_num=0]Training loss: 1.2118358612060547\n",
      "Epoch 6:  47%|████▋     | 99/212 [00:02<00:03, 35.16it/s, v_num=0]Training loss: 1.221845030784607\n",
      "Epoch 6:  47%|████▋     | 100/212 [00:02<00:03, 35.18it/s, v_num=0]Training loss: 1.1109334230422974\n",
      "Epoch 6:  48%|████▊     | 101/212 [00:02<00:03, 35.20it/s, v_num=0]Training loss: 0.8055219054222107\n",
      "Epoch 6:  48%|████▊     | 102/212 [00:02<00:03, 35.22it/s, v_num=0]Training loss: 1.4558621644973755\n",
      "Epoch 6:  49%|████▊     | 103/212 [00:02<00:03, 35.24it/s, v_num=0]Training loss: 1.0347685813903809\n",
      "Epoch 6:  49%|████▉     | 104/212 [00:02<00:03, 35.26it/s, v_num=0]Training loss: 1.0951869487762451\n",
      "Epoch 6:  50%|████▉     | 105/212 [00:02<00:03, 35.28it/s, v_num=0]Training loss: 0.8632015585899353\n",
      "Epoch 6:  50%|█████     | 106/212 [00:03<00:03, 35.29it/s, v_num=0]Training loss: 0.7707720398902893\n",
      "Epoch 6:  50%|█████     | 107/212 [00:03<00:02, 35.30it/s, v_num=0]Training loss: 0.9769925475120544\n",
      "Epoch 6:  51%|█████     | 108/212 [00:03<00:02, 35.30it/s, v_num=0]Training loss: 0.7572185397148132\n",
      "Epoch 6:  51%|█████▏    | 109/212 [00:03<00:02, 35.31it/s, v_num=0]Training loss: 1.2336924076080322\n",
      "Epoch 6:  52%|█████▏    | 110/212 [00:03<00:02, 35.32it/s, v_num=0]Training loss: 1.0928648710250854\n",
      "Epoch 6:  52%|█████▏    | 111/212 [00:03<00:02, 35.33it/s, v_num=0]Training loss: 0.8932188153266907\n",
      "Epoch 6:  53%|█████▎    | 112/212 [00:03<00:02, 35.33it/s, v_num=0]Training loss: 0.8275423049926758\n",
      "Epoch 6:  53%|█████▎    | 113/212 [00:03<00:02, 35.33it/s, v_num=0]Training loss: 0.841816782951355\n",
      "Epoch 6:  54%|█████▍    | 114/212 [00:03<00:02, 35.36it/s, v_num=0]Training loss: 0.7196885943412781\n",
      "Epoch 6:  54%|█████▍    | 115/212 [00:03<00:02, 35.38it/s, v_num=0]Training loss: 1.09368896484375\n",
      "Epoch 6:  55%|█████▍    | 116/212 [00:03<00:02, 35.40it/s, v_num=0]Training loss: 2.1671290397644043\n",
      "Epoch 6:  55%|█████▌    | 117/212 [00:03<00:02, 35.41it/s, v_num=0]Training loss: 0.7553074955940247\n",
      "Epoch 6:  56%|█████▌    | 118/212 [00:03<00:02, 35.43it/s, v_num=0]Training loss: 0.766471803188324\n",
      "Epoch 6:  56%|█████▌    | 119/212 [00:03<00:02, 35.45it/s, v_num=0]Training loss: 1.0916268825531006\n",
      "Epoch 6:  57%|█████▋    | 120/212 [00:03<00:02, 35.46it/s, v_num=0]Training loss: 3.3629231452941895\n",
      "Epoch 6:  57%|█████▋    | 121/212 [00:03<00:02, 35.48it/s, v_num=0]Training loss: 1.097044587135315\n",
      "Epoch 6:  58%|█████▊    | 122/212 [00:03<00:02, 35.49it/s, v_num=0]Training loss: 0.8672517538070679\n",
      "Epoch 6:  58%|█████▊    | 123/212 [00:03<00:02, 35.51it/s, v_num=0]Training loss: 0.616234540939331\n",
      "Epoch 6:  58%|█████▊    | 124/212 [00:03<00:02, 35.52it/s, v_num=0]Training loss: 0.8942456841468811\n",
      "Epoch 6:  59%|█████▉    | 125/212 [00:03<00:02, 35.54it/s, v_num=0]Training loss: 1.0169638395309448\n",
      "Epoch 6:  59%|█████▉    | 126/212 [00:03<00:02, 35.55it/s, v_num=0]Training loss: 1.1491565704345703\n",
      "Epoch 6:  60%|█████▉    | 127/212 [00:03<00:02, 35.57it/s, v_num=0]Training loss: 2.510035991668701\n",
      "Epoch 6:  60%|██████    | 128/212 [00:03<00:02, 35.58it/s, v_num=0]Training loss: 1.278849482536316\n",
      "Epoch 6:  61%|██████    | 129/212 [00:03<00:02, 35.60it/s, v_num=0]Training loss: 0.7523180842399597\n",
      "Epoch 6:  61%|██████▏   | 130/212 [00:03<00:02, 35.61it/s, v_num=0]Training loss: 0.8916005492210388\n",
      "Epoch 6:  62%|██████▏   | 131/212 [00:03<00:02, 35.63it/s, v_num=0]Training loss: 1.1646133661270142\n",
      "Epoch 6:  62%|██████▏   | 132/212 [00:03<00:02, 35.64it/s, v_num=0]Training loss: 0.7337967753410339\n",
      "Epoch 6:  63%|██████▎   | 133/212 [00:03<00:02, 35.65it/s, v_num=0]Training loss: 1.0364046096801758\n",
      "Epoch 6:  63%|██████▎   | 134/212 [00:03<00:02, 35.67it/s, v_num=0]Training loss: 1.0983799695968628\n",
      "Epoch 6:  64%|██████▎   | 135/212 [00:03<00:02, 35.68it/s, v_num=0]Training loss: 0.9166088700294495\n",
      "Epoch 6:  64%|██████▍   | 136/212 [00:03<00:02, 35.69it/s, v_num=0]Training loss: 0.8339344263076782\n",
      "Epoch 6:  65%|██████▍   | 137/212 [00:03<00:02, 35.71it/s, v_num=0]Training loss: 0.7051712274551392\n",
      "Epoch 6:  65%|██████▌   | 138/212 [00:03<00:02, 35.72it/s, v_num=0]Training loss: 0.9331783056259155\n",
      "Epoch 6:  66%|██████▌   | 139/212 [00:03<00:02, 35.73it/s, v_num=0]Training loss: 1.4468588829040527\n",
      "Epoch 6:  66%|██████▌   | 140/212 [00:03<00:02, 35.75it/s, v_num=0]Training loss: 1.126098394393921\n",
      "Epoch 6:  67%|██████▋   | 141/212 [00:03<00:01, 35.76it/s, v_num=0]Training loss: 0.8314861059188843\n",
      "Epoch 6:  67%|██████▋   | 142/212 [00:03<00:01, 35.77it/s, v_num=0]Training loss: 0.8937162160873413\n",
      "Epoch 6:  67%|██████▋   | 143/212 [00:03<00:01, 35.78it/s, v_num=0]Training loss: 0.7312930822372437\n",
      "Epoch 6:  68%|██████▊   | 144/212 [00:04<00:01, 35.79it/s, v_num=0]Training loss: 0.6415565609931946\n",
      "Epoch 6:  68%|██████▊   | 145/212 [00:04<00:01, 35.80it/s, v_num=0]Training loss: 1.1333250999450684\n",
      "Epoch 6:  69%|██████▉   | 146/212 [00:04<00:01, 35.80it/s, v_num=0]Training loss: 0.9053592681884766\n",
      "Epoch 6:  69%|██████▉   | 147/212 [00:04<00:01, 35.80it/s, v_num=0]Training loss: 0.8858060836791992\n",
      "Epoch 6:  70%|██████▉   | 148/212 [00:04<00:01, 35.81it/s, v_num=0]Training loss: 0.8250653147697449\n",
      "Epoch 6:  70%|███████   | 149/212 [00:04<00:01, 35.81it/s, v_num=0]Training loss: 0.9630333781242371\n",
      "Epoch 6:  71%|███████   | 150/212 [00:04<00:01, 35.81it/s, v_num=0]Training loss: 0.9496724009513855\n",
      "Epoch 6:  71%|███████   | 151/212 [00:04<00:01, 35.81it/s, v_num=0]Training loss: 0.6701010465621948\n",
      "Epoch 6:  72%|███████▏  | 152/212 [00:04<00:01, 35.81it/s, v_num=0]Training loss: 0.8039571046829224\n",
      "Epoch 6:  72%|███████▏  | 153/212 [00:04<00:01, 35.82it/s, v_num=0]Training loss: 1.2714686393737793\n",
      "Epoch 6:  73%|███████▎  | 154/212 [00:04<00:01, 35.83it/s, v_num=0]Training loss: 0.6902307271957397\n",
      "Epoch 6:  73%|███████▎  | 155/212 [00:04<00:01, 35.84it/s, v_num=0]Training loss: 1.1743242740631104\n",
      "Epoch 6:  74%|███████▎  | 156/212 [00:04<00:01, 35.85it/s, v_num=0]Training loss: 1.376913070678711\n",
      "Epoch 6:  74%|███████▍  | 157/212 [00:04<00:01, 35.86it/s, v_num=0]Training loss: 0.9444475173950195\n",
      "Epoch 6:  75%|███████▍  | 158/212 [00:04<00:01, 35.87it/s, v_num=0]Training loss: 0.8243597745895386\n",
      "Epoch 6:  75%|███████▌  | 159/212 [00:04<00:01, 35.88it/s, v_num=0]Training loss: 1.076160192489624\n",
      "Epoch 6:  75%|███████▌  | 160/212 [00:04<00:01, 35.89it/s, v_num=0]Training loss: 0.7794381380081177\n",
      "Epoch 6:  76%|███████▌  | 161/212 [00:04<00:01, 35.90it/s, v_num=0]Training loss: 0.7923198938369751\n",
      "Epoch 6:  76%|███████▋  | 162/212 [00:04<00:01, 35.91it/s, v_num=0]Training loss: 0.6774507761001587\n",
      "Epoch 6:  77%|███████▋  | 163/212 [00:04<00:01, 35.92it/s, v_num=0]Training loss: 1.6229045391082764\n",
      "Epoch 6:  77%|███████▋  | 164/212 [00:04<00:01, 35.92it/s, v_num=0]Training loss: 0.6509495973587036\n",
      "Epoch 6:  78%|███████▊  | 165/212 [00:04<00:01, 35.93it/s, v_num=0]Training loss: 2.411325216293335\n",
      "Epoch 6:  78%|███████▊  | 166/212 [00:04<00:01, 35.94it/s, v_num=0]Training loss: 0.8839545249938965\n",
      "Epoch 6:  79%|███████▉  | 167/212 [00:04<00:01, 35.95it/s, v_num=0]Training loss: 1.0346906185150146\n",
      "Epoch 6:  79%|███████▉  | 168/212 [00:04<00:01, 35.96it/s, v_num=0]Training loss: 0.9092473983764648\n",
      "Epoch 6:  80%|███████▉  | 169/212 [00:04<00:01, 35.97it/s, v_num=0]Training loss: 0.6016405820846558\n",
      "Epoch 6:  80%|████████  | 170/212 [00:04<00:01, 35.98it/s, v_num=0]Training loss: 1.1429213285446167\n",
      "Epoch 6:  81%|████████  | 171/212 [00:04<00:01, 35.99it/s, v_num=0]Training loss: 0.9948086738586426\n",
      "Epoch 6:  81%|████████  | 172/212 [00:04<00:01, 36.00it/s, v_num=0]Training loss: 1.3815877437591553\n",
      "Epoch 6:  82%|████████▏ | 173/212 [00:04<00:01, 36.01it/s, v_num=0]Training loss: 1.273439645767212\n",
      "Epoch 6:  82%|████████▏ | 174/212 [00:04<00:01, 36.01it/s, v_num=0]Training loss: 1.3139433860778809\n",
      "Epoch 6:  83%|████████▎ | 175/212 [00:04<00:01, 36.02it/s, v_num=0]Training loss: 0.7609526515007019\n",
      "Epoch 6:  83%|████████▎ | 176/212 [00:04<00:00, 36.03it/s, v_num=0]Training loss: 1.4764034748077393\n",
      "Epoch 6:  83%|████████▎ | 177/212 [00:04<00:00, 36.04it/s, v_num=0]Training loss: 1.392842173576355\n",
      "Epoch 6:  84%|████████▍ | 178/212 [00:04<00:00, 36.05it/s, v_num=0]Training loss: 9.768209457397461\n",
      "Epoch 6:  84%|████████▍ | 179/212 [00:04<00:00, 36.06it/s, v_num=0]Training loss: 1.0383833646774292\n",
      "Epoch 6:  85%|████████▍ | 180/212 [00:04<00:00, 36.06it/s, v_num=0]Training loss: 1.5266828536987305\n",
      "Epoch 6:  85%|████████▌ | 181/212 [00:05<00:00, 36.07it/s, v_num=0]Training loss: 1.1582562923431396\n",
      "Epoch 6:  86%|████████▌ | 182/212 [00:05<00:00, 36.08it/s, v_num=0]Training loss: 1.294066071510315\n",
      "Epoch 6:  86%|████████▋ | 183/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 1.171515941619873\n",
      "Epoch 6:  87%|████████▋ | 184/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.7250356078147888\n",
      "Epoch 6:  87%|████████▋ | 185/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.8663803339004517\n",
      "Epoch 6:  88%|████████▊ | 186/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.8262802958488464\n",
      "Epoch 6:  88%|████████▊ | 187/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.8824610114097595\n",
      "Epoch 6:  89%|████████▊ | 188/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 1.5140937566757202\n",
      "Epoch 6:  89%|████████▉ | 189/212 [00:05<00:00, 36.10it/s, v_num=0]Training loss: 0.8421680927276611\n",
      "Epoch 6:  90%|████████▉ | 190/212 [00:05<00:00, 36.10it/s, v_num=0]Training loss: 0.7313846349716187\n",
      "Epoch 6:  90%|█████████ | 191/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.8148981332778931\n",
      "Epoch 6:  91%|█████████ | 192/212 [00:05<00:00, 36.09it/s, v_num=0]Training loss: 0.6864421367645264\n",
      "Epoch 6:  91%|█████████ | 193/212 [00:05<00:00, 36.10it/s, v_num=0]Training loss: 0.810684323310852\n",
      "Epoch 6:  92%|█████████▏| 194/212 [00:05<00:00, 36.11it/s, v_num=0]Training loss: 0.6123133301734924\n",
      "Epoch 6:  92%|█████████▏| 195/212 [00:05<00:00, 36.12it/s, v_num=0]Training loss: 1.1183446645736694\n",
      "Epoch 6:  92%|█████████▏| 196/212 [00:05<00:00, 36.12it/s, v_num=0]Training loss: 1.3071134090423584\n",
      "Epoch 6:  93%|█████████▎| 197/212 [00:05<00:00, 36.13it/s, v_num=0]Training loss: 0.7640472650527954\n",
      "Epoch 6:  93%|█████████▎| 198/212 [00:05<00:00, 36.14it/s, v_num=0]Training loss: 1.7444392442703247\n",
      "Epoch 6:  94%|█████████▍| 199/212 [00:05<00:00, 36.14it/s, v_num=0]Training loss: 1.1302372217178345\n",
      "Epoch 6:  94%|█████████▍| 200/212 [00:05<00:00, 36.15it/s, v_num=0]Training loss: 2.2571961879730225\n",
      "Epoch 6:  95%|█████████▍| 201/212 [00:05<00:00, 36.16it/s, v_num=0]Training loss: 0.7538017630577087\n",
      "Epoch 6:  95%|█████████▌| 202/212 [00:05<00:00, 36.16it/s, v_num=0]Training loss: 1.1656404733657837\n",
      "Epoch 6:  96%|█████████▌| 203/212 [00:05<00:00, 36.17it/s, v_num=0]Training loss: 1.255582332611084\n",
      "Epoch 6:  96%|█████████▌| 204/212 [00:05<00:00, 36.17it/s, v_num=0]Training loss: 2.071119785308838\n",
      "Epoch 6:  97%|█████████▋| 205/212 [00:05<00:00, 36.18it/s, v_num=0]Training loss: 1.439024806022644\n",
      "Epoch 6:  97%|█████████▋| 206/212 [00:05<00:00, 36.19it/s, v_num=0]Training loss: 0.7155069708824158\n",
      "Epoch 6:  98%|█████████▊| 207/212 [00:05<00:00, 36.19it/s, v_num=0]Training loss: 1.4912303686141968\n",
      "Epoch 6:  98%|█████████▊| 208/212 [00:05<00:00, 36.20it/s, v_num=0]Training loss: 1.1516724824905396\n",
      "Epoch 6:  99%|█████████▊| 209/212 [00:05<00:00, 36.21it/s, v_num=0]Training loss: 0.623253345489502\n",
      "Epoch 6:  99%|█████████▉| 210/212 [00:05<00:00, 36.21it/s, v_num=0]Training loss: 0.8524615168571472\n",
      "Epoch 6: 100%|█████████▉| 211/212 [00:05<00:00, 36.22it/s, v_num=0]Training loss: 0.9415211081504822\n",
      "Epoch 6: 100%|██████████| 212/212 [00:05<00:00, 36.22it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.080850601196289\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 82.79it/s]\u001b[AValidation loss: 1.8043625354766846\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 92.22it/s]\u001b[AValidation loss: 0.8681179285049438\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 95.90it/s]\u001b[AValidation loss: 0.6374775767326355\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 97.82it/s]\u001b[AValidation loss: 0.6749736666679382\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 98.53it/s]\u001b[AValidation loss: 0.8330567479133606\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 99.58it/s]\u001b[AValidation loss: 1.1477336883544922\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 100.32it/s]\u001b[AValidation loss: 2.107808828353882\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 100.11it/s]\u001b[AValidation loss: 0.576519787311554\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 100.57it/s]\u001b[AValidation loss: 0.6853390336036682\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 100.83it/s]\u001b[AValidation loss: 0.6828314661979675\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 101.22it/s]\u001b[AValidation loss: 0.9825605750083923\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 101.55it/s]\u001b[AValidation loss: 1.0020402669906616\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 101.90it/s]\u001b[AValidation loss: 0.9077538847923279\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 102.09it/s]\u001b[AValidation loss: 0.7497427463531494\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 102.29it/s]\u001b[AValidation loss: 1.044560194015503\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 102.41it/s]\u001b[AValidation loss: 0.9917856454849243\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 102.57it/s]\u001b[AValidation loss: 1.4728251695632935\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.73it/s]\u001b[AValidation loss: 0.7044972777366638\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 102.88it/s]\u001b[AValidation loss: 1.181790828704834\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 102.67it/s]\u001b[AValidation loss: 1.0481035709381104\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 102.38it/s]\u001b[AValidation loss: 1.0704734325408936\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 102.20it/s]\u001b[AValidation loss: 1.0389362573623657\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 102.29it/s]\u001b[AValidation loss: 0.8850399255752563\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 102.41it/s]\u001b[AValidation loss: 1.3074417114257812\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 102.19it/s]\u001b[AValidation loss: 0.7079996466636658\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 102.29it/s]\u001b[AValidation loss: 0.8807192444801331\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 102.34it/s]\u001b[AValidation loss: 0.9335175156593323\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 102.44it/s]\u001b[AValidation loss: 0.9349240064620972\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 102.52it/s]\u001b[AValidation loss: 1.2199136018753052\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 0.7960279583930969\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.28it/s]\u001b[AValidation loss: 0.9546383619308472\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.11it/s]\u001b[AValidation loss: 0.8928802013397217\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 101.91it/s]\u001b[AValidation loss: 0.769925594329834\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 101.75it/s]\u001b[AValidation loss: 1.7892745733261108\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 101.58it/s]\u001b[AValidation loss: 0.5407640933990479\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 101.42it/s]\u001b[AValidation loss: 1.4882677793502808\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 101.25it/s]\u001b[AValidation loss: 0.605722963809967\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 100.87it/s]\u001b[AValidation loss: 0.9702156782150269\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 100.52it/s]\u001b[AValidation loss: 0.9049270153045654\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 100.38it/s]\u001b[AValidation loss: 0.7612486481666565\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 100.25it/s]\u001b[AValidation loss: 0.6872785091400146\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 100.13it/s]\u001b[AValidation loss: 0.7418767213821411\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 100.03it/s]\u001b[AValidation loss: 1.238165020942688\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 99.94it/s] \u001b[AValidation loss: 1.3546909093856812\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 99.86it/s]\u001b[AValidation loss: 0.6815890669822693\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 99.75it/s]\u001b[AValidation loss: 0.9125646948814392\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 99.66it/s]\u001b[AValidation loss: 0.8876275420188904\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 99.58it/s]\u001b[AValidation loss: 1.0876230001449585\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 99.50it/s]\u001b[AValidation loss: 1.3124834299087524\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 99.42it/s]\u001b[AValidation loss: 1.1738066673278809\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 99.38it/s]\u001b[AValidation loss: 1.0568310022354126\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 99.34it/s]\u001b[AValidation loss: 1.3193548917770386\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 99.36it/s]\u001b[AValidation loss: 1.0054019689559937\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 99.40it/s]\u001b[AValidation loss: 2.108283042907715\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 99.59it/s]\u001b[A\n",
      "Epoch 7:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.6614723205566406\n",
      "Epoch 7:   0%|          | 1/212 [00:00<00:04, 44.16it/s, v_num=0]Training loss: 1.0378714799880981\n",
      "Epoch 7:   1%|          | 2/212 [00:00<00:05, 39.84it/s, v_num=0]Training loss: 0.5264509916305542\n",
      "Epoch 7:   1%|▏         | 3/212 [00:00<00:05, 38.58it/s, v_num=0]Training loss: 1.2366183996200562\n",
      "Epoch 7:   2%|▏         | 4/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 1.2270739078521729\n",
      "Epoch 7:   2%|▏         | 5/212 [00:00<00:05, 37.63it/s, v_num=0]Training loss: 1.522098183631897\n",
      "Epoch 7:   3%|▎         | 6/212 [00:00<00:05, 37.39it/s, v_num=0]Training loss: 1.014642596244812\n",
      "Epoch 7:   3%|▎         | 7/212 [00:00<00:05, 37.20it/s, v_num=0]Training loss: 0.878895103931427\n",
      "Epoch 7:   4%|▍         | 8/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 1.1787015199661255\n",
      "Epoch 7:   4%|▍         | 9/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 0.8192929029464722\n",
      "Epoch 7:   5%|▍         | 10/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 1.0108375549316406\n",
      "Epoch 7:   5%|▌         | 11/212 [00:00<00:05, 37.27it/s, v_num=0]Training loss: 1.4883416891098022\n",
      "Epoch 7:   6%|▌         | 12/212 [00:00<00:05, 37.29it/s, v_num=0]Training loss: 0.9935353994369507\n",
      "Epoch 7:   6%|▌         | 13/212 [00:00<00:05, 37.31it/s, v_num=0]Training loss: 0.9446232318878174\n",
      "Epoch 7:   7%|▋         | 14/212 [00:00<00:05, 37.32it/s, v_num=0]Training loss: 0.9166914820671082\n",
      "Epoch 7:   7%|▋         | 15/212 [00:00<00:05, 37.32it/s, v_num=0]Training loss: 0.7604199647903442\n",
      "Epoch 7:   8%|▊         | 16/212 [00:00<00:05, 37.33it/s, v_num=0]Training loss: 0.6340405941009521\n",
      "Epoch 7:   8%|▊         | 17/212 [00:00<00:05, 37.34it/s, v_num=0]Training loss: 0.8500758409500122\n",
      "Epoch 7:   8%|▊         | 18/212 [00:00<00:05, 37.33it/s, v_num=0]Training loss: 1.241175889968872\n",
      "Epoch 7:   9%|▉         | 19/212 [00:00<00:05, 37.34it/s, v_num=0]Training loss: 1.1626449823379517\n",
      "Epoch 7:   9%|▉         | 20/212 [00:00<00:05, 37.34it/s, v_num=0]Training loss: 1.1840133666992188\n",
      "Epoch 7:  10%|▉         | 21/212 [00:00<00:05, 37.35it/s, v_num=0]Training loss: 1.4866633415222168\n",
      "Epoch 7:  10%|█         | 22/212 [00:00<00:05, 37.36it/s, v_num=0]Training loss: 0.9208611249923706\n",
      "Epoch 7:  11%|█         | 23/212 [00:00<00:05, 37.37it/s, v_num=0]Training loss: 0.6513485312461853\n",
      "Epoch 7:  11%|█▏        | 24/212 [00:00<00:05, 37.37it/s, v_num=0]Training loss: 0.7916789054870605\n",
      "Epoch 7:  12%|█▏        | 25/212 [00:00<00:05, 37.38it/s, v_num=0]Training loss: 0.7621924877166748\n",
      "Epoch 7:  12%|█▏        | 26/212 [00:00<00:04, 37.38it/s, v_num=0]Training loss: 0.9348704218864441\n",
      "Epoch 7:  13%|█▎        | 27/212 [00:00<00:04, 37.39it/s, v_num=0]Training loss: 0.7779628038406372\n",
      "Epoch 7:  13%|█▎        | 28/212 [00:00<00:04, 37.39it/s, v_num=0]Training loss: 1.0649378299713135\n",
      "Epoch 7:  14%|█▎        | 29/212 [00:00<00:04, 37.40it/s, v_num=0]Training loss: 1.389788031578064\n",
      "Epoch 7:  14%|█▍        | 30/212 [00:00<00:04, 37.40it/s, v_num=0]Training loss: 0.7584784030914307\n",
      "Epoch 7:  15%|█▍        | 31/212 [00:00<00:04, 37.41it/s, v_num=0]Training loss: 0.9937059879302979\n",
      "Epoch 7:  15%|█▌        | 32/212 [00:00<00:04, 37.41it/s, v_num=0]Training loss: 0.9285866618156433\n",
      "Epoch 7:  16%|█▌        | 33/212 [00:00<00:04, 37.42it/s, v_num=0]Training loss: 1.070300579071045\n",
      "Epoch 7:  16%|█▌        | 34/212 [00:00<00:04, 37.42it/s, v_num=0]Training loss: 3.320305585861206\n",
      "Epoch 7:  17%|█▋        | 35/212 [00:00<00:04, 37.43it/s, v_num=0]Training loss: 0.6432353258132935\n",
      "Epoch 7:  17%|█▋        | 36/212 [00:00<00:04, 37.43it/s, v_num=0]Training loss: 1.0692576169967651\n",
      "Epoch 7:  17%|█▋        | 37/212 [00:00<00:04, 37.43it/s, v_num=0]Training loss: 0.8683434724807739\n",
      "Epoch 7:  18%|█▊        | 38/212 [00:01<00:04, 37.43it/s, v_num=0]Training loss: 0.7723298668861389\n",
      "Epoch 7:  18%|█▊        | 39/212 [00:01<00:04, 37.42it/s, v_num=0]Training loss: 1.1410914659500122\n",
      "Epoch 7:  19%|█▉        | 40/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.7928453087806702\n",
      "Epoch 7:  19%|█▉        | 41/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.9404697418212891\n",
      "Epoch 7:  20%|█▉        | 42/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.0399965047836304\n",
      "Epoch 7:  20%|██        | 43/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.2019518613815308\n",
      "Epoch 7:  21%|██        | 44/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.3524019718170166\n",
      "Epoch 7:  21%|██        | 45/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.9870579838752747\n",
      "Epoch 7:  22%|██▏       | 46/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.566733181476593\n",
      "Epoch 7:  22%|██▏       | 47/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 1.7703369855880737\n",
      "Epoch 7:  23%|██▎       | 48/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.8060564994812012\n",
      "Epoch 7:  23%|██▎       | 49/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.1796351671218872\n",
      "Epoch 7:  24%|██▎       | 50/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.6375321745872498\n",
      "Epoch 7:  24%|██▍       | 51/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 2.3438310623168945\n",
      "Epoch 7:  25%|██▍       | 52/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.7988603711128235\n",
      "Epoch 7:  25%|██▌       | 53/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.6225488185882568\n",
      "Epoch 7:  25%|██▌       | 54/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.8169454336166382\n",
      "Epoch 7:  26%|██▌       | 55/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.7443044185638428\n",
      "Epoch 7:  26%|██▋       | 56/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.9457927942276001\n",
      "Epoch 7:  27%|██▋       | 57/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 1.0524342060089111\n",
      "Epoch 7:  27%|██▋       | 58/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 1.0628968477249146\n",
      "Epoch 7:  28%|██▊       | 59/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.7327557802200317\n",
      "Epoch 7:  28%|██▊       | 60/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 1.699082851409912\n",
      "Epoch 7:  29%|██▉       | 61/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 0.9208208918571472\n",
      "Epoch 7:  29%|██▉       | 62/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 0.9805992245674133\n",
      "Epoch 7:  30%|██▉       | 63/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 0.9867108464241028\n",
      "Epoch 7:  30%|███       | 64/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 1.3869614601135254\n",
      "Epoch 7:  31%|███       | 65/212 [00:01<00:03, 37.29it/s, v_num=0]Training loss: 0.9664239883422852\n",
      "Epoch 7:  31%|███       | 66/212 [00:01<00:03, 37.29it/s, v_num=0]Training loss: 0.8562078475952148\n",
      "Epoch 7:  32%|███▏      | 67/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 1.1423708200454712\n",
      "Epoch 7:  32%|███▏      | 68/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 0.6460915803909302\n",
      "Epoch 7:  33%|███▎      | 69/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 0.7569571733474731\n",
      "Epoch 7:  33%|███▎      | 70/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 10.111923217773438\n",
      "Epoch 7:  33%|███▎      | 71/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 0.7657798528671265\n",
      "Epoch 7:  34%|███▍      | 72/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 0.9581146836280823\n",
      "Epoch 7:  34%|███▍      | 73/212 [00:01<00:03, 37.32it/s, v_num=0]Training loss: 0.532517671585083\n",
      "Epoch 7:  35%|███▍      | 74/212 [00:01<00:03, 37.32it/s, v_num=0]Training loss: 1.3728961944580078\n",
      "Epoch 7:  35%|███▌      | 75/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.9464943408966064\n",
      "Epoch 7:  36%|███▌      | 76/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.8510937690734863\n",
      "Epoch 7:  36%|███▋      | 77/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 1.1596407890319824\n",
      "Epoch 7:  37%|███▋      | 78/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.6765462756156921\n",
      "Epoch 7:  37%|███▋      | 79/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.7003811001777649\n",
      "Epoch 7:  38%|███▊      | 80/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.8938047885894775\n",
      "Epoch 7:  38%|███▊      | 81/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.3602454662322998\n",
      "Epoch 7:  39%|███▊      | 82/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.924405038356781\n",
      "Epoch 7:  39%|███▉      | 83/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.489503413438797\n",
      "Epoch 7:  40%|███▉      | 84/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.9367853403091431\n",
      "Epoch 7:  40%|████      | 85/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.8062934875488281\n",
      "Epoch 7:  41%|████      | 86/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.9188280701637268\n",
      "Epoch 7:  41%|████      | 87/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.8916250467300415\n",
      "Epoch 7:  42%|████▏     | 88/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.6630428433418274\n",
      "Epoch 7:  42%|████▏     | 89/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.5925962328910828\n",
      "Epoch 7:  42%|████▏     | 90/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.4069499969482422\n",
      "Epoch 7:  43%|████▎     | 91/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.7183359861373901\n",
      "Epoch 7:  43%|████▎     | 92/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 1.0201005935668945\n",
      "Epoch 7:  44%|████▍     | 93/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.5172972083091736\n",
      "Epoch 7:  44%|████▍     | 94/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.7953557968139648\n",
      "Epoch 7:  45%|████▍     | 95/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.0603526830673218\n",
      "Epoch 7:  45%|████▌     | 96/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.1378037929534912\n",
      "Epoch 7:  46%|████▌     | 97/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.1940556764602661\n",
      "Epoch 7:  46%|████▌     | 98/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.9330848455429077\n",
      "Epoch 7:  47%|████▋     | 99/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.7403620481491089\n",
      "Epoch 7:  47%|████▋     | 100/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.7465691566467285\n",
      "Epoch 7:  48%|████▊     | 101/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 1.5539788007736206\n",
      "Epoch 7:  48%|████▊     | 102/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.9468809962272644\n",
      "Epoch 7:  49%|████▊     | 103/212 [00:02<00:02, 37.26it/s, v_num=0]Training loss: 0.5783706307411194\n",
      "Epoch 7:  49%|████▉     | 104/212 [00:02<00:02, 37.26it/s, v_num=0]Training loss: 0.693683922290802\n",
      "Epoch 7:  50%|████▉     | 105/212 [00:02<00:02, 37.26it/s, v_num=0]Training loss: 0.553056001663208\n",
      "Epoch 7:  50%|█████     | 106/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.1404025554656982\n",
      "Epoch 7:  50%|█████     | 107/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 0.669987142086029\n",
      "Epoch 7:  51%|█████     | 108/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.365737795829773\n",
      "Epoch 7:  51%|█████▏    | 109/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.407485008239746\n",
      "Epoch 7:  52%|█████▏    | 110/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 0.9709312319755554\n",
      "Epoch 7:  52%|█████▏    | 111/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.0436993837356567\n",
      "Epoch 7:  53%|█████▎    | 112/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.8640087842941284\n",
      "Epoch 7:  53%|█████▎    | 113/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.8709044456481934\n",
      "Epoch 7:  54%|█████▍    | 114/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.2112795114517212\n",
      "Epoch 7:  54%|█████▍    | 115/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.3870265483856201\n",
      "Epoch 7:  55%|█████▍    | 116/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.824192762374878\n",
      "Epoch 7:  55%|█████▌    | 117/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.3089017868041992\n",
      "Epoch 7:  56%|█████▌    | 118/212 [00:03<00:02, 37.30it/s, v_num=0]Training loss: 1.2699339389801025\n",
      "Epoch 7:  56%|█████▌    | 119/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.6063316464424133\n",
      "Epoch 7:  57%|█████▋    | 120/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.8245895504951477\n",
      "Epoch 7:  57%|█████▋    | 121/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8056729435920715\n",
      "Epoch 7:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0784345865249634\n",
      "Epoch 7:  58%|█████▊    | 123/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.4041314125061035\n",
      "Epoch 7:  58%|█████▊    | 124/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 9.949039459228516\n",
      "Epoch 7:  59%|█████▉    | 125/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.6328065395355225\n",
      "Epoch 7:  59%|█████▉    | 126/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.7535949349403381\n",
      "Epoch 7:  60%|█████▉    | 127/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.2770707607269287\n",
      "Epoch 7:  60%|██████    | 128/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.9533880352973938\n",
      "Epoch 7:  61%|██████    | 129/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9915255308151245\n",
      "Epoch 7:  61%|██████▏   | 130/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 2.4075183868408203\n",
      "Epoch 7:  62%|██████▏   | 131/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.7916222214698792\n",
      "Epoch 7:  62%|██████▏   | 132/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.0090423822402954\n",
      "Epoch 7:  63%|██████▎   | 133/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9749189615249634\n",
      "Epoch 7:  63%|██████▎   | 134/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.7469753623008728\n",
      "Epoch 7:  64%|██████▎   | 135/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1978784799575806\n",
      "Epoch 7:  64%|██████▍   | 136/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1299781799316406\n",
      "Epoch 7:  65%|██████▍   | 137/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.8600847125053406\n",
      "Epoch 7:  65%|██████▌   | 138/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 0.8670709729194641\n",
      "Epoch 7:  66%|██████▌   | 139/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 1.1159741878509521\n",
      "Epoch 7:  66%|██████▌   | 140/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 0.6084533929824829\n",
      "Epoch 7:  67%|██████▋   | 141/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 1.2999961376190186\n",
      "Epoch 7:  67%|██████▋   | 142/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 2.021880865097046\n",
      "Epoch 7:  67%|██████▋   | 143/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 2.2008979320526123\n",
      "Epoch 7:  68%|██████▊   | 144/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 1.0131803750991821\n",
      "Epoch 7:  68%|██████▊   | 145/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.9041836857795715\n",
      "Epoch 7:  69%|██████▉   | 146/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.8746688961982727\n",
      "Epoch 7:  69%|██████▉   | 147/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.7126103043556213\n",
      "Epoch 7:  70%|██████▉   | 148/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 1.2111775875091553\n",
      "Epoch 7:  70%|███████   | 149/212 [00:03<00:01, 37.26it/s, v_num=0]Training loss: 0.9056262373924255\n",
      "Epoch 7:  71%|███████   | 150/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.7373501062393188\n",
      "Epoch 7:  71%|███████   | 151/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.6125897169113159\n",
      "Epoch 7:  72%|███████▏  | 152/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8724995851516724\n",
      "Epoch 7:  72%|███████▏  | 153/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.768204927444458\n",
      "Epoch 7:  73%|███████▎  | 154/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.9072482585906982\n",
      "Epoch 7:  73%|███████▎  | 155/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.8325074911117554\n",
      "Epoch 7:  74%|███████▎  | 156/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 1.0273138284683228\n",
      "Epoch 7:  74%|███████▍  | 157/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.7214418053627014\n",
      "Epoch 7:  75%|███████▍  | 158/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 1.2316781282424927\n",
      "Epoch 7:  75%|███████▌  | 159/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8860687017440796\n",
      "Epoch 7:  75%|███████▌  | 160/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.9830467700958252\n",
      "Epoch 7:  76%|███████▌  | 161/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.2252355813980103\n",
      "Epoch 7:  76%|███████▋  | 162/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6884033679962158\n",
      "Epoch 7:  77%|███████▋  | 163/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.9932153224945068\n",
      "Epoch 7:  77%|███████▋  | 164/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8490400910377502\n",
      "Epoch 7:  78%|███████▊  | 165/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.123603343963623\n",
      "Epoch 7:  78%|███████▊  | 166/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.629195213317871\n",
      "Epoch 7:  79%|███████▉  | 167/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.1727285385131836\n",
      "Epoch 7:  79%|███████▉  | 168/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.138045310974121\n",
      "Epoch 7:  80%|███████▉  | 169/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8289826512336731\n",
      "Epoch 7:  80%|████████  | 170/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.9126261472702026\n",
      "Epoch 7:  81%|████████  | 171/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.7511964440345764\n",
      "Epoch 7:  81%|████████  | 172/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.5757016539573669\n",
      "Epoch 7:  82%|████████▏ | 173/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8280380368232727\n",
      "Epoch 7:  82%|████████▏ | 174/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.6159258484840393\n",
      "Epoch 7:  83%|████████▎ | 175/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.701397716999054\n",
      "Epoch 7:  83%|████████▎ | 176/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.780678927898407\n",
      "Epoch 7:  83%|████████▎ | 177/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 1.1448395252227783\n",
      "Epoch 7:  84%|████████▍ | 178/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.9351018667221069\n",
      "Epoch 7:  84%|████████▍ | 179/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 1.0128146409988403\n",
      "Epoch 7:  85%|████████▍ | 180/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.7902489900588989\n",
      "Epoch 7:  85%|████████▌ | 181/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.7246537208557129\n",
      "Epoch 7:  86%|████████▌ | 182/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 1.0340461730957031\n",
      "Epoch 7:  86%|████████▋ | 183/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.6104238629341125\n",
      "Epoch 7:  87%|████████▋ | 184/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.9084200859069824\n",
      "Epoch 7:  87%|████████▋ | 185/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.7187181115150452\n",
      "Epoch 7:  88%|████████▊ | 186/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 0.6795324087142944\n",
      "Epoch 7:  88%|████████▊ | 187/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.5783190727233887\n",
      "Epoch 7:  89%|████████▊ | 188/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9268300533294678\n",
      "Epoch 7:  89%|████████▉ | 189/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.6624792814254761\n",
      "Epoch 7:  90%|████████▉ | 190/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.7207183837890625\n",
      "Epoch 7:  90%|█████████ | 191/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.6976476907730103\n",
      "Epoch 7:  91%|█████████ | 192/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.211987018585205\n",
      "Epoch 7:  91%|█████████ | 193/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.7270488739013672\n",
      "Epoch 7:  92%|█████████▏| 194/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.6890887022018433\n",
      "Epoch 7:  92%|█████████▏| 195/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.0104875564575195\n",
      "Epoch 7:  92%|█████████▏| 196/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.9377618432044983\n",
      "Epoch 7:  93%|█████████▎| 197/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.1030765771865845\n",
      "Epoch 7:  93%|█████████▎| 198/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.6955488324165344\n",
      "Epoch 7:  94%|█████████▍| 199/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.0481863021850586\n",
      "Epoch 7:  94%|█████████▍| 200/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.4554389715194702\n",
      "Epoch 7:  95%|█████████▍| 201/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9019858837127686\n",
      "Epoch 7:  95%|█████████▌| 202/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.5411190986633301\n",
      "Epoch 7:  96%|█████████▌| 203/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 3.904804229736328\n",
      "Epoch 7:  96%|█████████▌| 204/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.7060686945915222\n",
      "Epoch 7:  97%|█████████▋| 205/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.1685186624526978\n",
      "Epoch 7:  97%|█████████▋| 206/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.780004620552063\n",
      "Epoch 7:  98%|█████████▊| 207/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.8315613865852356\n",
      "Epoch 7:  98%|█████████▊| 208/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.307397723197937\n",
      "Epoch 7:  99%|█████████▊| 209/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.9695153832435608\n",
      "Epoch 7:  99%|█████████▉| 210/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.7591685056686401\n",
      "Epoch 7: 100%|█████████▉| 211/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.9509531855583191\n",
      "Epoch 7: 100%|██████████| 212/212 [00:05<00:00, 37.23it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.0486104488372803\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 82.17it/s]\u001b[AValidation loss: 1.7650747299194336\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 91.16it/s]\u001b[AValidation loss: 0.824488639831543\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 94.83it/s]\u001b[AValidation loss: 0.6063187718391418\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 97.09it/s]\u001b[AValidation loss: 0.6523374319076538\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 98.33it/s]\u001b[AValidation loss: 0.8062233328819275\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 99.13it/s]\u001b[AValidation loss: 1.1188452243804932\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 99.78it/s]\u001b[AValidation loss: 2.0728213787078857\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 99.54it/s]\u001b[AValidation loss: 0.5587802529335022\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 99.90it/s]\u001b[AValidation loss: 0.6658134460449219\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 100.11it/s]\u001b[AValidation loss: 0.6609818935394287\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 100.33it/s]\u001b[AValidation loss: 0.9549574255943298\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 100.53it/s]\u001b[AValidation loss: 0.9854727983474731\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 100.77it/s]\u001b[AValidation loss: 0.8794115781784058\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 101.00it/s]\u001b[AValidation loss: 0.724666953086853\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 101.21it/s]\u001b[AValidation loss: 1.0186889171600342\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 101.25it/s]\u001b[AValidation loss: 0.9656798243522644\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 101.04it/s]\u001b[AValidation loss: 1.4350581169128418\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 101.19it/s]\u001b[AValidation loss: 0.6852995753288269\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 101.36it/s]\u001b[AValidation loss: 1.1612024307250977\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 101.15it/s]\u001b[AValidation loss: 1.0199462175369263\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 101.23it/s]\u001b[AValidation loss: 1.0416724681854248\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 101.36it/s]\u001b[AValidation loss: 1.0145225524902344\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 101.47it/s]\u001b[AValidation loss: 0.8593030571937561\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 101.54it/s]\u001b[AValidation loss: 1.2815912961959839\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 101.67it/s]\u001b[AValidation loss: 0.678138792514801\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.75it/s]\u001b[AValidation loss: 0.850369393825531\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.80it/s]\u001b[AValidation loss: 0.8989222049713135\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 101.88it/s]\u001b[AValidation loss: 0.8993049263954163\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 101.92it/s]\u001b[AValidation loss: 1.19502854347229\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 101.97it/s]\u001b[AValidation loss: 0.766903817653656\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.00it/s]\u001b[AValidation loss: 0.9419147968292236\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.06it/s]\u001b[AValidation loss: 0.8611140847206116\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 102.12it/s]\u001b[AValidation loss: 0.7389345765113831\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 102.17it/s]\u001b[AValidation loss: 1.7358533143997192\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 102.22it/s]\u001b[AValidation loss: 0.5191291570663452\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.27it/s]\u001b[AValidation loss: 1.4694218635559082\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.34it/s]\u001b[AValidation loss: 0.582342803478241\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.39it/s]\u001b[AValidation loss: 0.9363008141517639\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.43it/s]\u001b[AValidation loss: 0.8792243599891663\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.47it/s]\u001b[AValidation loss: 0.7334040999412537\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.38it/s]\u001b[AValidation loss: 0.661490261554718\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 0.7108278274536133\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 102.28it/s]\u001b[AValidation loss: 1.2150894403457642\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 102.31it/s]\u001b[AValidation loss: 1.324397325515747\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 0.6586182117462158\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 102.38it/s]\u001b[AValidation loss: 0.8778954148292542\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 102.43it/s]\u001b[AValidation loss: 0.8588939309120178\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 102.47it/s]\u001b[AValidation loss: 1.069736361503601\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 102.43it/s]\u001b[AValidation loss: 1.2874011993408203\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 102.47it/s]\u001b[AValidation loss: 1.1432504653930664\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 102.50it/s]\u001b[AValidation loss: 1.0409133434295654\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 102.46it/s]\u001b[AValidation loss: 1.2873613834381104\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 102.50it/s]\u001b[AValidation loss: 0.9759736657142639\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 102.52it/s]\u001b[AValidation loss: 2.082946538925171\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 102.67it/s]\u001b[A\n",
      "Epoch 8:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]                \u001b[ATraining loss: 0.9639942646026611\n",
      "Epoch 8:   0%|          | 1/212 [00:00<00:04, 45.40it/s, v_num=0]Training loss: 1.0334241390228271\n",
      "Epoch 8:   1%|          | 2/212 [00:00<00:05, 41.11it/s, v_num=0]Training loss: 2.176281452178955\n",
      "Epoch 8:   1%|▏         | 3/212 [00:00<00:05, 39.81it/s, v_num=0]Training loss: 0.6191583275794983\n",
      "Epoch 8:   2%|▏         | 4/212 [00:00<00:05, 39.28it/s, v_num=0]Training loss: 0.9327870607376099\n",
      "Epoch 8:   2%|▏         | 5/212 [00:00<00:05, 38.93it/s, v_num=0]Training loss: 1.1806622743606567\n",
      "Epoch 8:   3%|▎         | 6/212 [00:00<00:05, 38.69it/s, v_num=0]Training loss: 0.9584643840789795\n",
      "Epoch 8:   3%|▎         | 7/212 [00:00<00:05, 38.52it/s, v_num=0]Training loss: 0.7139039635658264\n",
      "Epoch 8:   4%|▍         | 8/212 [00:00<00:05, 38.29it/s, v_num=0]Training loss: 0.6947789788246155\n",
      "Epoch 8:   4%|▍         | 9/212 [00:00<00:05, 38.21it/s, v_num=0]Training loss: 0.9184677004814148\n",
      "Epoch 8:   5%|▍         | 10/212 [00:00<00:05, 38.14it/s, v_num=0]Training loss: 0.8724516034126282\n",
      "Epoch 8:   5%|▌         | 11/212 [00:00<00:05, 38.09it/s, v_num=0]Training loss: 0.950008749961853\n",
      "Epoch 8:   6%|▌         | 12/212 [00:00<00:05, 38.04it/s, v_num=0]Training loss: 0.6830275058746338\n",
      "Epoch 8:   6%|▌         | 13/212 [00:00<00:05, 38.01it/s, v_num=0]Training loss: 0.7362467646598816\n",
      "Epoch 8:   7%|▋         | 14/212 [00:00<00:05, 37.98it/s, v_num=0]Training loss: 0.8865729570388794\n",
      "Epoch 8:   7%|▋         | 15/212 [00:00<00:05, 37.80it/s, v_num=0]Training loss: 0.8327933549880981\n",
      "Epoch 8:   8%|▊         | 16/212 [00:00<00:05, 37.69it/s, v_num=0]Training loss: 2.2994604110717773\n",
      "Epoch 8:   8%|▊         | 17/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 1.212247610092163\n",
      "Epoch 8:   8%|▊         | 18/212 [00:00<00:05, 37.53it/s, v_num=0]Training loss: 0.6529850959777832\n",
      "Epoch 8:   9%|▉         | 19/212 [00:00<00:05, 37.47it/s, v_num=0]Training loss: 1.0800414085388184\n",
      "Epoch 8:   9%|▉         | 20/212 [00:00<00:05, 37.40it/s, v_num=0]Training loss: 1.304871916770935\n",
      "Epoch 8:  10%|▉         | 21/212 [00:00<00:05, 37.29it/s, v_num=0]Training loss: 1.183162808418274\n",
      "Epoch 8:  10%|█         | 22/212 [00:00<00:05, 37.23it/s, v_num=0]Training loss: 0.5383663177490234\n",
      "Epoch 8:  11%|█         | 23/212 [00:00<00:05, 37.29it/s, v_num=0]Training loss: 1.196908950805664\n",
      "Epoch 8:  11%|█▏        | 24/212 [00:00<00:05, 37.30it/s, v_num=0]Training loss: 1.1848198175430298\n",
      "Epoch 8:  12%|█▏        | 25/212 [00:00<00:05, 37.31it/s, v_num=0]Training loss: 1.1383798122406006\n",
      "Epoch 8:  12%|█▏        | 26/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.2676314115524292\n",
      "Epoch 8:  13%|█▎        | 27/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 1.4409350156784058\n",
      "Epoch 8:  13%|█▎        | 28/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 1.2454133033752441\n",
      "Epoch 8:  14%|█▎        | 29/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 0.9875045418739319\n",
      "Epoch 8:  14%|█▍        | 30/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.5752558708190918\n",
      "Epoch 8:  15%|█▍        | 31/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.0613867044448853\n",
      "Epoch 8:  15%|█▌        | 32/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.0455207824707031\n",
      "Epoch 8:  16%|█▌        | 33/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.6185501217842102\n",
      "Epoch 8:  16%|█▌        | 34/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.0404373407363892\n",
      "Epoch 8:  17%|█▋        | 35/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 0.7827877998352051\n",
      "Epoch 8:  17%|█▋        | 36/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 0.5281837582588196\n",
      "Epoch 8:  17%|█▋        | 37/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.618905782699585\n",
      "Epoch 8:  18%|█▊        | 38/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.4763754606246948\n",
      "Epoch 8:  18%|█▊        | 39/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.4440323114395142\n",
      "Epoch 8:  19%|█▉        | 40/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.7293261885643005\n",
      "Epoch 8:  19%|█▉        | 41/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.7866584062576294\n",
      "Epoch 8:  20%|█▉        | 42/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.9752020239830017\n",
      "Epoch 8:  20%|██        | 43/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.5882418751716614\n",
      "Epoch 8:  21%|██        | 44/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.2665939331054688\n",
      "Epoch 8:  21%|██        | 45/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.1358444690704346\n",
      "Epoch 8:  22%|██▏       | 46/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.9896310567855835\n",
      "Epoch 8:  22%|██▏       | 47/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.1267231702804565\n",
      "Epoch 8:  23%|██▎       | 48/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.0494637489318848\n",
      "Epoch 8:  23%|██▎       | 49/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 0.8834406137466431\n",
      "Epoch 8:  24%|██▎       | 50/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 0.6782516241073608\n",
      "Epoch 8:  24%|██▍       | 51/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 1.1104578971862793\n",
      "Epoch 8:  25%|██▍       | 52/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 1.0005446672439575\n",
      "Epoch 8:  25%|██▌       | 53/212 [00:01<00:04, 37.41it/s, v_num=0]Training loss: 0.7365540862083435\n",
      "Epoch 8:  25%|██▌       | 54/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.8649876713752747\n",
      "Epoch 8:  26%|██▌       | 55/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.9623054265975952\n",
      "Epoch 8:  26%|██▋       | 56/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.9633413553237915\n",
      "Epoch 8:  27%|██▋       | 57/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.45099675655365\n",
      "Epoch 8:  27%|██▋       | 58/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 0.8339875936508179\n",
      "Epoch 8:  28%|██▊       | 59/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.2888305187225342\n",
      "Epoch 8:  28%|██▊       | 60/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 1.1955175399780273\n",
      "Epoch 8:  29%|██▉       | 61/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 0.7511601448059082\n",
      "Epoch 8:  29%|██▉       | 62/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.9869897365570068\n",
      "Epoch 8:  30%|██▉       | 63/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.9230958819389343\n",
      "Epoch 8:  30%|███       | 64/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.9759353995323181\n",
      "Epoch 8:  31%|███       | 65/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.5152560472488403\n",
      "Epoch 8:  31%|███       | 66/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.9413465857505798\n",
      "Epoch 8:  32%|███▏      | 67/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.6106159090995789\n",
      "Epoch 8:  32%|███▏      | 68/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.030992865562439\n",
      "Epoch 8:  33%|███▎      | 69/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 1.8041177988052368\n",
      "Epoch 8:  33%|███▎      | 70/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.5552703142166138\n",
      "Epoch 8:  33%|███▎      | 71/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.6598195433616638\n",
      "Epoch 8:  34%|███▍      | 72/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.8643050193786621\n",
      "Epoch 8:  34%|███▍      | 73/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.861466109752655\n",
      "Epoch 8:  35%|███▍      | 74/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 3.408299684524536\n",
      "Epoch 8:  35%|███▌      | 75/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9096144437789917\n",
      "Epoch 8:  36%|███▌      | 76/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.1012974977493286\n",
      "Epoch 8:  36%|███▋      | 77/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.5494135618209839\n",
      "Epoch 8:  37%|███▋      | 78/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.4476438760757446\n",
      "Epoch 8:  37%|███▋      | 79/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9901925921440125\n",
      "Epoch 8:  38%|███▊      | 80/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.4327731132507324\n",
      "Epoch 8:  38%|███▊      | 81/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.6880549192428589\n",
      "Epoch 8:  39%|███▊      | 82/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.2658687829971313\n",
      "Epoch 8:  39%|███▉      | 83/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.947284460067749\n",
      "Epoch 8:  40%|███▉      | 84/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.102954387664795\n",
      "Epoch 8:  40%|████      | 85/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.8796691298484802\n",
      "Epoch 8:  41%|████      | 86/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.5809077024459839\n",
      "Epoch 8:  41%|████      | 87/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.022001028060913\n",
      "Epoch 8:  42%|████▏     | 88/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.0014008283615112\n",
      "Epoch 8:  42%|████▏     | 89/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.7079384922981262\n",
      "Epoch 8:  42%|████▏     | 90/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.705786943435669\n",
      "Epoch 8:  43%|████▎     | 91/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.5164654850959778\n",
      "Epoch 8:  43%|████▎     | 92/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.5455091595649719\n",
      "Epoch 8:  44%|████▍     | 93/212 [00:02<00:03, 37.33it/s, v_num=0]Training loss: 0.8656491637229919\n",
      "Epoch 8:  44%|████▍     | 94/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.8776208758354187\n",
      "Epoch 8:  45%|████▍     | 95/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.9673711061477661\n",
      "Epoch 8:  45%|████▌     | 96/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.018546462059021\n",
      "Epoch 8:  46%|████▌     | 97/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.0927419662475586\n",
      "Epoch 8:  46%|████▌     | 98/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.999252200126648\n",
      "Epoch 8:  47%|████▋     | 99/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.744554877281189\n",
      "Epoch 8:  47%|████▋     | 100/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.7576190829277039\n",
      "Epoch 8:  48%|████▊     | 101/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 0.8719555139541626\n",
      "Epoch 8:  48%|████▊     | 102/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.760211169719696\n",
      "Epoch 8:  49%|████▊     | 103/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.7521578073501587\n",
      "Epoch 8:  49%|████▉     | 104/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.4842435121536255\n",
      "Epoch 8:  50%|████▉     | 105/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.9885326027870178\n",
      "Epoch 8:  50%|█████     | 106/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 9.37646198272705\n",
      "Epoch 8:  50%|█████     | 107/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.6872329115867615\n",
      "Epoch 8:  51%|█████     | 108/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.9927369356155396\n",
      "Epoch 8:  51%|█████▏    | 109/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 2.115866184234619\n",
      "Epoch 8:  52%|█████▏    | 110/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.6807947158813477\n",
      "Epoch 8:  52%|█████▏    | 111/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.5453263521194458\n",
      "Epoch 8:  53%|█████▎    | 112/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.7715017795562744\n",
      "Epoch 8:  53%|█████▎    | 113/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8974855542182922\n",
      "Epoch 8:  54%|█████▍    | 114/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8402791619300842\n",
      "Epoch 8:  54%|█████▍    | 115/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.94026118516922\n",
      "Epoch 8:  55%|█████▍    | 116/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7269959449768066\n",
      "Epoch 8:  55%|█████▌    | 117/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.145425796508789\n",
      "Epoch 8:  56%|█████▌    | 118/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.6142881512641907\n",
      "Epoch 8:  56%|█████▌    | 119/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1231518983840942\n",
      "Epoch 8:  57%|█████▋    | 120/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.0029935836791992\n",
      "Epoch 8:  57%|█████▋    | 121/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.8753967881202698\n",
      "Epoch 8:  58%|█████▊    | 122/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.7048759460449219\n",
      "Epoch 8:  58%|█████▊    | 123/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.7849416732788086\n",
      "Epoch 8:  58%|█████▊    | 124/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.7688694596290588\n",
      "Epoch 8:  59%|█████▉    | 125/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0556236505508423\n",
      "Epoch 8:  59%|█████▉    | 126/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6678909063339233\n",
      "Epoch 8:  60%|█████▉    | 127/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.742137610912323\n",
      "Epoch 8:  60%|██████    | 128/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6885064244270325\n",
      "Epoch 8:  61%|██████    | 129/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8847818374633789\n",
      "Epoch 8:  61%|██████▏   | 130/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.4690508246421814\n",
      "Epoch 8:  62%|██████▏   | 131/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.937630295753479\n",
      "Epoch 8:  62%|██████▏   | 132/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0565091371536255\n",
      "Epoch 8:  63%|██████▎   | 133/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.946456253528595\n",
      "Epoch 8:  63%|██████▎   | 134/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.5553415417671204\n",
      "Epoch 8:  64%|██████▎   | 135/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.7923561334609985\n",
      "Epoch 8:  64%|██████▍   | 136/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.0081291198730469\n",
      "Epoch 8:  65%|██████▍   | 137/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.9173155426979065\n",
      "Epoch 8:  65%|██████▌   | 138/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 0.7455645203590393\n",
      "Epoch 8:  66%|██████▌   | 139/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.7864046096801758\n",
      "Epoch 8:  66%|██████▌   | 140/212 [00:03<00:01, 37.19it/s, v_num=0]Training loss: 0.8799393773078918\n",
      "Epoch 8:  67%|██████▋   | 141/212 [00:03<00:01, 37.18it/s, v_num=0]Training loss: 2.6148033142089844\n",
      "Epoch 8:  67%|██████▋   | 142/212 [00:03<00:01, 37.19it/s, v_num=0]Training loss: 0.7496001124382019\n",
      "Epoch 8:  67%|██████▋   | 143/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.9851354956626892\n",
      "Epoch 8:  68%|██████▊   | 144/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.7335261702537537\n",
      "Epoch 8:  68%|██████▊   | 145/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.7743706703186035\n",
      "Epoch 8:  69%|██████▉   | 146/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.5961979627609253\n",
      "Epoch 8:  69%|██████▉   | 147/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.7291157245635986\n",
      "Epoch 8:  70%|██████▉   | 148/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.703387975692749\n",
      "Epoch 8:  70%|███████   | 149/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.8297194242477417\n",
      "Epoch 8:  71%|███████   | 150/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.9017367959022522\n",
      "Epoch 8:  71%|███████   | 151/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0475108623504639\n",
      "Epoch 8:  72%|███████▏  | 152/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.8437126278877258\n",
      "Epoch 8:  72%|███████▏  | 153/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.3608269691467285\n",
      "Epoch 8:  73%|███████▎  | 154/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.5438896417617798\n",
      "Epoch 8:  73%|███████▎  | 155/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.8460150957107544\n",
      "Epoch 8:  74%|███████▎  | 156/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.7763888835906982\n",
      "Epoch 8:  74%|███████▍  | 157/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.7073292136192322\n",
      "Epoch 8:  75%|███████▍  | 158/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.8609559535980225\n",
      "Epoch 8:  75%|███████▌  | 159/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.9250715970993042\n",
      "Epoch 8:  75%|███████▌  | 160/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.0134646892547607\n",
      "Epoch 8:  76%|███████▌  | 161/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.7007003426551819\n",
      "Epoch 8:  76%|███████▋  | 162/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.301820993423462\n",
      "Epoch 8:  77%|███████▋  | 163/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.725587010383606\n",
      "Epoch 8:  77%|███████▋  | 164/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.9477712512016296\n",
      "Epoch 8:  78%|███████▊  | 165/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.9566949605941772\n",
      "Epoch 8:  78%|███████▊  | 166/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8208391070365906\n",
      "Epoch 8:  79%|███████▉  | 167/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2054722309112549\n",
      "Epoch 8:  79%|███████▉  | 168/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2519030570983887\n",
      "Epoch 8:  80%|███████▉  | 169/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8626801371574402\n",
      "Epoch 8:  80%|████████  | 170/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.4944351017475128\n",
      "Epoch 8:  81%|████████  | 171/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7043681740760803\n",
      "Epoch 8:  81%|████████  | 172/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.827475905418396\n",
      "Epoch 8:  82%|████████▏ | 173/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.0255296230316162\n",
      "Epoch 8:  82%|████████▏ | 174/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.315519094467163\n",
      "Epoch 8:  83%|████████▎ | 175/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.5881186723709106\n",
      "Epoch 8:  83%|████████▎ | 176/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.9683467149734497\n",
      "Epoch 8:  83%|████████▎ | 177/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.8910526037216187\n",
      "Epoch 8:  84%|████████▍ | 178/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 0.570045530796051\n",
      "Epoch 8:  84%|████████▍ | 179/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 0.8932462334632874\n",
      "Epoch 8:  85%|████████▍ | 180/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 1.0050498247146606\n",
      "Epoch 8:  85%|████████▌ | 181/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 3.355163097381592\n",
      "Epoch 8:  86%|████████▌ | 182/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 1.3089512586593628\n",
      "Epoch 8:  86%|████████▋ | 183/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 0.6544787883758545\n",
      "Epoch 8:  87%|████████▋ | 184/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 2.207265853881836\n",
      "Epoch 8:  87%|████████▋ | 185/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 1.1918294429779053\n",
      "Epoch 8:  88%|████████▊ | 186/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 0.5902702212333679\n",
      "Epoch 8:  88%|████████▊ | 187/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.1451141834259033\n",
      "Epoch 8:  89%|████████▊ | 188/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7474844455718994\n",
      "Epoch 8:  89%|████████▉ | 189/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7763243317604065\n",
      "Epoch 8:  90%|████████▉ | 190/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.5884458422660828\n",
      "Epoch 8:  90%|█████████ | 191/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.747675895690918\n",
      "Epoch 8:  91%|█████████ | 192/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.6355321407318115\n",
      "Epoch 8:  91%|█████████ | 193/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.3680834770202637\n",
      "Epoch 8:  92%|█████████▏| 194/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.7737729549407959\n",
      "Epoch 8:  92%|█████████▏| 195/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 10.207498550415039\n",
      "Epoch 8:  92%|█████████▏| 196/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.2990087270736694\n",
      "Epoch 8:  93%|█████████▎| 197/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.3203755617141724\n",
      "Epoch 8:  93%|█████████▎| 198/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.003170371055603\n",
      "Epoch 8:  94%|█████████▍| 199/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8266457319259644\n",
      "Epoch 8:  94%|█████████▍| 200/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.2072163820266724\n",
      "Epoch 8:  95%|█████████▍| 201/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.4012675285339355\n",
      "Epoch 8:  95%|█████████▌| 202/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.2419407367706299\n",
      "Epoch 8:  96%|█████████▌| 203/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8674876093864441\n",
      "Epoch 8:  96%|█████████▌| 204/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.1912376880645752\n",
      "Epoch 8:  97%|█████████▋| 205/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8476473093032837\n",
      "Epoch 8:  97%|█████████▋| 206/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8925755023956299\n",
      "Epoch 8:  98%|█████████▊| 207/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.1129703521728516\n",
      "Epoch 8:  98%|█████████▊| 208/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9687408208847046\n",
      "Epoch 8:  99%|█████████▊| 209/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7088729739189148\n",
      "Epoch 8:  99%|█████████▉| 210/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.6405874490737915\n",
      "Epoch 8: 100%|█████████▉| 211/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8824717402458191\n",
      "Epoch 8: 100%|██████████| 212/212 [00:05<00:00, 37.24it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.034207820892334\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 81.08it/s]\u001b[AValidation loss: 1.7395950555801392\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 87.21it/s]\u001b[AValidation loss: 0.7988555431365967\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 89.50it/s]\u001b[AValidation loss: 0.5935274362564087\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 91.09it/s]\u001b[AValidation loss: 0.6401011347770691\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 92.03it/s]\u001b[AValidation loss: 0.7880793809890747\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 92.60it/s]\u001b[AValidation loss: 1.0974820852279663\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 93.12it/s]\u001b[AValidation loss: 2.0438079833984375\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 93.50it/s]\u001b[AValidation loss: 0.5450279712677002\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 93.85it/s]\u001b[AValidation loss: 0.6527600884437561\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 93.33it/s]\u001b[AValidation loss: 0.6454986929893494\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 93.67it/s]\u001b[AValidation loss: 0.9376521706581116\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 93.89it/s]\u001b[AValidation loss: 0.9721791744232178\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 94.05it/s]\u001b[AValidation loss: 0.8578394651412964\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 94.20it/s]\u001b[AValidation loss: 0.7143355011940002\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 94.33it/s]\u001b[AValidation loss: 0.9912706613540649\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 94.45it/s]\u001b[AValidation loss: 0.9555205702781677\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 94.58it/s]\u001b[AValidation loss: 1.4121524095535278\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 94.60it/s]\u001b[AValidation loss: 0.6682131290435791\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 94.43it/s]\u001b[AValidation loss: 1.1487823724746704\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 94.53it/s]\u001b[AValidation loss: 1.0090080499649048\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 94.77it/s]\u001b[AValidation loss: 1.0252279043197632\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.06it/s]\u001b[AValidation loss: 0.990761935710907\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.34it/s]\u001b[AValidation loss: 0.833416759967804\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.61it/s]\u001b[AValidation loss: 1.2538551092147827\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.88it/s]\u001b[AValidation loss: 0.6596207618713379\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 96.12it/s]\u001b[AValidation loss: 0.8318156003952026\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 96.38it/s]\u001b[AValidation loss: 0.8738399744033813\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 96.61it/s]\u001b[AValidation loss: 0.8777141571044922\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 96.79it/s]\u001b[AValidation loss: 1.1819723844528198\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 96.74it/s]\u001b[AValidation loss: 0.7434963583946228\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 96.70it/s]\u001b[AValidation loss: 0.9318435788154602\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 96.69it/s]\u001b[AValidation loss: 0.8442965149879456\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 96.69it/s]\u001b[AValidation loss: 0.7190983891487122\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 96.68it/s]\u001b[AValidation loss: 1.7070493698120117\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 96.68it/s]\u001b[AValidation loss: 0.502594530582428\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 96.68it/s]\u001b[AValidation loss: 1.4610553979873657\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 96.77it/s]\u001b[AValidation loss: 0.5661720633506775\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 96.78it/s]\u001b[AValidation loss: 0.9126836657524109\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 96.77it/s]\u001b[AValidation loss: 0.8663043975830078\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 96.76it/s]\u001b[AValidation loss: 0.7150784730911255\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 96.75it/s]\u001b[AValidation loss: 0.6471061706542969\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 96.77it/s]\u001b[AValidation loss: 0.6895728707313538\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 96.77it/s]\u001b[AValidation loss: 1.1975600719451904\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 96.89it/s]\u001b[AValidation loss: 1.3115956783294678\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 97.03it/s]\u001b[AValidation loss: 0.6470907926559448\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 97.13it/s]\u001b[AValidation loss: 0.8642238974571228\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 97.24it/s]\u001b[AValidation loss: 0.8380572199821472\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 97.34it/s]\u001b[AValidation loss: 1.06014084815979\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 97.44it/s]\u001b[AValidation loss: 1.2633585929870605\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 97.42it/s]\u001b[AValidation loss: 1.1200000047683716\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 97.41it/s]\u001b[AValidation loss: 1.0203979015350342\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 97.40it/s]\u001b[AValidation loss: 1.2640608549118042\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 97.39it/s]\u001b[AValidation loss: 0.9559429883956909\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 97.40it/s]\u001b[AValidation loss: 2.068953275680542\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 97.53it/s]\u001b[A\n",
      "Epoch 9:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.7160565257072449\n",
      "Epoch 9:   0%|          | 1/212 [00:00<00:04, 45.14it/s, v_num=0]Training loss: 0.7816319465637207\n",
      "Epoch 9:   1%|          | 2/212 [00:00<00:05, 40.96it/s, v_num=0]Training loss: 1.0887025594711304\n",
      "Epoch 9:   1%|▏         | 3/212 [00:00<00:05, 39.70it/s, v_num=0]Training loss: 0.7590650916099548\n",
      "Epoch 9:   2%|▏         | 4/212 [00:00<00:05, 39.12it/s, v_num=0]Training loss: 0.9394786357879639\n",
      "Epoch 9:   2%|▏         | 5/212 [00:00<00:05, 38.79it/s, v_num=0]Training loss: 0.949622392654419\n",
      "Epoch 9:   3%|▎         | 6/212 [00:00<00:05, 38.51it/s, v_num=0]Training loss: 0.5609993934631348\n",
      "Epoch 9:   3%|▎         | 7/212 [00:00<00:05, 38.35it/s, v_num=0]Training loss: 0.6787063479423523\n",
      "Epoch 9:   4%|▍         | 8/212 [00:00<00:05, 38.12it/s, v_num=0]Training loss: 1.063115119934082\n",
      "Epoch 9:   4%|▍         | 9/212 [00:00<00:05, 38.06it/s, v_num=0]Training loss: 0.7365497350692749\n",
      "Epoch 9:   5%|▍         | 10/212 [00:00<00:05, 38.01it/s, v_num=0]Training loss: 1.074173092842102\n",
      "Epoch 9:   5%|▌         | 11/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 0.693416178226471\n",
      "Epoch 9:   6%|▌         | 12/212 [00:00<00:05, 37.93it/s, v_num=0]Training loss: 0.8330677151679993\n",
      "Epoch 9:   6%|▌         | 13/212 [00:00<00:05, 37.91it/s, v_num=0]Training loss: 0.8281433582305908\n",
      "Epoch 9:   7%|▋         | 14/212 [00:00<00:05, 37.88it/s, v_num=0]Training loss: 0.7682608962059021\n",
      "Epoch 9:   7%|▋         | 15/212 [00:00<00:05, 37.86it/s, v_num=0]Training loss: 0.9487239122390747\n",
      "Epoch 9:   8%|▊         | 16/212 [00:00<00:05, 37.82it/s, v_num=0]Training loss: 1.2753918170928955\n",
      "Epoch 9:   8%|▊         | 17/212 [00:00<00:05, 37.80it/s, v_num=0]Training loss: 0.7593350410461426\n",
      "Epoch 9:   8%|▊         | 18/212 [00:00<00:05, 37.79it/s, v_num=0]Training loss: 1.5420550107955933\n",
      "Epoch 9:   9%|▉         | 19/212 [00:00<00:05, 37.78it/s, v_num=0]Training loss: 0.5638633966445923\n",
      "Epoch 9:   9%|▉         | 20/212 [00:00<00:05, 37.77it/s, v_num=0]Training loss: 1.494483232498169\n",
      "Epoch 9:  10%|▉         | 21/212 [00:00<00:05, 37.76it/s, v_num=0]Training loss: 1.0235158205032349\n",
      "Epoch 9:  10%|█         | 22/212 [00:00<00:05, 37.75it/s, v_num=0]Training loss: 0.8730626106262207\n",
      "Epoch 9:  11%|█         | 23/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 0.650473952293396\n",
      "Epoch 9:  11%|█▏        | 24/212 [00:00<00:04, 37.73it/s, v_num=0]Training loss: 0.9701290726661682\n",
      "Epoch 9:  12%|█▏        | 25/212 [00:00<00:04, 37.72it/s, v_num=0]Training loss: 1.0367785692214966\n",
      "Epoch 9:  12%|█▏        | 26/212 [00:00<00:04, 37.72it/s, v_num=0]Training loss: 0.9979127049446106\n",
      "Epoch 9:  13%|█▎        | 27/212 [00:00<00:04, 37.70it/s, v_num=0]Training loss: 1.0113579034805298\n",
      "Epoch 9:  13%|█▎        | 28/212 [00:00<00:04, 37.63it/s, v_num=0]Training loss: 0.6062906980514526\n",
      "Epoch 9:  14%|█▎        | 29/212 [00:00<00:04, 37.58it/s, v_num=0]Training loss: 0.8411436080932617\n",
      "Epoch 9:  14%|█▍        | 30/212 [00:00<00:04, 37.53it/s, v_num=0]Training loss: 0.9586659669876099\n",
      "Epoch 9:  15%|█▍        | 31/212 [00:00<00:04, 37.49it/s, v_num=0]Training loss: 0.8421779870986938\n",
      "Epoch 9:  15%|█▌        | 32/212 [00:00<00:04, 37.45it/s, v_num=0]Training loss: 1.250369668006897\n",
      "Epoch 9:  16%|█▌        | 33/212 [00:00<00:04, 37.41it/s, v_num=0]Training loss: 0.817876398563385\n",
      "Epoch 9:  16%|█▌        | 34/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.813510000705719\n",
      "Epoch 9:  17%|█▋        | 35/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 1.6475839614868164\n",
      "Epoch 9:  17%|█▋        | 36/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 1.1271196603775024\n",
      "Epoch 9:  17%|█▋        | 37/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 1.4942741394042969\n",
      "Epoch 9:  18%|█▊        | 38/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.5137163400650024\n",
      "Epoch 9:  18%|█▊        | 39/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.4318332374095917\n",
      "Epoch 9:  19%|█▉        | 40/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.0283925533294678\n",
      "Epoch 9:  19%|█▉        | 41/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.6727099418640137\n",
      "Epoch 9:  20%|█▉        | 42/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.5724602937698364\n",
      "Epoch 9:  20%|██        | 43/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.347288727760315\n",
      "Epoch 9:  21%|██        | 44/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.3932437896728516\n",
      "Epoch 9:  21%|██        | 45/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.6577491760253906\n",
      "Epoch 9:  22%|██▏       | 46/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.0148361921310425\n",
      "Epoch 9:  22%|██▏       | 47/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.7880935668945312\n",
      "Epoch 9:  23%|██▎       | 48/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.1486515998840332\n",
      "Epoch 9:  23%|██▎       | 49/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.6513484716415405\n",
      "Epoch 9:  24%|██▎       | 50/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.5924217700958252\n",
      "Epoch 9:  24%|██▍       | 51/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.6133882403373718\n",
      "Epoch 9:  25%|██▍       | 52/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.6477988958358765\n",
      "Epoch 9:  25%|██▌       | 53/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.074337363243103\n",
      "Epoch 9:  25%|██▌       | 54/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.4548734128475189\n",
      "Epoch 9:  26%|██▌       | 55/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.7042809724807739\n",
      "Epoch 9:  26%|██▋       | 56/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.2350952625274658\n",
      "Epoch 9:  27%|██▋       | 57/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.7449244856834412\n",
      "Epoch 9:  27%|██▋       | 58/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.899912416934967\n",
      "Epoch 9:  28%|██▊       | 59/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.7628390192985535\n",
      "Epoch 9:  28%|██▊       | 60/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 2.1618776321411133\n",
      "Epoch 9:  29%|██▉       | 61/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 2.170701503753662\n",
      "Epoch 9:  29%|██▉       | 62/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.6553528904914856\n",
      "Epoch 9:  30%|██▉       | 63/212 [00:01<00:03, 37.38it/s, v_num=0]Training loss: 1.273059606552124\n",
      "Epoch 9:  30%|███       | 64/212 [00:01<00:03, 37.38it/s, v_num=0]Training loss: 0.5712602138519287\n",
      "Epoch 9:  31%|███       | 65/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 0.7954393625259399\n",
      "Epoch 9:  31%|███       | 66/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 1.5170526504516602\n",
      "Epoch 9:  32%|███▏      | 67/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 0.9837734699249268\n",
      "Epoch 9:  32%|███▏      | 68/212 [00:01<00:03, 37.36it/s, v_num=0]Training loss: 0.79522705078125\n",
      "Epoch 9:  33%|███▎      | 69/212 [00:01<00:03, 37.35it/s, v_num=0]Training loss: 1.0391353368759155\n",
      "Epoch 9:  33%|███▎      | 70/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 0.681589663028717\n",
      "Epoch 9:  33%|███▎      | 71/212 [00:01<00:03, 37.32it/s, v_num=0]Training loss: 1.0195896625518799\n",
      "Epoch 9:  34%|███▍      | 72/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 1.0985215902328491\n",
      "Epoch 9:  34%|███▍      | 73/212 [00:01<00:03, 37.29it/s, v_num=0]Training loss: 0.7169983983039856\n",
      "Epoch 9:  35%|███▍      | 74/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.653912365436554\n",
      "Epoch 9:  35%|███▌      | 75/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.6630949974060059\n",
      "Epoch 9:  36%|███▌      | 76/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.7894073724746704\n",
      "Epoch 9:  36%|███▋      | 77/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.6726582050323486\n",
      "Epoch 9:  37%|███▋      | 78/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9554498791694641\n",
      "Epoch 9:  37%|███▋      | 79/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.4196587800979614\n",
      "Epoch 9:  38%|███▊      | 80/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.7731274366378784\n",
      "Epoch 9:  38%|███▊      | 81/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.8541663885116577\n",
      "Epoch 9:  39%|███▊      | 82/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.1839693784713745\n",
      "Epoch 9:  39%|███▉      | 83/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9132188558578491\n",
      "Epoch 9:  40%|███▉      | 84/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7621413469314575\n",
      "Epoch 9:  40%|████      | 85/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9050723910331726\n",
      "Epoch 9:  41%|████      | 86/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.5611402988433838\n",
      "Epoch 9:  41%|████      | 87/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.267075777053833\n",
      "Epoch 9:  42%|████▏     | 88/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.2206603288650513\n",
      "Epoch 9:  42%|████▏     | 89/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9843252897262573\n",
      "Epoch 9:  42%|████▏     | 90/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.0151243209838867\n",
      "Epoch 9:  43%|████▎     | 91/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.4407325983047485\n",
      "Epoch 9:  43%|████▎     | 92/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.3849971294403076\n",
      "Epoch 9:  44%|████▍     | 93/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8762246370315552\n",
      "Epoch 9:  44%|████▍     | 94/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.184418797492981\n",
      "Epoch 9:  45%|████▍     | 95/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.9228740930557251\n",
      "Epoch 9:  45%|████▌     | 96/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.8218845725059509\n",
      "Epoch 9:  46%|████▌     | 97/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.8984547853469849\n",
      "Epoch 9:  46%|████▌     | 98/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.9077839255332947\n",
      "Epoch 9:  47%|████▋     | 99/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.8585157990455627\n",
      "Epoch 9:  47%|████▋     | 100/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.011565923690796\n",
      "Epoch 9:  48%|████▊     | 101/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 0.8603634238243103\n",
      "Epoch 9:  48%|████▊     | 102/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 0.8534701466560364\n",
      "Epoch 9:  49%|████▊     | 103/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.7108412981033325\n",
      "Epoch 9:  49%|████▉     | 104/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 3.480064868927002\n",
      "Epoch 9:  50%|████▉     | 105/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.4360899329185486\n",
      "Epoch 9:  50%|█████     | 106/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.7056425213813782\n",
      "Epoch 9:  50%|█████     | 107/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.9412199258804321\n",
      "Epoch 9:  51%|█████     | 108/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 0.9348700642585754\n",
      "Epoch 9:  51%|█████▏    | 109/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.395490050315857\n",
      "Epoch 9:  52%|█████▏    | 110/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 0.8049260377883911\n",
      "Epoch 9:  52%|█████▏    | 111/212 [00:02<00:02, 37.27it/s, v_num=0]Training loss: 1.092308759689331\n",
      "Epoch 9:  53%|█████▎    | 112/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.3130139112472534\n",
      "Epoch 9:  53%|█████▎    | 113/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8071333765983582\n",
      "Epoch 9:  54%|█████▍    | 114/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.8159946203231812\n",
      "Epoch 9:  54%|█████▍    | 115/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.785672664642334\n",
      "Epoch 9:  55%|█████▍    | 116/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.9492517709732056\n",
      "Epoch 9:  55%|█████▌    | 117/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.6985851526260376\n",
      "Epoch 9:  56%|█████▌    | 118/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6881348490715027\n",
      "Epoch 9:  56%|█████▌    | 119/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9234789609909058\n",
      "Epoch 9:  57%|█████▋    | 120/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6416664719581604\n",
      "Epoch 9:  57%|█████▋    | 121/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9881598353385925\n",
      "Epoch 9:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6992126703262329\n",
      "Epoch 9:  58%|█████▊    | 123/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9033165574073792\n",
      "Epoch 9:  58%|█████▊    | 124/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.6162238121032715\n",
      "Epoch 9:  59%|█████▉    | 125/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.9825134873390198\n",
      "Epoch 9:  59%|█████▉    | 126/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.630813479423523\n",
      "Epoch 9:  60%|█████▉    | 127/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.083293080329895\n",
      "Epoch 9:  60%|██████    | 128/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.6328588724136353\n",
      "Epoch 9:  61%|██████    | 129/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8386475443840027\n",
      "Epoch 9:  61%|██████▏   | 130/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7543292045593262\n",
      "Epoch 9:  62%|██████▏   | 131/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.9859949350357056\n",
      "Epoch 9:  62%|██████▏   | 132/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.2129456996917725\n",
      "Epoch 9:  63%|██████▎   | 133/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.7675700187683105\n",
      "Epoch 9:  63%|██████▎   | 134/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 10.154144287109375\n",
      "Epoch 9:  64%|██████▎   | 135/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.8094310760498047\n",
      "Epoch 9:  64%|██████▍   | 136/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.8583133220672607\n",
      "Epoch 9:  65%|██████▍   | 137/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.2522724866867065\n",
      "Epoch 9:  65%|██████▌   | 138/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 0.625329852104187\n",
      "Epoch 9:  66%|██████▌   | 139/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 0.847194492816925\n",
      "Epoch 9:  66%|██████▌   | 140/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 1.018210768699646\n",
      "Epoch 9:  67%|██████▋   | 141/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.6198796033859253\n",
      "Epoch 9:  67%|██████▋   | 142/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.7811641693115234\n",
      "Epoch 9:  67%|██████▋   | 143/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 1.0844660997390747\n",
      "Epoch 9:  68%|██████▊   | 144/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.8419530987739563\n",
      "Epoch 9:  68%|██████▊   | 145/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.8710156679153442\n",
      "Epoch 9:  69%|██████▉   | 146/212 [00:03<00:01, 37.30it/s, v_num=0]Training loss: 1.0902326107025146\n",
      "Epoch 9:  69%|██████▉   | 147/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 1.6726033687591553\n",
      "Epoch 9:  70%|██████▉   | 148/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 0.7345756888389587\n",
      "Epoch 9:  70%|███████   | 149/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 0.7901752591133118\n",
      "Epoch 9:  71%|███████   | 150/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.6582110524177551\n",
      "Epoch 9:  71%|███████   | 151/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.0526628494262695\n",
      "Epoch 9:  72%|███████▏  | 152/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.8328224420547485\n",
      "Epoch 9:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8194656372070312\n",
      "Epoch 9:  73%|███████▎  | 154/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.6416070461273193\n",
      "Epoch 9:  73%|███████▎  | 155/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.9934133887290955\n",
      "Epoch 9:  74%|███████▎  | 156/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 2.0467915534973145\n",
      "Epoch 9:  74%|███████▍  | 157/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.5336992144584656\n",
      "Epoch 9:  75%|███████▍  | 158/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8128984570503235\n",
      "Epoch 9:  75%|███████▌  | 159/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.1919118165969849\n",
      "Epoch 9:  75%|███████▌  | 160/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.502154529094696\n",
      "Epoch 9:  76%|███████▌  | 161/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.907898485660553\n",
      "Epoch 9:  76%|███████▋  | 162/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8915672302246094\n",
      "Epoch 9:  77%|███████▋  | 163/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7772207856178284\n",
      "Epoch 9:  77%|███████▋  | 164/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9413822889328003\n",
      "Epoch 9:  78%|███████▊  | 165/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.6974670886993408\n",
      "Epoch 9:  78%|███████▊  | 166/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.3221828937530518\n",
      "Epoch 9:  79%|███████▉  | 167/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9814985990524292\n",
      "Epoch 9:  79%|███████▉  | 168/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.8590706586837769\n",
      "Epoch 9:  80%|███████▉  | 169/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7435149550437927\n",
      "Epoch 9:  80%|████████  | 170/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9979746341705322\n",
      "Epoch 9:  81%|████████  | 171/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7851362228393555\n",
      "Epoch 9:  81%|████████  | 172/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.546587347984314\n",
      "Epoch 9:  82%|████████▏ | 173/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.49856218695640564\n",
      "Epoch 9:  82%|████████▏ | 174/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 3.873917579650879\n",
      "Epoch 9:  83%|████████▎ | 175/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.1344752311706543\n",
      "Epoch 9:  83%|████████▎ | 176/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.68865966796875\n",
      "Epoch 9:  83%|████████▎ | 177/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.4364385604858398\n",
      "Epoch 9:  84%|████████▍ | 178/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.7453383803367615\n",
      "Epoch 9:  84%|████████▍ | 179/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.2624365091323853\n",
      "Epoch 9:  85%|████████▍ | 180/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.449846863746643\n",
      "Epoch 9:  85%|████████▌ | 181/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.1652321815490723\n",
      "Epoch 9:  86%|████████▌ | 182/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.7675748467445374\n",
      "Epoch 9:  86%|████████▋ | 183/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.3586989641189575\n",
      "Epoch 9:  87%|████████▋ | 184/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.4495211839675903\n",
      "Epoch 9:  87%|████████▋ | 185/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.9157508015632629\n",
      "Epoch 9:  88%|████████▊ | 186/212 [00:04<00:00, 37.28it/s, v_num=0]Training loss: 1.261263370513916\n",
      "Epoch 9:  88%|████████▊ | 187/212 [00:05<00:00, 37.27it/s, v_num=0]Training loss: 0.9681367874145508\n",
      "Epoch 9:  89%|████████▊ | 188/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 10.312139511108398\n",
      "Epoch 9:  89%|████████▉ | 189/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.0540505647659302\n",
      "Epoch 9:  90%|████████▉ | 190/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.1154322624206543\n",
      "Epoch 9:  90%|█████████ | 191/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.2137361764907837\n",
      "Epoch 9:  91%|█████████ | 192/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.679484486579895\n",
      "Epoch 9:  91%|█████████ | 193/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6701797246932983\n",
      "Epoch 9:  92%|█████████▏| 194/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.212031364440918\n",
      "Epoch 9:  92%|█████████▏| 195/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.1659053564071655\n",
      "Epoch 9:  92%|█████████▏| 196/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.0541601181030273\n",
      "Epoch 9:  93%|█████████▎| 197/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.5028480291366577\n",
      "Epoch 9:  93%|█████████▎| 198/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8213792443275452\n",
      "Epoch 9:  94%|█████████▍| 199/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.46152201294898987\n",
      "Epoch 9:  94%|█████████▍| 200/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.2450776100158691\n",
      "Epoch 9:  95%|█████████▍| 201/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7280126810073853\n",
      "Epoch 9:  95%|█████████▌| 202/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9085522890090942\n",
      "Epoch 9:  96%|█████████▌| 203/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.4503347873687744\n",
      "Epoch 9:  96%|█████████▌| 204/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9725472927093506\n",
      "Epoch 9:  97%|█████████▋| 205/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.0424671173095703\n",
      "Epoch 9:  97%|█████████▋| 206/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 2.117598056793213\n",
      "Epoch 9:  98%|█████████▊| 207/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9277533888816833\n",
      "Epoch 9:  98%|█████████▊| 208/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.721630334854126\n",
      "Epoch 9:  99%|█████████▊| 209/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8519365787506104\n",
      "Epoch 9:  99%|█████████▉| 210/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.6844401359558105\n",
      "Epoch 9: 100%|█████████▉| 211/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.6001936197280884\n",
      "Epoch 9: 100%|██████████| 212/212 [00:05<00:00, 37.25it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.0174717903137207\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 79.20it/s]\u001b[AValidation loss: 1.719573974609375\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.10it/s]\u001b[AValidation loss: 0.7877389788627625\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 94.31it/s]\u001b[AValidation loss: 0.5789708495140076\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 96.67it/s]\u001b[AValidation loss: 0.6329987645149231\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 98.17it/s]\u001b[AValidation loss: 0.7787079811096191\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 99.33it/s]\u001b[AValidation loss: 1.0855196714401245\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 100.00it/s]\u001b[AValidation loss: 2.034416675567627\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 99.82it/s] \u001b[AValidation loss: 0.5383970141410828\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 100.31it/s]\u001b[AValidation loss: 0.6456066966056824\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 100.78it/s]\u001b[AValidation loss: 0.6343065500259399\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 101.09it/s]\u001b[AValidation loss: 0.9237650632858276\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 101.39it/s]\u001b[AValidation loss: 0.9674572944641113\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 101.66it/s]\u001b[AValidation loss: 0.8495977520942688\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 101.87it/s]\u001b[AValidation loss: 0.7055765986442566\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 102.10it/s]\u001b[AValidation loss: 0.9713443517684937\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 102.30it/s]\u001b[AValidation loss: 0.9462751150131226\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 102.50it/s]\u001b[AValidation loss: 1.4042363166809082\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.67it/s]\u001b[AValidation loss: 0.6574251651763916\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 102.83it/s]\u001b[AValidation loss: 1.1389565467834473\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 102.92it/s]\u001b[AValidation loss: 0.999871015548706\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 102.67it/s]\u001b[AValidation loss: 1.0161411762237549\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 102.44it/s]\u001b[AValidation loss: 0.9794977903366089\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 102.26it/s]\u001b[AValidation loss: 0.8243566155433655\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 102.38it/s]\u001b[AValidation loss: 1.2345112562179565\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 102.20it/s]\u001b[AValidation loss: 0.6496677398681641\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 0.8107424974441528\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 102.32it/s]\u001b[AValidation loss: 0.8622903227806091\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 102.40it/s]\u001b[AValidation loss: 0.8752470016479492\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 102.46it/s]\u001b[AValidation loss: 1.176661729812622\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 102.55it/s]\u001b[AValidation loss: 0.729904055595398\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.61it/s]\u001b[AValidation loss: 0.9278972148895264\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.67it/s]\u001b[AValidation loss: 0.8364700675010681\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 102.74it/s]\u001b[AValidation loss: 0.7090781927108765\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 102.79it/s]\u001b[AValidation loss: 1.6890499591827393\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 102.82it/s]\u001b[AValidation loss: 0.4934115409851074\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.86it/s]\u001b[AValidation loss: 1.4519537687301636\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.88it/s]\u001b[AValidation loss: 0.5591052174568176\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.90it/s]\u001b[AValidation loss: 0.896904706954956\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.75it/s]\u001b[AValidation loss: 0.8507174849510193\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.59it/s]\u001b[AValidation loss: 0.7011306285858154\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.43it/s]\u001b[AValidation loss: 0.6382027864456177\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.27it/s]\u001b[AValidation loss: 0.6801676750183105\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 102.11it/s]\u001b[AValidation loss: 1.1848829984664917\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 101.96it/s]\u001b[AValidation loss: 1.2863314151763916\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 101.79it/s]\u001b[AValidation loss: 0.640153169631958\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 101.62it/s]\u001b[AValidation loss: 0.8528329730033875\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 101.48it/s]\u001b[AValidation loss: 0.8234897255897522\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 101.34it/s]\u001b[AValidation loss: 1.0499322414398193\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 100.85it/s]\u001b[AValidation loss: 1.250944972038269\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 100.67it/s]\u001b[AValidation loss: 1.1020209789276123\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 100.55it/s]\u001b[AValidation loss: 1.015190839767456\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 100.45it/s]\u001b[AValidation loss: 1.2492923736572266\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 100.30it/s]\u001b[AValidation loss: 0.9438145756721497\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 100.20it/s]\u001b[AValidation loss: 2.0575873851776123\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 100.26it/s]\u001b[A\n",
      "Epoch 10:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.8641831874847412\n",
      "Epoch 10:   0%|          | 1/212 [00:00<00:04, 45.11it/s, v_num=0]Training loss: 1.0016298294067383\n",
      "Epoch 10:   1%|          | 2/212 [00:00<00:05, 41.05it/s, v_num=0]Training loss: 1.0528583526611328\n",
      "Epoch 10:   1%|▏         | 3/212 [00:00<00:05, 39.07it/s, v_num=0]Training loss: 0.9948322176933289\n",
      "Epoch 10:   2%|▏         | 4/212 [00:00<00:05, 38.33it/s, v_num=0]Training loss: 1.1732724905014038\n",
      "Epoch 10:   2%|▏         | 5/212 [00:00<00:05, 37.89it/s, v_num=0]Training loss: 0.6162189245223999\n",
      "Epoch 10:   3%|▎         | 6/212 [00:00<00:05, 37.62it/s, v_num=0]Training loss: 0.8561808466911316\n",
      "Epoch 10:   3%|▎         | 7/212 [00:00<00:05, 37.42it/s, v_num=0]Training loss: 0.5549094676971436\n",
      "Epoch 10:   4%|▍         | 8/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 0.865626335144043\n",
      "Epoch 10:   4%|▍         | 9/212 [00:00<00:05, 37.04it/s, v_num=0]Training loss: 1.0270558595657349\n",
      "Epoch 10:   5%|▍         | 10/212 [00:00<00:05, 36.93it/s, v_num=0]Training loss: 1.1257860660552979\n",
      "Epoch 10:   5%|▌         | 11/212 [00:00<00:05, 37.08it/s, v_num=0]Training loss: 0.8672581911087036\n",
      "Epoch 10:   6%|▌         | 12/212 [00:00<00:05, 37.10it/s, v_num=0]Training loss: 0.6938378214836121\n",
      "Epoch 10:   6%|▌         | 13/212 [00:00<00:05, 37.14it/s, v_num=0]Training loss: 1.0233815908432007\n",
      "Epoch 10:   7%|▋         | 14/212 [00:00<00:05, 37.15it/s, v_num=0]Training loss: 0.49437496066093445\n",
      "Epoch 10:   7%|▋         | 15/212 [00:00<00:05, 37.18it/s, v_num=0]Training loss: 0.44555899500846863\n",
      "Epoch 10:   8%|▊         | 16/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 0.9251026511192322\n",
      "Epoch 10:   8%|▊         | 17/212 [00:00<00:05, 37.20it/s, v_num=0]Training loss: 3.3105692863464355\n",
      "Epoch 10:   8%|▊         | 18/212 [00:00<00:05, 37.21it/s, v_num=0]Training loss: 0.5285492539405823\n",
      "Epoch 10:   9%|▉         | 19/212 [00:00<00:05, 37.23it/s, v_num=0]Training loss: 0.7244762182235718\n",
      "Epoch 10:   9%|▉         | 20/212 [00:00<00:05, 37.24it/s, v_num=0]Training loss: 1.2006844282150269\n",
      "Epoch 10:  10%|▉         | 21/212 [00:00<00:05, 37.24it/s, v_num=0]Training loss: 1.495474934577942\n",
      "Epoch 10:  10%|█         | 22/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 0.8231908082962036\n",
      "Epoch 10:  11%|█         | 23/212 [00:00<00:05, 37.26it/s, v_num=0]Training loss: 0.9386754035949707\n",
      "Epoch 10:  11%|█▏        | 24/212 [00:00<00:05, 37.27it/s, v_num=0]Training loss: 0.6863975524902344\n",
      "Epoch 10:  12%|█▏        | 25/212 [00:00<00:05, 37.27it/s, v_num=0]Training loss: 1.1361804008483887\n",
      "Epoch 10:  12%|█▏        | 26/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.9896804690361023\n",
      "Epoch 10:  13%|█▎        | 27/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 0.8477283716201782\n",
      "Epoch 10:  13%|█▎        | 28/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 0.8244441747665405\n",
      "Epoch 10:  14%|█▎        | 29/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 0.927975594997406\n",
      "Epoch 10:  14%|█▍        | 30/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 0.8171784281730652\n",
      "Epoch 10:  15%|█▍        | 31/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 0.8759816884994507\n",
      "Epoch 10:  15%|█▌        | 32/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.9467514753341675\n",
      "Epoch 10:  16%|█▌        | 33/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 0.7479061484336853\n",
      "Epoch 10:  16%|█▌        | 34/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 1.985198974609375\n",
      "Epoch 10:  17%|█▋        | 35/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.6926565170288086\n",
      "Epoch 10:  17%|█▋        | 36/212 [00:00<00:04, 37.36it/s, v_num=0]Training loss: 0.6252304315567017\n",
      "Epoch 10:  17%|█▋        | 37/212 [00:00<00:04, 37.37it/s, v_num=0]Training loss: 0.6497659087181091\n",
      "Epoch 10:  18%|█▊        | 38/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.8539919257164001\n",
      "Epoch 10:  18%|█▊        | 39/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.783973753452301\n",
      "Epoch 10:  19%|█▉        | 40/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.9025053977966309\n",
      "Epoch 10:  19%|█▉        | 41/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.0302504301071167\n",
      "Epoch 10:  20%|█▉        | 42/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.7558251619338989\n",
      "Epoch 10:  20%|██        | 43/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.6646208763122559\n",
      "Epoch 10:  21%|██        | 44/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.1071951389312744\n",
      "Epoch 10:  21%|██        | 45/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 1.003898024559021\n",
      "Epoch 10:  22%|██▏       | 46/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.9646680355072021\n",
      "Epoch 10:  22%|██▏       | 47/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.137453317642212\n",
      "Epoch 10:  23%|██▎       | 48/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.3794797658920288\n",
      "Epoch 10:  23%|██▎       | 49/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 1.8847044706344604\n",
      "Epoch 10:  24%|██▎       | 50/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 0.7580374479293823\n",
      "Epoch 10:  24%|██▍       | 51/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.9033598899841309\n",
      "Epoch 10:  25%|██▍       | 52/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.9814842343330383\n",
      "Epoch 10:  25%|██▌       | 53/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.7526005506515503\n",
      "Epoch 10:  25%|██▌       | 54/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 0.6973834037780762\n",
      "Epoch 10:  26%|██▌       | 55/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 0.8824629187583923\n",
      "Epoch 10:  26%|██▋       | 56/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.8840233087539673\n",
      "Epoch 10:  27%|██▋       | 57/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.9924461245536804\n",
      "Epoch 10:  27%|██▋       | 58/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.532374382019043\n",
      "Epoch 10:  28%|██▊       | 59/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.8476543426513672\n",
      "Epoch 10:  28%|██▊       | 60/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.7455571889877319\n",
      "Epoch 10:  29%|██▉       | 61/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.9382562637329102\n",
      "Epoch 10:  29%|██▉       | 62/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.5566476583480835\n",
      "Epoch 10:  30%|██▉       | 63/212 [00:01<00:04, 37.24it/s, v_num=0]Training loss: 1.8956034183502197\n",
      "Epoch 10:  30%|███       | 64/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 0.7717452049255371\n",
      "Epoch 10:  31%|███       | 65/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.7219390273094177\n",
      "Epoch 10:  31%|███       | 66/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.849661111831665\n",
      "Epoch 10:  32%|███▏      | 67/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.3184438943862915\n",
      "Epoch 10:  32%|███▏      | 68/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.8947438597679138\n",
      "Epoch 10:  33%|███▎      | 69/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 1.0196510553359985\n",
      "Epoch 10:  33%|███▎      | 70/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.8809592127799988\n",
      "Epoch 10:  33%|███▎      | 71/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.863711416721344\n",
      "Epoch 10:  34%|███▍      | 72/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.9792590141296387\n",
      "Epoch 10:  34%|███▍      | 73/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 0.9513254165649414\n",
      "Epoch 10:  35%|███▍      | 74/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 0.5877602696418762\n",
      "Epoch 10:  35%|███▌      | 75/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.7336617112159729\n",
      "Epoch 10:  36%|███▌      | 76/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.963401198387146\n",
      "Epoch 10:  36%|███▋      | 77/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8769229054450989\n",
      "Epoch 10:  37%|███▋      | 78/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.7459569573402405\n",
      "Epoch 10:  37%|███▋      | 79/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.5728841423988342\n",
      "Epoch 10:  38%|███▊      | 80/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.6480726003646851\n",
      "Epoch 10:  38%|███▊      | 81/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.7853060960769653\n",
      "Epoch 10:  39%|███▊      | 82/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7453852891921997\n",
      "Epoch 10:  39%|███▉      | 83/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.6302163600921631\n",
      "Epoch 10:  40%|███▉      | 84/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.160072684288025\n",
      "Epoch 10:  40%|████      | 85/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.7137375473976135\n",
      "Epoch 10:  41%|████      | 86/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.6527979373931885\n",
      "Epoch 10:  41%|████      | 87/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.6375996470451355\n",
      "Epoch 10:  42%|████▏     | 88/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 0.833660364151001\n",
      "Epoch 10:  42%|████▏     | 89/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.0695549249649048\n",
      "Epoch 10:  42%|████▏     | 90/212 [00:02<00:03, 37.18it/s, v_num=0]Training loss: 1.42201828956604\n",
      "Epoch 10:  43%|████▎     | 91/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 0.874966025352478\n",
      "Epoch 10:  43%|████▎     | 92/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.1138741970062256\n",
      "Epoch 10:  44%|████▍     | 93/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.0364584922790527\n",
      "Epoch 10:  44%|████▍     | 94/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 0.8127304315567017\n",
      "Epoch 10:  45%|████▍     | 95/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.9467794895172119\n",
      "Epoch 10:  45%|████▌     | 96/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.6132747530937195\n",
      "Epoch 10:  46%|████▌     | 97/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.5921694040298462\n",
      "Epoch 10:  46%|████▌     | 98/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.312119960784912\n",
      "Epoch 10:  47%|████▋     | 99/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.8695614337921143\n",
      "Epoch 10:  47%|████▋     | 100/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.2466785907745361\n",
      "Epoch 10:  48%|████▊     | 101/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.2049208879470825\n",
      "Epoch 10:  48%|████▊     | 102/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.2887901067733765\n",
      "Epoch 10:  49%|████▊     | 103/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 0.8081185817718506\n",
      "Epoch 10:  49%|████▉     | 104/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.1408363580703735\n",
      "Epoch 10:  50%|████▉     | 105/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.7665757536888123\n",
      "Epoch 10:  50%|█████     | 106/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 1.2224953174591064\n",
      "Epoch 10:  50%|█████     | 107/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 1.144745111465454\n",
      "Epoch 10:  51%|█████     | 108/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.5804958343505859\n",
      "Epoch 10:  51%|█████▏    | 109/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.0031249523162842\n",
      "Epoch 10:  52%|█████▏    | 110/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.7484318614006042\n",
      "Epoch 10:  52%|█████▏    | 111/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.433109164237976\n",
      "Epoch 10:  53%|█████▎    | 112/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.6391315460205078\n",
      "Epoch 10:  53%|█████▎    | 113/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 3.746466875076294\n",
      "Epoch 10:  54%|█████▍    | 114/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8554996848106384\n",
      "Epoch 10:  54%|█████▍    | 115/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9322288036346436\n",
      "Epoch 10:  55%|█████▍    | 116/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.275884985923767\n",
      "Epoch 10:  55%|█████▌    | 117/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0546202659606934\n",
      "Epoch 10:  56%|█████▌    | 118/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.9456843137741089\n",
      "Epoch 10:  56%|█████▌    | 119/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.6204513311386108\n",
      "Epoch 10:  57%|█████▋    | 120/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.5421913862228394\n",
      "Epoch 10:  57%|█████▋    | 121/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8574578762054443\n",
      "Epoch 10:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0758413076400757\n",
      "Epoch 10:  58%|█████▊    | 123/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1113579273223877\n",
      "Epoch 10:  58%|█████▊    | 124/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.5020127296447754\n",
      "Epoch 10:  59%|█████▉    | 125/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 2.1618497371673584\n",
      "Epoch 10:  59%|█████▉    | 126/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.5979526042938232\n",
      "Epoch 10:  60%|█████▉    | 127/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.614905834197998\n",
      "Epoch 10:  60%|██████    | 128/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.1411633491516113\n",
      "Epoch 10:  61%|██████    | 129/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 1.2823830842971802\n",
      "Epoch 10:  61%|██████▏   | 130/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 0.7520222067832947\n",
      "Epoch 10:  62%|██████▏   | 131/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 0.49504637718200684\n",
      "Epoch 10:  62%|██████▏   | 132/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 0.9710138440132141\n",
      "Epoch 10:  63%|██████▎   | 133/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.0567023754119873\n",
      "Epoch 10:  63%|██████▎   | 134/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 0.6245642900466919\n",
      "Epoch 10:  64%|██████▎   | 135/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.390415072441101\n",
      "Epoch 10:  64%|██████▍   | 136/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.2310737371444702\n",
      "Epoch 10:  65%|██████▍   | 137/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.6235151886940002\n",
      "Epoch 10:  65%|██████▌   | 138/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 1.1781768798828125\n",
      "Epoch 10:  66%|██████▌   | 139/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 0.8390083312988281\n",
      "Epoch 10:  66%|██████▌   | 140/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 1.1388121843338013\n",
      "Epoch 10:  67%|██████▋   | 141/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 0.6849144101142883\n",
      "Epoch 10:  67%|██████▋   | 142/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.0290391445159912\n",
      "Epoch 10:  67%|██████▋   | 143/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.013446569442749\n",
      "Epoch 10:  68%|██████▊   | 144/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.4297415018081665\n",
      "Epoch 10:  68%|██████▊   | 145/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.1396894454956055\n",
      "Epoch 10:  69%|██████▉   | 146/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.7682508230209351\n",
      "Epoch 10:  69%|██████▉   | 147/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 1.0548396110534668\n",
      "Epoch 10:  70%|██████▉   | 148/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.6761995553970337\n",
      "Epoch 10:  70%|███████   | 149/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.2122218608856201\n",
      "Epoch 10:  71%|███████   | 150/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.2841768264770508\n",
      "Epoch 10:  71%|███████   | 151/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.3047356605529785\n",
      "Epoch 10:  72%|███████▏  | 152/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8500135540962219\n",
      "Epoch 10:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8213976621627808\n",
      "Epoch 10:  73%|███████▎  | 154/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8900330066680908\n",
      "Epoch 10:  73%|███████▎  | 155/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2276116609573364\n",
      "Epoch 10:  74%|███████▎  | 156/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7759228348731995\n",
      "Epoch 10:  74%|███████▍  | 157/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7664936780929565\n",
      "Epoch 10:  75%|███████▍  | 158/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9020968079566956\n",
      "Epoch 10:  75%|███████▌  | 159/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.6708459258079529\n",
      "Epoch 10:  75%|███████▌  | 160/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9052116870880127\n",
      "Epoch 10:  76%|███████▌  | 161/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9643475413322449\n",
      "Epoch 10:  76%|███████▋  | 162/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9944512844085693\n",
      "Epoch 10:  77%|███████▋  | 163/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.5270591974258423\n",
      "Epoch 10:  77%|███████▋  | 164/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.105707049369812\n",
      "Epoch 10:  78%|███████▊  | 165/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.066267967224121\n",
      "Epoch 10:  78%|███████▊  | 166/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.2863214015960693\n",
      "Epoch 10:  79%|███████▉  | 167/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.7852272391319275\n",
      "Epoch 10:  79%|███████▉  | 168/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 9.292136192321777\n",
      "Epoch 10:  80%|███████▉  | 169/212 [00:04<00:01, 37.19it/s, v_num=0]Training loss: 0.8897672891616821\n",
      "Epoch 10:  80%|████████  | 170/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.9668567180633545\n",
      "Epoch 10:  81%|████████  | 171/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.9805060625076294\n",
      "Epoch 10:  81%|████████  | 172/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 0.8045985698699951\n",
      "Epoch 10:  82%|████████▏ | 173/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.1045218706130981\n",
      "Epoch 10:  82%|████████▏ | 174/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.0361157655715942\n",
      "Epoch 10:  83%|████████▎ | 175/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.7567954063415527\n",
      "Epoch 10:  83%|████████▎ | 176/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.6989814639091492\n",
      "Epoch 10:  83%|████████▎ | 177/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.9855836033821106\n",
      "Epoch 10:  84%|████████▍ | 178/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 1.1321403980255127\n",
      "Epoch 10:  84%|████████▍ | 179/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.8339064121246338\n",
      "Epoch 10:  85%|████████▍ | 180/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 9.917256355285645\n",
      "Epoch 10:  85%|████████▌ | 181/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 1.3951137065887451\n",
      "Epoch 10:  86%|████████▌ | 182/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.9763991832733154\n",
      "Epoch 10:  86%|████████▋ | 183/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 2.2004287242889404\n",
      "Epoch 10:  87%|████████▋ | 184/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.9316246509552002\n",
      "Epoch 10:  87%|████████▋ | 185/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.8787343502044678\n",
      "Epoch 10:  88%|████████▊ | 186/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.6833497881889343\n",
      "Epoch 10:  88%|████████▊ | 187/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.4380947351455688\n",
      "Epoch 10:  89%|████████▊ | 188/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.0731418132781982\n",
      "Epoch 10:  89%|████████▉ | 189/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.7657196521759033\n",
      "Epoch 10:  90%|████████▉ | 190/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9800771474838257\n",
      "Epoch 10:  90%|█████████ | 191/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.5009410381317139\n",
      "Epoch 10:  91%|█████████ | 192/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8841204643249512\n",
      "Epoch 10:  91%|█████████ | 193/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6825681328773499\n",
      "Epoch 10:  92%|█████████▏| 194/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8473914265632629\n",
      "Epoch 10:  92%|█████████▏| 195/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8823010325431824\n",
      "Epoch 10:  92%|█████████▏| 196/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.1594011783599854\n",
      "Epoch 10:  93%|█████████▎| 197/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.484464168548584\n",
      "Epoch 10:  93%|█████████▎| 198/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8341522216796875\n",
      "Epoch 10:  94%|█████████▍| 199/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.5916081666946411\n",
      "Epoch 10:  94%|█████████▍| 200/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.982279360294342\n",
      "Epoch 10:  95%|█████████▍| 201/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9165287017822266\n",
      "Epoch 10:  95%|█████████▌| 202/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7798347473144531\n",
      "Epoch 10:  96%|█████████▌| 203/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8398966193199158\n",
      "Epoch 10:  96%|█████████▌| 204/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.7974885702133179\n",
      "Epoch 10:  97%|█████████▋| 205/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.6901259422302246\n",
      "Epoch 10:  97%|█████████▋| 206/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.857498824596405\n",
      "Epoch 10:  98%|█████████▊| 207/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.8635286092758179\n",
      "Epoch 10:  98%|█████████▊| 208/212 [00:05<00:00, 37.19it/s, v_num=0]Training loss: 0.711450457572937\n",
      "Epoch 10:  99%|█████████▊| 209/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.2699216604232788\n",
      "Epoch 10:  99%|█████████▉| 210/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.658061683177948\n",
      "Epoch 10: 100%|█████████▉| 211/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.9221971035003662\n",
      "Epoch 10: 100%|██████████| 212/212 [00:05<00:00, 37.20it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.0055294036865234\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 84.70it/s]\u001b[AValidation loss: 1.711379051208496\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 92.68it/s]\u001b[AValidation loss: 0.7690262198448181\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 95.98it/s]\u001b[AValidation loss: 0.5743905901908875\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 97.79it/s]\u001b[AValidation loss: 0.627421498298645\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 97.58it/s]\u001b[AValidation loss: 0.7721158266067505\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 97.56it/s]\u001b[AValidation loss: 1.0714054107666016\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 98.39it/s]\u001b[AValidation loss: 2.0155410766601562\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 98.98it/s]\u001b[AValidation loss: 0.5352373123168945\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 99.45it/s]\u001b[AValidation loss: 0.6406514048576355\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 99.90it/s]\u001b[AValidation loss: 0.6293671131134033\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 100.28it/s]\u001b[AValidation loss: 0.916468620300293\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 100.54it/s]\u001b[AValidation loss: 0.9609124660491943\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 100.78it/s]\u001b[AValidation loss: 0.8394989967346191\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 100.97it/s]\u001b[AValidation loss: 0.7008868455886841\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 101.20it/s]\u001b[AValidation loss: 0.9667775630950928\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 101.36it/s]\u001b[AValidation loss: 0.9425731897354126\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 101.14it/s]\u001b[AValidation loss: 1.390463948249817\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 101.29it/s]\u001b[AValidation loss: 0.6541705131530762\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 101.41it/s]\u001b[AValidation loss: 1.130716323852539\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 101.18it/s]\u001b[AValidation loss: 0.9919127821922302\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 101.23it/s]\u001b[AValidation loss: 1.0058565139770508\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 101.34it/s]\u001b[AValidation loss: 0.9691135287284851\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 101.41it/s]\u001b[AValidation loss: 0.8125143647193909\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 101.53it/s]\u001b[AValidation loss: 1.2249869108200073\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 101.61it/s]\u001b[AValidation loss: 0.6420079469680786\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.67it/s]\u001b[AValidation loss: 0.802353024482727\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.74it/s]\u001b[AValidation loss: 0.8500723242759705\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 101.83it/s]\u001b[AValidation loss: 0.8638627529144287\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 101.91it/s]\u001b[AValidation loss: 1.1660315990447998\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 102.01it/s]\u001b[AValidation loss: 0.7212669253349304\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.06it/s]\u001b[AValidation loss: 0.9247106313705444\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.09it/s]\u001b[AValidation loss: 0.8315015435218811\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 102.12it/s]\u001b[AValidation loss: 0.7006723284721375\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 102.17it/s]\u001b[AValidation loss: 1.6708097457885742\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 102.21it/s]\u001b[AValidation loss: 0.4862311780452728\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 1.4387013912200928\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.31it/s]\u001b[AValidation loss: 0.5554279088973999\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 0.88975590467453\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.40it/s]\u001b[AValidation loss: 0.8447656631469727\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.46it/s]\u001b[AValidation loss: 0.6938948035240173\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.33it/s]\u001b[AValidation loss: 0.6333115100860596\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.37it/s]\u001b[AValidation loss: 0.6676168441772461\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 102.41it/s]\u001b[AValidation loss: 1.1774013042449951\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 102.31it/s]\u001b[AValidation loss: 1.281213402748108\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 102.36it/s]\u001b[AValidation loss: 0.6340828537940979\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 102.40it/s]\u001b[AValidation loss: 0.8432219624519348\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 102.44it/s]\u001b[AValidation loss: 0.8134141564369202\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 102.48it/s]\u001b[AValidation loss: 1.045432686805725\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 102.45it/s]\u001b[AValidation loss: 1.2373368740081787\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 102.48it/s]\u001b[AValidation loss: 1.096101999282837\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 102.49it/s]\u001b[AValidation loss: 1.0130656957626343\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 102.52it/s]\u001b[AValidation loss: 1.2426801919937134\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 102.56it/s]\u001b[AValidation loss: 0.9351001977920532\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 102.57it/s]\u001b[AValidation loss: 2.053499698638916\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 102.62it/s]\u001b[A\n",
      "Epoch 11:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.5561264157295227\n",
      "Epoch 11:   0%|          | 1/212 [00:00<00:04, 45.44it/s, v_num=0]Training loss: 0.5413985252380371\n",
      "Epoch 11:   1%|          | 2/212 [00:00<00:05, 41.10it/s, v_num=0]Training loss: 0.9172555804252625\n",
      "Epoch 11:   1%|▏         | 3/212 [00:00<00:05, 39.85it/s, v_num=0]Training loss: 0.7176439166069031\n",
      "Epoch 11:   2%|▏         | 4/212 [00:00<00:05, 39.25it/s, v_num=0]Training loss: 0.6069689989089966\n",
      "Epoch 11:   2%|▏         | 5/212 [00:00<00:05, 38.90it/s, v_num=0]Training loss: 0.9893240928649902\n",
      "Epoch 11:   3%|▎         | 6/212 [00:00<00:05, 38.68it/s, v_num=0]Training loss: 1.4776583909988403\n",
      "Epoch 11:   3%|▎         | 7/212 [00:00<00:05, 38.51it/s, v_num=0]Training loss: 0.5381537675857544\n",
      "Epoch 11:   4%|▍         | 8/212 [00:00<00:05, 38.28it/s, v_num=0]Training loss: 0.437923401594162\n",
      "Epoch 11:   4%|▍         | 9/212 [00:00<00:05, 38.21it/s, v_num=0]Training loss: 1.4653871059417725\n",
      "Epoch 11:   5%|▍         | 10/212 [00:00<00:05, 38.14it/s, v_num=0]Training loss: 1.3019932508468628\n",
      "Epoch 11:   5%|▌         | 11/212 [00:00<00:05, 38.08it/s, v_num=0]Training loss: 0.9027884602546692\n",
      "Epoch 11:   6%|▌         | 12/212 [00:00<00:05, 38.04it/s, v_num=0]Training loss: 1.1203484535217285\n",
      "Epoch 11:   6%|▌         | 13/212 [00:00<00:05, 38.00it/s, v_num=0]Training loss: 0.9713431000709534\n",
      "Epoch 11:   7%|▋         | 14/212 [00:00<00:05, 37.96it/s, v_num=0]Training loss: 0.47251588106155396\n",
      "Epoch 11:   7%|▋         | 15/212 [00:00<00:05, 37.94it/s, v_num=0]Training loss: 0.9479808807373047\n",
      "Epoch 11:   8%|▊         | 16/212 [00:00<00:05, 37.91it/s, v_num=0]Training loss: 0.8225039839744568\n",
      "Epoch 11:   8%|▊         | 17/212 [00:00<00:05, 37.76it/s, v_num=0]Training loss: 1.4086965322494507\n",
      "Epoch 11:   8%|▊         | 18/212 [00:00<00:05, 37.67it/s, v_num=0]Training loss: 0.7975748777389526\n",
      "Epoch 11:   9%|▉         | 19/212 [00:00<00:05, 37.60it/s, v_num=0]Training loss: 1.03106689453125\n",
      "Epoch 11:   9%|▉         | 20/212 [00:00<00:05, 37.52it/s, v_num=0]Training loss: 2.420391321182251\n",
      "Epoch 11:  10%|▉         | 21/212 [00:00<00:05, 37.46it/s, v_num=0]Training loss: 0.8035147786140442\n",
      "Epoch 11:  10%|█         | 22/212 [00:00<00:05, 37.41it/s, v_num=0]Training loss: 1.0425047874450684\n",
      "Epoch 11:  11%|█         | 23/212 [00:00<00:05, 37.31it/s, v_num=0]Training loss: 0.6224358081817627\n",
      "Epoch 11:  11%|█▏        | 24/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 0.990410327911377\n",
      "Epoch 11:  12%|█▏        | 25/212 [00:00<00:05, 37.31it/s, v_num=0]Training loss: 0.5793901085853577\n",
      "Epoch 11:  12%|█▏        | 26/212 [00:00<00:04, 37.31it/s, v_num=0]Training loss: 0.7790087461471558\n",
      "Epoch 11:  13%|█▎        | 27/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 1.950217366218567\n",
      "Epoch 11:  13%|█▎        | 28/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 0.6355431079864502\n",
      "Epoch 11:  14%|█▎        | 29/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.9249696731567383\n",
      "Epoch 11:  14%|█▍        | 30/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 0.7929136753082275\n",
      "Epoch 11:  15%|█▍        | 31/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 1.1880745887756348\n",
      "Epoch 11:  15%|█▌        | 32/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.5600671768188477\n",
      "Epoch 11:  16%|█▌        | 33/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.7592453360557556\n",
      "Epoch 11:  16%|█▌        | 34/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.7245990633964539\n",
      "Epoch 11:  17%|█▋        | 35/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.7830018997192383\n",
      "Epoch 11:  17%|█▋        | 36/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.9406878352165222\n",
      "Epoch 11:  17%|█▋        | 37/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 0.6127927303314209\n",
      "Epoch 11:  18%|█▊        | 38/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.8859924674034119\n",
      "Epoch 11:  18%|█▊        | 39/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.508169412612915\n",
      "Epoch 11:  19%|█▉        | 40/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 2.2147743701934814\n",
      "Epoch 11:  19%|█▉        | 41/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.4511537551879883\n",
      "Epoch 11:  20%|█▉        | 42/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.2239471673965454\n",
      "Epoch 11:  20%|██        | 43/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.1272541284561157\n",
      "Epoch 11:  21%|██        | 44/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.9191799163818359\n",
      "Epoch 11:  21%|██        | 45/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 0.9855268597602844\n",
      "Epoch 11:  22%|██▏       | 46/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.9738849401473999\n",
      "Epoch 11:  22%|██▏       | 47/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.180495262145996\n",
      "Epoch 11:  23%|██▎       | 48/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 1.0244754552841187\n",
      "Epoch 11:  23%|██▎       | 49/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 1.198013186454773\n",
      "Epoch 11:  24%|██▎       | 50/212 [00:01<00:04, 37.41it/s, v_num=0]Training loss: 1.0891776084899902\n",
      "Epoch 11:  24%|██▍       | 51/212 [00:01<00:04, 37.41it/s, v_num=0]Training loss: 1.0528860092163086\n",
      "Epoch 11:  25%|██▍       | 52/212 [00:01<00:04, 37.41it/s, v_num=0]Training loss: 0.5365228652954102\n",
      "Epoch 11:  25%|██▌       | 53/212 [00:01<00:04, 37.41it/s, v_num=0]Training loss: 0.7678909301757812\n",
      "Epoch 11:  25%|██▌       | 54/212 [00:01<00:04, 37.42it/s, v_num=0]Training loss: 0.8988217115402222\n",
      "Epoch 11:  26%|██▌       | 55/212 [00:01<00:04, 37.42it/s, v_num=0]Training loss: 0.7939713001251221\n",
      "Epoch 11:  26%|██▋       | 56/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.980027973651886\n",
      "Epoch 11:  27%|██▋       | 57/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.6635208129882812\n",
      "Epoch 11:  27%|██▋       | 58/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.1149543523788452\n",
      "Epoch 11:  28%|██▊       | 59/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.4932706952095032\n",
      "Epoch 11:  28%|██▊       | 60/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 0.6733476519584656\n",
      "Epoch 11:  29%|██▉       | 61/212 [00:01<00:04, 37.28it/s, v_num=0]Training loss: 0.6647728085517883\n",
      "Epoch 11:  29%|██▉       | 62/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 0.5807142853736877\n",
      "Epoch 11:  30%|██▉       | 63/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 1.1434998512268066\n",
      "Epoch 11:  30%|███       | 64/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.0611449480056763\n",
      "Epoch 11:  31%|███       | 65/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.6799998879432678\n",
      "Epoch 11:  31%|███       | 66/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.5188414454460144\n",
      "Epoch 11:  32%|███▏      | 67/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.8951858282089233\n",
      "Epoch 11:  32%|███▏      | 68/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.7671695351600647\n",
      "Epoch 11:  33%|███▎      | 69/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.5687224864959717\n",
      "Epoch 11:  33%|███▎      | 70/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 1.0376883745193481\n",
      "Epoch 11:  33%|███▎      | 71/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.8335301280021667\n",
      "Epoch 11:  34%|███▍      | 72/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 1.9896771907806396\n",
      "Epoch 11:  34%|███▍      | 73/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.6038919687271118\n",
      "Epoch 11:  35%|███▍      | 74/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.9329050183296204\n",
      "Epoch 11:  35%|███▌      | 75/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9011355638504028\n",
      "Epoch 11:  36%|███▌      | 76/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.1445822715759277\n",
      "Epoch 11:  36%|███▋      | 77/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.911874532699585\n",
      "Epoch 11:  37%|███▋      | 78/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.6925754547119141\n",
      "Epoch 11:  37%|███▋      | 79/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.346850872039795\n",
      "Epoch 11:  38%|███▊      | 80/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.0969575643539429\n",
      "Epoch 11:  38%|███▊      | 81/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.5798982381820679\n",
      "Epoch 11:  39%|███▊      | 82/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9674233198165894\n",
      "Epoch 11:  39%|███▉      | 83/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.5843443870544434\n",
      "Epoch 11:  40%|███▉      | 84/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.6392214894294739\n",
      "Epoch 11:  40%|████      | 85/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.8262237310409546\n",
      "Epoch 11:  41%|████      | 86/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.6338861584663391\n",
      "Epoch 11:  41%|████      | 87/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.8875467777252197\n",
      "Epoch 11:  42%|████▏     | 88/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.7271974682807922\n",
      "Epoch 11:  42%|████▏     | 89/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.5748476982116699\n",
      "Epoch 11:  42%|████▏     | 90/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.5914280414581299\n",
      "Epoch 11:  43%|████▎     | 91/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.9126197099685669\n",
      "Epoch 11:  43%|████▎     | 92/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 1.236203670501709\n",
      "Epoch 11:  44%|████▍     | 93/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.6126487851142883\n",
      "Epoch 11:  44%|████▍     | 94/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 3.294275999069214\n",
      "Epoch 11:  45%|████▍     | 95/212 [00:02<00:03, 37.32it/s, v_num=0]Training loss: 0.7203811407089233\n",
      "Epoch 11:  45%|████▌     | 96/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 0.533560037612915\n",
      "Epoch 11:  46%|████▌     | 97/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.7865535020828247\n",
      "Epoch 11:  46%|████▌     | 98/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.1240150928497314\n",
      "Epoch 11:  47%|████▋     | 99/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9152185320854187\n",
      "Epoch 11:  47%|████▋     | 100/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.793980598449707\n",
      "Epoch 11:  48%|████▊     | 101/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 1.2726584672927856\n",
      "Epoch 11:  48%|████▊     | 102/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.8816232085227966\n",
      "Epoch 11:  49%|████▊     | 103/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 0.736534595489502\n",
      "Epoch 11:  49%|████▉     | 104/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.7272213697433472\n",
      "Epoch 11:  50%|████▉     | 105/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.8090949654579163\n",
      "Epoch 11:  50%|█████     | 106/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 9.410430908203125\n",
      "Epoch 11:  50%|█████     | 107/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.0468658208847046\n",
      "Epoch 11:  51%|█████     | 108/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.4628993570804596\n",
      "Epoch 11:  51%|█████▏    | 109/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.1246628761291504\n",
      "Epoch 11:  52%|█████▏    | 110/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.74607253074646\n",
      "Epoch 11:  52%|█████▏    | 111/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 0.9277725219726562\n",
      "Epoch 11:  53%|█████▎    | 112/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6584497094154358\n",
      "Epoch 11:  53%|█████▎    | 113/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6652165651321411\n",
      "Epoch 11:  54%|█████▍    | 114/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1988564729690552\n",
      "Epoch 11:  54%|█████▍    | 115/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8387240171432495\n",
      "Epoch 11:  55%|█████▍    | 116/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9664610624313354\n",
      "Epoch 11:  55%|█████▌    | 117/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.9354127645492554\n",
      "Epoch 11:  56%|█████▌    | 118/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0979325771331787\n",
      "Epoch 11:  56%|█████▌    | 119/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.1514756679534912\n",
      "Epoch 11:  57%|█████▋    | 120/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.47558820247650146\n",
      "Epoch 11:  57%|█████▋    | 121/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.3759872913360596\n",
      "Epoch 11:  58%|█████▊    | 122/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.9979082942008972\n",
      "Epoch 11:  58%|█████▊    | 123/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.6513700485229492\n",
      "Epoch 11:  58%|█████▊    | 124/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.9297270774841309\n",
      "Epoch 11:  59%|█████▉    | 125/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.9426778554916382\n",
      "Epoch 11:  59%|█████▉    | 126/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.7006006836891174\n",
      "Epoch 11:  60%|█████▉    | 127/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.5246199369430542\n",
      "Epoch 11:  60%|██████    | 128/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.814892590045929\n",
      "Epoch 11:  61%|██████    | 129/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 1.2955738306045532\n",
      "Epoch 11:  61%|██████▏   | 130/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.7856691479682922\n",
      "Epoch 11:  62%|██████▏   | 131/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 0.774645209312439\n",
      "Epoch 11:  62%|██████▏   | 132/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 1.9157805442810059\n",
      "Epoch 11:  63%|██████▎   | 133/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 0.9185512661933899\n",
      "Epoch 11:  63%|██████▎   | 134/212 [00:03<00:02, 37.29it/s, v_num=0]Training loss: 0.8107452988624573\n",
      "Epoch 11:  64%|██████▎   | 135/212 [00:03<00:02, 37.28it/s, v_num=0]Training loss: 0.9825857281684875\n",
      "Epoch 11:  64%|██████▍   | 136/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.7989915609359741\n",
      "Epoch 11:  65%|██████▍   | 137/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.7028011679649353\n",
      "Epoch 11:  65%|██████▌   | 138/212 [00:03<00:01, 37.26it/s, v_num=0]Training loss: 0.7475643157958984\n",
      "Epoch 11:  66%|██████▌   | 139/212 [00:03<00:01, 37.25it/s, v_num=0]Training loss: 0.738578736782074\n",
      "Epoch 11:  66%|██████▌   | 140/212 [00:03<00:01, 37.24it/s, v_num=0]Training loss: 2.021765947341919\n",
      "Epoch 11:  67%|██████▋   | 141/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 1.0004286766052246\n",
      "Epoch 11:  67%|██████▋   | 142/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.8605436086654663\n",
      "Epoch 11:  67%|██████▋   | 143/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.7771767377853394\n",
      "Epoch 11:  68%|██████▊   | 144/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.6208418011665344\n",
      "Epoch 11:  68%|██████▊   | 145/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.9024081826210022\n",
      "Epoch 11:  69%|██████▉   | 146/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 1.1801824569702148\n",
      "Epoch 11:  69%|██████▉   | 147/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.9946570992469788\n",
      "Epoch 11:  70%|██████▉   | 148/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.9684115052223206\n",
      "Epoch 11:  70%|███████   | 149/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.7473317980766296\n",
      "Epoch 11:  71%|███████   | 150/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8659113049507141\n",
      "Epoch 11:  71%|███████   | 151/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8018269538879395\n",
      "Epoch 11:  72%|███████▏  | 152/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6584604382514954\n",
      "Epoch 11:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.189602255821228\n",
      "Epoch 11:  73%|███████▎  | 154/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.897179126739502\n",
      "Epoch 11:  73%|███████▎  | 155/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7902883887290955\n",
      "Epoch 11:  74%|███████▎  | 156/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7307454943656921\n",
      "Epoch 11:  74%|███████▍  | 157/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.446372389793396\n",
      "Epoch 11:  75%|███████▍  | 158/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.6737074851989746\n",
      "Epoch 11:  75%|███████▌  | 159/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.3154892921447754\n",
      "Epoch 11:  75%|███████▌  | 160/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.842255175113678\n",
      "Epoch 11:  76%|███████▌  | 161/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.6828545928001404\n",
      "Epoch 11:  76%|███████▋  | 162/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.0114024877548218\n",
      "Epoch 11:  77%|███████▋  | 163/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8395237326622009\n",
      "Epoch 11:  77%|███████▋  | 164/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.7479797601699829\n",
      "Epoch 11:  78%|███████▊  | 165/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8141432404518127\n",
      "Epoch 11:  78%|███████▊  | 166/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.9883401989936829\n",
      "Epoch 11:  79%|███████▉  | 167/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.5884809494018555\n",
      "Epoch 11:  79%|███████▉  | 168/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 1.2813888788223267\n",
      "Epoch 11:  80%|███████▉  | 169/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.6849777102470398\n",
      "Epoch 11:  80%|████████  | 170/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 2.1587769985198975\n",
      "Epoch 11:  81%|████████  | 171/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.6888672709465027\n",
      "Epoch 11:  81%|████████  | 172/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 3.6543967723846436\n",
      "Epoch 11:  82%|████████▏ | 173/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 1.2777894735336304\n",
      "Epoch 11:  82%|████████▏ | 174/212 [00:04<00:01, 37.27it/s, v_num=0]Training loss: 0.7769339680671692\n",
      "Epoch 11:  83%|████████▎ | 175/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.7870288491249084\n",
      "Epoch 11:  83%|████████▎ | 176/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.9319223761558533\n",
      "Epoch 11:  83%|████████▎ | 177/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 0.5965067744255066\n",
      "Epoch 11:  84%|████████▍ | 178/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 0.6567050814628601\n",
      "Epoch 11:  84%|████████▍ | 179/212 [00:04<00:00, 37.24it/s, v_num=0]Training loss: 0.6947651505470276\n",
      "Epoch 11:  85%|████████▍ | 180/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 1.0590474605560303\n",
      "Epoch 11:  85%|████████▌ | 181/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.1696879863739014\n",
      "Epoch 11:  86%|████████▌ | 182/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.8785747289657593\n",
      "Epoch 11:  86%|████████▋ | 183/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.7174469232559204\n",
      "Epoch 11:  87%|████████▋ | 184/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.7769852876663208\n",
      "Epoch 11:  87%|████████▋ | 185/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 1.128586769104004\n",
      "Epoch 11:  88%|████████▊ | 186/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.7031513452529907\n",
      "Epoch 11:  88%|████████▊ | 187/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.645273745059967\n",
      "Epoch 11:  89%|████████▊ | 188/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.1200840473175049\n",
      "Epoch 11:  89%|████████▉ | 189/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9638989567756653\n",
      "Epoch 11:  90%|████████▉ | 190/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9792490601539612\n",
      "Epoch 11:  90%|█████████ | 191/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.0223942995071411\n",
      "Epoch 11:  91%|█████████ | 192/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9746852517127991\n",
      "Epoch 11:  91%|█████████ | 193/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9283767342567444\n",
      "Epoch 11:  92%|█████████▏| 194/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.593781054019928\n",
      "Epoch 11:  92%|█████████▏| 195/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8658333420753479\n",
      "Epoch 11:  92%|█████████▏| 196/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.5180420875549316\n",
      "Epoch 11:  93%|█████████▎| 197/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 2.4094152450561523\n",
      "Epoch 11:  93%|█████████▎| 198/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9276834726333618\n",
      "Epoch 11:  94%|█████████▍| 199/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 10.288413047790527\n",
      "Epoch 11:  94%|█████████▍| 200/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8454719185829163\n",
      "Epoch 11:  95%|█████████▍| 201/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.9338736534118652\n",
      "Epoch 11:  95%|█████████▌| 202/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.7559403777122498\n",
      "Epoch 11:  96%|█████████▌| 203/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8523620367050171\n",
      "Epoch 11:  96%|█████████▌| 204/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.1268969774246216\n",
      "Epoch 11:  97%|█████████▋| 205/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.5559767484664917\n",
      "Epoch 11:  97%|█████████▋| 206/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.0522873401641846\n",
      "Epoch 11:  98%|█████████▊| 207/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.234596848487854\n",
      "Epoch 11:  98%|█████████▊| 208/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.45906198024749756\n",
      "Epoch 11:  99%|█████████▊| 209/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.2259494066238403\n",
      "Epoch 11:  99%|█████████▉| 210/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.0654373168945312\n",
      "Epoch 11: 100%|█████████▉| 211/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.9249829649925232\n",
      "Epoch 11: 100%|██████████| 212/212 [00:05<00:00, 37.26it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 2.002565622329712\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 79.44it/s]\u001b[AValidation loss: 1.7024004459381104\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.20it/s]\u001b[AValidation loss: 0.7652688026428223\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 94.33it/s]\u001b[AValidation loss: 0.569675862789154\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 95.07it/s]\u001b[AValidation loss: 0.6239051818847656\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 95.43it/s]\u001b[AValidation loss: 0.770831823348999\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 95.72it/s]\u001b[AValidation loss: 1.063681721687317\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 95.81it/s]\u001b[AValidation loss: 2.005303382873535\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 95.82it/s]\u001b[AValidation loss: 0.5303114652633667\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 95.91it/s]\u001b[AValidation loss: 0.634743332862854\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 95.91it/s]\u001b[AValidation loss: 0.6259178519248962\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 95.91it/s]\u001b[AValidation loss: 0.9079095721244812\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 95.91it/s]\u001b[AValidation loss: 0.9574660658836365\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 95.92it/s]\u001b[AValidation loss: 0.8341577649116516\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 96.00it/s]\u001b[AValidation loss: 0.6980078816413879\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 96.04it/s]\u001b[AValidation loss: 0.9545190334320068\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 96.03it/s]\u001b[AValidation loss: 0.9313598275184631\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 96.04it/s]\u001b[AValidation loss: 1.3819942474365234\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 96.07it/s]\u001b[AValidation loss: 0.6495893597602844\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 95.68it/s]\u001b[AValidation loss: 1.1254239082336426\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 95.65it/s]\u001b[AValidation loss: 0.9873068332672119\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 95.63it/s]\u001b[AValidation loss: 1.0019044876098633\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.64it/s]\u001b[AValidation loss: 0.9682536125183105\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.65it/s]\u001b[AValidation loss: 0.8069626688957214\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.67it/s]\u001b[AValidation loss: 1.2172929048538208\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.70it/s]\u001b[AValidation loss: 0.6350851655006409\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.85it/s]\u001b[AValidation loss: 0.7896629571914673\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 96.00it/s]\u001b[AValidation loss: 0.847734808921814\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 96.09it/s]\u001b[AValidation loss: 0.8555823564529419\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 96.31it/s]\u001b[AValidation loss: 1.1610413789749146\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 96.53it/s]\u001b[AValidation loss: 0.7115097045898438\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 96.71it/s]\u001b[AValidation loss: 0.9223407506942749\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 96.90it/s]\u001b[AValidation loss: 0.8260244727134705\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 97.08it/s]\u001b[AValidation loss: 0.6930539011955261\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 97.24it/s]\u001b[AValidation loss: 1.663301706314087\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 97.35it/s]\u001b[AValidation loss: 0.4815690815448761\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 97.31it/s]\u001b[AValidation loss: 1.4372049570083618\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 97.25it/s]\u001b[AValidation loss: 0.5500550270080566\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 97.24it/s]\u001b[AValidation loss: 0.8825610280036926\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 97.21it/s]\u001b[AValidation loss: 0.8386695384979248\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 97.21it/s]\u001b[AValidation loss: 0.6844774484634399\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 97.20it/s]\u001b[AValidation loss: 0.6271663904190063\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 97.20it/s]\u001b[AValidation loss: 0.6610081195831299\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 97.19it/s]\u001b[AValidation loss: 1.1726182699203491\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 97.16it/s]\u001b[AValidation loss: 1.2735813856124878\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 97.16it/s]\u001b[AValidation loss: 0.6310480833053589\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 97.16it/s]\u001b[AValidation loss: 0.8372166752815247\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 97.16it/s]\u001b[AValidation loss: 0.806611955165863\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 97.13it/s]\u001b[AValidation loss: 1.044759750366211\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 97.14it/s]\u001b[AValidation loss: 1.2281032800674438\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 97.23it/s]\u001b[AValidation loss: 1.0907540321350098\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 97.32it/s]\u001b[AValidation loss: 1.0068906545639038\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 97.42it/s]\u001b[AValidation loss: 1.2326455116271973\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 97.51it/s]\u001b[AValidation loss: 0.9327479600906372\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 97.59it/s]\u001b[AValidation loss: 2.0483238697052\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 97.81it/s]\u001b[A\n",
      "Epoch 12:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]              \u001b[ATraining loss: 1.0792070627212524\n",
      "Epoch 12:   0%|          | 1/212 [00:00<00:04, 44.03it/s, v_num=0]Training loss: 0.5565857291221619\n",
      "Epoch 12:   1%|          | 2/212 [00:00<00:05, 40.44it/s, v_num=0]Training loss: 0.8113855123519897\n",
      "Epoch 12:   1%|▏         | 3/212 [00:00<00:05, 39.41it/s, v_num=0]Training loss: 1.319367527961731\n",
      "Epoch 12:   2%|▏         | 4/212 [00:00<00:05, 38.88it/s, v_num=0]Training loss: 0.9936002492904663\n",
      "Epoch 12:   2%|▏         | 5/212 [00:00<00:05, 38.60it/s, v_num=0]Training loss: 0.949987530708313\n",
      "Epoch 12:   3%|▎         | 6/212 [00:00<00:05, 38.41it/s, v_num=0]Training loss: 1.260572910308838\n",
      "Epoch 12:   3%|▎         | 7/212 [00:00<00:05, 38.27it/s, v_num=0]Training loss: 0.747217059135437\n",
      "Epoch 12:   4%|▍         | 8/212 [00:00<00:05, 38.01it/s, v_num=0]Training loss: 1.1663237810134888\n",
      "Epoch 12:   4%|▍         | 9/212 [00:00<00:05, 37.97it/s, v_num=0]Training loss: 1.1863590478897095\n",
      "Epoch 12:   5%|▍         | 10/212 [00:00<00:05, 37.91it/s, v_num=0]Training loss: 1.083938717842102\n",
      "Epoch 12:   5%|▌         | 11/212 [00:00<00:05, 37.88it/s, v_num=0]Training loss: 1.0011186599731445\n",
      "Epoch 12:   6%|▌         | 12/212 [00:00<00:05, 37.86it/s, v_num=0]Training loss: 0.5347160696983337\n",
      "Epoch 12:   6%|▌         | 13/212 [00:00<00:05, 37.84it/s, v_num=0]Training loss: 0.8541553616523743\n",
      "Epoch 12:   7%|▋         | 14/212 [00:00<00:05, 37.82it/s, v_num=0]Training loss: 0.9550551772117615\n",
      "Epoch 12:   7%|▋         | 15/212 [00:00<00:05, 37.80it/s, v_num=0]Training loss: 0.5454267263412476\n",
      "Epoch 12:   8%|▊         | 16/212 [00:00<00:05, 37.78it/s, v_num=0]Training loss: 0.775088906288147\n",
      "Epoch 12:   8%|▊         | 17/212 [00:00<00:05, 37.77it/s, v_num=0]Training loss: 0.9155539870262146\n",
      "Epoch 12:   8%|▊         | 18/212 [00:00<00:05, 37.75it/s, v_num=0]Training loss: 3.009172201156616\n",
      "Epoch 12:   9%|▉         | 19/212 [00:00<00:05, 37.74it/s, v_num=0]Training loss: 0.6317216753959656\n",
      "Epoch 12:   9%|▉         | 20/212 [00:00<00:05, 37.73it/s, v_num=0]Training loss: 0.4615379273891449\n",
      "Epoch 12:  10%|▉         | 21/212 [00:00<00:05, 37.72it/s, v_num=0]Training loss: 0.955025315284729\n",
      "Epoch 12:  10%|█         | 22/212 [00:00<00:05, 37.72it/s, v_num=0]Training loss: 2.2091174125671387\n",
      "Epoch 12:  11%|█         | 23/212 [00:00<00:05, 37.71it/s, v_num=0]Training loss: 0.7493494153022766\n",
      "Epoch 12:  11%|█▏        | 24/212 [00:00<00:04, 37.70it/s, v_num=0]Training loss: 0.68447345495224\n",
      "Epoch 12:  12%|█▏        | 25/212 [00:00<00:04, 37.70it/s, v_num=0]Training loss: 1.147704839706421\n",
      "Epoch 12:  12%|█▏        | 26/212 [00:00<00:04, 37.69it/s, v_num=0]Training loss: 0.9835618138313293\n",
      "Epoch 12:  13%|█▎        | 27/212 [00:00<00:04, 37.68it/s, v_num=0]Training loss: 0.6550818085670471\n",
      "Epoch 12:  13%|█▎        | 28/212 [00:00<00:04, 37.68it/s, v_num=0]Training loss: 0.9689810872077942\n",
      "Epoch 12:  14%|█▎        | 29/212 [00:00<00:04, 37.60it/s, v_num=0]Training loss: 1.0662821531295776\n",
      "Epoch 12:  14%|█▍        | 30/212 [00:00<00:04, 37.55it/s, v_num=0]Training loss: 0.746397078037262\n",
      "Epoch 12:  15%|█▍        | 31/212 [00:00<00:04, 37.50it/s, v_num=0]Training loss: 1.2555491924285889\n",
      "Epoch 12:  15%|█▌        | 32/212 [00:00<00:04, 37.47it/s, v_num=0]Training loss: 1.1102511882781982\n",
      "Epoch 12:  16%|█▌        | 33/212 [00:00<00:04, 37.43it/s, v_num=0]Training loss: 0.7634949684143066\n",
      "Epoch 12:  16%|█▌        | 34/212 [00:00<00:04, 37.39it/s, v_num=0]Training loss: 1.1132900714874268\n",
      "Epoch 12:  17%|█▋        | 35/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 1.9357258081436157\n",
      "Epoch 12:  17%|█▋        | 36/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 1.0983864068984985\n",
      "Epoch 12:  17%|█▋        | 37/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.77294921875\n",
      "Epoch 12:  18%|█▊        | 38/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.0102671384811401\n",
      "Epoch 12:  18%|█▊        | 39/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 1.1256496906280518\n",
      "Epoch 12:  19%|█▉        | 40/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.9563210010528564\n",
      "Epoch 12:  19%|█▉        | 41/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.864459753036499\n",
      "Epoch 12:  20%|█▉        | 42/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.6122443675994873\n",
      "Epoch 12:  20%|██        | 43/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.3910488486289978\n",
      "Epoch 12:  21%|██        | 44/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.8958489298820496\n",
      "Epoch 12:  21%|██        | 45/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.7772784233093262\n",
      "Epoch 12:  22%|██▏       | 46/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.9276987910270691\n",
      "Epoch 12:  22%|██▏       | 47/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.7749426364898682\n",
      "Epoch 12:  23%|██▎       | 48/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.8507040143013\n",
      "Epoch 12:  23%|██▎       | 49/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 1.2233619689941406\n",
      "Epoch 12:  24%|██▎       | 50/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.8891822099685669\n",
      "Epoch 12:  24%|██▍       | 51/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.650099515914917\n",
      "Epoch 12:  25%|██▍       | 52/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.7238128781318665\n",
      "Epoch 12:  25%|██▌       | 53/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.4496515095233917\n",
      "Epoch 12:  25%|██▌       | 54/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 1.0258413553237915\n",
      "Epoch 12:  26%|██▌       | 55/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.9981701374053955\n",
      "Epoch 12:  26%|██▋       | 56/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.9734132289886475\n",
      "Epoch 12:  27%|██▋       | 57/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.1353124380111694\n",
      "Epoch 12:  27%|██▋       | 58/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.0526459217071533\n",
      "Epoch 12:  28%|██▊       | 59/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.1330493688583374\n",
      "Epoch 12:  28%|██▊       | 60/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.9478304982185364\n",
      "Epoch 12:  29%|██▉       | 61/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.618041455745697\n",
      "Epoch 12:  29%|██▉       | 62/212 [00:01<00:04, 37.38it/s, v_num=0]Training loss: 1.0861455202102661\n",
      "Epoch 12:  30%|██▉       | 63/212 [00:01<00:03, 37.38it/s, v_num=0]Training loss: 1.139087438583374\n",
      "Epoch 12:  30%|███       | 64/212 [00:01<00:03, 37.38it/s, v_num=0]Training loss: 2.1024677753448486\n",
      "Epoch 12:  31%|███       | 65/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 0.8325517177581787\n",
      "Epoch 12:  31%|███       | 66/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 0.6737459897994995\n",
      "Epoch 12:  32%|███▏      | 67/212 [00:01<00:03, 37.39it/s, v_num=0]Training loss: 0.9715314507484436\n",
      "Epoch 12:  32%|███▏      | 68/212 [00:01<00:03, 37.38it/s, v_num=0]Training loss: 0.7243756055831909\n",
      "Epoch 12:  33%|███▎      | 69/212 [00:01<00:03, 37.36it/s, v_num=0]Training loss: 1.1418673992156982\n",
      "Epoch 12:  33%|███▎      | 70/212 [00:01<00:03, 37.35it/s, v_num=0]Training loss: 4.176627159118652\n",
      "Epoch 12:  33%|███▎      | 71/212 [00:01<00:03, 37.33it/s, v_num=0]Training loss: 0.6895249485969543\n",
      "Epoch 12:  34%|███▍      | 72/212 [00:01<00:03, 37.31it/s, v_num=0]Training loss: 1.1752163171768188\n",
      "Epoch 12:  34%|███▍      | 73/212 [00:01<00:03, 37.30it/s, v_num=0]Training loss: 0.6673073768615723\n",
      "Epoch 12:  35%|███▍      | 74/212 [00:01<00:03, 37.28it/s, v_num=0]Training loss: 0.9806646704673767\n",
      "Epoch 12:  35%|███▌      | 75/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.9601433873176575\n",
      "Epoch 12:  36%|███▌      | 76/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.5756638646125793\n",
      "Epoch 12:  36%|███▋      | 77/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.7636690735816956\n",
      "Epoch 12:  37%|███▋      | 78/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.6313466429710388\n",
      "Epoch 12:  37%|███▋      | 79/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.9453242421150208\n",
      "Epoch 12:  38%|███▊      | 80/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.8332487344741821\n",
      "Epoch 12:  38%|███▊      | 81/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.004491925239563\n",
      "Epoch 12:  39%|███▊      | 82/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.2003886699676514\n",
      "Epoch 12:  39%|███▉      | 83/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.2637684345245361\n",
      "Epoch 12:  40%|███▉      | 84/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.5861499309539795\n",
      "Epoch 12:  40%|████      | 85/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.6321325898170471\n",
      "Epoch 12:  41%|████      | 86/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9337220191955566\n",
      "Epoch 12:  41%|████      | 87/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.6305526494979858\n",
      "Epoch 12:  42%|████▏     | 88/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.2548571825027466\n",
      "Epoch 12:  42%|████▏     | 89/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.7628657817840576\n",
      "Epoch 12:  42%|████▏     | 90/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 1.0921614170074463\n",
      "Epoch 12:  43%|████▎     | 91/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.6308822631835938\n",
      "Epoch 12:  43%|████▎     | 92/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.743594765663147\n",
      "Epoch 12:  44%|████▍     | 93/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 1.291199803352356\n",
      "Epoch 12:  44%|████▍     | 94/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8704508543014526\n",
      "Epoch 12:  45%|████▍     | 95/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.5814808011054993\n",
      "Epoch 12:  45%|████▌     | 96/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.43520987033844\n",
      "Epoch 12:  46%|████▌     | 97/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.3715271949768066\n",
      "Epoch 12:  46%|████▌     | 98/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 0.7204570174217224\n",
      "Epoch 12:  47%|████▋     | 99/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.1562494039535522\n",
      "Epoch 12:  47%|████▋     | 100/212 [00:02<00:03, 37.31it/s, v_num=0]Training loss: 1.1283917427062988\n",
      "Epoch 12:  48%|████▊     | 101/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 0.8668242692947388\n",
      "Epoch 12:  48%|████▊     | 102/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 1.252738118171692\n",
      "Epoch 12:  49%|████▊     | 103/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 1.0221412181854248\n",
      "Epoch 12:  49%|████▉     | 104/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 0.9376376271247864\n",
      "Epoch 12:  50%|████▉     | 105/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.6602216362953186\n",
      "Epoch 12:  50%|█████     | 106/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 0.833464503288269\n",
      "Epoch 12:  50%|█████     | 107/212 [00:02<00:02, 37.32it/s, v_num=0]Training loss: 1.3120783567428589\n",
      "Epoch 12:  51%|█████     | 108/212 [00:02<00:02, 37.31it/s, v_num=0]Training loss: 1.0217866897583008\n",
      "Epoch 12:  51%|█████▏    | 109/212 [00:02<00:02, 37.30it/s, v_num=0]Training loss: 0.5791335105895996\n",
      "Epoch 12:  52%|█████▏    | 110/212 [00:02<00:02, 37.29it/s, v_num=0]Training loss: 1.2433862686157227\n",
      "Epoch 12:  52%|█████▏    | 111/212 [00:02<00:02, 37.28it/s, v_num=0]Training loss: 1.0595512390136719\n",
      "Epoch 12:  53%|█████▎    | 112/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.9487700462341309\n",
      "Epoch 12:  53%|█████▎    | 113/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.3659392595291138\n",
      "Epoch 12:  54%|█████▍    | 114/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.60563725233078\n",
      "Epoch 12:  54%|█████▍    | 115/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9124732613563538\n",
      "Epoch 12:  55%|█████▍    | 116/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 9.610300064086914\n",
      "Epoch 12:  55%|█████▌    | 117/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1144832372665405\n",
      "Epoch 12:  56%|█████▌    | 118/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 2.205932378768921\n",
      "Epoch 12:  56%|█████▌    | 119/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.742992103099823\n",
      "Epoch 12:  57%|█████▋    | 120/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.3703032732009888\n",
      "Epoch 12:  57%|█████▋    | 121/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.7211692333221436\n",
      "Epoch 12:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8500134944915771\n",
      "Epoch 12:  58%|█████▊    | 123/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.334268569946289\n",
      "Epoch 12:  58%|█████▊    | 124/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9435330033302307\n",
      "Epoch 12:  59%|█████▉    | 125/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8474224209785461\n",
      "Epoch 12:  59%|█████▉    | 126/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.7455230355262756\n",
      "Epoch 12:  60%|█████▉    | 127/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6456668376922607\n",
      "Epoch 12:  60%|██████    | 128/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.5215392708778381\n",
      "Epoch 12:  61%|██████    | 129/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.8888876438140869\n",
      "Epoch 12:  61%|██████▏   | 130/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.5898165702819824\n",
      "Epoch 12:  62%|██████▏   | 131/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.5000847578048706\n",
      "Epoch 12:  62%|██████▏   | 132/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7067469358444214\n",
      "Epoch 12:  63%|██████▎   | 133/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0681016445159912\n",
      "Epoch 12:  63%|██████▎   | 134/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8040691018104553\n",
      "Epoch 12:  64%|██████▎   | 135/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.8695894479751587\n",
      "Epoch 12:  64%|██████▍   | 136/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.856134831905365\n",
      "Epoch 12:  65%|██████▍   | 137/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.0011893510818481\n",
      "Epoch 12:  65%|██████▌   | 138/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 0.831521213054657\n",
      "Epoch 12:  66%|██████▌   | 139/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 0.8734368681907654\n",
      "Epoch 12:  66%|██████▌   | 140/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 1.0552468299865723\n",
      "Epoch 12:  67%|██████▋   | 141/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 1.456003189086914\n",
      "Epoch 12:  67%|██████▋   | 142/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 0.5842726826667786\n",
      "Epoch 12:  67%|██████▋   | 143/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 1.1138827800750732\n",
      "Epoch 12:  68%|██████▊   | 144/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 0.7839009165763855\n",
      "Epoch 12:  68%|██████▊   | 145/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.7115596532821655\n",
      "Epoch 12:  69%|██████▉   | 146/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 1.3097456693649292\n",
      "Epoch 12:  69%|██████▉   | 147/212 [00:03<00:01, 37.29it/s, v_num=0]Training loss: 0.8113172054290771\n",
      "Epoch 12:  70%|██████▉   | 148/212 [00:03<00:01, 37.28it/s, v_num=0]Training loss: 1.4971755743026733\n",
      "Epoch 12:  70%|███████   | 149/212 [00:03<00:01, 37.27it/s, v_num=0]Training loss: 0.5417740941047668\n",
      "Epoch 12:  71%|███████   | 150/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 1.037109613418579\n",
      "Epoch 12:  71%|███████   | 151/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.6969837546348572\n",
      "Epoch 12:  72%|███████▏  | 152/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.3213974237442017\n",
      "Epoch 12:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.593985915184021\n",
      "Epoch 12:  73%|███████▎  | 154/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.1549856662750244\n",
      "Epoch 12:  73%|███████▎  | 155/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.7430852055549622\n",
      "Epoch 12:  74%|███████▎  | 156/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.0624275207519531\n",
      "Epoch 12:  74%|███████▍  | 157/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.6231909394264221\n",
      "Epoch 12:  75%|███████▍  | 158/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.885845422744751\n",
      "Epoch 12:  75%|███████▌  | 159/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 9.80121898651123\n",
      "Epoch 12:  75%|███████▌  | 160/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.393567442893982\n",
      "Epoch 12:  76%|███████▌  | 161/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7345635890960693\n",
      "Epoch 12:  76%|███████▋  | 162/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6101580262184143\n",
      "Epoch 12:  77%|███████▋  | 163/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2679249048233032\n",
      "Epoch 12:  77%|███████▋  | 164/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8126265406608582\n",
      "Epoch 12:  78%|███████▊  | 165/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6114634275436401\n",
      "Epoch 12:  78%|███████▊  | 166/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7507396340370178\n",
      "Epoch 12:  79%|███████▉  | 167/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.0257490873336792\n",
      "Epoch 12:  79%|███████▉  | 168/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6943530440330505\n",
      "Epoch 12:  80%|███████▉  | 169/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7356331944465637\n",
      "Epoch 12:  80%|████████  | 170/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.5814886093139648\n",
      "Epoch 12:  81%|████████  | 171/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7355855703353882\n",
      "Epoch 12:  81%|████████  | 172/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.7724002003669739\n",
      "Epoch 12:  82%|████████▏ | 173/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9980102181434631\n",
      "Epoch 12:  82%|████████▏ | 174/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.5411302447319031\n",
      "Epoch 12:  83%|████████▎ | 175/212 [00:04<00:00, 37.25it/s, v_num=0]Training loss: 1.0718252658843994\n",
      "Epoch 12:  83%|████████▎ | 176/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.0616209506988525\n",
      "Epoch 12:  83%|████████▎ | 177/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.4708225727081299\n",
      "Epoch 12:  84%|████████▍ | 178/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.3152612447738647\n",
      "Epoch 12:  84%|████████▍ | 179/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.5012118816375732\n",
      "Epoch 12:  85%|████████▍ | 180/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.48982834815979004\n",
      "Epoch 12:  85%|████████▌ | 181/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 1.2077445983886719\n",
      "Epoch 12:  86%|████████▌ | 182/212 [00:04<00:00, 37.26it/s, v_num=0]Training loss: 0.605273962020874\n",
      "Epoch 12:  86%|████████▋ | 183/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.9469115138053894\n",
      "Epoch 12:  87%|████████▋ | 184/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 0.6156038045883179\n",
      "Epoch 12:  87%|████████▋ | 185/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.255553126335144\n",
      "Epoch 12:  88%|████████▊ | 186/212 [00:04<00:00, 37.27it/s, v_num=0]Training loss: 1.1889774799346924\n",
      "Epoch 12:  88%|████████▊ | 187/212 [00:05<00:00, 37.27it/s, v_num=0]Training loss: 0.6913728713989258\n",
      "Epoch 12:  89%|████████▊ | 188/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 1.6098984479904175\n",
      "Epoch 12:  89%|████████▉ | 189/212 [00:05<00:00, 37.26it/s, v_num=0]Training loss: 0.5800130367279053\n",
      "Epoch 12:  90%|████████▉ | 190/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9259521961212158\n",
      "Epoch 12:  90%|█████████ | 191/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.7492980360984802\n",
      "Epoch 12:  91%|█████████ | 192/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.2829655408859253\n",
      "Epoch 12:  91%|█████████ | 193/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.7075471878051758\n",
      "Epoch 12:  92%|█████████▏| 194/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.5735558271408081\n",
      "Epoch 12:  92%|█████████▏| 195/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.5927245616912842\n",
      "Epoch 12:  92%|█████████▏| 196/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6671633720397949\n",
      "Epoch 12:  93%|█████████▎| 197/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6590375900268555\n",
      "Epoch 12:  93%|█████████▎| 198/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 2.2268664836883545\n",
      "Epoch 12:  94%|█████████▍| 199/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6746893525123596\n",
      "Epoch 12:  94%|█████████▍| 200/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.6982024908065796\n",
      "Epoch 12:  95%|█████████▍| 201/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7298161387443542\n",
      "Epoch 12:  95%|█████████▌| 202/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.47822603583335876\n",
      "Epoch 12:  96%|█████████▌| 203/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.4807679653167725\n",
      "Epoch 12:  96%|█████████▌| 204/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8586971759796143\n",
      "Epoch 12:  97%|█████████▋| 205/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.7718518376350403\n",
      "Epoch 12:  97%|█████████▋| 206/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.3765034675598145\n",
      "Epoch 12:  98%|█████████▊| 207/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.0523310899734497\n",
      "Epoch 12:  98%|█████████▊| 208/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.6636166572570801\n",
      "Epoch 12:  99%|█████████▊| 209/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8665174245834351\n",
      "Epoch 12:  99%|█████████▉| 210/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.0919795036315918\n",
      "Epoch 12: 100%|█████████▉| 211/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.1200298070907593\n",
      "Epoch 12: 100%|██████████| 212/212 [00:05<00:00, 37.24it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 1.9977384805679321\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 83.25it/s]\u001b[AValidation loss: 1.691269874572754\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.45it/s]\u001b[AValidation loss: 0.7537382245063782\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 93.03it/s]\u001b[AValidation loss: 0.5659433603286743\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 94.64it/s]\u001b[AValidation loss: 0.6217398643493652\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 96.58it/s]\u001b[AValidation loss: 0.758692741394043\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 96.93it/s]\u001b[AValidation loss: 1.0591466426849365\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 97.92it/s]\u001b[AValidation loss: 2.0050549507141113\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 98.77it/s]\u001b[AValidation loss: 0.5292437076568604\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 99.45it/s]\u001b[AValidation loss: 0.6345669031143188\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 99.99it/s]\u001b[AValidation loss: 0.62184739112854\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 100.40it/s]\u001b[AValidation loss: 0.9051406383514404\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 100.76it/s]\u001b[AValidation loss: 0.9564360976219177\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 101.04it/s]\u001b[AValidation loss: 0.8288248181343079\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 101.26it/s]\u001b[AValidation loss: 0.6941989660263062\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 101.54it/s]\u001b[AValidation loss: 0.9524793028831482\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 101.76it/s]\u001b[AValidation loss: 0.9292581677436829\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 101.92it/s]\u001b[AValidation loss: 1.3776881694793701\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 102.12it/s]\u001b[AValidation loss: 0.6480768918991089\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 101.91it/s]\u001b[AValidation loss: 1.120884895324707\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 102.00it/s]\u001b[AValidation loss: 0.982820987701416\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 102.07it/s]\u001b[AValidation loss: 0.9965060353279114\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 102.16it/s]\u001b[AValidation loss: 0.9620761275291443\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 102.24it/s]\u001b[AValidation loss: 0.7997642159461975\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 101.98it/s]\u001b[AValidation loss: 1.212698221206665\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 101.83it/s]\u001b[AValidation loss: 0.631084144115448\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.91it/s]\u001b[AValidation loss: 0.7875479459762573\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.98it/s]\u001b[AValidation loss: 0.8393611311912537\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 102.07it/s]\u001b[AValidation loss: 0.850190281867981\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 102.16it/s]\u001b[AValidation loss: 1.1588598489761353\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 0.7085733413696289\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 102.32it/s]\u001b[AValidation loss: 0.9196661710739136\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 102.17it/s]\u001b[AValidation loss: 0.8239686489105225\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 102.21it/s]\u001b[AValidation loss: 0.6885629296302795\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 1.6550565958023071\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 102.30it/s]\u001b[AValidation loss: 0.47764232754707336\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.37it/s]\u001b[AValidation loss: 1.435842752456665\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.41it/s]\u001b[AValidation loss: 0.5470376014709473\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.33it/s]\u001b[AValidation loss: 0.8763982653617859\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.37it/s]\u001b[AValidation loss: 0.8356629014015198\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 0.6808225512504578\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.20it/s]\u001b[AValidation loss: 0.6248140931129456\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.02it/s]\u001b[AValidation loss: 0.6554509401321411\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 101.87it/s]\u001b[AValidation loss: 1.1687164306640625\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 101.71it/s]\u001b[AValidation loss: 1.2722471952438354\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 101.59it/s]\u001b[AValidation loss: 0.6285269856452942\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 101.46it/s]\u001b[AValidation loss: 0.8353572487831116\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 101.34it/s]\u001b[AValidation loss: 0.8025538921356201\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 101.23it/s]\u001b[AValidation loss: 1.0402580499649048\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 100.96it/s]\u001b[AValidation loss: 1.2282923460006714\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 100.71it/s]\u001b[AValidation loss: 1.0838066339492798\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 100.61it/s]\u001b[AValidation loss: 1.0058401823043823\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 100.50it/s]\u001b[AValidation loss: 1.2234725952148438\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 100.42it/s]\u001b[AValidation loss: 0.9258610010147095\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 100.32it/s]\u001b[AValidation loss: 2.0453858375549316\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 100.39it/s]\u001b[A\n",
      "Epoch 13:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 1.4924781322479248\n",
      "Epoch 13:   0%|          | 1/212 [00:00<00:04, 45.24it/s, v_num=0]Training loss: 1.2040048837661743\n",
      "Epoch 13:   1%|          | 2/212 [00:00<00:05, 40.67it/s, v_num=0]Training loss: 0.6524735689163208\n",
      "Epoch 13:   1%|▏         | 3/212 [00:00<00:05, 39.02it/s, v_num=0]Training loss: 0.6789993047714233\n",
      "Epoch 13:   2%|▏         | 4/212 [00:00<00:05, 38.30it/s, v_num=0]Training loss: 0.574221670627594\n",
      "Epoch 13:   2%|▏         | 5/212 [00:00<00:05, 37.90it/s, v_num=0]Training loss: 0.549694836139679\n",
      "Epoch 13:   3%|▎         | 6/212 [00:00<00:05, 37.61it/s, v_num=0]Training loss: 0.5972672700881958\n",
      "Epoch 13:   3%|▎         | 7/212 [00:00<00:05, 37.45it/s, v_num=0]Training loss: 0.8124842643737793\n",
      "Epoch 13:   4%|▍         | 8/212 [00:00<00:05, 37.17it/s, v_num=0]Training loss: 1.162707805633545\n",
      "Epoch 13:   4%|▍         | 9/212 [00:00<00:05, 37.11it/s, v_num=0]Training loss: 0.644822359085083\n",
      "Epoch 13:   5%|▍         | 10/212 [00:00<00:05, 37.09it/s, v_num=0]Training loss: 0.806523323059082\n",
      "Epoch 13:   5%|▌         | 11/212 [00:00<00:05, 37.15it/s, v_num=0]Training loss: 0.6284142732620239\n",
      "Epoch 13:   6%|▌         | 12/212 [00:00<00:05, 37.19it/s, v_num=0]Training loss: 1.024224042892456\n",
      "Epoch 13:   6%|▌         | 13/212 [00:00<00:05, 37.22it/s, v_num=0]Training loss: 0.659713625907898\n",
      "Epoch 13:   7%|▋         | 14/212 [00:00<00:05, 37.22it/s, v_num=0]Training loss: 0.6023796796798706\n",
      "Epoch 13:   7%|▋         | 15/212 [00:00<00:05, 37.24it/s, v_num=0]Training loss: 0.6594812273979187\n",
      "Epoch 13:   8%|▊         | 16/212 [00:00<00:05, 37.26it/s, v_num=0]Training loss: 0.6989818811416626\n",
      "Epoch 13:   8%|▊         | 17/212 [00:00<00:05, 37.26it/s, v_num=0]Training loss: 0.9496870040893555\n",
      "Epoch 13:   8%|▊         | 18/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 0.5341323614120483\n",
      "Epoch 13:   9%|▉         | 19/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 0.7939805388450623\n",
      "Epoch 13:   9%|▉         | 20/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 0.623866856098175\n",
      "Epoch 13:  10%|▉         | 21/212 [00:00<00:05, 37.28it/s, v_num=0]Training loss: 0.9607908725738525\n",
      "Epoch 13:  10%|█         | 22/212 [00:00<00:05, 37.29it/s, v_num=0]Training loss: 0.7837709784507751\n",
      "Epoch 13:  11%|█         | 23/212 [00:00<00:05, 37.30it/s, v_num=0]Training loss: 0.6971803903579712\n",
      "Epoch 13:  11%|█▏        | 24/212 [00:00<00:05, 37.30it/s, v_num=0]Training loss: 0.9979180693626404\n",
      "Epoch 13:  12%|█▏        | 25/212 [00:00<00:05, 37.31it/s, v_num=0]Training loss: 0.7341157793998718\n",
      "Epoch 13:  12%|█▏        | 26/212 [00:00<00:04, 37.32it/s, v_num=0]Training loss: 1.0335341691970825\n",
      "Epoch 13:  13%|█▎        | 27/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.9227048754692078\n",
      "Epoch 13:  13%|█▎        | 28/212 [00:00<00:04, 37.33it/s, v_num=0]Training loss: 0.9057172536849976\n",
      "Epoch 13:  14%|█▎        | 29/212 [00:00<00:04, 37.34it/s, v_num=0]Training loss: 0.8224430680274963\n",
      "Epoch 13:  14%|█▍        | 30/212 [00:00<00:04, 37.35it/s, v_num=0]Training loss: 1.0043467283248901\n",
      "Epoch 13:  15%|█▍        | 31/212 [00:00<00:04, 37.36it/s, v_num=0]Training loss: 0.9512823820114136\n",
      "Epoch 13:  15%|█▌        | 32/212 [00:00<00:04, 37.36it/s, v_num=0]Training loss: 3.5155117511749268\n",
      "Epoch 13:  16%|█▌        | 33/212 [00:00<00:04, 37.37it/s, v_num=0]Training loss: 0.8672032952308655\n",
      "Epoch 13:  16%|█▌        | 34/212 [00:00<00:04, 37.37it/s, v_num=0]Training loss: 0.7854899168014526\n",
      "Epoch 13:  17%|█▋        | 35/212 [00:00<00:04, 37.38it/s, v_num=0]Training loss: 0.8929465413093567\n",
      "Epoch 13:  17%|█▋        | 36/212 [00:00<00:04, 37.38it/s, v_num=0]Training loss: 0.6927692890167236\n",
      "Epoch 13:  17%|█▋        | 37/212 [00:00<00:04, 37.39it/s, v_num=0]Training loss: 0.61717289686203\n",
      "Epoch 13:  18%|█▊        | 38/212 [00:01<00:04, 37.39it/s, v_num=0]Training loss: 0.5885217189788818\n",
      "Epoch 13:  18%|█▊        | 39/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 0.8930197358131409\n",
      "Epoch 13:  19%|█▉        | 40/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 0.8913113474845886\n",
      "Epoch 13:  19%|█▉        | 41/212 [00:01<00:04, 37.40it/s, v_num=0]Training loss: 9.863811492919922\n",
      "Epoch 13:  20%|█▉        | 42/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 0.7244398593902588\n",
      "Epoch 13:  20%|██        | 43/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.9708426594734192\n",
      "Epoch 13:  21%|██        | 44/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 0.7542988657951355\n",
      "Epoch 13:  21%|██        | 45/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 0.6986870169639587\n",
      "Epoch 13:  22%|██▏       | 46/212 [00:01<00:04, 37.26it/s, v_num=0]Training loss: 0.7318483591079712\n",
      "Epoch 13:  22%|██▏       | 47/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.8139845728874207\n",
      "Epoch 13:  23%|██▎       | 48/212 [00:01<00:04, 37.18it/s, v_num=0]Training loss: 0.6768892407417297\n",
      "Epoch 13:  23%|██▎       | 49/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.4839757680892944\n",
      "Epoch 13:  24%|██▎       | 50/212 [00:01<00:04, 37.16it/s, v_num=0]Training loss: 1.1442185640335083\n",
      "Epoch 13:  24%|██▍       | 51/212 [00:01<00:04, 37.17it/s, v_num=0]Training loss: 0.8490606546401978\n",
      "Epoch 13:  25%|██▍       | 52/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.873882532119751\n",
      "Epoch 13:  25%|██▌       | 53/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.6419768333435059\n",
      "Epoch 13:  25%|██▌       | 54/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.6640663743019104\n",
      "Epoch 13:  26%|██▌       | 55/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 0.7416397929191589\n",
      "Epoch 13:  26%|██▋       | 56/212 [00:01<00:04, 37.20it/s, v_num=0]Training loss: 1.1013187170028687\n",
      "Epoch 13:  27%|██▋       | 57/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 0.648638904094696\n",
      "Epoch 13:  27%|██▋       | 58/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.012292742729187\n",
      "Epoch 13:  28%|██▊       | 59/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 0.8934087753295898\n",
      "Epoch 13:  28%|██▊       | 60/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.2187496423721313\n",
      "Epoch 13:  29%|██▉       | 61/212 [00:01<00:04, 37.21it/s, v_num=0]Training loss: 1.936590552330017\n",
      "Epoch 13:  29%|██▉       | 62/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.7475135326385498\n",
      "Epoch 13:  30%|██▉       | 63/212 [00:01<00:04, 37.22it/s, v_num=0]Training loss: 0.4457376003265381\n",
      "Epoch 13:  30%|███       | 64/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.5057897567749023\n",
      "Epoch 13:  31%|███       | 65/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.084734559059143\n",
      "Epoch 13:  31%|███       | 66/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 0.9764105081558228\n",
      "Epoch 13:  32%|███▏      | 67/212 [00:01<00:03, 37.24it/s, v_num=0]Training loss: 1.7543591260910034\n",
      "Epoch 13:  32%|███▏      | 68/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.6299083232879639\n",
      "Epoch 13:  33%|███▎      | 69/212 [00:01<00:03, 37.25it/s, v_num=0]Training loss: 0.8020231127738953\n",
      "Epoch 13:  33%|███▎      | 70/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 0.6424160599708557\n",
      "Epoch 13:  33%|███▎      | 71/212 [00:01<00:03, 37.26it/s, v_num=0]Training loss: 3.395345687866211\n",
      "Epoch 13:  34%|███▍      | 72/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 1.094968557357788\n",
      "Epoch 13:  34%|███▍      | 73/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 0.8458102345466614\n",
      "Epoch 13:  35%|███▍      | 74/212 [00:01<00:03, 37.27it/s, v_num=0]Training loss: 1.206804633140564\n",
      "Epoch 13:  35%|███▌      | 75/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7826328873634338\n",
      "Epoch 13:  36%|███▌      | 76/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7454991936683655\n",
      "Epoch 13:  36%|███▋      | 77/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.5969277024269104\n",
      "Epoch 13:  37%|███▋      | 78/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.9966723322868347\n",
      "Epoch 13:  37%|███▋      | 79/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.6371318101882935\n",
      "Epoch 13:  38%|███▊      | 80/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.1331955194473267\n",
      "Epoch 13:  38%|███▊      | 81/212 [00:02<00:03, 37.30it/s, v_num=0]Training loss: 1.1901938915252686\n",
      "Epoch 13:  39%|███▊      | 82/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.9088432788848877\n",
      "Epoch 13:  39%|███▉      | 83/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.6681436896324158\n",
      "Epoch 13:  40%|███▉      | 84/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.8879828453063965\n",
      "Epoch 13:  40%|████      | 85/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.8168542385101318\n",
      "Epoch 13:  41%|████      | 86/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.4702358841896057\n",
      "Epoch 13:  41%|████      | 87/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.8025675415992737\n",
      "Epoch 13:  42%|████▏     | 88/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 1.0637198686599731\n",
      "Epoch 13:  42%|████▏     | 89/212 [00:02<00:03, 37.18it/s, v_num=0]Training loss: 0.6538897752761841\n",
      "Epoch 13:  42%|████▏     | 90/212 [00:02<00:03, 37.19it/s, v_num=0]Training loss: 0.7476440072059631\n",
      "Epoch 13:  43%|████▎     | 91/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.3402795791625977\n",
      "Epoch 13:  43%|████▎     | 92/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.2929003238677979\n",
      "Epoch 13:  44%|████▍     | 93/212 [00:02<00:03, 37.20it/s, v_num=0]Training loss: 1.004443883895874\n",
      "Epoch 13:  44%|████▍     | 94/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 1.7965937852859497\n",
      "Epoch 13:  45%|████▍     | 95/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.7016499638557434\n",
      "Epoch 13:  45%|████▌     | 96/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.4741436541080475\n",
      "Epoch 13:  46%|████▌     | 97/212 [00:02<00:03, 37.21it/s, v_num=0]Training loss: 0.5795660614967346\n",
      "Epoch 13:  46%|████▌     | 98/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.6570837497711182\n",
      "Epoch 13:  47%|████▋     | 99/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.2549155950546265\n",
      "Epoch 13:  47%|████▋     | 100/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 0.6818466186523438\n",
      "Epoch 13:  48%|████▊     | 101/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.09074866771698\n",
      "Epoch 13:  48%|████▊     | 102/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 2.080271005630493\n",
      "Epoch 13:  49%|████▊     | 103/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.6103273034095764\n",
      "Epoch 13:  49%|████▉     | 104/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.5669399499893188\n",
      "Epoch 13:  50%|████▉     | 105/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 0.8446199297904968\n",
      "Epoch 13:  50%|█████     | 106/212 [00:02<00:02, 37.23it/s, v_num=0]Training loss: 1.2005090713500977\n",
      "Epoch 13:  50%|█████     | 107/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 1.7693129777908325\n",
      "Epoch 13:  51%|█████     | 108/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.8199683427810669\n",
      "Epoch 13:  51%|█████▏    | 109/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.9792468547821045\n",
      "Epoch 13:  52%|█████▏    | 110/212 [00:02<00:02, 37.24it/s, v_num=0]Training loss: 0.7278631925582886\n",
      "Epoch 13:  52%|█████▏    | 111/212 [00:02<00:02, 37.25it/s, v_num=0]Training loss: 1.5115365982055664\n",
      "Epoch 13:  53%|█████▎    | 112/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.851845383644104\n",
      "Epoch 13:  53%|█████▎    | 113/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.834732174873352\n",
      "Epoch 13:  54%|█████▍    | 114/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.830383539199829\n",
      "Epoch 13:  54%|█████▍    | 115/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8711054921150208\n",
      "Epoch 13:  55%|█████▍    | 116/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 2.180142402648926\n",
      "Epoch 13:  55%|█████▌    | 117/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.7459583282470703\n",
      "Epoch 13:  56%|█████▌    | 118/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.3051568269729614\n",
      "Epoch 13:  56%|█████▌    | 119/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 0.7296367287635803\n",
      "Epoch 13:  57%|█████▋    | 120/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.0023185014724731\n",
      "Epoch 13:  57%|█████▋    | 121/212 [00:03<00:02, 37.27it/s, v_num=0]Training loss: 1.3408774137496948\n",
      "Epoch 13:  58%|█████▊    | 122/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.6499072909355164\n",
      "Epoch 13:  58%|█████▊    | 123/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.0882145166397095\n",
      "Epoch 13:  58%|█████▊    | 124/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 0.6437215209007263\n",
      "Epoch 13:  59%|█████▉    | 125/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.2172304391860962\n",
      "Epoch 13:  59%|█████▉    | 126/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.9599369764328003\n",
      "Epoch 13:  60%|█████▉    | 127/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.116762399673462\n",
      "Epoch 13:  60%|██████    | 128/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.0710608959197998\n",
      "Epoch 13:  61%|██████    | 129/212 [00:03<00:02, 37.19it/s, v_num=0]Training loss: 0.735490083694458\n",
      "Epoch 13:  61%|██████▏   | 130/212 [00:03<00:02, 37.20it/s, v_num=0]Training loss: 1.0901947021484375\n",
      "Epoch 13:  62%|██████▏   | 131/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.7219375967979431\n",
      "Epoch 13:  62%|██████▏   | 132/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.1551721096038818\n",
      "Epoch 13:  63%|██████▎   | 133/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.8624593019485474\n",
      "Epoch 13:  63%|██████▎   | 134/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.5618166923522949\n",
      "Epoch 13:  64%|██████▎   | 135/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 0.8953937888145447\n",
      "Epoch 13:  64%|██████▍   | 136/212 [00:03<00:02, 37.21it/s, v_num=0]Training loss: 1.1885251998901367\n",
      "Epoch 13:  65%|██████▍   | 137/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 2.60546612739563\n",
      "Epoch 13:  65%|██████▌   | 138/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.8298478722572327\n",
      "Epoch 13:  66%|██████▌   | 139/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.2343173027038574\n",
      "Epoch 13:  66%|██████▌   | 140/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 1.5764914751052856\n",
      "Epoch 13:  67%|██████▋   | 141/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.8244427442550659\n",
      "Epoch 13:  67%|██████▋   | 142/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.5087393522262573\n",
      "Epoch 13:  67%|██████▋   | 143/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.7227829694747925\n",
      "Epoch 13:  68%|██████▊   | 144/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.6776278018951416\n",
      "Epoch 13:  68%|██████▊   | 145/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.8453977108001709\n",
      "Epoch 13:  69%|██████▉   | 146/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.8528013229370117\n",
      "Epoch 13:  69%|██████▉   | 147/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 1.0555554628372192\n",
      "Epoch 13:  70%|██████▉   | 148/212 [00:03<00:01, 37.23it/s, v_num=0]Training loss: 0.7204936146736145\n",
      "Epoch 13:  70%|███████   | 149/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.9427096843719482\n",
      "Epoch 13:  71%|███████   | 150/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.3045299053192139\n",
      "Epoch 13:  71%|███████   | 151/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.0662211179733276\n",
      "Epoch 13:  72%|███████▏  | 152/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.8532008528709412\n",
      "Epoch 13:  72%|███████▏  | 153/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6843434572219849\n",
      "Epoch 13:  73%|███████▎  | 154/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.0458424091339111\n",
      "Epoch 13:  73%|███████▎  | 155/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.9543690085411072\n",
      "Epoch 13:  74%|███████▎  | 156/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 0.722235918045044\n",
      "Epoch 13:  74%|███████▍  | 157/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.0375666618347168\n",
      "Epoch 13:  75%|███████▍  | 158/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8758281469345093\n",
      "Epoch 13:  75%|███████▌  | 159/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 0.8674178123474121\n",
      "Epoch 13:  75%|███████▌  | 160/212 [00:04<00:01, 37.26it/s, v_num=0]Training loss: 2.249711275100708\n",
      "Epoch 13:  76%|███████▌  | 161/212 [00:04<00:01, 37.25it/s, v_num=0]Training loss: 1.6708877086639404\n",
      "Epoch 13:  76%|███████▋  | 162/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7597770094871521\n",
      "Epoch 13:  77%|███████▋  | 163/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.625591516494751\n",
      "Epoch 13:  77%|███████▋  | 164/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.935204267501831\n",
      "Epoch 13:  78%|███████▊  | 165/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.1150195598602295\n",
      "Epoch 13:  78%|███████▊  | 166/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.6132644414901733\n",
      "Epoch 13:  79%|███████▉  | 167/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.517947256565094\n",
      "Epoch 13:  79%|███████▉  | 168/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.1293954849243164\n",
      "Epoch 13:  80%|███████▉  | 169/212 [00:04<00:01, 37.20it/s, v_num=0]Training loss: 1.0087409019470215\n",
      "Epoch 13:  80%|████████  | 170/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.3913216590881348\n",
      "Epoch 13:  81%|████████  | 171/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.156904935836792\n",
      "Epoch 13:  81%|████████  | 172/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.3755838871002197\n",
      "Epoch 13:  82%|████████▏ | 173/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.7768009901046753\n",
      "Epoch 13:  82%|████████▏ | 174/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.154447078704834\n",
      "Epoch 13:  83%|████████▎ | 175/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.7644304037094116\n",
      "Epoch 13:  83%|████████▎ | 176/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.9249652028083801\n",
      "Epoch 13:  83%|████████▎ | 177/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.6020375490188599\n",
      "Epoch 13:  84%|████████▍ | 178/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 10.553030967712402\n",
      "Epoch 13:  84%|████████▍ | 179/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.161395788192749\n",
      "Epoch 13:  85%|████████▍ | 180/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.0089606046676636\n",
      "Epoch 13:  85%|████████▌ | 181/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.6207682490348816\n",
      "Epoch 13:  86%|████████▌ | 182/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.1925849914550781\n",
      "Epoch 13:  86%|████████▋ | 183/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.221482515335083\n",
      "Epoch 13:  87%|████████▋ | 184/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.7703585028648376\n",
      "Epoch 13:  87%|████████▋ | 185/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.47054198384284973\n",
      "Epoch 13:  88%|████████▊ | 186/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.8513713479042053\n",
      "Epoch 13:  88%|████████▊ | 187/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9587070941925049\n",
      "Epoch 13:  89%|████████▊ | 188/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.653496265411377\n",
      "Epoch 13:  89%|████████▉ | 189/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8425490260124207\n",
      "Epoch 13:  90%|████████▉ | 190/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.2405153512954712\n",
      "Epoch 13:  90%|█████████ | 191/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.6777580380439758\n",
      "Epoch 13:  91%|█████████ | 192/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8005509376525879\n",
      "Epoch 13:  91%|█████████ | 193/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.584037184715271\n",
      "Epoch 13:  92%|█████████▏| 194/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 1.205675721168518\n",
      "Epoch 13:  92%|█████████▏| 195/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.5870844721794128\n",
      "Epoch 13:  92%|█████████▏| 196/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8631758689880371\n",
      "Epoch 13:  93%|█████████▎| 197/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.9728443622589111\n",
      "Epoch 13:  93%|█████████▎| 198/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.8228651881217957\n",
      "Epoch 13:  94%|█████████▍| 199/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 1.1462607383728027\n",
      "Epoch 13:  94%|█████████▍| 200/212 [00:05<00:00, 37.25it/s, v_num=0]Training loss: 0.7471410036087036\n",
      "Epoch 13:  95%|█████████▍| 201/212 [00:05<00:00, 37.24it/s, v_num=0]Training loss: 0.8461350798606873\n",
      "Epoch 13:  95%|█████████▌| 202/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.8304364085197449\n",
      "Epoch 13:  96%|█████████▌| 203/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 1.1940844058990479\n",
      "Epoch 13:  96%|█████████▌| 204/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8662934899330139\n",
      "Epoch 13:  97%|█████████▋| 205/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8931331038475037\n",
      "Epoch 13:  97%|█████████▋| 206/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9516429305076599\n",
      "Epoch 13:  98%|█████████▊| 207/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9942141175270081\n",
      "Epoch 13:  98%|█████████▊| 208/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7266148328781128\n",
      "Epoch 13:  99%|█████████▊| 209/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9177772998809814\n",
      "Epoch 13:  99%|█████████▉| 210/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.5603078007698059\n",
      "Epoch 13: 100%|█████████▉| 211/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.223105788230896\n",
      "Epoch 13: 100%|██████████| 212/212 [00:05<00:00, 37.21it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 1.9995306730270386\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 83.91it/s]\u001b[AValidation loss: 1.6928789615631104\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 92.36it/s]\u001b[AValidation loss: 0.7500900030136108\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 95.88it/s]\u001b[AValidation loss: 0.5626053810119629\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 97.81it/s]\u001b[AValidation loss: 0.6194300651550293\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 96.88it/s]\u001b[AValidation loss: 0.7602713108062744\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 97.39it/s]\u001b[AValidation loss: 1.0550888776779175\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 98.27it/s]\u001b[AValidation loss: 1.9990663528442383\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 98.89it/s]\u001b[AValidation loss: 0.5263283252716064\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 99.35it/s]\u001b[AValidation loss: 0.6306285262107849\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 99.87it/s]\u001b[AValidation loss: 0.6181399822235107\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 100.23it/s]\u001b[AValidation loss: 0.8999471068382263\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 100.55it/s]\u001b[AValidation loss: 0.9544768333435059\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 100.79it/s]\u001b[AValidation loss: 0.8252497315406799\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 100.93it/s]\u001b[AValidation loss: 0.6923300623893738\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 101.10it/s]\u001b[AValidation loss: 0.9459225535392761\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 101.26it/s]\u001b[AValidation loss: 0.9288964867591858\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 101.05it/s]\u001b[AValidation loss: 1.3736711740493774\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 101.19it/s]\u001b[AValidation loss: 0.643105685710907\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 101.14it/s]\u001b[AValidation loss: 1.1183929443359375\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 100.85it/s]\u001b[AValidation loss: 0.9799664616584778\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 100.95it/s]\u001b[AValidation loss: 0.9939351677894592\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 101.03it/s]\u001b[AValidation loss: 0.9585011601448059\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 101.14it/s]\u001b[AValidation loss: 0.7969679236412048\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 101.22it/s]\u001b[AValidation loss: 1.206178903579712\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 101.32it/s]\u001b[AValidation loss: 0.6274305582046509\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 101.39it/s]\u001b[AValidation loss: 0.7819086313247681\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 27/55 [00:00<00:00, 101.45it/s]\u001b[AValidation loss: 0.8330912590026855\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 28/55 [00:00<00:00, 101.53it/s]\u001b[AValidation loss: 0.8439855575561523\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 29/55 [00:00<00:00, 101.59it/s]\u001b[AValidation loss: 1.1558306217193604\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 30/55 [00:00<00:00, 101.67it/s]\u001b[AValidation loss: 0.7042632102966309\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 31/55 [00:00<00:00, 101.75it/s]\u001b[AValidation loss: 0.9178823232650757\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 32/55 [00:00<00:00, 101.79it/s]\u001b[AValidation loss: 0.8207805752754211\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 33/55 [00:00<00:00, 101.85it/s]\u001b[AValidation loss: 0.6845125555992126\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 34/55 [00:00<00:00, 101.90it/s]\u001b[AValidation loss: 1.6488127708435059\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 35/55 [00:00<00:00, 101.97it/s]\u001b[AValidation loss: 0.4754038155078888\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 36/55 [00:00<00:00, 102.04it/s]\u001b[AValidation loss: 1.4379361867904663\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 37/55 [00:00<00:00, 102.06it/s]\u001b[AValidation loss: 0.5465346574783325\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 38/55 [00:00<00:00, 102.05it/s]\u001b[AValidation loss: 0.868772029876709\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 39/55 [00:00<00:00, 102.12it/s]\u001b[AValidation loss: 0.8295894861221313\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 40/55 [00:00<00:00, 102.15it/s]\u001b[AValidation loss: 0.6766711473464966\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 41/55 [00:00<00:00, 102.19it/s]\u001b[AValidation loss: 0.6216303110122681\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 42/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 0.6523256897926331\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 43/55 [00:00<00:00, 102.13it/s]\u001b[AValidation loss: 1.1655504703521729\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 44/55 [00:00<00:00, 102.20it/s]\u001b[AValidation loss: 1.265718936920166\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 45/55 [00:00<00:00, 102.25it/s]\u001b[AValidation loss: 0.6256546974182129\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 46/55 [00:00<00:00, 102.29it/s]\u001b[AValidation loss: 0.83003830909729\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 47/55 [00:00<00:00, 102.37it/s]\u001b[AValidation loss: 0.7982731461524963\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 48/55 [00:00<00:00, 102.29it/s]\u001b[AValidation loss: 1.0383315086364746\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 49/55 [00:00<00:00, 102.32it/s]\u001b[AValidation loss: 1.221286654472351\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 50/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 1.0814077854156494\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 51/55 [00:00<00:00, 102.40it/s]\u001b[AValidation loss: 1.0025525093078613\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 52/55 [00:00<00:00, 102.45it/s]\u001b[AValidation loss: 1.2203726768493652\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 53/55 [00:00<00:00, 102.35it/s]\u001b[AValidation loss: 0.9238995909690857\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 54/55 [00:00<00:00, 102.39it/s]\u001b[AValidation loss: 2.0441975593566895\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 102.43it/s]\u001b[A\n",
      "Epoch 14:   0%|          | 0/212 [00:00<?, ?it/s, v_num=0]               \u001b[ATraining loss: 0.72822505235672\n",
      "Epoch 14:   0%|          | 1/212 [00:00<00:04, 44.49it/s, v_num=0]Training loss: 0.6304550170898438\n",
      "Epoch 14:   1%|          | 2/212 [00:00<00:05, 40.69it/s, v_num=0]Training loss: 0.8329776525497437\n",
      "Epoch 14:   1%|▏         | 3/212 [00:00<00:05, 39.55it/s, v_num=0]Training loss: 0.7945128083229065\n",
      "Epoch 14:   2%|▏         | 4/212 [00:00<00:05, 39.04it/s, v_num=0]Training loss: 1.6194359064102173\n",
      "Epoch 14:   2%|▏         | 5/212 [00:00<00:05, 38.74it/s, v_num=0]Training loss: 0.64866703748703\n",
      "Epoch 14:   3%|▎         | 6/212 [00:00<00:05, 38.54it/s, v_num=0]Training loss: 0.8452586531639099\n",
      "Epoch 14:   3%|▎         | 7/212 [00:00<00:05, 38.39it/s, v_num=0]Training loss: 0.6404048204421997\n",
      "Epoch 14:   4%|▍         | 8/212 [00:00<00:05, 38.18it/s, v_num=0]Training loss: 0.9526647329330444\n",
      "Epoch 14:   4%|▍         | 9/212 [00:00<00:05, 38.10it/s, v_num=0]Training loss: 0.8482951521873474\n",
      "Epoch 14:   5%|▍         | 10/212 [00:00<00:05, 38.04it/s, v_num=0]Training loss: 0.6236617565155029\n",
      "Epoch 14:   5%|▌         | 11/212 [00:00<00:05, 38.00it/s, v_num=0]Training loss: 0.530132532119751\n",
      "Epoch 14:   6%|▌         | 12/212 [00:00<00:05, 37.96it/s, v_num=0]Training loss: 0.8035503625869751\n",
      "Epoch 14:   6%|▌         | 13/212 [00:00<00:05, 37.93it/s, v_num=0]Training loss: 0.5881449580192566\n",
      "Epoch 14:   7%|▋         | 14/212 [00:00<00:05, 37.90it/s, v_num=0]Training loss: 0.9255385994911194\n",
      "Epoch 14:   7%|▋         | 15/212 [00:00<00:05, 37.87it/s, v_num=0]Training loss: 0.6799407601356506\n",
      "Epoch 14:   8%|▊         | 16/212 [00:00<00:05, 37.72it/s, v_num=0]Training loss: 0.6825879216194153\n",
      "Epoch 14:   8%|▊         | 17/212 [00:00<00:05, 37.64it/s, v_num=0]Training loss: 0.9802970290184021\n",
      "Epoch 14:   8%|▊         | 18/212 [00:00<00:05, 37.55it/s, v_num=0]Training loss: 0.6490652561187744\n",
      "Epoch 14:   9%|▉         | 19/212 [00:00<00:05, 37.48it/s, v_num=0]Training loss: 0.9945569038391113\n",
      "Epoch 14:   9%|▉         | 20/212 [00:00<00:05, 37.41it/s, v_num=0]Training loss: 1.2231247425079346\n",
      "Epoch 14:  10%|▉         | 21/212 [00:00<00:05, 37.35it/s, v_num=0]Training loss: 0.9347553253173828\n",
      "Epoch 14:  10%|█         | 22/212 [00:00<00:05, 37.25it/s, v_num=0]Training loss: 0.9092168211936951\n",
      "Epoch 14:  11%|█         | 23/212 [00:00<00:05, 37.20it/s, v_num=0]Training loss: 0.5333141684532166\n",
      "Epoch 14:  11%|█▏        | 24/212 [00:00<00:05, 37.22it/s, v_num=0]Training loss: 0.864152193069458\n",
      "Epoch 14:  12%|█▏        | 25/212 [00:00<00:05, 37.26it/s, v_num=0]Training loss: 1.0432119369506836\n",
      "Epoch 14:  12%|█▏        | 26/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 0.5886167883872986\n",
      "Epoch 14:  13%|█▎        | 27/212 [00:00<00:04, 37.25it/s, v_num=0]Training loss: 0.6691328287124634\n",
      "Epoch 14:  13%|█▎        | 28/212 [00:00<00:04, 37.26it/s, v_num=0]Training loss: 0.7208911776542664\n",
      "Epoch 14:  14%|█▎        | 29/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 0.9245858192443848\n",
      "Epoch 14:  14%|█▍        | 30/212 [00:00<00:04, 37.27it/s, v_num=0]Training loss: 1.4407782554626465\n",
      "Epoch 14:  15%|█▍        | 31/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 0.947308361530304\n",
      "Epoch 14:  15%|█▌        | 32/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 1.370957612991333\n",
      "Epoch 14:  16%|█▌        | 33/212 [00:00<00:04, 37.28it/s, v_num=0]Training loss: 1.1874363422393799\n",
      "Epoch 14:  16%|█▌        | 34/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 1.2936816215515137\n",
      "Epoch 14:  17%|█▋        | 35/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.7693198919296265\n",
      "Epoch 14:  17%|█▋        | 36/212 [00:00<00:04, 37.29it/s, v_num=0]Training loss: 0.8342841863632202\n",
      "Epoch 14:  17%|█▋        | 37/212 [00:00<00:04, 37.30it/s, v_num=0]Training loss: 3.4143991470336914\n",
      "Epoch 14:  18%|█▊        | 38/212 [00:01<00:04, 37.30it/s, v_num=0]Training loss: 1.3509517908096313\n",
      "Epoch 14:  18%|█▊        | 39/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 0.6227222681045532\n",
      "Epoch 14:  19%|█▉        | 40/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.0381590127944946\n",
      "Epoch 14:  19%|█▉        | 41/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.6017729043960571\n",
      "Epoch 14:  20%|█▉        | 42/212 [00:01<00:04, 37.32it/s, v_num=0]Training loss: 0.5026618242263794\n",
      "Epoch 14:  20%|██        | 43/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.7880218029022217\n",
      "Epoch 14:  21%|██        | 44/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 2.8183670043945312\n",
      "Epoch 14:  21%|██        | 45/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 1.115950345993042\n",
      "Epoch 14:  22%|██▏       | 46/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.711625874042511\n",
      "Epoch 14:  22%|██▏       | 47/212 [00:01<00:04, 37.34it/s, v_num=0]Training loss: 0.7746442556381226\n",
      "Epoch 14:  23%|██▎       | 48/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.475581556558609\n",
      "Epoch 14:  23%|██▎       | 49/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.6873616576194763\n",
      "Epoch 14:  24%|██▎       | 50/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.099858045578003\n",
      "Epoch 14:  24%|██▍       | 51/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.4330021142959595\n",
      "Epoch 14:  25%|██▍       | 52/212 [00:01<00:04, 37.36it/s, v_num=0]Training loss: 1.2413065433502197\n",
      "Epoch 14:  25%|██▌       | 53/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 0.5050216913223267\n",
      "Epoch 14:  25%|██▌       | 54/212 [00:01<00:04, 37.37it/s, v_num=0]Training loss: 1.2801114320755005\n",
      "Epoch 14:  26%|██▌       | 55/212 [00:01<00:04, 37.35it/s, v_num=0]Training loss: 0.75990891456604\n",
      "Epoch 14:  26%|██▋       | 56/212 [00:01<00:04, 37.33it/s, v_num=0]Training loss: 0.7161375284194946\n",
      "Epoch 14:  27%|██▋       | 57/212 [00:01<00:04, 37.31it/s, v_num=0]Training loss: 1.7679557800292969\n",
      "Epoch 14:  27%|██▋       | 58/212 [00:01<00:04, 37.29it/s, v_num=0]Training loss: 0.6222422122955322\n",
      "Epoch 14:  28%|██▊       | 59/212 [00:01<00:04, 37.27it/s, v_num=0]Training loss: 1.0098600387573242\n",
      "Epoch 14:  28%|██▊       | 60/212 [00:01<00:04, 37.25it/s, v_num=0]Training loss: 1.7189311981201172\n",
      "Epoch 14:  29%|██▉       | 61/212 [00:01<00:04, 37.23it/s, v_num=0]Training loss: 0.5379089713096619\n",
      "Epoch 14:  29%|██▉       | 62/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.6540518999099731\n",
      "Epoch 14:  30%|██▉       | 63/212 [00:01<00:04, 37.19it/s, v_num=0]Training loss: 0.837388277053833\n",
      "Epoch 14:  30%|███       | 64/212 [00:01<00:03, 37.21it/s, v_num=0]Training loss: 0.7637165188789368\n",
      "Epoch 14:  31%|███       | 65/212 [00:01<00:03, 37.21it/s, v_num=0]Training loss: 0.9394157528877258\n",
      "Epoch 14:  31%|███       | 66/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.9556899666786194\n",
      "Epoch 14:  32%|███▏      | 67/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.6741603016853333\n",
      "Epoch 14:  32%|███▏      | 68/212 [00:01<00:03, 37.22it/s, v_num=0]Training loss: 0.6547933220863342\n",
      "Epoch 14:  33%|███▎      | 69/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.784203827381134\n",
      "Epoch 14:  33%|███▎      | 70/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.7737092971801758\n",
      "Epoch 14:  33%|███▎      | 71/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 1.115501046180725\n",
      "Epoch 14:  34%|███▍      | 72/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 9.675710678100586\n",
      "Epoch 14:  34%|███▍      | 73/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.5324525833129883\n",
      "Epoch 14:  35%|███▍      | 74/212 [00:01<00:03, 37.23it/s, v_num=0]Training loss: 0.9795730113983154\n",
      "Epoch 14:  35%|███▌      | 75/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.8791567087173462\n",
      "Epoch 14:  36%|███▌      | 76/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.1133041381835938\n",
      "Epoch 14:  36%|███▋      | 77/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.5275182127952576\n",
      "Epoch 14:  37%|███▋      | 78/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 0.9966760873794556\n",
      "Epoch 14:  37%|███▋      | 79/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 0.9787477254867554\n",
      "Epoch 14:  38%|███▊      | 80/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.2650309801101685\n",
      "Epoch 14:  38%|███▊      | 81/212 [00:02<00:03, 37.25it/s, v_num=0]Training loss: 1.0947314500808716\n",
      "Epoch 14:  39%|███▊      | 82/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.5925411581993103\n",
      "Epoch 14:  39%|███▉      | 83/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.7158157825469971\n",
      "Epoch 14:  40%|███▉      | 84/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.8775417804718018\n",
      "Epoch 14:  40%|████      | 85/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.7278397679328918\n",
      "Epoch 14:  41%|████      | 86/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 1.214231252670288\n",
      "Epoch 14:  41%|████      | 87/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.7786920666694641\n",
      "Epoch 14:  42%|████▏     | 88/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7308165431022644\n",
      "Epoch 14:  42%|████▏     | 89/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 1.3230029344558716\n",
      "Epoch 14:  42%|████▏     | 90/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.7136471271514893\n",
      "Epoch 14:  43%|████▎     | 91/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 2.1042356491088867\n",
      "Epoch 14:  43%|████▎     | 92/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8842927813529968\n",
      "Epoch 14:  44%|████▍     | 93/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8466655015945435\n",
      "Epoch 14:  44%|████▍     | 94/212 [00:02<00:03, 37.29it/s, v_num=0]Training loss: 0.8521271347999573\n",
      "Epoch 14:  45%|████▍     | 95/212 [00:02<00:03, 37.28it/s, v_num=0]Training loss: 0.690161406993866\n",
      "Epoch 14:  45%|████▌     | 96/212 [00:02<00:03, 37.27it/s, v_num=0]Training loss: 0.4136447608470917\n",
      "Epoch 14:  46%|████▌     | 97/212 [00:02<00:03, 37.26it/s, v_num=0]Training loss: 0.7327671051025391\n",
      "Epoch 14:  46%|████▌     | 98/212 [00:02<00:03, 37.24it/s, v_num=0]Training loss: 1.248386263847351\n",
      "Epoch 14:  47%|████▋     | 99/212 [00:02<00:03, 37.23it/s, v_num=0]Training loss: 0.945175051689148\n",
      "Epoch 14:  47%|████▋     | 100/212 [00:02<00:03, 37.22it/s, v_num=0]Training loss: 1.059702754020691\n",
      "Epoch 14:  48%|████▊     | 101/212 [00:02<00:02, 37.20it/s, v_num=0]Training loss: 0.8310007452964783\n",
      "Epoch 14:  48%|████▊     | 102/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 0.5815198421478271\n",
      "Epoch 14:  49%|████▊     | 103/212 [00:02<00:02, 37.19it/s, v_num=0]Training loss: 0.6364728212356567\n",
      "Epoch 14:  49%|████▉     | 104/212 [00:02<00:02, 37.20it/s, v_num=0]Training loss: 1.5202281475067139\n",
      "Epoch 14:  50%|████▉     | 105/212 [00:02<00:02, 37.20it/s, v_num=0]Training loss: 0.8597841262817383\n",
      "Epoch 14:  50%|█████     | 106/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.8312962651252747\n",
      "Epoch 14:  50%|█████     | 107/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 1.1540745496749878\n",
      "Epoch 14:  51%|█████     | 108/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.8334288001060486\n",
      "Epoch 14:  51%|█████▏    | 109/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.6790109872817993\n",
      "Epoch 14:  52%|█████▏    | 110/212 [00:02<00:02, 37.21it/s, v_num=0]Training loss: 0.8596178293228149\n",
      "Epoch 14:  52%|█████▏    | 111/212 [00:02<00:02, 37.22it/s, v_num=0]Training loss: 1.50763738155365\n",
      "Epoch 14:  53%|█████▎    | 112/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.6362473964691162\n",
      "Epoch 14:  53%|█████▎    | 113/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.8741120100021362\n",
      "Epoch 14:  54%|█████▍    | 114/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.0635385513305664\n",
      "Epoch 14:  54%|█████▍    | 115/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 1.2322101593017578\n",
      "Epoch 14:  55%|█████▍    | 116/212 [00:03<00:02, 37.22it/s, v_num=0]Training loss: 0.6295762062072754\n",
      "Epoch 14:  55%|█████▌    | 117/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.143892526626587\n",
      "Epoch 14:  56%|█████▌    | 118/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9239712357521057\n",
      "Epoch 14:  56%|█████▌    | 119/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.9950886368751526\n",
      "Epoch 14:  57%|█████▋    | 120/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.0001838207244873\n",
      "Epoch 14:  57%|█████▋    | 121/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.979933500289917\n",
      "Epoch 14:  58%|█████▊    | 122/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 0.8343068957328796\n",
      "Epoch 14:  58%|█████▊    | 123/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1613467931747437\n",
      "Epoch 14:  58%|█████▊    | 124/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.0514026880264282\n",
      "Epoch 14:  59%|█████▉    | 125/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.4102391004562378\n",
      "Epoch 14:  59%|█████▉    | 126/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.2012436389923096\n",
      "Epoch 14:  60%|█████▉    | 127/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.9213994741439819\n",
      "Epoch 14:  60%|██████    | 128/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1454156637191772\n",
      "Epoch 14:  61%|██████    | 129/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 1.1397507190704346\n",
      "Epoch 14:  61%|██████▏   | 130/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.681520402431488\n",
      "Epoch 14:  62%|██████▏   | 131/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.226157546043396\n",
      "Epoch 14:  62%|██████▏   | 132/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 0.8997812271118164\n",
      "Epoch 14:  63%|██████▎   | 133/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.134238362312317\n",
      "Epoch 14:  63%|██████▎   | 134/212 [00:03<00:02, 37.26it/s, v_num=0]Training loss: 1.0233583450317383\n",
      "Epoch 14:  64%|██████▎   | 135/212 [00:03<00:02, 37.25it/s, v_num=0]Training loss: 0.610064685344696\n",
      "Epoch 14:  64%|██████▍   | 136/212 [00:03<00:02, 37.24it/s, v_num=0]Training loss: 1.1647526025772095\n",
      "Epoch 14:  65%|██████▍   | 137/212 [00:03<00:02, 37.23it/s, v_num=0]Training loss: 1.1623035669326782\n",
      "Epoch 14:  65%|██████▌   | 138/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.9050062298774719\n",
      "Epoch 14:  66%|██████▌   | 139/212 [00:03<00:01, 37.22it/s, v_num=0]Training loss: 0.6468493342399597\n",
      "Epoch 14:  66%|██████▌   | 140/212 [00:03<00:01, 37.21it/s, v_num=0]Training loss: 0.6811494827270508\n",
      "Epoch 14:  67%|██████▋   | 141/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.7059586048126221\n",
      "Epoch 14:  67%|██████▋   | 142/212 [00:03<00:01, 37.19it/s, v_num=0]Training loss: 1.436739444732666\n",
      "Epoch 14:  67%|██████▋   | 143/212 [00:03<00:01, 37.19it/s, v_num=0]Training loss: 0.6744679808616638\n",
      "Epoch 14:  68%|██████▊   | 144/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.9815995693206787\n",
      "Epoch 14:  68%|██████▊   | 145/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.9595742225646973\n",
      "Epoch 14:  69%|██████▉   | 146/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.167040467262268\n",
      "Epoch 14:  69%|██████▉   | 147/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 0.9604125618934631\n",
      "Epoch 14:  70%|██████▉   | 148/212 [00:03<00:01, 37.20it/s, v_num=0]Training loss: 1.013504981994629\n",
      "Epoch 14:  70%|███████   | 149/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.5797104835510254\n",
      "Epoch 14:  71%|███████   | 150/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0948110818862915\n",
      "Epoch 14:  71%|███████   | 151/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.7540715932846069\n",
      "Epoch 14:  72%|███████▏  | 152/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.1437195539474487\n",
      "Epoch 14:  72%|███████▏  | 153/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.111080527305603\n",
      "Epoch 14:  73%|███████▎  | 154/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.9372133612632751\n",
      "Epoch 14:  73%|███████▎  | 155/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.826123058795929\n",
      "Epoch 14:  74%|███████▎  | 156/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 1.0204726457595825\n",
      "Epoch 14:  74%|███████▍  | 157/212 [00:04<00:01, 37.21it/s, v_num=0]Training loss: 0.9259166121482849\n",
      "Epoch 14:  75%|███████▍  | 158/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.873114824295044\n",
      "Epoch 14:  75%|███████▌  | 159/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.7453954219818115\n",
      "Epoch 14:  75%|███████▌  | 160/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.5613576769828796\n",
      "Epoch 14:  76%|███████▌  | 161/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 0.6728625297546387\n",
      "Epoch 14:  76%|███████▋  | 162/212 [00:04<00:01, 37.22it/s, v_num=0]Training loss: 1.2174708843231201\n",
      "Epoch 14:  77%|███████▋  | 163/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.5972809195518494\n",
      "Epoch 14:  77%|███████▋  | 164/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8900310397148132\n",
      "Epoch 14:  78%|███████▊  | 165/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 1.0075830221176147\n",
      "Epoch 14:  78%|███████▊  | 166/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.6414821743965149\n",
      "Epoch 14:  79%|███████▉  | 167/212 [00:04<00:01, 37.23it/s, v_num=0]Training loss: 0.8889282941818237\n",
      "Epoch 14:  79%|███████▉  | 168/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.6090322732925415\n",
      "Epoch 14:  80%|███████▉  | 169/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.2015531063079834\n",
      "Epoch 14:  80%|████████  | 170/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.010833978652954\n",
      "Epoch 14:  81%|████████  | 171/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.898500382900238\n",
      "Epoch 14:  81%|████████  | 172/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.5441330075263977\n",
      "Epoch 14:  82%|████████▏ | 173/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 0.7061524391174316\n",
      "Epoch 14:  82%|████████▏ | 174/212 [00:04<00:01, 37.24it/s, v_num=0]Training loss: 1.327043056488037\n",
      "Epoch 14:  83%|████████▎ | 175/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 1.0751181840896606\n",
      "Epoch 14:  83%|████████▎ | 176/212 [00:04<00:00, 37.23it/s, v_num=0]Training loss: 0.9436825513839722\n",
      "Epoch 14:  83%|████████▎ | 177/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 0.7935449481010437\n",
      "Epoch 14:  84%|████████▍ | 178/212 [00:04<00:00, 37.22it/s, v_num=0]Training loss: 1.2374411821365356\n",
      "Epoch 14:  84%|████████▍ | 179/212 [00:04<00:00, 37.21it/s, v_num=0]Training loss: 0.9684455394744873\n",
      "Epoch 14:  85%|████████▍ | 180/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 2.3604891300201416\n",
      "Epoch 14:  85%|████████▌ | 181/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 0.7380822896957397\n",
      "Epoch 14:  86%|████████▌ | 182/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 0.8740266561508179\n",
      "Epoch 14:  86%|████████▋ | 183/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 0.6989006996154785\n",
      "Epoch 14:  87%|████████▋ | 184/212 [00:04<00:00, 37.19it/s, v_num=0]Training loss: 0.6756548285484314\n",
      "Epoch 14:  87%|████████▋ | 185/212 [00:04<00:00, 37.20it/s, v_num=0]Training loss: 3.834857940673828\n",
      "Epoch 14:  88%|████████▊ | 186/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.9840262532234192\n",
      "Epoch 14:  88%|████████▊ | 187/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.0009570121765137\n",
      "Epoch 14:  89%|████████▊ | 188/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.337174892425537\n",
      "Epoch 14:  89%|████████▉ | 189/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 9.694101333618164\n",
      "Epoch 14:  90%|████████▉ | 190/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.4937018156051636\n",
      "Epoch 14:  90%|█████████ | 191/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.9091664552688599\n",
      "Epoch 14:  91%|█████████ | 192/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7201130390167236\n",
      "Epoch 14:  91%|█████████ | 193/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 0.7040324807167053\n",
      "Epoch 14:  92%|█████████▏| 194/212 [00:05<00:00, 37.20it/s, v_num=0]Training loss: 1.1233668327331543\n",
      "Epoch 14:  92%|█████████▏| 195/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.621579110622406\n",
      "Epoch 14:  92%|█████████▏| 196/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.977831244468689\n",
      "Epoch 14:  93%|█████████▎| 197/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.776872456073761\n",
      "Epoch 14:  93%|█████████▎| 198/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.9781681299209595\n",
      "Epoch 14:  94%|█████████▍| 199/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.8192553520202637\n",
      "Epoch 14:  94%|█████████▍| 200/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 1.2231225967407227\n",
      "Epoch 14:  95%|█████████▍| 201/212 [00:05<00:00, 37.21it/s, v_num=0]Training loss: 0.7956686019897461\n",
      "Epoch 14:  95%|█████████▌| 202/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.0805584192276\n",
      "Epoch 14:  96%|█████████▌| 203/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 1.0971989631652832\n",
      "Epoch 14:  96%|█████████▌| 204/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.8551515340805054\n",
      "Epoch 14:  97%|█████████▋| 205/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.6898079514503479\n",
      "Epoch 14:  97%|█████████▋| 206/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.809032678604126\n",
      "Epoch 14:  98%|█████████▊| 207/212 [00:05<00:00, 37.22it/s, v_num=0]Training loss: 0.7376965284347534\n",
      "Epoch 14:  98%|█████████▊| 208/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7205125093460083\n",
      "Epoch 14:  99%|█████████▊| 209/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.7411238551139832\n",
      "Epoch 14:  99%|█████████▉| 210/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.644114077091217\n",
      "Epoch 14: 100%|█████████▉| 211/212 [00:05<00:00, 37.23it/s, v_num=0]Training loss: 0.9716542363166809\n",
      "Epoch 14: 100%|██████████| 212/212 [00:05<00:00, 37.23it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[AValidation loss: 1.993988037109375\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 1/55 [00:00<00:00, 79.43it/s]\u001b[AValidation loss: 1.683191180229187\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 2/55 [00:00<00:00, 90.19it/s]\u001b[AValidation loss: 0.7491878271102905\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 3/55 [00:00<00:00, 93.23it/s]\u001b[AValidation loss: 0.5614163279533386\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 4/55 [00:00<00:00, 94.77it/s]\u001b[AValidation loss: 0.6182805895805359\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 5/55 [00:00<00:00, 96.56it/s]\u001b[AValidation loss: 0.7575650811195374\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 6/55 [00:00<00:00, 96.09it/s]\u001b[AValidation loss: 1.0514702796936035\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 7/55 [00:00<00:00, 96.18it/s]\u001b[AValidation loss: 1.9923202991485596\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 8/55 [00:00<00:00, 96.00it/s]\u001b[AValidation loss: 0.5243526101112366\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 9/55 [00:00<00:00, 96.09it/s]\u001b[AValidation loss: 0.6292189359664917\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 10/55 [00:00<00:00, 96.19it/s]\u001b[AValidation loss: 0.6158960461616516\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 11/55 [00:00<00:00, 96.25it/s]\u001b[AValidation loss: 0.8974650502204895\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 12/55 [00:00<00:00, 96.31it/s]\u001b[AValidation loss: 0.9511984586715698\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 13/55 [00:00<00:00, 96.33it/s]\u001b[AValidation loss: 0.823101818561554\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 14/55 [00:00<00:00, 96.36it/s]\u001b[AValidation loss: 0.6898624897003174\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 15/55 [00:00<00:00, 96.38it/s]\u001b[AValidation loss: 0.9470760226249695\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 16/55 [00:00<00:00, 96.36it/s]\u001b[AValidation loss: 0.9236501455307007\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 17/55 [00:00<00:00, 96.35it/s]\u001b[AValidation loss: 1.3670352697372437\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 18/55 [00:00<00:00, 96.00it/s]\u001b[AValidation loss: 0.6417521238327026\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 19/55 [00:00<00:00, 95.69it/s]\u001b[AValidation loss: 1.112764596939087\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 20/55 [00:00<00:00, 95.65it/s]\u001b[AValidation loss: 0.9771363735198975\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 21/55 [00:00<00:00, 95.68it/s]\u001b[AValidation loss: 0.9899837970733643\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 22/55 [00:00<00:00, 95.70it/s]\u001b[AValidation loss: 0.9540692567825317\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 23/55 [00:00<00:00, 95.73it/s]\u001b[AValidation loss: 0.7939920425415039\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 24/55 [00:00<00:00, 95.73it/s]\u001b[AValidation loss: 1.2014118432998657\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 25/55 [00:00<00:00, 95.67it/s]\u001b[AValidation loss: 0.6249402165412903\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 26/55 [00:00<00:00, 95.65it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "\n",
    "data_size = '5000'\n",
    "dataset_path = 'data/v2/m1/'\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    \n",
    "    # This loads both X (features) and y (labels) from the saved files\n",
    "    train_tensors = torch.load(\"/\".join([dataset_path, data_size, \"train_dataset_tensors.pt\"]))  # Loads (X_train, y_train)\n",
    "    val_tensors = torch.load(\"/\".join([dataset_path, data_size, \"val_dataset_tensors.pt\"]))      # Loads (X_val, y_val)\n",
    "    test_tensors = torch.load(\"/\".join([dataset_path, data_size, \"test_dataset_tensors.pt\"]))    # Loads (X_test, y_test)\n",
    "    \n",
    "    # Recreate the TensorDataset\n",
    "    train_dataset = TensorDataset(*train_tensors)\n",
    "    val_dataset = TensorDataset(*val_tensors)\n",
    "    test_dataset = TensorDataset(*test_tensors)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "    \n",
    "    # Create a logger\n",
    "    logger = loggers.CSVLogger('lightning_logs/', name=f'v2_m1_vae_{data_size}')\n",
    "    \n",
    "    \n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Initialize the VAE Lightning model\n",
    "    input_dim = train_tensors[0].shape[1]  # The number of input features\n",
    "    # input_dim = X_train_tensor.shape[1]  # The number of input features\n",
    "    latent_dim = 256  # Latent dimension size, can be tuned\n",
    "    hidden_dims = [2048, 1024, 512]\n",
    "    dropout_rate = 0.2\n",
    "    lr = 1e-6\n",
    "    \n",
    "    model = VAE_Lightning(\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout_rate=dropout_rate,\n",
    "        lr=lr)\n",
    "    \n",
    "    # Training\n",
    "    loss_history_callback = LossHistoryCallback()\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        dirpath=f'{logger.save_dir}/{logger.name}/version_{logger.version}/checkpoints/',\n",
    "        filename='m1-vae-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=200,\n",
    "        gradient_clip_val=0.1,  # Clip gradients to avoid explosion\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, loss_history_callback],\n",
    "        precision=32,\n",
    "        accelerator='gpu',          # Use 'gpu' or 'cpu'\n",
    "        devices=1 if torch.cuda.is_available() else 'auto',  # Use 1 GPU or CPU ('auto' will pick the appropriate one)\n",
    "        deterministic=True,  # Ensure reproducibility\n",
    "        logger=logger\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41d126-b304-44d2-a04e-2acb99ad917b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3d0f77-c8b8-4caa-a189-56b80f990dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'df_logs' is your DataFrame with the logs\n",
    "# Drop the rows with NaN values for plotting each loss curve separately\n",
    "df_logs = pd.read_csv(\"lightning_logs/m1_vae/version_0/metrics.csv\")\n",
    "\n",
    "# Extract relevant columns\n",
    "epochs = df_logs['epoch'].unique()  # Get unique epoch values\n",
    "train_loss = df_logs['train_loss'].dropna()  # Drop NaN values for train loss\n",
    "val_loss = df_logs['val_loss'].dropna()      # Drop NaN values for validation loss\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs[:len(val_loss)], val_loss, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c431f599-80a3-41d2-94f0-d91882af4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': 0.2, 'hidden_dims': [2048, 1024, 512], 'input_dim': 451747, 'latent_dim': 256, 'lr': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the hyperparameters from the hparams.yaml file\n",
    "hparams_path = 'lightning_logs/m1_vae/version_0/hparams.yaml'  # Replace with the correct path\n",
    "with open(hparams_path) as file:\n",
    "    hparams = yaml.safe_load(file)\n",
    "\n",
    "print(hparams)  # To inspect the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b179030-528c-486f-b322-ac02c9078e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb373265-2411-49c0-ab8e-b739615c35c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_Lightning(\n",
       "  (model): VAE(\n",
       "    (encoder_layers): Sequential(\n",
       "      (0): Linear(in_features=451747, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_logvar): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_output): Linear(in_features=2048, out_features=451747, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "checkpoint_path = \"lightning_logs/m1_vae/version_0/checkpoints/m1-vae-epoch=21-val_loss=0.96.ckpt\"\n",
    "vae_model = VAE_Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    **hparams\n",
    "    )\n",
    "\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7151a777-f314-4cc2-8456-aa01e7b220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_embeddings(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x,y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Replace NaNs with zero or another neutral value for forward pass\n",
    "            x_filled = replace_nan_with_mean(x)\n",
    "            # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "            \n",
    "            z, _, _ = model.forward(x_filled)\n",
    "            embeddings.append(z)\n",
    "            labels.append(y)\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = get_latent_embeddings(vae_model, train_loader)\n",
    "val_embeddings, val_labels = get_latent_embeddings(vae_model, val_loader)\n",
    "test_embeddings, test_labels = get_latent_embeddings(vae_model, test_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbab96-e2f0-44ee-bf0c-63e74b8659dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meth]",
   "language": "python",
   "name": "conda-env-meth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
