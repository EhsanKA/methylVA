{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f18039-d090-4590-8147-7015aea29fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4052d57-369b-41d8-a387-4fd099f72314",
   "metadata": {},
   "source": [
    "### For Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9f4e60-85f2-4786-b816-cd2ba384f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Disables fast auto-tuning\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25597bcb-917e-4df6-bc83-6e282da50a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab27605-9859-4893-ba42-abe75439ac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e41a7a-1388-4584-a057-7d8c6bf81684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285847dd-5b4b-4988-94b8-61b7e0d8c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 22s, sys: 5.48 s, total: 9min 27s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df1 = pd.read_pickle('data/methyl_scores_v1_HM450k_1.pkl', compression=\"bz2\")\n",
    "df2 = pd.read_pickle('data/methyl_scores_v1_HM450k_2.pkl', compression=\"bz2\")\n",
    "df3 = pd.read_pickle('data/methyl_scores_v1_HM450k_3.pkl', compression=\"bz2\")\n",
    "df4 = pd.read_pickle('data/methyl_scores_v1_HM450k_4.pkl', compression=\"bz2\")\n",
    "df5 = pd.read_pickle('data/methyl_scores_v1_HM450k_5.pkl', compression=\"bz2\")\n",
    "df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "# df = pd.read_pickle('data/methyl_scores_v1_HM450k_1.pkl', compression=\"bz2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b0c2f0-cd67-40bd-ad72-5bc71d829f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11694, 485590)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9665d9a-ffb3-47bd-b6e5-973309d09f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geo_accession</th>\n",
       "      <th>title</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>disease</th>\n",
       "      <th>tissue</th>\n",
       "      <th>geo_platform</th>\n",
       "      <th>inferred_sex</th>\n",
       "      <th>inferred_age_Hannum</th>\n",
       "      <th>inferred_age_SkinBlood</th>\n",
       "      <th>inferred_age_Horvath353</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM3944747</th>\n",
       "      <td>103392</td>\n",
       "      <td>GSM3944747</td>\n",
       "      <td>AD, CBL sample 03--34</td>\n",
       "      <td>female</td>\n",
       "      <td>88</td>\n",
       "      <td>None</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>brain</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>F</td>\n",
       "      <td>34.520802</td>\n",
       "      <td>9.871158</td>\n",
       "      <td>65.031621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM3944748</th>\n",
       "      <td>103393</td>\n",
       "      <td>GSM3944748</td>\n",
       "      <td>AD, CBL sample 08--45</td>\n",
       "      <td>male</td>\n",
       "      <td>78</td>\n",
       "      <td>None</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>brain</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>M</td>\n",
       "      <td>30.496641</td>\n",
       "      <td>8.858558</td>\n",
       "      <td>60.821105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM3944749</th>\n",
       "      <td>103394</td>\n",
       "      <td>GSM3944749</td>\n",
       "      <td>AD, CBL sample 08--59</td>\n",
       "      <td>male</td>\n",
       "      <td>89</td>\n",
       "      <td>None</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>brain</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>M</td>\n",
       "      <td>37.546386</td>\n",
       "      <td>11.725778</td>\n",
       "      <td>63.846017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM3944750</th>\n",
       "      <td>103395</td>\n",
       "      <td>GSM3944750</td>\n",
       "      <td>AD, CBL sample 08--01</td>\n",
       "      <td>male</td>\n",
       "      <td>90</td>\n",
       "      <td>None</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>brain</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>M</td>\n",
       "      <td>31.407073</td>\n",
       "      <td>11.791566</td>\n",
       "      <td>59.617576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM3944751</th>\n",
       "      <td>103396</td>\n",
       "      <td>GSM3944751</td>\n",
       "      <td>AD, CBL sample 01--45</td>\n",
       "      <td>female</td>\n",
       "      <td>87</td>\n",
       "      <td>None</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>brain</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>F</td>\n",
       "      <td>30.523229</td>\n",
       "      <td>13.329728</td>\n",
       "      <td>65.524861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4317289</th>\n",
       "      <td>114835</td>\n",
       "      <td>GSM4317289</td>\n",
       "      <td>200078650258_R01C02</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>control</td>\n",
       "      <td>blood</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>M</td>\n",
       "      <td>69.873144</td>\n",
       "      <td>70.297545</td>\n",
       "      <td>71.124282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4317290</th>\n",
       "      <td>114836</td>\n",
       "      <td>GSM4317290</td>\n",
       "      <td>200078650258_R03C02</td>\n",
       "      <td>female</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>control</td>\n",
       "      <td>blood</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>F</td>\n",
       "      <td>75.717975</td>\n",
       "      <td>74.210118</td>\n",
       "      <td>77.139598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4317291</th>\n",
       "      <td>114837</td>\n",
       "      <td>GSM4317291</td>\n",
       "      <td>200078650258_R04C02</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Parkinson's disease</td>\n",
       "      <td>blood</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>M</td>\n",
       "      <td>73.383265</td>\n",
       "      <td>67.509537</td>\n",
       "      <td>70.919180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4317292</th>\n",
       "      <td>114838</td>\n",
       "      <td>GSM4317292</td>\n",
       "      <td>200078650258_R05C02</td>\n",
       "      <td>female</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>control</td>\n",
       "      <td>blood</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>F</td>\n",
       "      <td>76.227048</td>\n",
       "      <td>72.524829</td>\n",
       "      <td>72.624740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM4317293</th>\n",
       "      <td>114839</td>\n",
       "      <td>GSM4317293</td>\n",
       "      <td>200078650258_R06C02</td>\n",
       "      <td>female</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Parkinson's disease</td>\n",
       "      <td>blood</td>\n",
       "      <td>GPL13534</td>\n",
       "      <td>F</td>\n",
       "      <td>73.663465</td>\n",
       "      <td>74.175218</td>\n",
       "      <td>72.883815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11694 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id geo_accession                  title     sex   age  race  \\\n",
       "GSM3944747  103392    GSM3944747  AD, CBL sample 03--34  female    88  None   \n",
       "GSM3944748  103393    GSM3944748  AD, CBL sample 08--45    male    78  None   \n",
       "GSM3944749  103394    GSM3944749  AD, CBL sample 08--59    male    89  None   \n",
       "GSM3944750  103395    GSM3944750  AD, CBL sample 08--01    male    90  None   \n",
       "GSM3944751  103396    GSM3944751  AD, CBL sample 01--45  female    87  None   \n",
       "...            ...           ...                    ...     ...   ...   ...   \n",
       "GSM4317289  114835    GSM4317289    200078650258_R01C02    male  None  None   \n",
       "GSM4317290  114836    GSM4317290    200078650258_R03C02  female  None  None   \n",
       "GSM4317291  114837    GSM4317291    200078650258_R04C02    male  None  None   \n",
       "GSM4317292  114838    GSM4317292    200078650258_R05C02  female  None  None   \n",
       "GSM4317293  114839    GSM4317293    200078650258_R06C02  female  None  None   \n",
       "\n",
       "                        disease tissue geo_platform inferred_sex  \\\n",
       "GSM3944747  Alzheimer's disease  brain     GPL13534            F   \n",
       "GSM3944748  Alzheimer's disease  brain     GPL13534            M   \n",
       "GSM3944749  Alzheimer's disease  brain     GPL13534            M   \n",
       "GSM3944750  Alzheimer's disease  brain     GPL13534            M   \n",
       "GSM3944751  Alzheimer's disease  brain     GPL13534            F   \n",
       "...                         ...    ...          ...          ...   \n",
       "GSM4317289              control  blood     GPL13534            M   \n",
       "GSM4317290              control  blood     GPL13534            F   \n",
       "GSM4317291  Parkinson's disease  blood     GPL13534            M   \n",
       "GSM4317292              control  blood     GPL13534            F   \n",
       "GSM4317293  Parkinson's disease  blood     GPL13534            F   \n",
       "\n",
       "            inferred_age_Hannum  inferred_age_SkinBlood  \\\n",
       "GSM3944747            34.520802                9.871158   \n",
       "GSM3944748            30.496641                8.858558   \n",
       "GSM3944749            37.546386               11.725778   \n",
       "GSM3944750            31.407073               11.791566   \n",
       "GSM3944751            30.523229               13.329728   \n",
       "...                         ...                     ...   \n",
       "GSM4317289            69.873144               70.297545   \n",
       "GSM4317290            75.717975               74.210118   \n",
       "GSM4317291            73.383265               67.509537   \n",
       "GSM4317292            76.227048               72.524829   \n",
       "GSM4317293            73.663465               74.175218   \n",
       "\n",
       "            inferred_age_Horvath353  \n",
       "GSM3944747                65.031621  \n",
       "GSM3944748                60.821105  \n",
       "GSM3944749                63.846017  \n",
       "GSM3944750                59.617576  \n",
       "GSM3944751                65.524861  \n",
       "...                             ...  \n",
       "GSM4317289                71.124282  \n",
       "GSM4317290                77.139598  \n",
       "GSM4317291                70.919180  \n",
       "GSM4317292                72.624740  \n",
       "GSM4317293                72.883815  \n",
       "\n",
       "[11694 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['id', 'geo_accession', 'title', 'sex', 'age', 'race', 'disease',\n",
    "       'tissue', 'geo_platform', 'inferred_sex', 'inferred_age_Hannum',\n",
    "       'inferred_age_SkinBlood', 'inferred_age_Horvath353']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461eab8-829d-4fcb-bc5f-427a39e2d3ba",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cca18a-9d43-48be-9926-2b34f38c9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "metadata_columns = ['id', 'geo_accession', 'title', 'sex', 'age', 'race',\n",
    "                    'tissue', 'geo_platform', 'inferred_age_Hannum',\n",
    "                    'inferred_age_SkinBlood', 'inferred_age_Horvath353']  # list of metadata columns\n",
    "\n",
    "label_column = 'disease'  # column with target values for classification/regression\n",
    "condition_column = 'inferred_sex'\n",
    "numerical_data = df.drop(metadata_columns + [label_column] + [condition_column], axis=1)  # features for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a32489-463c-4167-9b5f-b118e722a85a",
   "metadata": {},
   "source": [
    "Fill in the NA values in the `label_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7488b8-4ffa-48b9-b016-aad9f5390627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7389856.1.gpu.q/ipykernel_2683849/744962163.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[label_column].fillna(default_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "default_value = 'no_label'\n",
    "df[label_column].fillna(default_value, inplace=True)\n",
    "\n",
    "labels = df[label_column]  # target/label for model training\n",
    "conditions = df[condition_column]  # target/label for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d3d066-d303-4a4f-be53-fef6f8ced205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disease\n",
       "control                                  5380\n",
       "no_label                                 1471\n",
       "Alzheimer's disease                      1204\n",
       "Parkinson's disease                       959\n",
       "rheumatoid arthritis                      718\n",
       "osteoporosis                              264\n",
       "multiple sclerosis                        240\n",
       "Crohn's disease                           177\n",
       "diabetes mellitus                         143\n",
       "gestational diabetes                      122\n",
       "Creutzfeldt-Jakob disease                 114\n",
       "fetal alcohol spectrum disorder           112\n",
       "mild cognitive impairment                 111\n",
       "hepatocellular carcinoma                  105\n",
       "ulcerative colitis                         87\n",
       "glioblastoma                               72\n",
       "pre-eclampsia                              71\n",
       "type 2 diabetes mellitus                   65\n",
       "lupus erythematosus                        57\n",
       "chronic obstructive pulmonary disease      50\n",
       "major depressive disorder                  40\n",
       "substance-related disorder                 37\n",
       "epilepsy                                   30\n",
       "obsessive-compulsive disorder              30\n",
       "depressive disorder                        22\n",
       "Lewy body dementia                          8\n",
       "pulmonary hypertension                      5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a6365-ab37-4a7c-9fd3-25e0fb273dd2",
   "metadata": {},
   "source": [
    "## Detect and drop unreliable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574c0d03-0791-46a2-98ff-ab2b4fe8e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3d4837-9f4d-45c5-bba1-4bb1d4c2a795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # Calculate the percentage of NaN values in each column\n",
    "nan_percentage = numerical_data.isna().sum(axis=0) / numerical_data.shape[0] * 100\n",
    "\n",
    "# Plot the histogram of the percentage of NaN values per column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(nan_percentage, bins=50, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Histogram of Percentage of NaN Values Per Column\")\n",
    "plt.xlabel(\"Percentage of NaN values\")  \n",
    "plt.ylabel(\"Number of Columns\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9dfa51f-7009-4628-96e0-56379bc43fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 1% NaN: 192121\n",
      "more than 5% NaN: 62648\n",
      "more than 10% NaN: 33830\n",
      "more than 15% NaN: 21686\n",
      "more than 20% NaN: 15110\n",
      "more than 30% NaN: 8532\n"
     ]
    }
   ],
   "source": [
    "print(f\"more than 1% NaN: {(nan_percentage>1).sum()}\")\n",
    "print(f\"more than 5% NaN: {(nan_percentage>5).sum()}\")\n",
    "print(f\"more than 10% NaN: {(nan_percentage>10).sum()}\")\n",
    "print(f\"more than 15% NaN: {(nan_percentage>15).sum()}\")\n",
    "print(f\"more than 20% NaN: {(nan_percentage>20).sum()}\")\n",
    "print(f\"more than 30% NaN: {(nan_percentage>30).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d520711a-e424-4ecd-ba2b-0771a4460fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of columns where NaN percentage is less than 10%\n",
    "selected_columns = nan_percentage[nan_percentage < 10].index.tolist()\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "numerical_data_filtered = numerical_data[selected_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d32e8-a353-4fee-9cb6-71bc0f962dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5862f95-701b-4ccd-98fd-525ebbf2e2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d40df9a1-49e4-41a4-83aa-8aa67c0cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Convert categorical labels to one-hot vectors\n",
    "labels_onehot = onehot_encoder.fit_transform(labels.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af488e12-0c6e-4b75-9bb2-028948dd4e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11694, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd4520a-deb7-4889-b16b-57be32332afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SCALE = True\n",
    "# ## With SCALE = False, the loss would explode! therefore, we scale the data to control the range of the loss and the gradients\n",
    "# if SCALE:\n",
    "#     scaler = StandardScaler()\n",
    "#     features_scaled = scaler.fit_transform(numerical_data_filtered)\n",
    "# else:\n",
    "#     features_scaled = numerical_data_filtered.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "707a140e-879d-4afd-9a81-a86d2fa41a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Todo Should separate scaling for train, validation, and test: like the following\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)  # Fit only on train data\n",
    "# X_val_scaled = scaler.transform(X_val)  # Apply the same scaler to validation and test data\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dd697-3d2e-472c-bbbe-58336cb18dd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2552d6e6-2d73-4c95-8ef8-8f1b2d94d781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa61bfb9-80c2-4a39-b733-e064574ed050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 38s, sys: 28.4 s, total: 5min 6s\n",
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Start by assuming your data is unscaled\n",
    "numerical_data_filtered = numerical_data_filtered.values  # Assuming this is your data\n",
    "labels_onehot = labels_onehot  # Assuming these are your labels\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "labels_class = np.argmax(labels_onehot, axis=1)  # Convert one-hot to class labels\n",
    "\n",
    "# Stratified shuffle split (30% test set, 70% train) based on class indices\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(numerical_data_filtered, labels_class):\n",
    "    X_train, X_temp = numerical_data_filtered[train_index], numerical_data_filtered[test_index]\n",
    "    y_train, y_temp = labels_onehot[train_index], labels_onehot[test_index]  # Use one-hot labels for actual training\n",
    "\n",
    "# Split the temp set into validation and test sets (15% val, 15% test)\n",
    "splitter_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in splitter_val_test.split(X_temp, np.argmax(y_temp, axis=1)):  # Use class labels here\n",
    "    X_val, X_test = X_temp[val_index], X_temp[test_index]\n",
    "    y_val, y_test = y_temp[val_index], y_temp[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c5ef9-8f1f-433e-8251-def8cfc34459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scale the data\n",
    "SCALE = True\n",
    "if SCALE:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler only on the training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Apply the same scaler to the validation and test sets\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    # If scaling is turned off, use the raw data\n",
    "    X_train_scaled = X_train\n",
    "    X_val_scaled = X_val\n",
    "    X_test_scaled = X_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b47349ad-ddbe-4803-abec-6114f2fd865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (8185, 451747)\n",
      "Validation set size: (1754, 451747)\n",
      "Test set size: (1755, 451747)\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "\n",
    "# Check the stratified split\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ad77-9006-41ec-9ff8-7b1a6d0753ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b485a-57b5-4d4f-af6b-7c5168286ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fbd7193-a7fe-4e11-92c0-dec7c94a9fc9",
   "metadata": {},
   "source": [
    "# Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a42ceb7a-386e-45e9-b4ac-97ed51a518be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable inside the script\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1344de36-d29d-432d-b4cb-8f4e31882834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims=[2048,1024,512], dropout_rate=0.2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = self.build_layers(input_dim, hidden_dims, dropout_rate)\n",
    "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)  # for mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)  # for log variance\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_hidden_dims = hidden_dims[::-1]\n",
    "        self.decoder_layers =self.build_layers(latent_dim, decoder_hidden_dims, dropout_rate)\n",
    "        self.fc_output = nn.Linear(hidden_dims[0], input_dim)\n",
    "        # self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        # self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def build_layers(self, input_dim, hidden_dims, dropout_rate):\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = h_dim\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder_layers(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Check if logvar has NaN or Inf values\n",
    "        if torch.isnan(logvar).any() or torch.isinf(logvar).any():\n",
    "            print(f\"NaN or Inf detected in logvar: logvar={logvar}\")\n",
    "        \n",
    "        # Clamp logvar to prevent extreme values\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        \n",
    "        # Calculate std from logvar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Check if std has NaN or Inf values\n",
    "        if torch.isnan(std).any() or torch.isinf(std).any():\n",
    "            print(f\"NaN or Inf detected in std computation: std={std}\")\n",
    "        \n",
    "        # Sample from the latent space\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Check if z has NaN or Inf values\n",
    "        if torch.isnan(z).any() or torch.isinf(z).any():\n",
    "            print(f\"NaN or Inf detected in z computation: z={z}\")\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_layers(z)\n",
    "        # h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc_output(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Method to get the latent embedding (the `z` vector) for an input.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)  # this is the embedding\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b62bd86a-8aff-492d-9745-fe7071e3446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Lightning(pl.LightningModule):\n",
    "    def __init__(self, input_dim=485577, latent_dim=128, hidden_dims=[2048, 1024, 512], dropout_rate=0.2, lr=1e-6):\n",
    "        super(VAE_Lightning, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()  # Save hyperparameters for checkpointing\n",
    "\n",
    "        self.model = VAE(input_dim, latent_dim, hidden_dims, dropout_rate)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.model.encode(x)\n",
    "        z = self.model.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def get_latent_embedding(self, x):\n",
    "        return self.model.get_latent_embedding(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)  # mask where values are not NaN\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 1: Create mask before replacing NaN values\n",
    "        mask = ~torch.isnan(x)\n",
    "\n",
    "        # Step 2: Replace NaNs with zero or another neutral value for forward pass\n",
    "        x_filled = replace_nan_with_mean(x)\n",
    "        # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Step 3: Pass through the model with filled values\n",
    "        z, mu, logvar = self.forward(x_filled)\n",
    "        x_hat, _, _ = self.model(x_filled)\n",
    "\n",
    "        # Step 4: Use the original x (with NaNs) and mask to calculate the loss\n",
    "        loss = self._vae_loss(x, x_hat, mu, logvar, mask)\n",
    "        print(f\"Validation loss: {loss.item()}\")\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "  \n",
    "\n",
    "    def _vae_loss(self, original_x, x_hat, mu, logvar, mask):\n",
    "        # Apply mask to ignore NaN values in the loss calculation\n",
    "        recon_loss = F.mse_loss(x_hat[mask], original_x[mask], reduction='mean')\n",
    "    \n",
    "        # Scale the KL divergence to balance the losses\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss / original_x.shape[0]  # Normalize by batch size or apply weighting\n",
    "    \n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7d2c96b-8ddb-4e73-a3c2-7743bf5e4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last training epoch from the logs\n",
    "        train_loss = trainer.callback_metrics.get('train_loss')\n",
    "        if train_loss is not None:\n",
    "            self.train_losses.append(train_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Access the loss for the last validation epoch from the logs\n",
    "        val_loss = trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "def replace_nan_with_mean(x):\n",
    "    # Calculate the column-wise mean, ignoring NaNs\n",
    "    col_mean = torch.nanmean(x, dim=0)\n",
    "    \n",
    "    # Find where NaN values are located\n",
    "    nan_mask = torch.isnan(x)\n",
    "    \n",
    "    # Replace NaNs with the corresponding column means\n",
    "    x[nan_mask] = torch.take(col_mean, nan_mask.nonzero()[:, 1])\n",
    "    \n",
    "    # Check if there are still NaN or Inf values\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in the input data after imputation!\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b9194-7cc9-4121-8b43-649794d892c3",
   "metadata": {},
   "source": [
    "## Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32624f45-a42b-40e1-a495-8eda0f11cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfd2ceef-19cf-48e1-81d8-ee67dd3c2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509808f-8e5e-4a7a-a9c4-acd96c9e87c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d648b65-faac-49ce-a8c2-ad4970ca0785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 1.9 B  | train\n",
      "---------------------------------------\n",
      "1.9 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 B     Total params\n",
      "7,425.805 Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Validation loss: 1.7909674644470215\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.86it/s]Validation loss: 1.2646608352661133\n",
      "Epoch 0:   0%|          | 0/512 [00:00<?, ?it/s]                           Training loss: 1.7406187057495117\n",
      "Epoch 0:   0%|          | 1/512 [00:01<10:24,  0.82it/s, v_num=0]Training loss: 1.8607882261276245\n",
      "Epoch 0:   0%|          | 2/512 [00:01<06:08,  1.38it/s, v_num=0]Training loss: 2.048858165740967\n",
      "Epoch 0:   1%|          | 3/512 [00:01<05:05,  1.66it/s, v_num=0]Training loss: 2.7867136001586914\n",
      "Epoch 0:   1%|          | 4/512 [00:02<04:38,  1.82it/s, v_num=0]Training loss: 1.8669546842575073\n",
      "Epoch 0:   1%|          | 5/512 [00:02<04:21,  1.94it/s, v_num=0]Training loss: 1.8472330570220947\n",
      "Epoch 0:   1%|          | 6/512 [00:02<04:10,  2.02it/s, v_num=0]Training loss: 1.7870906591415405\n",
      "Epoch 0:   1%|▏         | 7/512 [00:03<04:02,  2.08it/s, v_num=0]Training loss: 2.1885530948638916\n",
      "Epoch 0:   2%|▏         | 8/512 [00:03<03:56,  2.13it/s, v_num=0]Training loss: 1.453842043876648\n",
      "Epoch 0:   2%|▏         | 9/512 [00:04<03:51,  2.17it/s, v_num=0]Training loss: 2.0112228393554688\n",
      "Epoch 0:   2%|▏         | 10/512 [00:04<03:47,  2.21it/s, v_num=0]Training loss: 1.5796096324920654\n",
      "Epoch 0:   2%|▏         | 11/512 [00:04<03:44,  2.24it/s, v_num=0]Training loss: 1.4355809688568115\n",
      "Epoch 0:   2%|▏         | 12/512 [00:05<03:41,  2.26it/s, v_num=0]Training loss: 1.6012274026870728\n",
      "Epoch 0:   3%|▎         | 13/512 [00:05<03:38,  2.28it/s, v_num=0]Training loss: 2.064741849899292\n",
      "Epoch 0:   3%|▎         | 14/512 [00:06<03:36,  2.30it/s, v_num=0]Training loss: 1.0290948152542114\n",
      "Epoch 0:   3%|▎         | 15/512 [00:06<03:34,  2.32it/s, v_num=0]Training loss: 2.1081466674804688\n",
      "Epoch 0:   3%|▎         | 16/512 [00:06<03:32,  2.33it/s, v_num=0]Training loss: 2.107151746749878\n",
      "Epoch 0:   3%|▎         | 17/512 [00:07<03:31,  2.34it/s, v_num=0]Training loss: 2.8658857345581055\n",
      "Epoch 0:   4%|▎         | 18/512 [00:07<03:29,  2.35it/s, v_num=0]Training loss: 2.0449235439300537\n",
      "Epoch 0:   4%|▎         | 19/512 [00:08<03:28,  2.36it/s, v_num=0]Training loss: 1.8022617101669312\n",
      "Epoch 0:   4%|▍         | 20/512 [00:08<03:27,  2.37it/s, v_num=0]Training loss: 1.7641183137893677\n",
      "Epoch 0:   4%|▍         | 21/512 [00:08<03:26,  2.38it/s, v_num=0]Training loss: 1.3620108366012573\n",
      "Epoch 0:   4%|▍         | 22/512 [00:09<03:25,  2.39it/s, v_num=0]Training loss: 1.0724457502365112\n",
      "Epoch 0:   4%|▍         | 23/512 [00:09<03:24,  2.40it/s, v_num=0]Training loss: 1.187183141708374\n",
      "Epoch 0:   5%|▍         | 24/512 [00:09<03:23,  2.40it/s, v_num=0]Training loss: 1.750023603439331\n",
      "Epoch 0:   5%|▍         | 25/512 [00:10<03:22,  2.41it/s, v_num=0]Training loss: 1.2803449630737305\n",
      "Epoch 0:   5%|▌         | 26/512 [00:10<03:21,  2.42it/s, v_num=0]Training loss: 1.1786155700683594\n",
      "Epoch 0:   5%|▌         | 27/512 [00:11<03:20,  2.42it/s, v_num=0]Training loss: 1.6708149909973145\n",
      "Epoch 0:   5%|▌         | 28/512 [00:11<03:19,  2.42it/s, v_num=0]Training loss: 1.4317495822906494\n",
      "Epoch 0:   6%|▌         | 29/512 [00:11<03:18,  2.43it/s, v_num=0]Training loss: 1.4132366180419922\n",
      "Epoch 0:   6%|▌         | 30/512 [00:12<03:18,  2.43it/s, v_num=0]Training loss: 1.2850724458694458\n",
      "Epoch 0:   6%|▌         | 31/512 [00:12<03:17,  2.44it/s, v_num=0]Training loss: 1.9911456108093262\n",
      "Epoch 0:   6%|▋         | 32/512 [00:13<03:16,  2.44it/s, v_num=0]Training loss: 1.5117151737213135\n",
      "Epoch 0:   6%|▋         | 33/512 [00:13<03:15,  2.45it/s, v_num=0]Training loss: 1.5419834852218628\n",
      "Epoch 0:   7%|▋         | 34/512 [00:13<03:15,  2.45it/s, v_num=0]Training loss: 1.7655102014541626\n",
      "Epoch 0:   7%|▋         | 35/512 [00:14<03:14,  2.45it/s, v_num=0]Training loss: 1.896330714225769\n",
      "Epoch 0:   7%|▋         | 36/512 [00:14<03:13,  2.46it/s, v_num=0]Training loss: 1.8347469568252563\n",
      "Epoch 0:   7%|▋         | 37/512 [00:15<03:13,  2.46it/s, v_num=0]Training loss: 1.5671249628067017\n",
      "Epoch 0:   7%|▋         | 38/512 [00:15<03:12,  2.46it/s, v_num=0]Training loss: 1.8230971097946167\n",
      "Epoch 0:   8%|▊         | 39/512 [00:15<03:12,  2.46it/s, v_num=0]Training loss: 1.366502285003662\n",
      "Epoch 0:   8%|▊         | 40/512 [00:16<03:11,  2.47it/s, v_num=0]Training loss: 1.57558012008667\n",
      "Epoch 0:   8%|▊         | 41/512 [00:16<03:10,  2.47it/s, v_num=0]Training loss: 1.1818413734436035\n",
      "Epoch 0:   8%|▊         | 42/512 [00:17<03:10,  2.47it/s, v_num=0]Training loss: 1.4769316911697388\n",
      "Epoch 0:   8%|▊         | 43/512 [00:17<03:09,  2.47it/s, v_num=0]Training loss: 1.302053451538086\n",
      "Epoch 0:   9%|▊         | 44/512 [00:17<03:09,  2.47it/s, v_num=0]Training loss: 1.33256196975708\n",
      "Epoch 0:   9%|▉         | 45/512 [00:18<03:08,  2.48it/s, v_num=0]Training loss: 1.1128871440887451\n",
      "Epoch 0:   9%|▉         | 46/512 [00:18<03:08,  2.48it/s, v_num=0]Training loss: 1.6678920984268188\n",
      "Epoch 0:   9%|▉         | 47/512 [00:18<03:07,  2.48it/s, v_num=0]Training loss: 1.4767192602157593\n",
      "Epoch 0:   9%|▉         | 48/512 [00:19<03:06,  2.48it/s, v_num=0]Training loss: 1.5420643091201782\n",
      "Epoch 0:  10%|▉         | 49/512 [00:19<03:06,  2.48it/s, v_num=0]Training loss: 1.3061716556549072\n",
      "Epoch 0:  10%|▉         | 50/512 [00:20<03:05,  2.49it/s, v_num=0]Training loss: 1.952255368232727\n",
      "Epoch 0:  10%|▉         | 51/512 [00:20<03:05,  2.49it/s, v_num=0]Training loss: 2.0211877822875977\n",
      "Epoch 0:  10%|█         | 52/512 [00:20<03:04,  2.49it/s, v_num=0]Training loss: 1.6556936502456665\n",
      "Epoch 0:  10%|█         | 53/512 [00:21<03:04,  2.49it/s, v_num=0]Training loss: 1.5210471153259277\n",
      "Epoch 0:  11%|█         | 54/512 [00:21<03:03,  2.49it/s, v_num=0]Training loss: 1.5651938915252686\n",
      "Epoch 0:  11%|█         | 55/512 [00:22<03:03,  2.49it/s, v_num=0]Training loss: 1.2719297409057617\n",
      "Epoch 0:  11%|█         | 56/512 [00:22<03:02,  2.49it/s, v_num=0]Training loss: 1.415602445602417\n",
      "Epoch 0:  11%|█         | 57/512 [00:22<03:02,  2.49it/s, v_num=0]Training loss: 1.1992313861846924\n",
      "Epoch 0:  11%|█▏        | 58/512 [00:23<03:01,  2.50it/s, v_num=0]Training loss: 1.213016152381897\n",
      "Epoch 0:  12%|█▏        | 59/512 [00:23<03:01,  2.50it/s, v_num=0]Training loss: 1.2480019330978394\n",
      "Epoch 0:  12%|█▏        | 60/512 [00:24<03:00,  2.50it/s, v_num=0]Training loss: 1.322170615196228\n",
      "Epoch 0:  12%|█▏        | 61/512 [00:24<03:00,  2.50it/s, v_num=0]Training loss: 1.3362042903900146\n",
      "Epoch 0:  12%|█▏        | 62/512 [00:24<02:59,  2.50it/s, v_num=0]Training loss: 1.4582247734069824\n",
      "Epoch 0:  12%|█▏        | 63/512 [00:25<02:59,  2.50it/s, v_num=0]Training loss: 1.4323214292526245\n",
      "Epoch 0:  12%|█▎        | 64/512 [00:25<02:59,  2.50it/s, v_num=0]Training loss: 1.37335205078125\n",
      "Epoch 0:  13%|█▎        | 65/512 [00:25<02:58,  2.50it/s, v_num=0]Training loss: 1.461761474609375\n",
      "Epoch 0:  13%|█▎        | 66/512 [00:26<02:58,  2.50it/s, v_num=0]Training loss: 1.4682517051696777\n",
      "Epoch 0:  13%|█▎        | 67/512 [00:26<02:57,  2.51it/s, v_num=0]Training loss: 1.9666776657104492\n",
      "Epoch 0:  13%|█▎        | 68/512 [00:27<02:57,  2.51it/s, v_num=0]Training loss: 1.155247688293457\n",
      "Epoch 0:  13%|█▎        | 69/512 [00:27<02:56,  2.51it/s, v_num=0]Training loss: 1.181624412536621\n",
      "Epoch 0:  14%|█▎        | 70/512 [00:27<02:56,  2.51it/s, v_num=0]Training loss: 1.2450741529464722\n",
      "Epoch 0:  14%|█▍        | 71/512 [00:28<02:55,  2.51it/s, v_num=0]Training loss: 1.6161973476409912\n",
      "Epoch 0:  14%|█▍        | 72/512 [00:28<02:55,  2.51it/s, v_num=0]Training loss: 1.1415116786956787\n",
      "Epoch 0:  14%|█▍        | 73/512 [00:29<02:54,  2.51it/s, v_num=0]Training loss: 2.2283713817596436\n",
      "Epoch 0:  14%|█▍        | 74/512 [00:29<02:54,  2.51it/s, v_num=0]Training loss: 1.4153389930725098\n",
      "Epoch 0:  15%|█▍        | 75/512 [00:29<02:54,  2.51it/s, v_num=0]Training loss: 1.792510747909546\n",
      "Epoch 0:  15%|█▍        | 76/512 [00:30<02:53,  2.51it/s, v_num=0]Training loss: 1.738131046295166\n",
      "Epoch 0:  15%|█▌        | 77/512 [00:30<02:53,  2.51it/s, v_num=0]Training loss: 0.8663198947906494\n",
      "Epoch 0:  15%|█▌        | 78/512 [00:31<02:52,  2.51it/s, v_num=0]Training loss: 1.1369645595550537\n",
      "Epoch 0:  15%|█▌        | 79/512 [00:31<02:52,  2.51it/s, v_num=0]Training loss: 1.3960299491882324\n",
      "Epoch 0:  16%|█▌        | 80/512 [00:31<02:51,  2.51it/s, v_num=0]Training loss: 1.1816701889038086\n",
      "Epoch 0:  16%|█▌        | 81/512 [00:32<02:51,  2.52it/s, v_num=0]Training loss: 1.8756873607635498\n",
      "Epoch 0:  16%|█▌        | 82/512 [00:32<02:50,  2.52it/s, v_num=0]Training loss: 1.4242134094238281\n",
      "Epoch 0:  16%|█▌        | 83/512 [00:32<02:50,  2.52it/s, v_num=0]Training loss: 1.5318400859832764\n",
      "Epoch 0:  16%|█▋        | 84/512 [00:33<02:50,  2.52it/s, v_num=0]Training loss: 1.3129563331604004\n",
      "Epoch 0:  17%|█▋        | 85/512 [00:33<02:49,  2.52it/s, v_num=0]Training loss: 1.4318046569824219\n",
      "Epoch 0:  17%|█▋        | 86/512 [00:34<02:49,  2.52it/s, v_num=0]Training loss: 1.5083285570144653\n",
      "Epoch 0:  17%|█▋        | 87/512 [00:34<02:48,  2.52it/s, v_num=0]Training loss: 1.086432695388794\n",
      "Epoch 0:  17%|█▋        | 88/512 [00:34<02:48,  2.52it/s, v_num=0]Training loss: 1.6643660068511963\n",
      "Epoch 0:  17%|█▋        | 89/512 [00:35<02:47,  2.52it/s, v_num=0]Training loss: 1.1065596342086792\n",
      "Epoch 0:  18%|█▊        | 90/512 [00:35<02:47,  2.52it/s, v_num=0]Training loss: 1.631380558013916\n",
      "Epoch 0:  18%|█▊        | 91/512 [00:36<02:47,  2.52it/s, v_num=0]Training loss: 1.362331748008728\n",
      "Epoch 0:  18%|█▊        | 92/512 [00:36<02:46,  2.52it/s, v_num=0]Training loss: 1.2490997314453125\n",
      "Epoch 0:  18%|█▊        | 93/512 [00:36<02:46,  2.52it/s, v_num=0]Training loss: 1.0691300630569458\n",
      "Epoch 0:  18%|█▊        | 94/512 [00:37<02:45,  2.52it/s, v_num=0]Training loss: 1.5742621421813965\n",
      "Epoch 0:  19%|█▊        | 95/512 [00:37<02:45,  2.52it/s, v_num=0]Training loss: 1.635905146598816\n",
      "Epoch 0:  19%|█▉        | 96/512 [00:38<02:44,  2.52it/s, v_num=0]Training loss: 1.3995376825332642\n",
      "Epoch 0:  19%|█▉        | 97/512 [00:38<02:44,  2.52it/s, v_num=0]Training loss: 2.1613101959228516\n",
      "Epoch 0:  19%|█▉        | 98/512 [00:38<02:44,  2.52it/s, v_num=0]Training loss: 1.2039082050323486\n",
      "Epoch 0:  19%|█▉        | 99/512 [00:39<02:43,  2.52it/s, v_num=0]Training loss: 1.1578096151351929\n",
      "Epoch 0:  20%|█▉        | 100/512 [00:39<02:43,  2.52it/s, v_num=0]Training loss: 1.1689373254776\n",
      "Epoch 0:  20%|█▉        | 101/512 [00:40<02:42,  2.52it/s, v_num=0]Training loss: 1.2282686233520508\n",
      "Epoch 0:  20%|█▉        | 102/512 [00:40<02:42,  2.52it/s, v_num=0]Training loss: 1.9062752723693848\n",
      "Epoch 0:  20%|██        | 103/512 [00:40<02:41,  2.53it/s, v_num=0]Training loss: 1.4209537506103516\n",
      "Epoch 0:  20%|██        | 104/512 [00:41<02:41,  2.53it/s, v_num=0]Training loss: 1.768606424331665\n",
      "Epoch 0:  21%|██        | 105/512 [00:41<02:41,  2.53it/s, v_num=0]Training loss: 1.5835199356079102\n",
      "Epoch 0:  21%|██        | 106/512 [00:41<02:40,  2.53it/s, v_num=0]Training loss: 1.1795899868011475\n",
      "Epoch 0:  21%|██        | 107/512 [00:42<02:40,  2.53it/s, v_num=0]Training loss: 1.178619384765625\n",
      "Epoch 0:  21%|██        | 108/512 [00:42<02:39,  2.53it/s, v_num=0]Training loss: 1.2150139808654785\n",
      "Epoch 0:  21%|██▏       | 109/512 [00:43<02:39,  2.53it/s, v_num=0]Training loss: 1.7450443506240845\n",
      "Epoch 0:  21%|██▏       | 110/512 [00:43<02:39,  2.53it/s, v_num=0]Training loss: 1.5956608057022095\n",
      "Epoch 0:  22%|██▏       | 111/512 [00:43<02:38,  2.53it/s, v_num=0]Training loss: 1.9604213237762451\n",
      "Epoch 0:  22%|██▏       | 112/512 [00:44<02:38,  2.53it/s, v_num=0]Training loss: 1.6608848571777344\n",
      "Epoch 0:  22%|██▏       | 113/512 [00:44<02:37,  2.53it/s, v_num=0]Training loss: 1.3136260509490967\n",
      "Epoch 0:  22%|██▏       | 114/512 [00:45<02:37,  2.53it/s, v_num=0]Training loss: 1.6284059286117554\n",
      "Epoch 0:  22%|██▏       | 115/512 [00:45<02:36,  2.53it/s, v_num=0]Training loss: 2.130201816558838\n",
      "Epoch 0:  23%|██▎       | 116/512 [00:45<02:36,  2.53it/s, v_num=0]Training loss: 1.1121867895126343\n",
      "Epoch 0:  23%|██▎       | 117/512 [00:46<02:36,  2.53it/s, v_num=0]Training loss: 0.9092350006103516\n",
      "Epoch 0:  23%|██▎       | 118/512 [00:46<02:35,  2.53it/s, v_num=0]Training loss: 2.0197956562042236\n",
      "Epoch 0:  23%|██▎       | 119/512 [00:47<02:35,  2.53it/s, v_num=0]Training loss: 1.3171041011810303\n",
      "Epoch 0:  23%|██▎       | 120/512 [00:47<02:34,  2.53it/s, v_num=0]Training loss: 1.5527912378311157\n",
      "Epoch 0:  24%|██▎       | 121/512 [00:47<02:34,  2.53it/s, v_num=0]Training loss: 1.5846738815307617\n",
      "Epoch 0:  24%|██▍       | 122/512 [00:48<02:34,  2.53it/s, v_num=0]Training loss: 1.3386174440383911\n",
      "Epoch 0:  24%|██▍       | 123/512 [00:48<02:33,  2.53it/s, v_num=0]Training loss: 1.0828357934951782\n",
      "Epoch 0:  24%|██▍       | 124/512 [00:48<02:33,  2.53it/s, v_num=0]Training loss: 1.406743049621582\n",
      "Epoch 0:  24%|██▍       | 125/512 [00:49<02:32,  2.53it/s, v_num=0]Training loss: 1.3294751644134521\n",
      "Epoch 0:  25%|██▍       | 126/512 [00:49<02:32,  2.53it/s, v_num=0]Training loss: 2.2321393489837646\n",
      "Epoch 0:  25%|██▍       | 127/512 [00:50<02:31,  2.53it/s, v_num=0]Training loss: 1.5536651611328125\n",
      "Epoch 0:  25%|██▌       | 128/512 [00:50<02:31,  2.53it/s, v_num=0]Training loss: 1.436701774597168\n",
      "Epoch 0:  25%|██▌       | 129/512 [00:50<02:31,  2.53it/s, v_num=0]Training loss: 1.2586487531661987\n",
      "Epoch 0:  25%|██▌       | 130/512 [00:51<02:30,  2.53it/s, v_num=0]Training loss: 1.4322595596313477\n",
      "Epoch 0:  26%|██▌       | 131/512 [00:51<02:30,  2.53it/s, v_num=0]Training loss: 1.6468944549560547\n",
      "Epoch 0:  26%|██▌       | 132/512 [00:52<02:29,  2.53it/s, v_num=0]Training loss: 1.4839186668395996\n",
      "Epoch 0:  26%|██▌       | 133/512 [00:52<02:29,  2.53it/s, v_num=0]Training loss: 1.5744025707244873\n",
      "Epoch 0:  26%|██▌       | 134/512 [00:52<02:29,  2.53it/s, v_num=0]Training loss: 1.9179835319519043\n",
      "Epoch 0:  26%|██▋       | 135/512 [00:53<02:28,  2.53it/s, v_num=0]Training loss: 1.4487801790237427\n",
      "Epoch 0:  27%|██▋       | 136/512 [00:53<02:28,  2.54it/s, v_num=0]Training loss: 1.5999493598937988\n",
      "Epoch 0:  27%|██▋       | 137/512 [00:54<02:27,  2.54it/s, v_num=0]Training loss: 1.7857019901275635\n",
      "Epoch 0:  27%|██▋       | 138/512 [00:54<02:27,  2.54it/s, v_num=0]Training loss: 1.8289005756378174\n",
      "Epoch 0:  27%|██▋       | 139/512 [00:54<02:27,  2.54it/s, v_num=0]Training loss: 1.8060929775238037\n",
      "Epoch 0:  27%|██▋       | 140/512 [00:55<02:26,  2.54it/s, v_num=0]Training loss: 1.4513517618179321\n",
      "Epoch 0:  28%|██▊       | 141/512 [00:55<02:26,  2.54it/s, v_num=0]Training loss: 1.4826452732086182\n",
      "Epoch 0:  28%|██▊       | 142/512 [00:55<02:25,  2.54it/s, v_num=0]Training loss: 1.6148666143417358\n",
      "Epoch 0:  28%|██▊       | 143/512 [00:56<02:25,  2.54it/s, v_num=0]Training loss: 1.9224565029144287\n",
      "Epoch 0:  28%|██▊       | 144/512 [00:56<02:25,  2.54it/s, v_num=0]Training loss: 1.8670426607131958\n",
      "Epoch 0:  28%|██▊       | 145/512 [00:57<02:24,  2.54it/s, v_num=0]Training loss: 1.110487937927246\n",
      "Epoch 0:  29%|██▊       | 146/512 [00:57<02:24,  2.54it/s, v_num=0]Training loss: 1.46923828125\n",
      "Epoch 0:  29%|██▊       | 147/512 [00:57<02:23,  2.54it/s, v_num=0]Training loss: 1.1228902339935303\n",
      "Epoch 0:  29%|██▉       | 148/512 [00:58<02:23,  2.54it/s, v_num=0]Training loss: 1.6980406045913696\n",
      "Epoch 0:  29%|██▉       | 149/512 [00:58<02:23,  2.54it/s, v_num=0]Training loss: 0.8825507760047913\n",
      "Epoch 0:  29%|██▉       | 150/512 [00:59<02:22,  2.54it/s, v_num=0]Training loss: 1.4777017831802368\n",
      "Epoch 0:  29%|██▉       | 151/512 [00:59<02:22,  2.54it/s, v_num=0]Training loss: 1.4472002983093262\n",
      "Epoch 0:  30%|██▉       | 152/512 [00:59<02:21,  2.54it/s, v_num=0]Training loss: 1.5219621658325195\n",
      "Epoch 0:  30%|██▉       | 153/512 [01:00<02:21,  2.54it/s, v_num=0]Training loss: 1.5396373271942139\n",
      "Epoch 0:  30%|███       | 154/512 [01:00<02:21,  2.54it/s, v_num=0]Training loss: 2.172417640686035\n",
      "Epoch 0:  30%|███       | 155/512 [01:01<02:20,  2.54it/s, v_num=0]Training loss: 1.6158955097198486\n",
      "Epoch 0:  30%|███       | 156/512 [01:01<02:20,  2.54it/s, v_num=0]Training loss: 1.3800065517425537\n",
      "Epoch 0:  31%|███       | 157/512 [01:01<02:19,  2.54it/s, v_num=0]Training loss: 0.9344748854637146\n",
      "Epoch 0:  31%|███       | 158/512 [01:02<02:19,  2.54it/s, v_num=0]Training loss: 1.413580298423767\n",
      "Epoch 0:  31%|███       | 159/512 [01:02<02:18,  2.54it/s, v_num=0]Training loss: 1.0109175443649292\n",
      "Epoch 0:  31%|███▏      | 160/512 [01:02<02:18,  2.54it/s, v_num=0]Training loss: 1.7990739345550537\n",
      "Epoch 0:  31%|███▏      | 161/512 [01:03<02:18,  2.54it/s, v_num=0]Training loss: 1.216428279876709\n",
      "Epoch 0:  32%|███▏      | 162/512 [01:03<02:17,  2.54it/s, v_num=0]Training loss: 1.2912778854370117\n",
      "Epoch 0:  32%|███▏      | 163/512 [01:04<02:17,  2.54it/s, v_num=0]Training loss: 1.3535606861114502\n",
      "Epoch 0:  32%|███▏      | 164/512 [01:04<02:16,  2.54it/s, v_num=0]Training loss: 1.4548561573028564\n",
      "Epoch 0:  32%|███▏      | 165/512 [01:04<02:16,  2.54it/s, v_num=0]Training loss: 1.4324469566345215\n",
      "Epoch 0:  32%|███▏      | 166/512 [01:05<02:16,  2.54it/s, v_num=0]Training loss: 2.0291922092437744\n",
      "Epoch 0:  33%|███▎      | 167/512 [01:05<02:15,  2.54it/s, v_num=0]Training loss: 1.3259823322296143\n",
      "Epoch 0:  33%|███▎      | 168/512 [01:06<02:15,  2.54it/s, v_num=0]Training loss: 1.1819517612457275\n",
      "Epoch 0:  33%|███▎      | 169/512 [01:06<02:14,  2.54it/s, v_num=0]Training loss: 1.2805135250091553\n",
      "Epoch 0:  33%|███▎      | 170/512 [01:06<02:14,  2.54it/s, v_num=0]Training loss: 1.6208044290542603\n",
      "Epoch 0:  33%|███▎      | 171/512 [01:07<02:14,  2.54it/s, v_num=0]Training loss: 1.6399825811386108\n",
      "Epoch 0:  34%|███▎      | 172/512 [01:07<02:13,  2.54it/s, v_num=0]Training loss: 1.4036121368408203\n",
      "Epoch 0:  34%|███▍      | 173/512 [01:08<02:13,  2.54it/s, v_num=0]Training loss: 1.5335171222686768\n",
      "Epoch 0:  34%|███▍      | 174/512 [01:08<02:12,  2.54it/s, v_num=0]Training loss: 1.4912540912628174\n",
      "Epoch 0:  34%|███▍      | 175/512 [01:08<02:12,  2.54it/s, v_num=0]Training loss: 1.237351655960083\n",
      "Epoch 0:  34%|███▍      | 176/512 [01:09<02:12,  2.54it/s, v_num=0]Training loss: 1.9648473262786865\n",
      "Epoch 0:  35%|███▍      | 177/512 [01:09<02:11,  2.54it/s, v_num=0]Training loss: 1.374228835105896\n",
      "Epoch 0:  35%|███▍      | 178/512 [01:10<02:11,  2.54it/s, v_num=0]Training loss: 1.5746434926986694\n",
      "Epoch 0:  35%|███▍      | 179/512 [01:10<02:10,  2.54it/s, v_num=0]Training loss: 1.5354739427566528\n",
      "Epoch 0:  35%|███▌      | 180/512 [01:10<02:10,  2.54it/s, v_num=0]Training loss: 2.1058614253997803\n",
      "Epoch 0:  35%|███▌      | 181/512 [01:11<02:10,  2.54it/s, v_num=0]Training loss: 0.9685822129249573\n",
      "Epoch 0:  36%|███▌      | 182/512 [01:11<02:09,  2.54it/s, v_num=0]Training loss: 1.6351505517959595\n",
      "Epoch 0:  36%|███▌      | 183/512 [01:11<02:09,  2.54it/s, v_num=0]Training loss: 2.060054302215576\n",
      "Epoch 0:  36%|███▌      | 184/512 [01:12<02:08,  2.54it/s, v_num=0]Training loss: 1.3894274234771729\n",
      "Epoch 0:  36%|███▌      | 185/512 [01:12<02:08,  2.54it/s, v_num=0]Training loss: 1.7223259210586548\n",
      "Epoch 0:  36%|███▋      | 186/512 [01:13<02:08,  2.54it/s, v_num=0]Training loss: 1.6248944997787476\n",
      "Epoch 0:  37%|███▋      | 187/512 [01:13<02:07,  2.54it/s, v_num=0]Training loss: 1.7115885019302368\n",
      "Epoch 0:  37%|███▋      | 188/512 [01:13<02:07,  2.54it/s, v_num=0]Training loss: 1.5067075490951538\n",
      "Epoch 0:  37%|███▋      | 189/512 [01:14<02:06,  2.54it/s, v_num=0]Training loss: 1.6187424659729004\n",
      "Epoch 0:  37%|███▋      | 190/512 [01:14<02:06,  2.54it/s, v_num=0]Training loss: 1.4721325635910034\n",
      "Epoch 0:  37%|███▋      | 191/512 [01:15<02:06,  2.54it/s, v_num=0]Training loss: 1.3577502965927124\n",
      "Epoch 0:  38%|███▊      | 192/512 [01:15<02:05,  2.54it/s, v_num=0]Training loss: 1.3743958473205566\n",
      "Epoch 0:  38%|███▊      | 193/512 [01:15<02:05,  2.54it/s, v_num=0]Training loss: 2.527984857559204\n",
      "Epoch 0:  38%|███▊      | 194/512 [01:16<02:04,  2.54it/s, v_num=0]Training loss: 1.0632115602493286\n",
      "Epoch 0:  38%|███▊      | 195/512 [01:16<02:04,  2.54it/s, v_num=0]Training loss: 1.1607170104980469\n",
      "Epoch 0:  38%|███▊      | 196/512 [01:17<02:04,  2.54it/s, v_num=0]Training loss: 1.590741753578186\n",
      "Epoch 0:  38%|███▊      | 197/512 [01:17<02:03,  2.54it/s, v_num=0]Training loss: 1.2742105722427368\n",
      "Epoch 0:  39%|███▊      | 198/512 [01:17<02:03,  2.54it/s, v_num=0]Training loss: 1.16494882106781\n",
      "Epoch 0:  39%|███▉      | 199/512 [01:18<02:02,  2.55it/s, v_num=0]Training loss: 1.4523791074752808\n",
      "Epoch 0:  39%|███▉      | 200/512 [01:18<02:02,  2.55it/s, v_num=0]Training loss: 1.8151466846466064\n",
      "Epoch 0:  39%|███▉      | 201/512 [01:18<02:02,  2.55it/s, v_num=0]Training loss: 1.2074071168899536\n",
      "Epoch 0:  39%|███▉      | 202/512 [01:19<02:01,  2.55it/s, v_num=0]Training loss: 1.2359882593154907\n",
      "Epoch 0:  40%|███▉      | 203/512 [01:19<02:01,  2.55it/s, v_num=0]Training loss: 1.148984432220459\n",
      "Epoch 0:  40%|███▉      | 204/512 [01:20<02:00,  2.55it/s, v_num=0]Training loss: 0.9143669605255127\n",
      "Epoch 0:  40%|████      | 205/512 [01:20<02:00,  2.55it/s, v_num=0]Training loss: 1.5824096202850342\n",
      "Epoch 0:  40%|████      | 206/512 [01:20<02:00,  2.55it/s, v_num=0]Training loss: 1.1698063611984253\n",
      "Epoch 0:  40%|████      | 207/512 [01:21<01:59,  2.55it/s, v_num=0]Training loss: 1.5915570259094238\n",
      "Epoch 0:  41%|████      | 208/512 [01:21<01:59,  2.55it/s, v_num=0]Training loss: 1.4458949565887451\n",
      "Epoch 0:  41%|████      | 209/512 [01:22<01:59,  2.55it/s, v_num=0]Training loss: 1.0687376260757446\n",
      "Epoch 0:  41%|████      | 210/512 [01:22<01:58,  2.55it/s, v_num=0]Training loss: 1.647686243057251\n",
      "Epoch 0:  41%|████      | 211/512 [01:22<01:58,  2.55it/s, v_num=0]Training loss: 1.2989808320999146\n",
      "Epoch 0:  41%|████▏     | 212/512 [01:23<01:57,  2.55it/s, v_num=0]Training loss: 1.260741114616394\n",
      "Epoch 0:  42%|████▏     | 213/512 [01:23<01:57,  2.55it/s, v_num=0]Training loss: 1.4977096319198608\n",
      "Epoch 0:  42%|████▏     | 214/512 [01:24<01:57,  2.55it/s, v_num=0]Training loss: 4.575191497802734\n",
      "Epoch 0:  42%|████▏     | 215/512 [01:24<01:56,  2.55it/s, v_num=0]Training loss: 1.5455379486083984\n",
      "Epoch 0:  42%|████▏     | 216/512 [01:24<01:56,  2.55it/s, v_num=0]Training loss: 1.6431884765625\n",
      "Epoch 0:  42%|████▏     | 217/512 [01:25<01:55,  2.55it/s, v_num=0]Training loss: 1.913280725479126\n",
      "Epoch 0:  43%|████▎     | 218/512 [01:25<01:55,  2.55it/s, v_num=0]Training loss: 1.760814905166626\n",
      "Epoch 0:  43%|████▎     | 219/512 [01:25<01:55,  2.55it/s, v_num=0]Training loss: 1.0266225337982178\n",
      "Epoch 0:  43%|████▎     | 220/512 [01:26<01:54,  2.55it/s, v_num=0]Training loss: 1.6809680461883545\n",
      "Epoch 0:  43%|████▎     | 221/512 [01:26<01:54,  2.55it/s, v_num=0]Training loss: 1.5074793100357056\n",
      "Epoch 0:  43%|████▎     | 222/512 [01:27<01:53,  2.55it/s, v_num=0]Training loss: 1.2896440029144287\n",
      "Epoch 0:  44%|████▎     | 223/512 [01:27<01:53,  2.55it/s, v_num=0]Training loss: 1.9072951078414917\n",
      "Epoch 0:  44%|████▍     | 224/512 [01:27<01:53,  2.55it/s, v_num=0]Training loss: 1.7241610288619995\n",
      "Epoch 0:  44%|████▍     | 225/512 [01:28<01:52,  2.55it/s, v_num=0]Training loss: 2.0878453254699707\n",
      "Epoch 0:  44%|████▍     | 226/512 [01:28<01:52,  2.55it/s, v_num=0]Training loss: 1.5483227968215942\n",
      "Epoch 0:  44%|████▍     | 227/512 [01:29<01:51,  2.55it/s, v_num=0]Training loss: 1.565274953842163\n",
      "Epoch 0:  45%|████▍     | 228/512 [01:29<01:51,  2.55it/s, v_num=0]Training loss: 0.9688623547554016\n",
      "Epoch 0:  45%|████▍     | 229/512 [01:29<01:51,  2.55it/s, v_num=0]Training loss: 1.5118324756622314\n",
      "Epoch 0:  45%|████▍     | 230/512 [01:30<01:50,  2.55it/s, v_num=0]Training loss: 1.6941642761230469\n",
      "Epoch 0:  45%|████▌     | 231/512 [01:30<01:50,  2.55it/s, v_num=0]Training loss: 1.9815952777862549\n",
      "Epoch 0:  45%|████▌     | 232/512 [01:31<01:49,  2.55it/s, v_num=0]Training loss: 1.2623144388198853\n",
      "Epoch 0:  46%|████▌     | 233/512 [01:31<01:49,  2.55it/s, v_num=0]Training loss: 1.3784089088439941\n",
      "Epoch 0:  46%|████▌     | 234/512 [01:31<01:49,  2.55it/s, v_num=0]Training loss: 1.205169439315796\n",
      "Epoch 0:  46%|████▌     | 235/512 [01:32<01:48,  2.55it/s, v_num=0]Training loss: 1.3684746026992798\n",
      "Epoch 0:  46%|████▌     | 236/512 [01:32<01:48,  2.55it/s, v_num=0]Training loss: 1.2185695171356201\n",
      "Epoch 0:  46%|████▋     | 237/512 [01:32<01:47,  2.55it/s, v_num=0]Training loss: 1.015371322631836\n",
      "Epoch 0:  46%|████▋     | 238/512 [01:33<01:47,  2.55it/s, v_num=0]Training loss: 1.126755952835083\n",
      "Epoch 0:  47%|████▋     | 239/512 [01:33<01:47,  2.55it/s, v_num=0]Training loss: 1.795047640800476\n",
      "Epoch 0:  47%|████▋     | 240/512 [01:34<01:46,  2.55it/s, v_num=0]Training loss: 1.3077821731567383\n",
      "Epoch 0:  47%|████▋     | 241/512 [01:34<01:46,  2.55it/s, v_num=0]Training loss: 1.2634128332138062\n",
      "Epoch 0:  47%|████▋     | 242/512 [01:34<01:45,  2.55it/s, v_num=0]Training loss: 2.0511555671691895\n",
      "Epoch 0:  47%|████▋     | 243/512 [01:35<01:45,  2.55it/s, v_num=0]Training loss: 1.1432403326034546\n",
      "Epoch 0:  48%|████▊     | 244/512 [01:35<01:45,  2.55it/s, v_num=0]Training loss: 16.86851692199707\n",
      "Epoch 0:  48%|████▊     | 245/512 [01:36<01:44,  2.55it/s, v_num=0]Training loss: 1.726806402206421\n",
      "Epoch 0:  48%|████▊     | 246/512 [01:36<01:44,  2.55it/s, v_num=0]Training loss: 1.2306572198867798\n",
      "Epoch 0:  48%|████▊     | 247/512 [01:36<01:43,  2.55it/s, v_num=0]Training loss: 1.8699811697006226\n",
      "Epoch 0:  48%|████▊     | 248/512 [01:37<01:43,  2.55it/s, v_num=0]Training loss: 1.116835117340088\n",
      "Epoch 0:  49%|████▊     | 249/512 [01:37<01:43,  2.55it/s, v_num=0]Training loss: 1.2970080375671387\n",
      "Epoch 0:  49%|████▉     | 250/512 [01:38<01:42,  2.55it/s, v_num=0]Training loss: 1.1689808368682861\n",
      "Epoch 0:  49%|████▉     | 251/512 [01:38<01:42,  2.55it/s, v_num=0]Training loss: 1.6350398063659668\n",
      "Epoch 0:  49%|████▉     | 252/512 [01:38<01:41,  2.55it/s, v_num=0]Training loss: 2.0482850074768066\n",
      "Epoch 0:  49%|████▉     | 253/512 [01:39<01:41,  2.55it/s, v_num=0]Training loss: 0.8801330327987671\n",
      "Epoch 0:  50%|████▉     | 254/512 [01:39<01:41,  2.55it/s, v_num=0]Training loss: 1.6605567932128906\n",
      "Epoch 0:  50%|████▉     | 255/512 [01:40<01:40,  2.55it/s, v_num=0]Training loss: 1.831013560295105\n",
      "Epoch 0:  50%|█████     | 256/512 [01:40<01:40,  2.55it/s, v_num=0]Training loss: 1.5415318012237549\n",
      "Epoch 0:  50%|█████     | 257/512 [01:40<01:39,  2.55it/s, v_num=0]Training loss: 1.5228207111358643\n",
      "Epoch 0:  50%|█████     | 258/512 [01:41<01:39,  2.55it/s, v_num=0]Training loss: 1.2496459484100342\n",
      "Epoch 0:  51%|█████     | 259/512 [01:41<01:39,  2.55it/s, v_num=0]Training loss: 1.8085416555404663\n",
      "Epoch 0:  51%|█████     | 260/512 [01:41<01:38,  2.55it/s, v_num=0]Training loss: 1.2037479877471924\n",
      "Epoch 0:  51%|█████     | 261/512 [01:42<01:38,  2.55it/s, v_num=0]Training loss: 1.4450544118881226\n",
      "Epoch 0:  51%|█████     | 262/512 [01:42<01:38,  2.55it/s, v_num=0]Training loss: 1.3035433292388916\n",
      "Epoch 0:  51%|█████▏    | 263/512 [01:43<01:37,  2.55it/s, v_num=0]Training loss: 1.192915916442871\n",
      "Epoch 0:  52%|█████▏    | 264/512 [01:43<01:37,  2.55it/s, v_num=0]Training loss: 0.9719254374504089\n",
      "Epoch 0:  52%|█████▏    | 265/512 [01:43<01:36,  2.55it/s, v_num=0]Training loss: 1.2763566970825195\n",
      "Epoch 0:  52%|█████▏    | 266/512 [01:44<01:36,  2.55it/s, v_num=0]Training loss: 1.9611804485321045\n",
      "Epoch 0:  52%|█████▏    | 267/512 [01:44<01:36,  2.55it/s, v_num=0]Training loss: 1.5919806957244873\n",
      "Epoch 0:  52%|█████▏    | 268/512 [01:45<01:35,  2.55it/s, v_num=0]Training loss: 1.7740399837493896\n",
      "Epoch 0:  53%|█████▎    | 269/512 [01:45<01:35,  2.55it/s, v_num=0]Training loss: 1.6523525714874268\n",
      "Epoch 0:  53%|█████▎    | 270/512 [01:45<01:34,  2.55it/s, v_num=0]Training loss: 1.3668493032455444\n",
      "Epoch 0:  53%|█████▎    | 271/512 [01:46<01:34,  2.55it/s, v_num=0]Training loss: 1.0174158811569214\n",
      "Epoch 0:  53%|█████▎    | 272/512 [01:46<01:34,  2.55it/s, v_num=0]Training loss: 1.2725942134857178\n",
      "Epoch 0:  53%|█████▎    | 273/512 [01:47<01:33,  2.55it/s, v_num=0]Training loss: 1.0411497354507446\n",
      "Epoch 0:  54%|█████▎    | 274/512 [01:47<01:33,  2.55it/s, v_num=0]Training loss: 1.670019507408142\n",
      "Epoch 0:  54%|█████▎    | 275/512 [01:47<01:32,  2.55it/s, v_num=0]Training loss: 1.0217831134796143\n",
      "Epoch 0:  54%|█████▍    | 276/512 [01:48<01:32,  2.55it/s, v_num=0]Training loss: 1.0016143321990967\n",
      "Epoch 0:  54%|█████▍    | 277/512 [01:48<01:32,  2.55it/s, v_num=0]Training loss: 1.4067473411560059\n",
      "Epoch 0:  54%|█████▍    | 278/512 [01:48<01:31,  2.55it/s, v_num=0]Training loss: 1.7991329431533813\n",
      "Epoch 0:  54%|█████▍    | 279/512 [01:49<01:31,  2.55it/s, v_num=0]Training loss: 1.2648262977600098\n",
      "Epoch 0:  55%|█████▍    | 280/512 [01:49<01:30,  2.55it/s, v_num=0]Training loss: 1.105922818183899\n",
      "Epoch 0:  55%|█████▍    | 281/512 [01:50<01:30,  2.55it/s, v_num=0]Training loss: 3.4044787883758545\n",
      "Epoch 0:  55%|█████▌    | 282/512 [01:50<01:30,  2.55it/s, v_num=0]Training loss: 1.8632802963256836\n",
      "Epoch 0:  55%|█████▌    | 283/512 [01:50<01:29,  2.55it/s, v_num=0]Training loss: 1.617904543876648\n",
      "Epoch 0:  55%|█████▌    | 284/512 [01:51<01:29,  2.55it/s, v_num=0]Training loss: 1.9954555034637451\n",
      "Epoch 0:  56%|█████▌    | 285/512 [01:51<01:28,  2.55it/s, v_num=0]Training loss: 1.2699209451675415\n",
      "Epoch 0:  56%|█████▌    | 286/512 [01:52<01:28,  2.55it/s, v_num=0]Training loss: 1.4108684062957764\n",
      "Epoch 0:  56%|█████▌    | 287/512 [01:52<01:28,  2.55it/s, v_num=0]Training loss: 1.3363134860992432\n",
      "Epoch 0:  56%|█████▋    | 288/512 [01:52<01:27,  2.55it/s, v_num=0]Training loss: 1.2711315155029297\n",
      "Epoch 0:  56%|█████▋    | 289/512 [01:53<01:27,  2.55it/s, v_num=0]Training loss: 1.6138876676559448\n",
      "Epoch 0:  57%|█████▋    | 290/512 [01:53<01:26,  2.55it/s, v_num=0]Training loss: 1.1237424612045288\n",
      "Epoch 0:  57%|█████▋    | 291/512 [01:54<01:26,  2.55it/s, v_num=0]Training loss: 1.2649049758911133\n",
      "Epoch 0:  57%|█████▋    | 292/512 [01:54<01:26,  2.55it/s, v_num=0]Training loss: 1.0964181423187256\n",
      "Epoch 0:  57%|█████▋    | 293/512 [01:54<01:25,  2.55it/s, v_num=0]Training loss: 1.0623763799667358\n",
      "Epoch 0:  57%|█████▋    | 294/512 [01:55<01:25,  2.55it/s, v_num=0]Training loss: 1.6549054384231567\n",
      "Epoch 0:  58%|█████▊    | 295/512 [01:55<01:25,  2.55it/s, v_num=0]Training loss: 1.6534302234649658\n",
      "Epoch 0:  58%|█████▊    | 296/512 [01:55<01:24,  2.55it/s, v_num=0]Training loss: 1.7960078716278076\n",
      "Epoch 0:  58%|█████▊    | 297/512 [01:56<01:24,  2.55it/s, v_num=0]Training loss: 1.3251171112060547\n",
      "Epoch 0:  58%|█████▊    | 298/512 [01:56<01:23,  2.55it/s, v_num=0]Training loss: 1.4726347923278809\n",
      "Epoch 0:  58%|█████▊    | 299/512 [01:57<01:23,  2.55it/s, v_num=0]Training loss: 1.09260892868042\n",
      "Epoch 0:  59%|█████▊    | 300/512 [01:57<01:23,  2.55it/s, v_num=0]Training loss: 0.9860900044441223\n",
      "Epoch 0:  59%|█████▉    | 301/512 [01:57<01:22,  2.55it/s, v_num=0]Training loss: 1.1535429954528809\n",
      "Epoch 0:  59%|█████▉    | 302/512 [01:58<01:22,  2.55it/s, v_num=0]Training loss: 2.0421018600463867\n",
      "Epoch 0:  59%|█████▉    | 303/512 [01:58<01:21,  2.55it/s, v_num=0]Training loss: 1.8432071208953857\n",
      "Epoch 0:  59%|█████▉    | 304/512 [01:59<01:21,  2.55it/s, v_num=0]Training loss: 1.1946660280227661\n",
      "Epoch 0:  60%|█████▉    | 305/512 [01:59<01:21,  2.55it/s, v_num=0]Training loss: 1.1030912399291992\n",
      "Epoch 0:  60%|█████▉    | 306/512 [01:59<01:20,  2.55it/s, v_num=0]Training loss: 1.0268157720565796\n",
      "Epoch 0:  60%|█████▉    | 307/512 [02:00<01:20,  2.55it/s, v_num=0]Training loss: 1.2117785215377808\n",
      "Epoch 0:  60%|██████    | 308/512 [02:00<01:19,  2.55it/s, v_num=0]Training loss: 1.2401642799377441\n",
      "Epoch 0:  60%|██████    | 309/512 [02:01<01:19,  2.55it/s, v_num=0]Training loss: 0.8842964172363281\n",
      "Epoch 0:  61%|██████    | 310/512 [02:01<01:19,  2.55it/s, v_num=0]Training loss: 1.1295784711837769\n",
      "Epoch 0:  61%|██████    | 311/512 [02:01<01:18,  2.55it/s, v_num=0]Training loss: 1.2881237268447876\n",
      "Epoch 0:  61%|██████    | 312/512 [02:02<01:18,  2.55it/s, v_num=0]Training loss: 1.3237909078598022\n",
      "Epoch 0:  61%|██████    | 313/512 [02:02<01:17,  2.55it/s, v_num=0]Training loss: 1.3492951393127441\n",
      "Epoch 0:  61%|██████▏   | 314/512 [02:02<01:17,  2.55it/s, v_num=0]Training loss: 1.2612720727920532\n",
      "Epoch 0:  62%|██████▏   | 315/512 [02:03<01:17,  2.55it/s, v_num=0]Training loss: 1.7872114181518555\n",
      "Epoch 0:  62%|██████▏   | 316/512 [02:03<01:16,  2.55it/s, v_num=0]Training loss: 1.2977486848831177\n",
      "Epoch 0:  62%|██████▏   | 317/512 [02:04<01:16,  2.55it/s, v_num=0]Training loss: 1.3206247091293335\n",
      "Epoch 0:  62%|██████▏   | 318/512 [02:04<01:15,  2.55it/s, v_num=0]Training loss: 1.7105755805969238\n",
      "Epoch 0:  62%|██████▏   | 319/512 [02:04<01:15,  2.55it/s, v_num=0]Training loss: 1.0925058126449585\n",
      "Epoch 0:  62%|██████▎   | 320/512 [02:05<01:15,  2.55it/s, v_num=0]Training loss: 1.2094632387161255\n",
      "Epoch 0:  63%|██████▎   | 321/512 [02:05<01:14,  2.55it/s, v_num=0]Training loss: 1.2844562530517578\n",
      "Epoch 0:  63%|██████▎   | 322/512 [02:06<01:14,  2.55it/s, v_num=0]Training loss: 1.2353315353393555\n",
      "Epoch 0:  63%|██████▎   | 323/512 [02:06<01:14,  2.55it/s, v_num=0]Training loss: 1.4248597621917725\n",
      "Epoch 0:  63%|██████▎   | 324/512 [02:06<01:13,  2.55it/s, v_num=0]Training loss: 1.0457763671875\n",
      "Epoch 0:  63%|██████▎   | 325/512 [02:07<01:13,  2.55it/s, v_num=0]Training loss: 1.402259111404419\n",
      "Epoch 0:  64%|██████▎   | 326/512 [02:07<01:12,  2.55it/s, v_num=0]Training loss: 0.8661251068115234\n",
      "Epoch 0:  64%|██████▍   | 327/512 [02:08<01:12,  2.55it/s, v_num=0]Training loss: 1.5477814674377441\n",
      "Epoch 0:  64%|██████▍   | 328/512 [02:08<01:12,  2.55it/s, v_num=0]Training loss: 1.296992540359497\n",
      "Epoch 0:  64%|██████▍   | 329/512 [02:08<01:11,  2.55it/s, v_num=0]Training loss: 1.5442222356796265\n",
      "Epoch 0:  64%|██████▍   | 330/512 [02:09<01:11,  2.55it/s, v_num=0]Training loss: 1.3306457996368408\n",
      "Epoch 0:  65%|██████▍   | 331/512 [02:09<01:10,  2.55it/s, v_num=0]Training loss: 1.2743239402770996\n",
      "Epoch 0:  65%|██████▍   | 332/512 [02:09<01:10,  2.55it/s, v_num=0]Training loss: 1.5115416049957275\n",
      "Epoch 0:  65%|██████▌   | 333/512 [02:10<01:10,  2.55it/s, v_num=0]Training loss: 1.6091610193252563\n",
      "Epoch 0:  65%|██████▌   | 334/512 [02:10<01:09,  2.55it/s, v_num=0]Training loss: 1.418744683265686\n",
      "Epoch 0:  65%|██████▌   | 335/512 [02:11<01:09,  2.55it/s, v_num=0]Training loss: 1.0917094945907593\n",
      "Epoch 0:  66%|██████▌   | 336/512 [02:11<01:08,  2.55it/s, v_num=0]Training loss: 1.0586609840393066\n",
      "Epoch 0:  66%|██████▌   | 337/512 [02:11<01:08,  2.55it/s, v_num=0]Training loss: 1.079939842224121\n",
      "Epoch 0:  66%|██████▌   | 338/512 [02:12<01:08,  2.55it/s, v_num=0]Training loss: 1.131853699684143\n",
      "Epoch 0:  66%|██████▌   | 339/512 [02:12<01:07,  2.55it/s, v_num=0]Training loss: 1.857973575592041\n",
      "Epoch 0:  66%|██████▋   | 340/512 [02:13<01:07,  2.55it/s, v_num=0]Training loss: 0.9060192108154297\n",
      "Epoch 0:  67%|██████▋   | 341/512 [02:13<01:06,  2.55it/s, v_num=0]Training loss: 1.341955304145813\n",
      "Epoch 0:  67%|██████▋   | 342/512 [02:13<01:06,  2.55it/s, v_num=0]Training loss: 1.6249207258224487\n",
      "Epoch 0:  67%|██████▋   | 343/512 [02:14<01:06,  2.55it/s, v_num=0]Training loss: 1.1293240785598755\n",
      "Epoch 0:  67%|██████▋   | 344/512 [02:14<01:05,  2.55it/s, v_num=0]Training loss: 1.0015113353729248\n",
      "Epoch 0:  67%|██████▋   | 345/512 [02:15<01:05,  2.55it/s, v_num=0]Training loss: 1.5414624214172363\n",
      "Epoch 0:  68%|██████▊   | 346/512 [02:15<01:04,  2.55it/s, v_num=0]Training loss: 1.3659324645996094\n",
      "Epoch 0:  68%|██████▊   | 347/512 [02:15<01:04,  2.55it/s, v_num=0]Training loss: 1.0725704431533813\n",
      "Epoch 0:  68%|██████▊   | 348/512 [02:16<01:04,  2.55it/s, v_num=0]Training loss: 1.0641652345657349\n",
      "Epoch 0:  68%|██████▊   | 349/512 [02:16<01:03,  2.55it/s, v_num=0]Training loss: 1.4053049087524414\n",
      "Epoch 0:  68%|██████▊   | 350/512 [02:17<01:03,  2.55it/s, v_num=0]Training loss: 0.9428348541259766\n",
      "Epoch 0:  69%|██████▊   | 351/512 [02:17<01:03,  2.55it/s, v_num=0]Training loss: 1.8257479667663574\n",
      "Epoch 0:  69%|██████▉   | 352/512 [02:17<01:02,  2.55it/s, v_num=0]Training loss: 0.986789345741272\n",
      "Epoch 0:  69%|██████▉   | 353/512 [02:18<01:02,  2.55it/s, v_num=0]Training loss: 1.9234066009521484\n",
      "Epoch 0:  69%|██████▉   | 354/512 [02:18<01:01,  2.55it/s, v_num=0]Training loss: 1.3594014644622803\n",
      "Epoch 0:  69%|██████▉   | 355/512 [02:18<01:01,  2.55it/s, v_num=0]Training loss: 1.0809850692749023\n",
      "Epoch 0:  70%|██████▉   | 356/512 [02:19<01:01,  2.55it/s, v_num=0]Training loss: 1.6110926866531372\n",
      "Epoch 0:  70%|██████▉   | 357/512 [02:19<01:00,  2.55it/s, v_num=0]Training loss: 1.460007905960083\n",
      "Epoch 0:  70%|██████▉   | 358/512 [02:20<01:00,  2.55it/s, v_num=0]Training loss: 1.1645636558532715\n",
      "Epoch 0:  70%|███████   | 359/512 [02:20<00:59,  2.55it/s, v_num=0]Training loss: 1.2549004554748535\n",
      "Epoch 0:  70%|███████   | 360/512 [02:20<00:59,  2.55it/s, v_num=0]Training loss: 1.6171400547027588\n",
      "Epoch 0:  71%|███████   | 361/512 [02:21<00:59,  2.55it/s, v_num=0]Training loss: 1.1636755466461182\n",
      "Epoch 0:  71%|███████   | 362/512 [02:21<00:58,  2.55it/s, v_num=0]Training loss: 1.561802625656128\n",
      "Epoch 0:  71%|███████   | 363/512 [02:22<00:58,  2.55it/s, v_num=0]Training loss: 1.4843966960906982\n",
      "Epoch 0:  71%|███████   | 364/512 [02:22<00:57,  2.56it/s, v_num=0]Training loss: 1.8793399333953857\n",
      "Epoch 0:  71%|███████▏  | 365/512 [02:22<00:57,  2.56it/s, v_num=0]Training loss: 1.2073699235916138\n",
      "Epoch 0:  71%|███████▏  | 366/512 [02:23<00:57,  2.56it/s, v_num=0]Training loss: 1.1577457189559937\n",
      "Epoch 0:  72%|███████▏  | 367/512 [02:23<00:56,  2.56it/s, v_num=0]Training loss: 1.1005135774612427\n",
      "Epoch 0:  72%|███████▏  | 368/512 [02:24<00:56,  2.56it/s, v_num=0]Training loss: 2.2904999256134033\n",
      "Epoch 0:  72%|███████▏  | 369/512 [02:24<00:55,  2.56it/s, v_num=0]Training loss: 1.529512643814087\n",
      "Epoch 0:  72%|███████▏  | 370/512 [02:24<00:55,  2.56it/s, v_num=0]Training loss: 1.2737581729888916\n",
      "Epoch 0:  72%|███████▏  | 371/512 [02:25<00:55,  2.56it/s, v_num=0]Training loss: 1.2541407346725464\n",
      "Epoch 0:  73%|███████▎  | 372/512 [02:25<00:54,  2.56it/s, v_num=0]Training loss: 1.160161018371582\n",
      "Epoch 0:  73%|███████▎  | 373/512 [02:25<00:54,  2.56it/s, v_num=0]Training loss: 1.4002522230148315\n",
      "Epoch 0:  73%|███████▎  | 374/512 [02:26<00:54,  2.56it/s, v_num=0]Training loss: 1.6689821481704712\n",
      "Epoch 0:  73%|███████▎  | 375/512 [02:26<00:53,  2.56it/s, v_num=0]Training loss: 1.3726099729537964\n",
      "Epoch 0:  73%|███████▎  | 376/512 [02:27<00:53,  2.56it/s, v_num=0]Training loss: 1.3540620803833008\n",
      "Epoch 0:  74%|███████▎  | 377/512 [02:27<00:52,  2.56it/s, v_num=0]Training loss: 1.23971688747406\n",
      "Epoch 0:  74%|███████▍  | 378/512 [02:27<00:52,  2.56it/s, v_num=0]Training loss: 1.5491493940353394\n",
      "Epoch 0:  74%|███████▍  | 379/512 [02:28<00:52,  2.56it/s, v_num=0]Training loss: 0.7982895374298096\n",
      "Epoch 0:  74%|███████▍  | 380/512 [02:28<00:51,  2.56it/s, v_num=0]Training loss: 1.4961650371551514\n",
      "Epoch 0:  74%|███████▍  | 381/512 [02:29<00:51,  2.56it/s, v_num=0]Training loss: 1.2461967468261719\n",
      "Epoch 0:  75%|███████▍  | 382/512 [02:29<00:50,  2.56it/s, v_num=0]Training loss: 1.4034278392791748\n",
      "Epoch 0:  75%|███████▍  | 383/512 [02:29<00:50,  2.56it/s, v_num=0]Training loss: 2.6883840560913086\n",
      "Epoch 0:  75%|███████▌  | 384/512 [02:30<00:50,  2.56it/s, v_num=0]Training loss: 1.498018741607666\n",
      "Epoch 0:  75%|███████▌  | 385/512 [02:30<00:49,  2.56it/s, v_num=0]Training loss: 2.1920065879821777\n",
      "Epoch 0:  75%|███████▌  | 386/512 [02:31<00:49,  2.56it/s, v_num=0]Training loss: 1.0825903415679932\n",
      "Epoch 0:  76%|███████▌  | 387/512 [02:31<00:48,  2.56it/s, v_num=0]Training loss: 1.4736535549163818\n",
      "Epoch 0:  76%|███████▌  | 388/512 [02:31<00:48,  2.56it/s, v_num=0]Training loss: 1.2139654159545898\n",
      "Epoch 0:  76%|███████▌  | 389/512 [02:32<00:48,  2.56it/s, v_num=0]Training loss: 1.2308449745178223\n",
      "Epoch 0:  76%|███████▌  | 390/512 [02:32<00:47,  2.56it/s, v_num=0]Training loss: 2.0804104804992676\n",
      "Epoch 0:  76%|███████▋  | 391/512 [02:32<00:47,  2.56it/s, v_num=0]Training loss: 1.8112773895263672\n",
      "Epoch 0:  77%|███████▋  | 392/512 [02:33<00:46,  2.56it/s, v_num=0]Training loss: 1.5164029598236084\n",
      "Epoch 0:  77%|███████▋  | 393/512 [02:33<00:46,  2.56it/s, v_num=0]Training loss: 1.541511058807373\n",
      "Epoch 0:  77%|███████▋  | 394/512 [02:34<00:46,  2.56it/s, v_num=0]Training loss: 1.7714695930480957\n",
      "Epoch 0:  77%|███████▋  | 395/512 [02:34<00:45,  2.56it/s, v_num=0]Training loss: 1.2032208442687988\n",
      "Epoch 0:  77%|███████▋  | 396/512 [02:34<00:45,  2.56it/s, v_num=0]Training loss: 1.0616685152053833\n",
      "Epoch 0:  78%|███████▊  | 397/512 [02:35<00:44,  2.56it/s, v_num=0]Training loss: 1.6424803733825684\n",
      "Epoch 0:  78%|███████▊  | 398/512 [02:35<00:44,  2.56it/s, v_num=0]Training loss: 1.6342018842697144\n",
      "Epoch 0:  78%|███████▊  | 399/512 [02:36<00:44,  2.56it/s, v_num=0]Training loss: 0.9036116600036621\n",
      "Epoch 0:  78%|███████▊  | 400/512 [02:36<00:43,  2.56it/s, v_num=0]Training loss: 1.7575336694717407\n",
      "Epoch 0:  78%|███████▊  | 401/512 [02:36<00:43,  2.56it/s, v_num=0]Training loss: 1.2633955478668213\n",
      "Epoch 0:  79%|███████▊  | 402/512 [02:37<00:43,  2.56it/s, v_num=0]Training loss: 1.3256840705871582\n",
      "Epoch 0:  79%|███████▊  | 403/512 [02:37<00:42,  2.56it/s, v_num=0]Training loss: 1.1834899187088013\n",
      "Epoch 0:  79%|███████▉  | 404/512 [02:38<00:42,  2.56it/s, v_num=0]Training loss: 1.1457809209823608\n",
      "Epoch 0:  79%|███████▉  | 405/512 [02:38<00:41,  2.56it/s, v_num=0]Training loss: 1.2530863285064697\n",
      "Epoch 0:  79%|███████▉  | 406/512 [02:38<00:41,  2.56it/s, v_num=0]Training loss: 1.0599085092544556\n",
      "Epoch 0:  79%|███████▉  | 407/512 [02:39<00:41,  2.56it/s, v_num=0]Training loss: 2.0373685359954834\n",
      "Epoch 0:  80%|███████▉  | 408/512 [02:39<00:40,  2.56it/s, v_num=0]Training loss: 1.4103922843933105\n",
      "Epoch 0:  80%|███████▉  | 409/512 [02:40<00:40,  2.56it/s, v_num=0]Training loss: 1.8313392400741577\n",
      "Epoch 0:  80%|████████  | 410/512 [02:40<00:39,  2.56it/s, v_num=0]Training loss: 1.4634500741958618\n",
      "Epoch 0:  80%|████████  | 411/512 [02:40<00:39,  2.56it/s, v_num=0]Training loss: 1.0604171752929688\n",
      "Epoch 0:  80%|████████  | 412/512 [02:41<00:39,  2.56it/s, v_num=0]Training loss: 1.6299762725830078\n",
      "Epoch 0:  81%|████████  | 413/512 [02:41<00:38,  2.56it/s, v_num=0]Training loss: 1.1761513948440552\n",
      "Epoch 0:  81%|████████  | 414/512 [02:41<00:38,  2.56it/s, v_num=0]Training loss: 1.2053232192993164\n",
      "Epoch 0:  81%|████████  | 415/512 [02:42<00:37,  2.56it/s, v_num=0]Training loss: 1.641198992729187\n",
      "Epoch 0:  81%|████████▏ | 416/512 [02:42<00:37,  2.56it/s, v_num=0]Training loss: 0.8818443417549133\n",
      "Epoch 0:  81%|████████▏ | 417/512 [02:43<00:37,  2.56it/s, v_num=0]Training loss: 1.0218391418457031\n",
      "Epoch 0:  82%|████████▏ | 418/512 [02:43<00:36,  2.56it/s, v_num=0]Training loss: 1.5773437023162842\n",
      "Epoch 0:  82%|████████▏ | 419/512 [02:43<00:36,  2.56it/s, v_num=0]Training loss: 0.792763352394104\n",
      "Epoch 0:  82%|████████▏ | 420/512 [02:44<00:35,  2.56it/s, v_num=0]Training loss: 1.4550135135650635\n",
      "Epoch 0:  82%|████████▏ | 421/512 [02:44<00:35,  2.56it/s, v_num=0]Training loss: 0.9388784170150757\n",
      "Epoch 0:  82%|████████▏ | 422/512 [02:45<00:35,  2.56it/s, v_num=0]Training loss: 1.121457576751709\n",
      "Epoch 0:  83%|████████▎ | 423/512 [02:45<00:34,  2.56it/s, v_num=0]Training loss: 1.4630944728851318\n",
      "Epoch 0:  83%|████████▎ | 424/512 [02:45<00:34,  2.56it/s, v_num=0]Training loss: 1.9822425842285156\n",
      "Epoch 0:  83%|████████▎ | 425/512 [02:46<00:34,  2.56it/s, v_num=0]Training loss: 1.3749037981033325\n",
      "Epoch 0:  83%|████████▎ | 426/512 [02:46<00:33,  2.56it/s, v_num=0]Training loss: 1.501094937324524\n",
      "Epoch 0:  83%|████████▎ | 427/512 [02:47<00:33,  2.56it/s, v_num=0]Training loss: 1.2347207069396973\n",
      "Epoch 0:  84%|████████▎ | 428/512 [02:47<00:32,  2.56it/s, v_num=0]Training loss: 1.9433515071868896\n",
      "Epoch 0:  84%|████████▍ | 429/512 [02:47<00:32,  2.56it/s, v_num=0]Training loss: 1.6108825206756592\n",
      "Epoch 0:  84%|████████▍ | 430/512 [02:48<00:32,  2.56it/s, v_num=0]Training loss: 1.0185579061508179\n",
      "Epoch 0:  84%|████████▍ | 431/512 [02:48<00:31,  2.56it/s, v_num=0]Training loss: 0.9726834297180176\n",
      "Epoch 0:  84%|████████▍ | 432/512 [02:48<00:31,  2.56it/s, v_num=0]Training loss: 1.0140575170516968\n",
      "Epoch 0:  85%|████████▍ | 433/512 [02:49<00:30,  2.56it/s, v_num=0]Training loss: 1.304674506187439\n",
      "Epoch 0:  85%|████████▍ | 434/512 [02:49<00:30,  2.56it/s, v_num=0]Training loss: 1.0110273361206055\n",
      "Epoch 0:  85%|████████▍ | 435/512 [02:50<00:30,  2.56it/s, v_num=0]Training loss: 1.322203278541565\n",
      "Epoch 0:  85%|████████▌ | 436/512 [02:50<00:29,  2.56it/s, v_num=0]Training loss: 0.9989740252494812\n",
      "Epoch 0:  85%|████████▌ | 437/512 [02:50<00:29,  2.56it/s, v_num=0]Training loss: 1.135770559310913\n",
      "Epoch 0:  86%|████████▌ | 438/512 [02:51<00:28,  2.56it/s, v_num=0]Training loss: 1.082564115524292\n",
      "Epoch 0:  86%|████████▌ | 439/512 [02:51<00:28,  2.56it/s, v_num=0]Training loss: 1.336577296257019\n",
      "Epoch 0:  86%|████████▌ | 440/512 [02:52<00:28,  2.56it/s, v_num=0]Training loss: 1.6770093441009521\n",
      "Epoch 0:  86%|████████▌ | 441/512 [02:52<00:27,  2.56it/s, v_num=0]Training loss: 1.4784770011901855\n",
      "Epoch 0:  86%|████████▋ | 442/512 [02:52<00:27,  2.56it/s, v_num=0]Training loss: 1.258514642715454\n",
      "Epoch 0:  87%|████████▋ | 443/512 [02:53<00:26,  2.56it/s, v_num=0]Training loss: 1.077836036682129\n",
      "Epoch 0:  87%|████████▋ | 444/512 [02:53<00:26,  2.56it/s, v_num=0]Training loss: 0.9379453659057617\n",
      "Epoch 0:  87%|████████▋ | 445/512 [02:54<00:26,  2.56it/s, v_num=0]Training loss: 2.0486321449279785\n",
      "Epoch 0:  87%|████████▋ | 446/512 [02:54<00:25,  2.56it/s, v_num=0]Training loss: 1.595402479171753\n",
      "Epoch 0:  87%|████████▋ | 447/512 [02:54<00:25,  2.56it/s, v_num=0]Training loss: 1.3954391479492188\n",
      "Epoch 0:  88%|████████▊ | 448/512 [02:55<00:25,  2.56it/s, v_num=0]Training loss: 1.1193418502807617\n",
      "Epoch 0:  88%|████████▊ | 449/512 [02:55<00:24,  2.56it/s, v_num=0]Training loss: 0.8095151782035828\n",
      "Epoch 0:  88%|████████▊ | 450/512 [02:55<00:24,  2.56it/s, v_num=0]Training loss: 1.1283804178237915\n",
      "Epoch 0:  88%|████████▊ | 451/512 [02:56<00:23,  2.56it/s, v_num=0]Training loss: 2.116673707962036\n",
      "Epoch 0:  88%|████████▊ | 452/512 [02:56<00:23,  2.56it/s, v_num=0]Training loss: 1.7541722059249878\n",
      "Epoch 0:  88%|████████▊ | 453/512 [02:57<00:23,  2.56it/s, v_num=0]Training loss: 0.9742904901504517\n",
      "Epoch 0:  89%|████████▊ | 454/512 [02:57<00:22,  2.56it/s, v_num=0]Training loss: 1.5125874280929565\n",
      "Epoch 0:  89%|████████▉ | 455/512 [02:57<00:22,  2.56it/s, v_num=0]Training loss: 1.1690810918807983\n",
      "Epoch 0:  89%|████████▉ | 456/512 [02:58<00:21,  2.56it/s, v_num=0]Training loss: 1.3709220886230469\n",
      "Epoch 0:  89%|████████▉ | 457/512 [02:58<00:21,  2.56it/s, v_num=0]Training loss: 0.8698679208755493\n",
      "Epoch 0:  89%|████████▉ | 458/512 [02:59<00:21,  2.56it/s, v_num=0]Training loss: 1.6035165786743164\n",
      "Epoch 0:  90%|████████▉ | 459/512 [02:59<00:20,  2.56it/s, v_num=0]Training loss: 1.7620224952697754\n",
      "Epoch 0:  90%|████████▉ | 460/512 [02:59<00:20,  2.56it/s, v_num=0]Training loss: 1.0989675521850586\n",
      "Epoch 0:  90%|█████████ | 461/512 [03:00<00:19,  2.56it/s, v_num=0]Training loss: 1.4929373264312744\n",
      "Epoch 0:  90%|█████████ | 462/512 [03:00<00:19,  2.56it/s, v_num=0]Training loss: 1.1901705265045166\n",
      "Epoch 0:  90%|█████████ | 463/512 [03:01<00:19,  2.56it/s, v_num=0]Training loss: 1.0313587188720703\n",
      "Epoch 0:  91%|█████████ | 464/512 [03:01<00:18,  2.56it/s, v_num=0]Training loss: 0.7466622591018677\n",
      "Epoch 0:  91%|█████████ | 465/512 [03:01<00:18,  2.56it/s, v_num=0]Training loss: 1.7444202899932861\n",
      "Epoch 0:  91%|█████████ | 466/512 [03:02<00:17,  2.56it/s, v_num=0]Training loss: 0.9541051387786865\n",
      "Epoch 0:  91%|█████████ | 467/512 [03:02<00:17,  2.56it/s, v_num=0]Training loss: 1.9737300872802734\n",
      "Epoch 0:  91%|█████████▏| 468/512 [03:03<00:17,  2.56it/s, v_num=0]Training loss: 1.1105083227157593\n",
      "Epoch 0:  92%|█████████▏| 469/512 [03:03<00:16,  2.56it/s, v_num=0]Training loss: 1.2154779434204102\n",
      "Epoch 0:  92%|█████████▏| 470/512 [03:03<00:16,  2.56it/s, v_num=0]Training loss: 1.4454480409622192\n",
      "Epoch 0:  92%|█████████▏| 471/512 [03:04<00:16,  2.56it/s, v_num=0]Training loss: 1.3032145500183105\n",
      "Epoch 0:  92%|█████████▏| 472/512 [03:04<00:15,  2.56it/s, v_num=0]Training loss: 1.0366054773330688\n",
      "Epoch 0:  92%|█████████▏| 473/512 [03:04<00:15,  2.56it/s, v_num=0]Training loss: 1.2602890729904175\n",
      "Epoch 0:  93%|█████████▎| 474/512 [03:05<00:14,  2.56it/s, v_num=0]Training loss: 0.8238152265548706\n",
      "Epoch 0:  93%|█████████▎| 475/512 [03:05<00:14,  2.56it/s, v_num=0]Training loss: 1.8962287902832031\n",
      "Epoch 0:  93%|█████████▎| 476/512 [03:06<00:14,  2.56it/s, v_num=0]Training loss: 1.284291386604309\n",
      "Epoch 0:  93%|█████████▎| 477/512 [03:06<00:13,  2.56it/s, v_num=0]Training loss: 1.0019125938415527\n",
      "Epoch 0:  93%|█████████▎| 478/512 [03:06<00:13,  2.56it/s, v_num=0]Training loss: 1.3836705684661865\n",
      "Epoch 0:  94%|█████████▎| 479/512 [03:07<00:12,  2.56it/s, v_num=0]Training loss: 1.1521787643432617\n",
      "Epoch 0:  94%|█████████▍| 480/512 [03:07<00:12,  2.56it/s, v_num=0]Training loss: 1.1665537357330322\n",
      "Epoch 0:  94%|█████████▍| 481/512 [03:08<00:12,  2.56it/s, v_num=0]Training loss: 1.4194201231002808\n",
      "Epoch 0:  94%|█████████▍| 482/512 [03:08<00:11,  2.56it/s, v_num=0]Training loss: 1.1479014158248901\n",
      "Epoch 0:  94%|█████████▍| 483/512 [03:08<00:11,  2.56it/s, v_num=0]Training loss: 1.5335407257080078\n",
      "Epoch 0:  95%|█████████▍| 484/512 [03:09<00:10,  2.56it/s, v_num=0]Training loss: 1.8664350509643555\n",
      "Epoch 0:  95%|█████████▍| 485/512 [03:09<00:10,  2.56it/s, v_num=0]Training loss: 1.1870139837265015\n",
      "Epoch 0:  95%|█████████▍| 486/512 [03:10<00:10,  2.56it/s, v_num=0]Training loss: 1.4433000087738037\n",
      "Epoch 0:  95%|█████████▌| 487/512 [03:10<00:09,  2.56it/s, v_num=0]Training loss: 1.154191493988037\n",
      "Epoch 0:  95%|█████████▌| 488/512 [03:10<00:09,  2.56it/s, v_num=0]Training loss: 1.1301708221435547\n",
      "Epoch 0:  96%|█████████▌| 489/512 [03:11<00:08,  2.56it/s, v_num=0]Training loss: 1.2541991472244263\n",
      "Epoch 0:  96%|█████████▌| 490/512 [03:11<00:08,  2.56it/s, v_num=0]Training loss: 0.9247100353240967\n",
      "Epoch 0:  96%|█████████▌| 491/512 [03:11<00:08,  2.56it/s, v_num=0]Training loss: 0.9839382171630859\n",
      "Epoch 0:  96%|█████████▌| 492/512 [03:12<00:07,  2.56it/s, v_num=0]Training loss: 1.3495770692825317\n",
      "Epoch 0:  96%|█████████▋| 493/512 [03:12<00:07,  2.56it/s, v_num=0]Training loss: 1.5288819074630737\n",
      "Epoch 0:  96%|█████████▋| 494/512 [03:13<00:07,  2.56it/s, v_num=0]Training loss: 1.2885358333587646\n",
      "Epoch 0:  97%|█████████▋| 495/512 [03:13<00:06,  2.56it/s, v_num=0]Training loss: 1.9212610721588135\n",
      "Epoch 0:  97%|█████████▋| 496/512 [03:13<00:06,  2.56it/s, v_num=0]Training loss: 2.1610233783721924\n",
      "Epoch 0:  97%|█████████▋| 497/512 [03:14<00:05,  2.56it/s, v_num=0]Training loss: 1.4389019012451172\n",
      "Epoch 0:  97%|█████████▋| 498/512 [03:14<00:05,  2.56it/s, v_num=0]Training loss: 1.1457397937774658\n",
      "Epoch 0:  97%|█████████▋| 499/512 [03:15<00:05,  2.56it/s, v_num=0]Training loss: 1.062143087387085\n",
      "Epoch 0:  98%|█████████▊| 500/512 [03:15<00:04,  2.56it/s, v_num=0]Training loss: 1.209057331085205\n",
      "Epoch 0:  98%|█████████▊| 501/512 [03:15<00:04,  2.56it/s, v_num=0]Training loss: 1.7496520280838013\n",
      "Epoch 0:  98%|█████████▊| 502/512 [03:16<00:03,  2.56it/s, v_num=0]Training loss: 1.7121973037719727\n",
      "Epoch 0:  98%|█████████▊| 503/512 [03:16<00:03,  2.56it/s, v_num=0]Training loss: 1.4162484407424927\n",
      "Epoch 0:  98%|█████████▊| 504/512 [03:17<00:03,  2.56it/s, v_num=0]Training loss: 1.5827522277832031\n",
      "Epoch 0:  99%|█████████▊| 505/512 [03:17<00:02,  2.56it/s, v_num=0]Training loss: 1.4580961465835571\n",
      "Epoch 0:  99%|█████████▉| 506/512 [03:17<00:02,  2.56it/s, v_num=0]Training loss: 1.5630215406417847\n",
      "Epoch 0:  99%|█████████▉| 507/512 [03:18<00:01,  2.56it/s, v_num=0]Training loss: 1.5825810432434082\n",
      "Epoch 0:  99%|█████████▉| 508/512 [03:18<00:01,  2.56it/s, v_num=0]Training loss: 1.3287708759307861\n",
      "Epoch 0:  99%|█████████▉| 509/512 [03:18<00:01,  2.56it/s, v_num=0]Training loss: 1.3055622577667236\n",
      "Epoch 0: 100%|█████████▉| 510/512 [03:19<00:00,  2.56it/s, v_num=0]Training loss: 1.4527909755706787\n",
      "Epoch 0: 100%|█████████▉| 511/512 [03:19<00:00,  2.56it/s, v_num=0]Training loss: 1.1677063703536987\n",
      "Epoch 0: 100%|██████████| 512/512 [03:20<00:00,  2.56it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.451522707939148\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 29.46it/s]\u001b[AValidation loss: 1.0252137184143066\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 25.12it/s]\u001b[AValidation loss: 1.5632591247558594\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 24.05it/s]\u001b[AValidation loss: 1.3815284967422485\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 23.55it/s]\u001b[AValidation loss: 1.0560461282730103\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 22.76it/s]\u001b[AValidation loss: 1.4290745258331299\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 22.61it/s]\u001b[AValidation loss: 1.141378402709961\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 22.64it/s]\u001b[AValidation loss: 1.3507944345474243\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 22.59it/s]\u001b[AValidation loss: 1.4501780271530151\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.52it/s]\u001b[AValidation loss: 1.2510769367218018\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.56it/s]\u001b[AValidation loss: 1.0178658962249756\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.53it/s]\u001b[AValidation loss: 0.9085730314254761\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.49it/s]\u001b[AValidation loss: 1.1232490539550781\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.52it/s]\u001b[AValidation loss: 1.3009432554244995\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 22.33it/s]\u001b[AValidation loss: 1.2651686668395996\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 22.30it/s]\u001b[AValidation loss: 1.4635429382324219\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 22.29it/s]\u001b[AValidation loss: 1.2128963470458984\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 22.28it/s]\u001b[AValidation loss: 0.9420518279075623\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 22.27it/s]\u001b[AValidation loss: 0.7903674840927124\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 22.27it/s]\u001b[AValidation loss: 0.914893627166748\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:04, 22.26it/s]\u001b[AValidation loss: 1.7023392915725708\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:04, 22.24it/s]\u001b[AValidation loss: 0.9329405426979065\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:03, 22.13it/s]\u001b[AValidation loss: 1.2278587818145752\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:03, 22.03it/s]\u001b[AValidation loss: 1.210071086883545\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:03, 22.02it/s]\u001b[AValidation loss: 1.0312591791152954\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 22.05it/s]\u001b[AValidation loss: 0.9365740418434143\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 21.98it/s]\u001b[AValidation loss: 1.6299479007720947\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:01<00:03, 21.97it/s]\u001b[AValidation loss: 1.0654938220977783\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:01<00:03, 21.98it/s]\u001b[AValidation loss: 1.304330825805664\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:01<00:03, 22.01it/s]\u001b[AValidation loss: 2.4850361347198486\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:01<00:03, 22.01it/s]\u001b[AValidation loss: 1.635751724243164\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:03, 22.04it/s]\u001b[AValidation loss: 1.5981028079986572\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:03, 22.04it/s]\u001b[AValidation loss: 1.2359634637832642\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:03, 22.05it/s]\u001b[AValidation loss: 1.2725825309753418\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:03, 22.07it/s]\u001b[AValidation loss: 1.2977116107940674\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:03, 22.07it/s]\u001b[AValidation loss: 0.9179537296295166\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:03, 22.08it/s]\u001b[AValidation loss: 1.0816935300827026\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:03, 22.07it/s]\u001b[AValidation loss: 1.4509999752044678\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:03, 22.02it/s]\u001b[AValidation loss: 1.119720458984375\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:03, 22.02it/s]\u001b[AValidation loss: 0.9192307591438293\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:03, 22.04it/s]\u001b[AValidation loss: 1.1574753522872925\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:03, 21.99it/s]\u001b[AValidation loss: 1.1650952100753784\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:03, 21.98it/s]\u001b[AValidation loss: 3.582594156265259\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:03, 21.99it/s]\u001b[AValidation loss: 1.4095772504806519\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:02<00:03, 21.99it/s]\u001b[AValidation loss: 0.9172409176826477\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 1.1831196546554565\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 0.8308870792388916\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 1.867334008216858\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 1.1772916316986084\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 0.9264917969703674\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 1.7417973279953003\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 1.920862078666687\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 1.4540303945541382\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:02<00:02, 21.96it/s]\u001b[AValidation loss: 1.619875192642212\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:02<00:02, 21.96it/s]\u001b[AValidation loss: 1.8270319700241089\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:02<00:02, 21.96it/s]\u001b[AValidation loss: 1.1333112716674805\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:02<00:02, 21.97it/s]\u001b[AValidation loss: 0.9140956401824951\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:02<00:02, 21.97it/s]\u001b[AValidation loss: 1.316397786140442\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:02<00:02, 21.97it/s]\u001b[AValidation loss: 1.0409111976623535\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:02<00:02, 21.98it/s]\u001b[AValidation loss: 1.4758342504501343\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 1.5075074434280396\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:02<00:02, 21.98it/s]\u001b[AValidation loss: 1.2360725402832031\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:02<00:02, 21.96it/s]\u001b[AValidation loss: 1.841172695159912\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:02, 21.92it/s]\u001b[AValidation loss: 2.46347713470459\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:02, 21.89it/s]\u001b[AValidation loss: 0.9452001452445984\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:02, 21.86it/s]\u001b[AValidation loss: 1.4956570863723755\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:03<00:02, 21.86it/s]\u001b[AValidation loss: 2.006688356399536\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 0.9334196448326111\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 1.41888427734375\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:03<00:01, 21.88it/s]\u001b[AValidation loss: 1.4039608240127563\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 1.2587536573410034\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:03<00:01, 21.85it/s]\u001b[AValidation loss: 1.0437474250793457\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:03<00:01, 21.85it/s]\u001b[AValidation loss: 0.8586171865463257\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 1.2962212562561035\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 1.4005950689315796\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:03<00:01, 21.87it/s]\u001b[AValidation loss: 1.132549524307251\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:03<00:01, 21.89it/s]\u001b[AValidation loss: 1.0911520719528198\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:03<00:01, 21.86it/s]\u001b[AValidation loss: 0.7374625205993652\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:03<00:01, 21.86it/s]\u001b[AValidation loss: 0.7557283043861389\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:03<00:01, 21.86it/s]\u001b[AValidation loss: 1.2739628553390503\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:03<00:01, 21.88it/s]\u001b[AValidation loss: 1.5892603397369385\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:03<00:01, 21.88it/s]\u001b[AValidation loss: 1.166538953781128\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:03<00:01, 21.88it/s]\u001b[AValidation loss: 0.9079260230064392\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:03<00:01, 21.86it/s]\u001b[AValidation loss: 1.0750044584274292\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:03<00:01, 21.83it/s]\u001b[AValidation loss: 0.915271520614624\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:03<00:01, 21.84it/s]\u001b[AValidation loss: 1.0604416131973267\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:03<00:01, 21.84it/s]\u001b[AValidation loss: 1.0791466236114502\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:03<00:01, 21.85it/s]\u001b[AValidation loss: 0.9921815395355225\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:04<00:01, 21.85it/s]\u001b[AValidation loss: 1.683067798614502\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:04<00:00, 21.86it/s]\u001b[AValidation loss: 2.1566965579986572\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:04<00:00, 21.84it/s]\u001b[AValidation loss: 1.5642180442810059\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:04<00:00, 21.84it/s]\u001b[AValidation loss: 0.9586101770401001\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:04<00:00, 21.85it/s]\u001b[AValidation loss: 1.3863978385925293\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:04<00:00, 21.85it/s]\u001b[AValidation loss: 1.8481180667877197\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:04<00:00, 21.85it/s]\u001b[AValidation loss: 2.0241518020629883\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:04<00:00, 21.83it/s]\u001b[AValidation loss: 1.3257646560668945\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:04<00:00, 21.80it/s]\u001b[AValidation loss: 1.3922302722930908\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:04<00:00, 21.80it/s]\u001b[AValidation loss: 0.8853507041931152\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:04<00:00, 21.81it/s]\u001b[AValidation loss: 1.246927261352539\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:04<00:00, 21.82it/s]\u001b[AValidation loss: 1.0471775531768799\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:04<00:00, 21.80it/s]\u001b[AValidation loss: 0.9427425265312195\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:04<00:00, 21.80it/s]\u001b[AValidation loss: 1.3153512477874756\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:04<00:00, 21.80it/s]\u001b[AValidation loss: 1.084238886833191\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:04<00:00, 21.81it/s]\u001b[AValidation loss: 1.401139497756958\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:04<00:00, 21.81it/s]\u001b[AValidation loss: 1.0879353284835815\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:04<00:00, 21.82it/s]\u001b[AValidation loss: 1.3366882801055908\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:04<00:00, 21.82it/s]\u001b[AValidation loss: 1.063124656677246\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:04<00:00, 21.82it/s]\u001b[AValidation loss: 1.2544413805007935\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:04<00:00, 21.83it/s]\u001b[AValidation loss: 1.0140613317489624\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:04<00:00, 21.83it/s]\u001b[AValidation loss: 1.2905570268630981\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:05<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/512 [00:00<?, ?it/s, v_num=0]                 \u001b[ATraining loss: 1.0721015930175781\n",
      "Epoch 1:   0%|          | 1/512 [00:00<00:36, 13.84it/s, v_num=0]Training loss: 1.8350328207015991\n",
      "Epoch 1:   0%|          | 2/512 [00:00<01:56,  4.38it/s, v_num=0]Training loss: 1.3514220714569092\n",
      "Epoch 1:   1%|          | 3/512 [00:00<02:23,  3.55it/s, v_num=0]Training loss: 3.2873735427856445\n",
      "Epoch 1:   1%|          | 4/512 [00:01<02:37,  3.23it/s, v_num=0]Training loss: 1.735490083694458\n",
      "Epoch 1:   1%|          | 5/512 [00:01<02:44,  3.07it/s, v_num=0]Training loss: 1.200962781906128\n",
      "Epoch 1:   1%|          | 6/512 [00:02<02:49,  2.98it/s, v_num=0]Training loss: 1.272056221961975\n",
      "Epoch 1:   1%|▏         | 7/512 [00:02<02:53,  2.91it/s, v_num=0]Training loss: 0.9694456458091736\n",
      "Epoch 1:   2%|▏         | 8/512 [00:02<02:56,  2.86it/s, v_num=0]Training loss: 2.0543534755706787\n",
      "Epoch 1:   2%|▏         | 9/512 [00:03<02:58,  2.82it/s, v_num=0]Training loss: 0.9763157963752747\n",
      "Epoch 1:   2%|▏         | 10/512 [00:03<02:59,  2.79it/s, v_num=0]Training loss: 1.8762024641036987\n",
      "Epoch 1:   2%|▏         | 11/512 [00:03<03:00,  2.77it/s, v_num=0]Training loss: 1.7554652690887451\n",
      "Epoch 1:   2%|▏         | 12/512 [00:04<03:01,  2.75it/s, v_num=0]Training loss: 1.519314169883728\n",
      "Epoch 1:   3%|▎         | 13/512 [00:04<03:02,  2.74it/s, v_num=0]Training loss: 1.170107126235962\n",
      "Epoch 1:   3%|▎         | 14/512 [00:05<03:02,  2.73it/s, v_num=0]Training loss: 1.4833757877349854\n",
      "Epoch 1:   3%|▎         | 15/512 [00:05<03:03,  2.71it/s, v_num=0]Training loss: 1.4145796298980713\n",
      "Epoch 1:   3%|▎         | 16/512 [00:05<03:03,  2.70it/s, v_num=0]Training loss: 1.0872448682785034\n",
      "Epoch 1:   3%|▎         | 17/512 [00:06<03:03,  2.70it/s, v_num=0]Training loss: 1.0570720434188843\n",
      "Epoch 1:   4%|▎         | 18/512 [00:06<03:03,  2.69it/s, v_num=0]Training loss: 1.4440792798995972\n",
      "Epoch 1:   4%|▎         | 19/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 1.1828618049621582\n",
      "Epoch 1:   4%|▍         | 20/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 1.864356279373169\n",
      "Epoch 1:   4%|▍         | 21/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 0.9709436297416687\n",
      "Epoch 1:   4%|▍         | 22/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 1.0070128440856934\n",
      "Epoch 1:   4%|▍         | 23/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 1.237911581993103\n",
      "Epoch 1:   5%|▍         | 24/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 1.4107720851898193\n",
      "Epoch 1:   5%|▍         | 25/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.314172387123108\n",
      "Epoch 1:   5%|▌         | 26/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.7198822498321533\n",
      "Epoch 1:   5%|▌         | 27/512 [00:10<03:03,  2.65it/s, v_num=0]Training loss: 1.0129363536834717\n",
      "Epoch 1:   5%|▌         | 28/512 [00:10<03:03,  2.64it/s, v_num=0]Training loss: 1.879492998123169\n",
      "Epoch 1:   6%|▌         | 29/512 [00:10<03:02,  2.64it/s, v_num=0]Training loss: 1.5320346355438232\n",
      "Epoch 1:   6%|▌         | 30/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 2.1946299076080322\n",
      "Epoch 1:   6%|▌         | 31/512 [00:11<03:02,  2.63it/s, v_num=0]Training loss: 1.3690333366394043\n",
      "Epoch 1:   6%|▋         | 32/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.2408511638641357\n",
      "Epoch 1:   6%|▋         | 33/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.063374638557434\n",
      "Epoch 1:   7%|▋         | 34/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 1.3560786247253418\n",
      "Epoch 1:   7%|▋         | 35/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.4892940521240234\n",
      "Epoch 1:   7%|▋         | 36/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 0.9098938703536987\n",
      "Epoch 1:   7%|▋         | 37/512 [00:14<03:01,  2.62it/s, v_num=0]Training loss: 1.0092575550079346\n",
      "Epoch 1:   7%|▋         | 38/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.4129014015197754\n",
      "Epoch 1:   8%|▊         | 39/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.0574281215667725\n",
      "Epoch 1:   8%|▊         | 40/512 [00:15<03:00,  2.62it/s, v_num=0]Training loss: 1.240064024925232\n",
      "Epoch 1:   8%|▊         | 41/512 [00:15<02:59,  2.62it/s, v_num=0]Training loss: 0.881303071975708\n",
      "Epoch 1:   8%|▊         | 42/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.030771255493164\n",
      "Epoch 1:   8%|▊         | 43/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.1724364757537842\n",
      "Epoch 1:   9%|▊         | 44/512 [00:16<02:58,  2.61it/s, v_num=0]Training loss: 0.8375972509384155\n",
      "Epoch 1:   9%|▉         | 45/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.260020136833191\n",
      "Epoch 1:   9%|▉         | 46/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.3131450414657593\n",
      "Epoch 1:   9%|▉         | 47/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.09847092628479\n",
      "Epoch 1:   9%|▉         | 48/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.3917996883392334\n",
      "Epoch 1:  10%|▉         | 49/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.2706767320632935\n",
      "Epoch 1:  10%|▉         | 50/512 [00:19<02:57,  2.61it/s, v_num=0]Training loss: 1.3367116451263428\n",
      "Epoch 1:  10%|▉         | 51/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.309427261352539\n",
      "Epoch 1:  10%|█         | 52/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.4053698778152466\n",
      "Epoch 1:  10%|█         | 53/512 [00:20<02:56,  2.61it/s, v_num=0]Training loss: 1.176435947418213\n",
      "Epoch 1:  11%|█         | 54/512 [00:20<02:55,  2.61it/s, v_num=0]Training loss: 1.2821331024169922\n",
      "Epoch 1:  11%|█         | 55/512 [00:21<02:55,  2.61it/s, v_num=0]Training loss: 0.8045241832733154\n",
      "Epoch 1:  11%|█         | 56/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 1.3861267566680908\n",
      "Epoch 1:  11%|█         | 57/512 [00:21<02:54,  2.60it/s, v_num=0]Training loss: 1.0597177743911743\n",
      "Epoch 1:  11%|█▏        | 58/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 1.2726378440856934\n",
      "Epoch 1:  12%|█▏        | 59/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 1.9290424585342407\n",
      "Epoch 1:  12%|█▏        | 60/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.3510421514511108\n",
      "Epoch 1:  12%|█▏        | 61/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.451309323310852\n",
      "Epoch 1:  12%|█▏        | 62/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.0542998313903809\n",
      "Epoch 1:  12%|█▏        | 63/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.753299355506897\n",
      "Epoch 1:  12%|█▎        | 64/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.3198190927505493\n",
      "Epoch 1:  13%|█▎        | 65/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.5932085514068604\n",
      "Epoch 1:  13%|█▎        | 66/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.4118032455444336\n",
      "Epoch 1:  13%|█▎        | 67/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.3270323276519775\n",
      "Epoch 1:  13%|█▎        | 68/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.2664294242858887\n",
      "Epoch 1:  13%|█▎        | 69/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.0012410879135132\n",
      "Epoch 1:  14%|█▎        | 70/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.0567797422409058\n",
      "Epoch 1:  14%|█▍        | 71/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.2991784811019897\n",
      "Epoch 1:  14%|█▍        | 72/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.5006520748138428\n",
      "Epoch 1:  14%|█▍        | 73/512 [00:28<02:49,  2.60it/s, v_num=0]Training loss: 0.9927712678909302\n",
      "Epoch 1:  14%|█▍        | 74/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.2235735654830933\n",
      "Epoch 1:  15%|█▍        | 75/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.2493972778320312\n",
      "Epoch 1:  15%|█▍        | 76/512 [00:29<02:48,  2.59it/s, v_num=0]Training loss: 0.8579160571098328\n",
      "Epoch 1:  15%|█▌        | 77/512 [00:29<02:47,  2.59it/s, v_num=0]Training loss: 1.697296380996704\n",
      "Epoch 1:  15%|█▌        | 78/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.019671082496643\n",
      "Epoch 1:  15%|█▌        | 79/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.1835105419158936\n",
      "Epoch 1:  16%|█▌        | 80/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 1.2118961811065674\n",
      "Epoch 1:  16%|█▌        | 81/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 1.312854290008545\n",
      "Epoch 1:  16%|█▌        | 82/512 [00:31<02:45,  2.59it/s, v_num=0]Training loss: 1.3183796405792236\n",
      "Epoch 1:  16%|█▌        | 83/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.146310806274414\n",
      "Epoch 1:  16%|█▋        | 84/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.5201979875564575\n",
      "Epoch 1:  17%|█▋        | 85/512 [00:32<02:44,  2.59it/s, v_num=0]Training loss: 1.09950590133667\n",
      "Epoch 1:  17%|█▋        | 86/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.2154648303985596\n",
      "Epoch 1:  17%|█▋        | 87/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.4157726764678955\n",
      "Epoch 1:  17%|█▋        | 88/512 [00:33<02:43,  2.59it/s, v_num=0]Training loss: 1.1892229318618774\n",
      "Epoch 1:  17%|█▋        | 89/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.116294503211975\n",
      "Epoch 1:  18%|█▊        | 90/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.6285548210144043\n",
      "Epoch 1:  18%|█▊        | 91/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.0778999328613281\n",
      "Epoch 1:  18%|█▊        | 92/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.1751060485839844\n",
      "Epoch 1:  18%|█▊        | 93/512 [00:35<02:41,  2.59it/s, v_num=0]Training loss: 1.2263282537460327\n",
      "Epoch 1:  18%|█▊        | 94/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.2278947830200195\n",
      "Epoch 1:  19%|█▊        | 95/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.7221508026123047\n",
      "Epoch 1:  19%|█▉        | 96/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.226885199546814\n",
      "Epoch 1:  19%|█▉        | 97/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.6531174182891846\n",
      "Epoch 1:  19%|█▉        | 98/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.693021297454834\n",
      "Epoch 1:  19%|█▉        | 99/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 1.4072903394699097\n",
      "Epoch 1:  20%|█▉        | 100/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 1.8660883903503418\n",
      "Epoch 1:  20%|█▉        | 101/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.2038906812667847\n",
      "Epoch 1:  20%|█▉        | 102/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.5152732133865356\n",
      "Epoch 1:  20%|██        | 103/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.0731011629104614\n",
      "Epoch 1:  20%|██        | 104/512 [00:40<02:37,  2.59it/s, v_num=0]Training loss: 1.6438367366790771\n",
      "Epoch 1:  21%|██        | 105/512 [00:40<02:37,  2.59it/s, v_num=0]Training loss: 1.6863528490066528\n",
      "Epoch 1:  21%|██        | 106/512 [00:41<02:37,  2.59it/s, v_num=0]Training loss: 1.5159647464752197\n",
      "Epoch 1:  21%|██        | 107/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.4215829372406006\n",
      "Epoch 1:  21%|██        | 108/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.3402332067489624\n",
      "Epoch 1:  21%|██▏       | 109/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.0136220455169678\n",
      "Epoch 1:  21%|██▏       | 110/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 0.9835661053657532\n",
      "Epoch 1:  22%|██▏       | 111/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.2551239728927612\n",
      "Epoch 1:  22%|██▏       | 112/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 0.9865202307701111\n",
      "Epoch 1:  22%|██▏       | 113/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.4403362274169922\n",
      "Epoch 1:  22%|██▏       | 114/512 [00:44<02:34,  2.58it/s, v_num=0]Training loss: 1.4820330142974854\n",
      "Epoch 1:  22%|██▏       | 115/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 1.7592999935150146\n",
      "Epoch 1:  23%|██▎       | 116/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 1.1378192901611328\n",
      "Epoch 1:  23%|██▎       | 117/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.2774733304977417\n",
      "Epoch 1:  23%|██▎       | 118/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.6994632482528687\n",
      "Epoch 1:  23%|██▎       | 119/512 [00:46<02:32,  2.58it/s, v_num=0]Training loss: 1.3707629442214966\n",
      "Epoch 1:  23%|██▎       | 120/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.9518865942955017\n",
      "Epoch 1:  24%|██▎       | 121/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 1.3475513458251953\n",
      "Epoch 1:  24%|██▍       | 122/512 [00:47<02:31,  2.58it/s, v_num=0]Training loss: 1.4203952550888062\n",
      "Epoch 1:  24%|██▍       | 123/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 1.1915929317474365\n",
      "Epoch 1:  24%|██▍       | 124/512 [00:48<02:30,  2.58it/s, v_num=0]Training loss: 1.1446194648742676\n",
      "Epoch 1:  24%|██▍       | 125/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.2200335264205933\n",
      "Epoch 1:  25%|██▍       | 126/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.3959753513336182\n",
      "Epoch 1:  25%|██▍       | 127/512 [00:49<02:29,  2.58it/s, v_num=0]Training loss: 1.4270169734954834\n",
      "Epoch 1:  25%|██▌       | 128/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 1.2903863191604614\n",
      "Epoch 1:  25%|██▌       | 129/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 0.9177739024162292\n",
      "Epoch 1:  25%|██▌       | 130/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 1.200215220451355\n",
      "Epoch 1:  26%|██▌       | 131/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 1.3071993589401245\n",
      "Epoch 1:  26%|██▌       | 132/512 [00:51<02:27,  2.58it/s, v_num=0]Training loss: 1.476025104522705\n",
      "Epoch 1:  26%|██▌       | 133/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.4956648349761963\n",
      "Epoch 1:  26%|██▌       | 134/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.388079047203064\n",
      "Epoch 1:  26%|██▋       | 135/512 [00:52<02:26,  2.58it/s, v_num=0]Training loss: 1.0890748500823975\n",
      "Epoch 1:  27%|██▋       | 136/512 [00:52<02:25,  2.58it/s, v_num=0]Training loss: 1.446569800376892\n",
      "Epoch 1:  27%|██▋       | 137/512 [00:53<02:25,  2.58it/s, v_num=0]Training loss: 1.4594393968582153\n",
      "Epoch 1:  27%|██▋       | 138/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 1.593773603439331\n",
      "Epoch 1:  27%|██▋       | 139/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 0.9414398074150085\n",
      "Epoch 1:  27%|██▋       | 140/512 [00:54<02:24,  2.58it/s, v_num=0]Training loss: 2.072925567626953\n",
      "Epoch 1:  28%|██▊       | 141/512 [00:54<02:23,  2.58it/s, v_num=0]Training loss: 0.9254817962646484\n",
      "Epoch 1:  28%|██▊       | 142/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.2226303815841675\n",
      "Epoch 1:  28%|██▊       | 143/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.2108019590377808\n",
      "Epoch 1:  28%|██▊       | 144/512 [00:55<02:22,  2.58it/s, v_num=0]Training loss: 1.7591910362243652\n",
      "Epoch 1:  28%|██▊       | 145/512 [00:56<02:22,  2.58it/s, v_num=0]Training loss: 1.5259370803833008\n",
      "Epoch 1:  29%|██▊       | 146/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 1.7991689443588257\n",
      "Epoch 1:  29%|██▊       | 147/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 1.9409734010696411\n",
      "Epoch 1:  29%|██▉       | 148/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 1.554115891456604\n",
      "Epoch 1:  29%|██▉       | 149/512 [00:57<02:20,  2.58it/s, v_num=0]Training loss: 2.184849262237549\n",
      "Epoch 1:  29%|██▉       | 150/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 1.8295161724090576\n",
      "Epoch 1:  29%|██▉       | 151/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 1.7257381677627563\n",
      "Epoch 1:  30%|██▉       | 152/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 1.0374447107315063\n",
      "Epoch 1:  30%|██▉       | 153/512 [00:59<02:19,  2.58it/s, v_num=0]Training loss: 1.2827481031417847\n",
      "Epoch 1:  30%|███       | 154/512 [00:59<02:18,  2.58it/s, v_num=0]Training loss: 1.571205496788025\n",
      "Epoch 1:  30%|███       | 155/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.1538296937942505\n",
      "Epoch 1:  30%|███       | 156/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.1413451433181763\n",
      "Epoch 1:  31%|███       | 157/512 [01:00<02:17,  2.58it/s, v_num=0]Training loss: 1.1576309204101562\n",
      "Epoch 1:  31%|███       | 158/512 [01:01<02:17,  2.58it/s, v_num=0]Training loss: 1.5033518075942993\n",
      "Epoch 1:  31%|███       | 159/512 [01:01<02:16,  2.58it/s, v_num=0]Training loss: 1.7762348651885986\n",
      "Epoch 1:  31%|███▏      | 160/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 2.8215749263763428\n",
      "Epoch 1:  31%|███▏      | 161/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 1.0331752300262451\n",
      "Epoch 1:  32%|███▏      | 162/512 [01:02<02:15,  2.58it/s, v_num=0]Training loss: 1.1618015766143799\n",
      "Epoch 1:  32%|███▏      | 163/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 1.310349941253662\n",
      "Epoch 1:  32%|███▏      | 164/512 [01:03<02:14,  2.58it/s, v_num=0]Training loss: 1.60540771484375\n",
      "Epoch 1:  32%|███▏      | 165/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 1.039308786392212\n",
      "Epoch 1:  32%|███▏      | 166/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 1.2256476879119873\n",
      "Epoch 1:  33%|███▎      | 167/512 [01:04<02:13,  2.58it/s, v_num=0]Training loss: 1.2647062540054321\n",
      "Epoch 1:  33%|███▎      | 168/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.2261449098587036\n",
      "Epoch 1:  33%|███▎      | 169/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.7894083261489868\n",
      "Epoch 1:  33%|███▎      | 170/512 [01:05<02:12,  2.58it/s, v_num=0]Training loss: 1.4395917654037476\n",
      "Epoch 1:  33%|███▎      | 171/512 [01:06<02:12,  2.58it/s, v_num=0]Training loss: 1.582108497619629\n",
      "Epoch 1:  34%|███▎      | 172/512 [01:06<02:11,  2.58it/s, v_num=0]Training loss: 1.4475195407867432\n",
      "Epoch 1:  34%|███▍      | 173/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.5907841920852661\n",
      "Epoch 1:  34%|███▍      | 174/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.5492054224014282\n",
      "Epoch 1:  34%|███▍      | 175/512 [01:07<02:10,  2.58it/s, v_num=0]Training loss: 1.2521936893463135\n",
      "Epoch 1:  34%|███▍      | 176/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.5173895359039307\n",
      "Epoch 1:  35%|███▍      | 177/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.4231055974960327\n",
      "Epoch 1:  35%|███▍      | 178/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 1.2395696640014648\n",
      "Epoch 1:  35%|███▍      | 179/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 1.5821599960327148\n",
      "Epoch 1:  35%|███▌      | 180/512 [01:09<02:08,  2.58it/s, v_num=0]Training loss: 0.9532618522644043\n",
      "Epoch 1:  35%|███▌      | 181/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 1.5932401418685913\n",
      "Epoch 1:  36%|███▌      | 182/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 1.1960656642913818\n",
      "Epoch 1:  36%|███▌      | 183/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.004948377609253\n",
      "Epoch 1:  36%|███▌      | 184/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.2117537260055542\n",
      "Epoch 1:  36%|███▌      | 185/512 [01:11<02:06,  2.58it/s, v_num=0]Training loss: 1.7460271120071411\n",
      "Epoch 1:  36%|███▋      | 186/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 1.6010677814483643\n",
      "Epoch 1:  37%|███▋      | 187/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 1.3704643249511719\n",
      "Epoch 1:  37%|███▋      | 188/512 [01:12<02:05,  2.58it/s, v_num=0]Training loss: 0.931380033493042\n",
      "Epoch 1:  37%|███▋      | 189/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 1.3171879053115845\n",
      "Epoch 1:  37%|███▋      | 190/512 [01:13<02:04,  2.58it/s, v_num=0]Training loss: 0.9878190755844116\n",
      "Epoch 1:  37%|███▋      | 191/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 0.9355229735374451\n",
      "Epoch 1:  38%|███▊      | 192/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.7727153301239014\n",
      "Epoch 1:  38%|███▊      | 193/512 [01:14<02:03,  2.58it/s, v_num=0]Training loss: 1.3759410381317139\n",
      "Epoch 1:  38%|███▊      | 194/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 1.1331156492233276\n",
      "Epoch 1:  38%|███▊      | 195/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 1.364923119544983\n",
      "Epoch 1:  38%|███▊      | 196/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.1351784467697144\n",
      "Epoch 1:  38%|███▊      | 197/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.4393742084503174\n",
      "Epoch 1:  39%|███▊      | 198/512 [01:16<02:01,  2.58it/s, v_num=0]Training loss: 1.3593418598175049\n",
      "Epoch 1:  39%|███▉      | 199/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 0.8718281388282776\n",
      "Epoch 1:  39%|███▉      | 200/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 1.6589969396591187\n",
      "Epoch 1:  39%|███▉      | 201/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 1.6112427711486816\n",
      "Epoch 1:  39%|███▉      | 202/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 1.3235671520233154\n",
      "Epoch 1:  40%|███▉      | 203/512 [01:18<01:59,  2.58it/s, v_num=0]Training loss: 1.178769826889038\n",
      "Epoch 1:  40%|███▉      | 204/512 [01:19<01:59,  2.58it/s, v_num=0]Training loss: 1.3676276206970215\n",
      "Epoch 1:  40%|████      | 205/512 [01:19<01:59,  2.58it/s, v_num=0]Training loss: 1.343509554862976\n",
      "Epoch 1:  40%|████      | 206/512 [01:19<01:58,  2.58it/s, v_num=0]Training loss: 2.196868658065796\n",
      "Epoch 1:  40%|████      | 207/512 [01:20<01:58,  2.58it/s, v_num=0]Training loss: 0.8595690727233887\n",
      "Epoch 1:  41%|████      | 208/512 [01:20<01:58,  2.58it/s, v_num=0]Training loss: 1.524437427520752\n",
      "Epoch 1:  41%|████      | 209/512 [01:21<01:57,  2.58it/s, v_num=0]Training loss: 0.9630721807479858\n",
      "Epoch 1:  41%|████      | 210/512 [01:21<01:57,  2.58it/s, v_num=0]Training loss: 1.5840611457824707\n",
      "Epoch 1:  41%|████      | 211/512 [01:21<01:56,  2.57it/s, v_num=0]Training loss: 1.21113121509552\n",
      "Epoch 1:  41%|████▏     | 212/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 1.2434430122375488\n",
      "Epoch 1:  42%|████▏     | 213/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 1.0123841762542725\n",
      "Epoch 1:  42%|████▏     | 214/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.0472346544265747\n",
      "Epoch 1:  42%|████▏     | 215/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.6948282718658447\n",
      "Epoch 1:  42%|████▏     | 216/512 [01:23<01:54,  2.57it/s, v_num=0]Training loss: 1.1251511573791504\n",
      "Epoch 1:  42%|████▏     | 217/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 0.8811112642288208\n",
      "Epoch 1:  43%|████▎     | 218/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.5671658515930176\n",
      "Epoch 1:  43%|████▎     | 219/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.4974133968353271\n",
      "Epoch 1:  43%|████▎     | 220/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.5935258865356445\n",
      "Epoch 1:  43%|████▎     | 221/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.9873409271240234\n",
      "Epoch 1:  43%|████▎     | 222/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 1.1815779209136963\n",
      "Epoch 1:  44%|████▎     | 223/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 1.1176475286483765\n",
      "Epoch 1:  44%|████▍     | 224/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.152451992034912\n",
      "Epoch 1:  44%|████▍     | 225/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.207887887954712\n",
      "Epoch 1:  44%|████▍     | 226/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.9789739847183228\n",
      "Epoch 1:  44%|████▍     | 227/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 1.449297547340393\n",
      "Epoch 1:  45%|████▍     | 228/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 1.1907987594604492\n",
      "Epoch 1:  45%|████▍     | 229/512 [01:28<01:49,  2.57it/s, v_num=0]Training loss: 1.222329020500183\n",
      "Epoch 1:  45%|████▍     | 230/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 0.9937485456466675\n",
      "Epoch 1:  45%|████▌     | 231/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 1.2936863899230957\n",
      "Epoch 1:  45%|████▌     | 232/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.616863489151001\n",
      "Epoch 1:  46%|████▌     | 233/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.4923653602600098\n",
      "Epoch 1:  46%|████▌     | 234/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.9192075729370117\n",
      "Epoch 1:  46%|████▌     | 235/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.0426982641220093\n",
      "Epoch 1:  46%|████▌     | 236/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.0796462297439575\n",
      "Epoch 1:  46%|████▋     | 237/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.6771974563598633\n",
      "Epoch 1:  46%|████▋     | 238/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.0375783443450928\n",
      "Epoch 1:  47%|████▋     | 239/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.2081857919692993\n",
      "Epoch 1:  47%|████▋     | 240/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.6261987686157227\n",
      "Epoch 1:  47%|████▋     | 241/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.2871370315551758\n",
      "Epoch 1:  47%|████▋     | 242/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.5834479331970215\n",
      "Epoch 1:  47%|████▋     | 243/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.1464471817016602\n",
      "Epoch 1:  48%|████▊     | 244/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.454148292541504\n",
      "Epoch 1:  48%|████▊     | 245/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 1.1294851303100586\n",
      "Epoch 1:  48%|████▊     | 246/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 0.9436575174331665\n",
      "Epoch 1:  48%|████▊     | 247/512 [01:35<01:42,  2.57it/s, v_num=0]Training loss: 0.9298028945922852\n",
      "Epoch 1:  48%|████▊     | 248/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.218610167503357\n",
      "Epoch 1:  49%|████▊     | 249/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.1344068050384521\n",
      "Epoch 1:  49%|████▉     | 250/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.07062566280365\n",
      "Epoch 1:  49%|████▉     | 251/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.2547845840454102\n",
      "Epoch 1:  49%|████▉     | 252/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.31220543384552\n",
      "Epoch 1:  49%|████▉     | 253/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.4842562675476074\n",
      "Epoch 1:  50%|████▉     | 254/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.654739499092102\n",
      "Epoch 1:  50%|████▉     | 255/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.9027737379074097\n",
      "Epoch 1:  50%|█████     | 256/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.186623215675354\n",
      "Epoch 1:  50%|█████     | 257/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.0912576913833618\n",
      "Epoch 1:  50%|█████     | 258/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 1.5755538940429688\n",
      "Epoch 1:  51%|█████     | 259/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 1.2325435876846313\n",
      "Epoch 1:  51%|█████     | 260/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 1.4955370426177979\n",
      "Epoch 1:  51%|█████     | 261/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 16.250844955444336\n",
      "Epoch 1:  51%|█████     | 262/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 1.2102065086364746\n",
      "Epoch 1:  51%|█████▏    | 263/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.8267450332641602\n",
      "Epoch 1:  52%|█████▏    | 264/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.5800877809524536\n",
      "Epoch 1:  52%|█████▏    | 265/512 [01:42<01:35,  2.57it/s, v_num=0]Training loss: 1.9137680530548096\n",
      "Epoch 1:  52%|█████▏    | 266/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 1.693428635597229\n",
      "Epoch 1:  52%|█████▏    | 267/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 1.2708768844604492\n",
      "Epoch 1:  52%|█████▏    | 268/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.1351944208145142\n",
      "Epoch 1:  53%|█████▎    | 269/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.0195839405059814\n",
      "Epoch 1:  53%|█████▎    | 270/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.0891106128692627\n",
      "Epoch 1:  53%|█████▎    | 271/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 3.1295528411865234\n",
      "Epoch 1:  53%|█████▎    | 272/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 1.1228898763656616\n",
      "Epoch 1:  53%|█████▎    | 273/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 2.8362393379211426\n",
      "Epoch 1:  54%|█████▎    | 274/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.0897305011749268\n",
      "Epoch 1:  54%|█████▎    | 275/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.6248222589492798\n",
      "Epoch 1:  54%|█████▍    | 276/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.1039936542510986\n",
      "Epoch 1:  54%|█████▍    | 277/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.4294530153274536\n",
      "Epoch 1:  54%|█████▍    | 278/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.3308724164962769\n",
      "Epoch 1:  54%|█████▍    | 279/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.0525301694869995\n",
      "Epoch 1:  55%|█████▍    | 280/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.5589491128921509\n",
      "Epoch 1:  55%|█████▍    | 281/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.276934266090393\n",
      "Epoch 1:  55%|█████▌    | 282/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.2275865077972412\n",
      "Epoch 1:  55%|█████▌    | 283/512 [01:50<01:29,  2.57it/s, v_num=0]Training loss: 1.0106472969055176\n",
      "Epoch 1:  55%|█████▌    | 284/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.568868637084961\n",
      "Epoch 1:  56%|█████▌    | 285/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.096335530281067\n",
      "Epoch 1:  56%|█████▌    | 286/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.375292181968689\n",
      "Epoch 1:  56%|█████▌    | 287/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.2014799118041992\n",
      "Epoch 1:  56%|█████▋    | 288/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.6903241872787476\n",
      "Epoch 1:  56%|█████▋    | 289/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.1655668020248413\n",
      "Epoch 1:  57%|█████▋    | 290/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 0.9401149749755859\n",
      "Epoch 1:  57%|█████▋    | 291/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.0595977306365967\n",
      "Epoch 1:  57%|█████▋    | 292/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 0.958065927028656\n",
      "Epoch 1:  57%|█████▋    | 293/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.156059980392456\n",
      "Epoch 1:  57%|█████▋    | 294/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 1.8851807117462158\n",
      "Epoch 1:  58%|█████▊    | 295/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 1.1285752058029175\n",
      "Epoch 1:  58%|█████▊    | 296/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.3721429109573364\n",
      "Epoch 1:  58%|█████▊    | 297/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.1540237665176392\n",
      "Epoch 1:  58%|█████▊    | 298/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.7855807542800903\n",
      "Epoch 1:  58%|█████▊    | 299/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.3262118101119995\n",
      "Epoch 1:  59%|█████▊    | 300/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.15920090675354\n",
      "Epoch 1:  59%|█████▉    | 301/512 [01:57<01:22,  2.57it/s, v_num=0]Training loss: 1.0571389198303223\n",
      "Epoch 1:  59%|█████▉    | 302/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 1.2783843278884888\n",
      "Epoch 1:  59%|█████▉    | 303/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.8278902173042297\n",
      "Epoch 1:  59%|█████▉    | 304/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.3169031143188477\n",
      "Epoch 1:  60%|█████▉    | 305/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 2.0378549098968506\n",
      "Epoch 1:  60%|█████▉    | 306/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.3813954591751099\n",
      "Epoch 1:  60%|█████▉    | 307/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 0.9578853249549866\n",
      "Epoch 1:  60%|██████    | 308/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.5235823392868042\n",
      "Epoch 1:  60%|██████    | 309/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.359257459640503\n",
      "Epoch 1:  61%|██████    | 310/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.738400936126709\n",
      "Epoch 1:  61%|██████    | 311/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.2791038751602173\n",
      "Epoch 1:  61%|██████    | 312/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.6048243045806885\n",
      "Epoch 1:  61%|██████    | 313/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.122321367263794\n",
      "Epoch 1:  61%|██████▏   | 314/512 [02:02<01:17,  2.57it/s, v_num=0]Training loss: 2.0084547996520996\n",
      "Epoch 1:  62%|██████▏   | 315/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.290961742401123\n",
      "Epoch 1:  62%|██████▏   | 316/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.1167865991592407\n",
      "Epoch 1:  62%|██████▏   | 317/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.3570592403411865\n",
      "Epoch 1:  62%|██████▏   | 318/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.4567228555679321\n",
      "Epoch 1:  62%|██████▏   | 319/512 [02:04<01:15,  2.57it/s, v_num=0]Training loss: 1.2174125909805298\n",
      "Epoch 1:  62%|██████▎   | 320/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 0.7119045257568359\n",
      "Epoch 1:  63%|██████▎   | 321/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.4831843376159668\n",
      "Epoch 1:  63%|██████▎   | 322/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.6407573223114014\n",
      "Epoch 1:  63%|██████▎   | 323/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 0.9858224391937256\n",
      "Epoch 1:  63%|██████▎   | 324/512 [02:06<01:13,  2.57it/s, v_num=0]Training loss: 0.9935355186462402\n",
      "Epoch 1:  63%|██████▎   | 325/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.3132679462432861\n",
      "Epoch 1:  64%|██████▎   | 326/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 0.9444015622138977\n",
      "Epoch 1:  64%|██████▍   | 327/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.1327412128448486\n",
      "Epoch 1:  64%|██████▍   | 328/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.5829325914382935\n",
      "Epoch 1:  64%|██████▍   | 329/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.4094862937927246\n",
      "Epoch 1:  64%|██████▍   | 330/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.6691533327102661\n",
      "Epoch 1:  65%|██████▍   | 331/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.9457529783248901\n",
      "Epoch 1:  65%|██████▍   | 332/512 [02:09<01:10,  2.57it/s, v_num=0]Training loss: 1.3314554691314697\n",
      "Epoch 1:  65%|██████▌   | 333/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.0286870002746582\n",
      "Epoch 1:  65%|██████▌   | 334/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.1374680995941162\n",
      "Epoch 1:  65%|██████▌   | 335/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.9895389676094055\n",
      "Epoch 1:  66%|██████▌   | 336/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.961906373500824\n",
      "Epoch 1:  66%|██████▌   | 337/512 [02:11<01:08,  2.57it/s, v_num=0]Training loss: 0.9273777604103088\n",
      "Epoch 1:  66%|██████▌   | 338/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.1155935525894165\n",
      "Epoch 1:  66%|██████▌   | 339/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 2.1665687561035156\n",
      "Epoch 1:  66%|██████▋   | 340/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.512747049331665\n",
      "Epoch 1:  67%|██████▋   | 341/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.1763629913330078\n",
      "Epoch 1:  67%|██████▋   | 342/512 [02:13<01:06,  2.57it/s, v_num=0]Training loss: 0.8384150266647339\n",
      "Epoch 1:  67%|██████▋   | 343/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.1395529508590698\n",
      "Epoch 1:  67%|██████▋   | 344/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.3358755111694336\n",
      "Epoch 1:  67%|██████▋   | 345/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.2663308382034302\n",
      "Epoch 1:  68%|██████▊   | 346/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.7968364357948303\n",
      "Epoch 1:  68%|██████▊   | 347/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.7879815101623535\n",
      "Epoch 1:  68%|██████▊   | 348/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 1.2116212844848633\n",
      "Epoch 1:  68%|██████▊   | 349/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 1.2363591194152832\n",
      "Epoch 1:  68%|██████▊   | 350/512 [02:16<01:03,  2.57it/s, v_num=0]Training loss: 0.9536749124526978\n",
      "Epoch 1:  69%|██████▊   | 351/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.0153419971466064\n",
      "Epoch 1:  69%|██████▉   | 352/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.3133149147033691\n",
      "Epoch 1:  69%|██████▉   | 353/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 1.0649293661117554\n",
      "Epoch 1:  69%|██████▉   | 354/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.9546442627906799\n",
      "Epoch 1:  69%|██████▉   | 355/512 [02:18<01:01,  2.57it/s, v_num=0]Training loss: 1.445105791091919\n",
      "Epoch 1:  70%|██████▉   | 356/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.669736623764038\n",
      "Epoch 1:  70%|██████▉   | 357/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.4785715341567993\n",
      "Epoch 1:  70%|██████▉   | 358/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.0724756717681885\n",
      "Epoch 1:  70%|███████   | 359/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.3269429206848145\n",
      "Epoch 1:  70%|███████   | 360/512 [02:20<00:59,  2.57it/s, v_num=0]Training loss: 1.3675053119659424\n",
      "Epoch 1:  71%|███████   | 361/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 0.9103261232376099\n",
      "Epoch 1:  71%|███████   | 362/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 1.3918609619140625\n",
      "Epoch 1:  71%|███████   | 363/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.570538878440857\n",
      "Epoch 1:  71%|███████   | 364/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.2300224304199219\n",
      "Epoch 1:  71%|███████▏  | 365/512 [02:22<00:57,  2.57it/s, v_num=0]Training loss: 1.4951688051223755\n",
      "Epoch 1:  71%|███████▏  | 366/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 1.0585453510284424\n",
      "Epoch 1:  72%|███████▏  | 367/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 1.180770754814148\n",
      "Epoch 1:  72%|███████▏  | 368/512 [02:23<00:56,  2.57it/s, v_num=0]Training loss: 1.414024829864502\n",
      "Epoch 1:  72%|███████▏  | 369/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.5086021423339844\n",
      "Epoch 1:  72%|███████▏  | 370/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.02385675907135\n",
      "Epoch 1:  72%|███████▏  | 371/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.2185150384902954\n",
      "Epoch 1:  73%|███████▎  | 372/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 0.8989952802658081\n",
      "Epoch 1:  73%|███████▎  | 373/512 [02:25<00:54,  2.57it/s, v_num=0]Training loss: 1.402025818824768\n",
      "Epoch 1:  73%|███████▎  | 374/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 2.1656622886657715\n",
      "Epoch 1:  73%|███████▎  | 375/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.6373417377471924\n",
      "Epoch 1:  73%|███████▎  | 376/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 1.47402822971344\n",
      "Epoch 1:  74%|███████▎  | 377/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 1.0139005184173584\n",
      "Epoch 1:  74%|███████▍  | 378/512 [02:27<00:52,  2.57it/s, v_num=0]Training loss: 2.0128912925720215\n",
      "Epoch 1:  74%|███████▍  | 379/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.5702385902404785\n",
      "Epoch 1:  74%|███████▍  | 380/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.1375031471252441\n",
      "Epoch 1:  74%|███████▍  | 381/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.1860350370407104\n",
      "Epoch 1:  75%|███████▍  | 382/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.4425618648529053\n",
      "Epoch 1:  75%|███████▍  | 383/512 [02:29<00:50,  2.57it/s, v_num=0]Training loss: 1.448075294494629\n",
      "Epoch 1:  75%|███████▌  | 384/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.0583453178405762\n",
      "Epoch 1:  75%|███████▌  | 385/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.4338942766189575\n",
      "Epoch 1:  75%|███████▌  | 386/512 [02:30<00:49,  2.57it/s, v_num=0]Training loss: 1.0042343139648438\n",
      "Epoch 1:  76%|███████▌  | 387/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.2938097715377808\n",
      "Epoch 1:  76%|███████▌  | 388/512 [02:31<00:48,  2.57it/s, v_num=0]Training loss: 1.1670230627059937\n",
      "Epoch 1:  76%|███████▌  | 389/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.154276728630066\n",
      "Epoch 1:  76%|███████▌  | 390/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.2731244564056396\n",
      "Epoch 1:  76%|███████▋  | 391/512 [02:32<00:47,  2.57it/s, v_num=0]Training loss: 0.8772192001342773\n",
      "Epoch 1:  77%|███████▋  | 392/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.2852717638015747\n",
      "Epoch 1:  77%|███████▋  | 393/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 0.8810994029045105\n",
      "Epoch 1:  77%|███████▋  | 394/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.3205809593200684\n",
      "Epoch 1:  77%|███████▋  | 395/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.8891777992248535\n",
      "Epoch 1:  77%|███████▋  | 396/512 [02:34<00:45,  2.57it/s, v_num=0]Training loss: 0.9827991724014282\n",
      "Epoch 1:  78%|███████▊  | 397/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.3622031211853027\n",
      "Epoch 1:  78%|███████▊  | 398/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.680429458618164\n",
      "Epoch 1:  78%|███████▊  | 399/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 1.0943105220794678\n",
      "Epoch 1:  78%|███████▊  | 400/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 1.1028475761413574\n",
      "Epoch 1:  78%|███████▊  | 401/512 [02:36<00:43,  2.57it/s, v_num=0]Training loss: 1.0064611434936523\n",
      "Epoch 1:  79%|███████▊  | 402/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.118168592453003\n",
      "Epoch 1:  79%|███████▊  | 403/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.0632927417755127\n",
      "Epoch 1:  79%|███████▉  | 404/512 [02:37<00:42,  2.57it/s, v_num=0]Training loss: 1.402061104774475\n",
      "Epoch 1:  79%|███████▉  | 405/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.0445525646209717\n",
      "Epoch 1:  79%|███████▉  | 406/512 [02:38<00:41,  2.57it/s, v_num=0]Training loss: 1.3591002225875854\n",
      "Epoch 1:  79%|███████▉  | 407/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 1.6422432661056519\n",
      "Epoch 1:  80%|███████▉  | 408/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 1.4686816930770874\n",
      "Epoch 1:  80%|███████▉  | 409/512 [02:39<00:40,  2.57it/s, v_num=0]Training loss: 0.7795442938804626\n",
      "Epoch 1:  80%|████████  | 410/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 1.699100136756897\n",
      "Epoch 1:  80%|████████  | 411/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 2.392834424972534\n",
      "Epoch 1:  80%|████████  | 412/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 1.4917633533477783\n",
      "Epoch 1:  81%|████████  | 413/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 3.929844617843628\n",
      "Epoch 1:  81%|████████  | 414/512 [02:41<00:38,  2.57it/s, v_num=0]Training loss: 0.9866543412208557\n",
      "Epoch 1:  81%|████████  | 415/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.3019139766693115\n",
      "Epoch 1:  81%|████████▏ | 416/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.0157593488693237\n",
      "Epoch 1:  81%|████████▏ | 417/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.4462294578552246\n",
      "Epoch 1:  82%|████████▏ | 418/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.523795485496521\n",
      "Epoch 1:  82%|████████▏ | 419/512 [02:43<00:36,  2.57it/s, v_num=0]Training loss: 1.2966516017913818\n",
      "Epoch 1:  82%|████████▏ | 420/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 1.4844624996185303\n",
      "Epoch 1:  82%|████████▏ | 421/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 1.133813500404358\n",
      "Epoch 1:  82%|████████▏ | 422/512 [02:44<00:35,  2.57it/s, v_num=0]Training loss: 1.4054844379425049\n",
      "Epoch 1:  83%|████████▎ | 423/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 2.2047924995422363\n",
      "Epoch 1:  83%|████████▎ | 424/512 [02:45<00:34,  2.57it/s, v_num=0]Training loss: 1.419448971748352\n",
      "Epoch 1:  83%|████████▎ | 425/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 0.9636269807815552\n",
      "Epoch 1:  83%|████████▎ | 426/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 1.2376513481140137\n",
      "Epoch 1:  83%|████████▎ | 427/512 [02:46<00:33,  2.57it/s, v_num=0]Training loss: 0.9456378221511841\n",
      "Epoch 1:  84%|████████▎ | 428/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.2202606201171875\n",
      "Epoch 1:  84%|████████▍ | 429/512 [02:47<00:32,  2.57it/s, v_num=0]Training loss: 1.5796278715133667\n",
      "Epoch 1:  84%|████████▍ | 430/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.1008431911468506\n",
      "Epoch 1:  84%|████████▍ | 431/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.0476213693618774\n",
      "Epoch 1:  84%|████████▍ | 432/512 [02:48<00:31,  2.57it/s, v_num=0]Training loss: 1.5198676586151123\n",
      "Epoch 1:  85%|████████▍ | 433/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.8743148446083069\n",
      "Epoch 1:  85%|████████▍ | 434/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.9192813634872437\n",
      "Epoch 1:  85%|████████▍ | 435/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.3891065120697021\n",
      "Epoch 1:  85%|████████▌ | 436/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.2787659168243408\n",
      "Epoch 1:  85%|████████▌ | 437/512 [02:50<00:29,  2.57it/s, v_num=0]Training loss: 1.280622959136963\n",
      "Epoch 1:  86%|████████▌ | 438/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.2519557476043701\n",
      "Epoch 1:  86%|████████▌ | 439/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.6823831796646118\n",
      "Epoch 1:  86%|████████▌ | 440/512 [02:51<00:28,  2.57it/s, v_num=0]Training loss: 1.209436058998108\n",
      "Epoch 1:  86%|████████▌ | 441/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 1.1231492757797241\n",
      "Epoch 1:  86%|████████▋ | 442/512 [02:52<00:27,  2.57it/s, v_num=0]Training loss: 1.7586909532546997\n",
      "Epoch 1:  87%|████████▋ | 443/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 1.3527470827102661\n",
      "Epoch 1:  87%|████████▋ | 444/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 0.950263500213623\n",
      "Epoch 1:  87%|████████▋ | 445/512 [02:53<00:26,  2.57it/s, v_num=0]Training loss: 1.1503266096115112\n",
      "Epoch 1:  87%|████████▋ | 446/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 0.9748879671096802\n",
      "Epoch 1:  87%|████████▋ | 447/512 [02:54<00:25,  2.57it/s, v_num=0]Training loss: 2.859095811843872\n",
      "Epoch 1:  88%|████████▊ | 448/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 1.1447041034698486\n",
      "Epoch 1:  88%|████████▊ | 449/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 0.6424281597137451\n",
      "Epoch 1:  88%|████████▊ | 450/512 [02:55<00:24,  2.57it/s, v_num=0]Training loss: 1.2689341306686401\n",
      "Epoch 1:  88%|████████▊ | 451/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 1.9021189212799072\n",
      "Epoch 1:  88%|████████▊ | 452/512 [02:56<00:23,  2.57it/s, v_num=0]Training loss: 1.3026925325393677\n",
      "Epoch 1:  88%|████████▊ | 453/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 1.014130711555481\n",
      "Epoch 1:  89%|████████▊ | 454/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 1.1555477380752563\n",
      "Epoch 1:  89%|████████▉ | 455/512 [02:57<00:22,  2.57it/s, v_num=0]Training loss: 0.7612130045890808\n",
      "Epoch 1:  89%|████████▉ | 456/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.328033447265625\n",
      "Epoch 1:  89%|████████▉ | 457/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.043752908706665\n",
      "Epoch 1:  89%|████████▉ | 458/512 [02:58<00:21,  2.57it/s, v_num=0]Training loss: 0.9086231589317322\n",
      "Epoch 1:  90%|████████▉ | 459/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 1.555345058441162\n",
      "Epoch 1:  90%|████████▉ | 460/512 [02:59<00:20,  2.57it/s, v_num=0]Training loss: 2.0121171474456787\n",
      "Epoch 1:  90%|█████████ | 461/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 2.6153204441070557\n",
      "Epoch 1:  90%|█████████ | 462/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.0818780660629272\n",
      "Epoch 1:  90%|█████████ | 463/512 [03:00<00:19,  2.57it/s, v_num=0]Training loss: 1.3433829545974731\n",
      "Epoch 1:  91%|█████████ | 464/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 1.0569884777069092\n",
      "Epoch 1:  91%|█████████ | 465/512 [03:01<00:18,  2.57it/s, v_num=0]Training loss: 0.8757367730140686\n",
      "Epoch 1:  91%|█████████ | 466/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 1.1487902402877808\n",
      "Epoch 1:  91%|█████████ | 467/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 1.2363730669021606\n",
      "Epoch 1:  91%|█████████▏| 468/512 [03:02<00:17,  2.57it/s, v_num=0]Training loss: 1.5574562549591064\n",
      "Epoch 1:  92%|█████████▏| 469/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 0.9419742226600647\n",
      "Epoch 1:  92%|█████████▏| 470/512 [03:03<00:16,  2.57it/s, v_num=0]Training loss: 1.9599021673202515\n",
      "Epoch 1:  92%|█████████▏| 471/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.004883050918579\n",
      "Epoch 1:  92%|█████████▏| 472/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.4951688051223755\n",
      "Epoch 1:  92%|█████████▏| 473/512 [03:04<00:15,  2.57it/s, v_num=0]Training loss: 0.9709641933441162\n",
      "Epoch 1:  93%|█████████▎| 474/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 2.006453514099121\n",
      "Epoch 1:  93%|█████████▎| 475/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.733980417251587\n",
      "Epoch 1:  93%|█████████▎| 476/512 [03:05<00:14,  2.57it/s, v_num=0]Training loss: 0.8918048143386841\n",
      "Epoch 1:  93%|█████████▎| 477/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 1.104089617729187\n",
      "Epoch 1:  93%|█████████▎| 478/512 [03:06<00:13,  2.57it/s, v_num=0]Training loss: 1.155019760131836\n",
      "Epoch 1:  94%|█████████▎| 479/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.3827811479568481\n",
      "Epoch 1:  94%|█████████▍| 480/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.5116393566131592\n",
      "Epoch 1:  94%|█████████▍| 481/512 [03:07<00:12,  2.57it/s, v_num=0]Training loss: 1.0547150373458862\n",
      "Epoch 1:  94%|█████████▍| 482/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 1.2642810344696045\n",
      "Epoch 1:  94%|█████████▍| 483/512 [03:08<00:11,  2.57it/s, v_num=0]Training loss: 1.07699716091156\n",
      "Epoch 1:  95%|█████████▍| 484/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.0877935886383057\n",
      "Epoch 1:  95%|█████████▍| 485/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.3900761604309082\n",
      "Epoch 1:  95%|█████████▍| 486/512 [03:09<00:10,  2.57it/s, v_num=0]Training loss: 1.113276481628418\n",
      "Epoch 1:  95%|█████████▌| 487/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 0.8460730910301208\n",
      "Epoch 1:  95%|█████████▌| 488/512 [03:10<00:09,  2.57it/s, v_num=0]Training loss: 1.660404920578003\n",
      "Epoch 1:  96%|█████████▌| 489/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.4056488275527954\n",
      "Epoch 1:  96%|█████████▌| 490/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 0.8392086029052734\n",
      "Epoch 1:  96%|█████████▌| 491/512 [03:11<00:08,  2.57it/s, v_num=0]Training loss: 1.4274168014526367\n",
      "Epoch 1:  96%|█████████▌| 492/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.1917005777359009\n",
      "Epoch 1:  96%|█████████▋| 493/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 0.9424203038215637\n",
      "Epoch 1:  96%|█████████▋| 494/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 1.3800188302993774\n",
      "Epoch 1:  97%|█████████▋| 495/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 1.2799731492996216\n",
      "Epoch 1:  97%|█████████▋| 496/512 [03:13<00:06,  2.57it/s, v_num=0]Training loss: 1.1337182521820068\n",
      "Epoch 1:  97%|█████████▋| 497/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 1.2654650211334229\n",
      "Epoch 1:  97%|█████████▋| 498/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 1.2166399955749512\n",
      "Epoch 1:  97%|█████████▋| 499/512 [03:14<00:05,  2.57it/s, v_num=0]Training loss: 1.0504873991012573\n",
      "Epoch 1:  98%|█████████▊| 500/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 1.0669208765029907\n",
      "Epoch 1:  98%|█████████▊| 501/512 [03:15<00:04,  2.57it/s, v_num=0]Training loss: 0.9827663898468018\n",
      "Epoch 1:  98%|█████████▊| 502/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 1.1400642395019531\n",
      "Epoch 1:  98%|█████████▊| 503/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 0.8024365305900574\n",
      "Epoch 1:  98%|█████████▊| 504/512 [03:16<00:03,  2.57it/s, v_num=0]Training loss: 1.6489232778549194\n",
      "Epoch 1:  99%|█████████▊| 505/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 1.0622330904006958\n",
      "Epoch 1:  99%|█████████▉| 506/512 [03:17<00:02,  2.57it/s, v_num=0]Training loss: 1.1168100833892822\n",
      "Epoch 1:  99%|█████████▉| 507/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.1675631999969482\n",
      "Epoch 1:  99%|█████████▉| 508/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.0218912363052368\n",
      "Epoch 1:  99%|█████████▉| 509/512 [03:18<00:01,  2.57it/s, v_num=0]Training loss: 1.9128015041351318\n",
      "Epoch 1: 100%|█████████▉| 510/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 1.7437125444412231\n",
      "Epoch 1: 100%|█████████▉| 511/512 [03:19<00:00,  2.57it/s, v_num=0]Training loss: 1.0007065534591675\n",
      "Epoch 1: 100%|██████████| 512/512 [03:19<00:00,  2.57it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.3912116289138794\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 29.72it/s]\u001b[AValidation loss: 0.9767307639122009\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 23.85it/s]\u001b[AValidation loss: 1.5059170722961426\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 23.23it/s]\u001b[AValidation loss: 1.3248114585876465\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 23.10it/s]\u001b[AValidation loss: 1.0100332498550415\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 22.92it/s]\u001b[AValidation loss: 1.373271107673645\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 22.90it/s]\u001b[AValidation loss: 1.0901635885238647\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 22.81it/s]\u001b[AValidation loss: 1.3004176616668701\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 22.80it/s]\u001b[AValidation loss: 1.386640191078186\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.74it/s]\u001b[AValidation loss: 1.1994513273239136\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.73it/s]\u001b[AValidation loss: 0.9717592000961304\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.68it/s]\u001b[AValidation loss: 0.8650257587432861\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.68it/s]\u001b[AValidation loss: 1.071845293045044\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.66it/s]\u001b[AValidation loss: 1.2423794269561768\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 22.66it/s]\u001b[AValidation loss: 1.2128326892852783\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 22.62it/s]\u001b[AValidation loss: 1.4067058563232422\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 22.36it/s]\u001b[AValidation loss: 1.1595633029937744\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 22.35it/s]\u001b[AValidation loss: 0.89634770154953\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 22.36it/s]\u001b[AValidation loss: 0.7485190629959106\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 22.35it/s]\u001b[AValidation loss: 0.8687223196029663\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:04, 22.36it/s]\u001b[AValidation loss: 1.639967679977417\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:03, 22.29it/s]\u001b[AValidation loss: 0.887493371963501\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:03, 22.31it/s]\u001b[AValidation loss: 1.1796725988388062\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:03, 22.31it/s]\u001b[AValidation loss: 1.1599292755126953\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 0.9859111905097961\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 0.888749897480011\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 1.5722681283950806\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 1.0173145532608032\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:01<00:03, 22.33it/s]\u001b[AValidation loss: 1.2534043788909912\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:01<00:03, 22.33it/s]\u001b[AValidation loss: 2.422644853591919\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:01<00:03, 22.34it/s]\u001b[AValidation loss: 1.5778506994247437\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:03, 22.34it/s]\u001b[AValidation loss: 1.5450773239135742\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:03, 22.30it/s]\u001b[AValidation loss: 1.180161714553833\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:03, 22.30it/s]\u001b[AValidation loss: 1.2207090854644775\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:03, 22.31it/s]\u001b[AValidation loss: 1.2426490783691406\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:03, 22.31it/s]\u001b[AValidation loss: 0.870983898639679\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 1.0315666198730469\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:03, 22.32it/s]\u001b[AValidation loss: 1.3939390182495117\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:03, 22.33it/s]\u001b[AValidation loss: 1.0678951740264893\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:03, 22.33it/s]\u001b[AValidation loss: 0.8727865219116211\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:03, 22.34it/s]\u001b[AValidation loss: 1.1095600128173828\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:03, 22.34it/s]\u001b[AValidation loss: 1.1065771579742432\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:03, 22.31it/s]\u001b[AValidation loss: 3.4633893966674805\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:03, 22.31it/s]\u001b[AValidation loss: 1.3511428833007812\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 22.32it/s]\u001b[AValidation loss: 0.8713076710700989\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:02<00:02, 22.32it/s]\u001b[AValidation loss: 1.1268701553344727\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:02<00:02, 22.32it/s]\u001b[AValidation loss: 0.7860333323478699\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:02<00:02, 22.32it/s]\u001b[AValidation loss: 1.8006335496902466\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:02<00:02, 22.33it/s]\u001b[AValidation loss: 1.1269065141677856\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:02<00:02, 22.33it/s]\u001b[AValidation loss: 0.8838174343109131\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:02<00:02, 22.34it/s]\u001b[AValidation loss: 1.68559730052948\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:02<00:02, 22.34it/s]\u001b[AValidation loss: 1.8545438051223755\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:02<00:02, 22.34it/s]\u001b[AValidation loss: 1.3953157663345337\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:02<00:02, 22.34it/s]\u001b[AValidation loss: 1.5566093921661377\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:02<00:02, 22.35it/s]\u001b[AValidation loss: 1.7605822086334229\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:02<00:02, 22.34it/s]\u001b[AValidation loss: 1.0817738771438599\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:02<00:02, 22.35it/s]\u001b[AValidation loss: 0.8707848191261292\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:02<00:02, 22.35it/s]\u001b[AValidation loss: 1.265479326248169\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:02<00:02, 22.35it/s]\u001b[AValidation loss: 0.9944089651107788\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:02<00:02, 22.35it/s]\u001b[AValidation loss: 1.417189598083496\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 1.4499350786209106\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 1.1840256452560425\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 1.7756448984146118\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 2.3492236137390137\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 0.898868978023529\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:02, 22.36it/s]\u001b[AValidation loss: 1.438318133354187\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:02<00:01, 22.36it/s]\u001b[AValidation loss: 1.9403371810913086\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:02<00:01, 22.36it/s]\u001b[AValidation loss: 0.8889298439025879\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:03<00:01, 22.36it/s]\u001b[AValidation loss: 1.3551115989685059\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:03<00:01, 22.36it/s]\u001b[AValidation loss: 1.3481636047363281\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:03<00:01, 22.36it/s]\u001b[AValidation loss: 1.2042856216430664\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:03<00:01, 22.36it/s]\u001b[AValidation loss: 0.99660325050354\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 0.8173481822013855\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.2415951490402222\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.3387014865875244\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.0847625732421875\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.0388191938400269\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 0.692761242389679\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 0.7165490984916687\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.2221157550811768\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.5310122966766357\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:03<00:01, 22.37it/s]\u001b[AValidation loss: 1.120666265487671\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 0.8650822043418884\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 1.021493673324585\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 0.8681446313858032\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 1.0111162662506104\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 1.0329246520996094\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:03<00:01, 22.38it/s]\u001b[AValidation loss: 0.9456413984298706\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:03<00:00, 22.38it/s]\u001b[AValidation loss: 1.6268367767333984\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:03<00:00, 22.38it/s]\u001b[AValidation loss: 2.083275079727173\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 1.5129915475845337\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 0.9125632047653198\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 1.3362799882888794\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.7866790294647217\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.9527446031570435\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.2703098058700562\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.3372129201889038\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 0.8436684608459473\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.1914918422698975\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.0019922256469727\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 0.9013626575469971\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 1.2544423341751099\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 1.0330232381820679\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.3531800508499146\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.035840630531311\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.2812021970748901\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.014780044555664\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 1.2014316320419312\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:04<00:00, 22.39it/s]\u001b[AValidation loss: 0.964798629283905\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:04<00:00, 22.38it/s]\u001b[AValidation loss: 1.2371567487716675\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:04<00:00, 22.41it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/512 [00:00<?, ?it/s, v_num=0]                 \u001b[ATraining loss: 1.91740882396698\n",
      "Epoch 2:   0%|          | 1/512 [00:00<00:35, 14.56it/s, v_num=0]Training loss: 1.5659971237182617\n",
      "Epoch 2:   0%|          | 2/512 [00:00<01:55,  4.42it/s, v_num=0]Training loss: 1.8142597675323486\n",
      "Epoch 2:   1%|          | 3/512 [00:00<02:22,  3.56it/s, v_num=0]Training loss: 1.8226984739303589\n",
      "Epoch 2:   1%|          | 4/512 [00:01<02:36,  3.25it/s, v_num=0]Training loss: 1.4618639945983887\n",
      "Epoch 2:   1%|          | 5/512 [00:01<02:44,  3.08it/s, v_num=0]Training loss: 0.8466767072677612\n",
      "Epoch 2:   1%|          | 6/512 [00:02<02:49,  2.98it/s, v_num=0]Training loss: 1.3637452125549316\n",
      "Epoch 2:   1%|▏         | 7/512 [00:02<02:53,  2.91it/s, v_num=0]Training loss: 1.7213897705078125\n",
      "Epoch 2:   2%|▏         | 8/512 [00:02<02:55,  2.87it/s, v_num=0]Training loss: 1.6367790699005127\n",
      "Epoch 2:   2%|▏         | 9/512 [00:03<02:57,  2.83it/s, v_num=0]Training loss: 1.5207661390304565\n",
      "Epoch 2:   2%|▏         | 10/512 [00:03<02:59,  2.80it/s, v_num=0]Training loss: 1.6971299648284912\n",
      "Epoch 2:   2%|▏         | 11/512 [00:03<03:00,  2.78it/s, v_num=0]Training loss: 1.4541215896606445\n",
      "Epoch 2:   2%|▏         | 12/512 [00:04<03:01,  2.76it/s, v_num=0]Training loss: 1.628434419631958\n",
      "Epoch 2:   3%|▎         | 13/512 [00:04<03:01,  2.74it/s, v_num=0]Training loss: 0.9769896268844604\n",
      "Epoch 2:   3%|▎         | 14/512 [00:05<03:02,  2.73it/s, v_num=0]Training loss: 0.9795141220092773\n",
      "Epoch 2:   3%|▎         | 15/512 [00:05<03:02,  2.72it/s, v_num=0]Training loss: 1.3294270038604736\n",
      "Epoch 2:   3%|▎         | 16/512 [00:05<03:03,  2.71it/s, v_num=0]Training loss: 1.1268986463546753\n",
      "Epoch 2:   3%|▎         | 17/512 [00:06<03:03,  2.70it/s, v_num=0]Training loss: 0.8414726257324219\n",
      "Epoch 2:   4%|▎         | 18/512 [00:06<03:03,  2.69it/s, v_num=0]Training loss: 1.1883800029754639\n",
      "Epoch 2:   4%|▎         | 19/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 1.0967204570770264\n",
      "Epoch 2:   4%|▍         | 20/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 0.9834024906158447\n",
      "Epoch 2:   4%|▍         | 21/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 1.4991540908813477\n",
      "Epoch 2:   4%|▍         | 22/512 [00:08<03:03,  2.67it/s, v_num=0]Training loss: 1.6447696685791016\n",
      "Epoch 2:   4%|▍         | 23/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 1.2134575843811035\n",
      "Epoch 2:   5%|▍         | 24/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 1.7863154411315918\n",
      "Epoch 2:   5%|▍         | 25/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.3180344104766846\n",
      "Epoch 2:   5%|▌         | 26/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.0010559558868408\n",
      "Epoch 2:   5%|▌         | 27/512 [00:10<03:03,  2.65it/s, v_num=0]Training loss: 1.6173361539840698\n",
      "Epoch 2:   5%|▌         | 28/512 [00:10<03:03,  2.64it/s, v_num=0]Training loss: 1.6063939332962036\n",
      "Epoch 2:   6%|▌         | 29/512 [00:10<03:02,  2.64it/s, v_num=0]Training loss: 0.8627915382385254\n",
      "Epoch 2:   6%|▌         | 30/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 1.2600831985473633\n",
      "Epoch 2:   6%|▌         | 31/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 1.3559794425964355\n",
      "Epoch 2:   6%|▋         | 32/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 0.7749509811401367\n",
      "Epoch 2:   6%|▋         | 33/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.0935670137405396\n",
      "Epoch 2:   7%|▋         | 34/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 1.9031790494918823\n",
      "Epoch 2:   7%|▋         | 35/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.4394934177398682\n",
      "Epoch 2:   7%|▋         | 36/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.2207409143447876\n",
      "Epoch 2:   7%|▋         | 37/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 0.7578050494194031\n",
      "Epoch 2:   7%|▋         | 38/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 0.8773795962333679\n",
      "Epoch 2:   8%|▊         | 39/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.3330169916152954\n",
      "Epoch 2:   8%|▊         | 40/512 [00:15<03:00,  2.62it/s, v_num=0]Training loss: 1.632132649421692\n",
      "Epoch 2:   8%|▊         | 41/512 [00:15<02:59,  2.62it/s, v_num=0]Training loss: 1.0593194961547852\n",
      "Epoch 2:   8%|▊         | 42/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.9003651142120361\n",
      "Epoch 2:   8%|▊         | 43/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 0.6962818503379822\n",
      "Epoch 2:   9%|▊         | 44/512 [00:16<02:58,  2.61it/s, v_num=0]Training loss: 1.0335993766784668\n",
      "Epoch 2:   9%|▉         | 45/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.030336856842041\n",
      "Epoch 2:   9%|▉         | 46/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 0.9876692295074463\n",
      "Epoch 2:   9%|▉         | 47/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 0.9821354746818542\n",
      "Epoch 2:   9%|▉         | 48/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.1305161714553833\n",
      "Epoch 2:  10%|▉         | 49/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.0690721273422241\n",
      "Epoch 2:  10%|▉         | 50/512 [00:19<02:57,  2.61it/s, v_num=0]Training loss: 1.1476600170135498\n",
      "Epoch 2:  10%|▉         | 51/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 0.7902307510375977\n",
      "Epoch 2:  10%|█         | 52/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.1195510625839233\n",
      "Epoch 2:  10%|█         | 53/512 [00:20<02:56,  2.61it/s, v_num=0]Training loss: 1.0430365800857544\n",
      "Epoch 2:  11%|█         | 54/512 [00:20<02:55,  2.61it/s, v_num=0]Training loss: 1.1957768201828003\n",
      "Epoch 2:  11%|█         | 55/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 1.4877772331237793\n",
      "Epoch 2:  11%|█         | 56/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 1.201497197151184\n",
      "Epoch 2:  11%|█         | 57/512 [00:21<02:54,  2.60it/s, v_num=0]Training loss: 1.2683324813842773\n",
      "Epoch 2:  11%|█▏        | 58/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 1.28466796875\n",
      "Epoch 2:  12%|█▏        | 59/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 0.7444099187850952\n",
      "Epoch 2:  12%|█▏        | 60/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 0.8839640021324158\n",
      "Epoch 2:  12%|█▏        | 61/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.0307707786560059\n",
      "Epoch 2:  12%|█▏        | 62/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 0.9109885096549988\n",
      "Epoch 2:  12%|█▏        | 63/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 0.7700635194778442\n",
      "Epoch 2:  12%|█▎        | 64/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.4329291582107544\n",
      "Epoch 2:  13%|█▎        | 65/512 [00:25<02:52,  2.60it/s, v_num=0]Training loss: 1.461963415145874\n",
      "Epoch 2:  13%|█▎        | 66/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.090849757194519\n",
      "Epoch 2:  13%|█▎        | 67/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.0309700965881348\n",
      "Epoch 2:  13%|█▎        | 68/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 0.7728496193885803\n",
      "Epoch 2:  13%|█▎        | 69/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.2458086013793945\n",
      "Epoch 2:  14%|█▎        | 70/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.5055761337280273\n",
      "Epoch 2:  14%|█▍        | 71/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.0606207847595215\n",
      "Epoch 2:  14%|█▍        | 72/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.0122263431549072\n",
      "Epoch 2:  14%|█▍        | 73/512 [00:28<02:49,  2.59it/s, v_num=0]Training loss: 1.3783540725708008\n",
      "Epoch 2:  14%|█▍        | 74/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.5820837020874023\n",
      "Epoch 2:  15%|█▍        | 75/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.228934645652771\n",
      "Epoch 2:  15%|█▍        | 76/512 [00:29<02:48,  2.59it/s, v_num=0]Training loss: 1.2258596420288086\n",
      "Epoch 2:  15%|█▌        | 77/512 [00:29<02:47,  2.59it/s, v_num=0]Training loss: 1.3049077987670898\n",
      "Epoch 2:  15%|█▌        | 78/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.1738159656524658\n",
      "Epoch 2:  15%|█▌        | 79/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.5950678586959839\n",
      "Epoch 2:  16%|█▌        | 80/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 0.8388372659683228\n",
      "Epoch 2:  16%|█▌        | 81/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 0.8890690803527832\n",
      "Epoch 2:  16%|█▌        | 82/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 1.7013592720031738\n",
      "Epoch 2:  16%|█▌        | 83/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.049684762954712\n",
      "Epoch 2:  16%|█▋        | 84/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.334840178489685\n",
      "Epoch 2:  17%|█▋        | 85/512 [00:32<02:44,  2.59it/s, v_num=0]Training loss: 1.1192160844802856\n",
      "Epoch 2:  17%|█▋        | 86/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 0.9816576242446899\n",
      "Epoch 2:  17%|█▋        | 87/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.418206810951233\n",
      "Epoch 2:  17%|█▋        | 88/512 [00:33<02:43,  2.59it/s, v_num=0]Training loss: 1.378760576248169\n",
      "Epoch 2:  17%|█▋        | 89/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.5281423330307007\n",
      "Epoch 2:  18%|█▊        | 90/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 0.7653851509094238\n",
      "Epoch 2:  18%|█▊        | 91/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.317365050315857\n",
      "Epoch 2:  18%|█▊        | 92/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 0.9642118215560913\n",
      "Epoch 2:  18%|█▊        | 93/512 [00:35<02:41,  2.59it/s, v_num=0]Training loss: 1.1234264373779297\n",
      "Epoch 2:  18%|█▊        | 94/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.655285120010376\n",
      "Epoch 2:  19%|█▊        | 95/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 0.8242985010147095\n",
      "Epoch 2:  19%|█▉        | 96/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.2678475379943848\n",
      "Epoch 2:  19%|█▉        | 97/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.0885095596313477\n",
      "Epoch 2:  19%|█▉        | 98/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.3136976957321167\n",
      "Epoch 2:  19%|█▉        | 99/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 1.0502605438232422\n",
      "Epoch 2:  20%|█▉        | 100/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 1.8924626111984253\n",
      "Epoch 2:  20%|█▉        | 101/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.7965571880340576\n",
      "Epoch 2:  20%|█▉        | 102/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.4272263050079346\n",
      "Epoch 2:  20%|██        | 103/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.6338038444519043\n",
      "Epoch 2:  20%|██        | 104/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.0673770904541016\n",
      "Epoch 2:  21%|██        | 105/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.8664473295211792\n",
      "Epoch 2:  21%|██        | 106/512 [00:41<02:37,  2.58it/s, v_num=0]Training loss: 1.2791446447372437\n",
      "Epoch 2:  21%|██        | 107/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.2022336721420288\n",
      "Epoch 2:  21%|██        | 108/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.26186203956604\n",
      "Epoch 2:  21%|██▏       | 109/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 0.9530647397041321\n",
      "Epoch 2:  21%|██▏       | 110/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.278843641281128\n",
      "Epoch 2:  22%|██▏       | 111/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 0.9717464447021484\n",
      "Epoch 2:  22%|██▏       | 112/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.0893837213516235\n",
      "Epoch 2:  22%|██▏       | 113/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.5949265956878662\n",
      "Epoch 2:  22%|██▏       | 114/512 [00:44<02:34,  2.58it/s, v_num=0]Training loss: 1.45194673538208\n",
      "Epoch 2:  22%|██▏       | 115/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 1.0183982849121094\n",
      "Epoch 2:  23%|██▎       | 116/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 1.7440388202667236\n",
      "Epoch 2:  23%|██▎       | 117/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.1980794668197632\n",
      "Epoch 2:  23%|██▎       | 118/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.3277852535247803\n",
      "Epoch 2:  23%|██▎       | 119/512 [00:46<02:32,  2.58it/s, v_num=0]Training loss: 1.1788392066955566\n",
      "Epoch 2:  23%|██▎       | 120/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 3.7252869606018066\n",
      "Epoch 2:  24%|██▎       | 121/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.9639235138893127\n",
      "Epoch 2:  24%|██▍       | 122/512 [00:47<02:31,  2.58it/s, v_num=0]Training loss: 1.034963607788086\n",
      "Epoch 2:  24%|██▍       | 123/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 2.048417329788208\n",
      "Epoch 2:  24%|██▍       | 124/512 [00:48<02:30,  2.58it/s, v_num=0]Training loss: 1.6182975769042969\n",
      "Epoch 2:  24%|██▍       | 125/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.4782413244247437\n",
      "Epoch 2:  25%|██▍       | 126/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.3335000276565552\n",
      "Epoch 2:  25%|██▍       | 127/512 [00:49<02:29,  2.58it/s, v_num=0]Training loss: 1.1942417621612549\n",
      "Epoch 2:  25%|██▌       | 128/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 2.2584457397460938\n",
      "Epoch 2:  25%|██▌       | 129/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 1.0680750608444214\n",
      "Epoch 2:  25%|██▌       | 130/512 [00:50<02:28,  2.58it/s, v_num=0]Training loss: 1.5383394956588745\n",
      "Epoch 2:  26%|██▌       | 131/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 1.0393646955490112\n",
      "Epoch 2:  26%|██▌       | 132/512 [00:51<02:27,  2.58it/s, v_num=0]Training loss: 1.2560832500457764\n",
      "Epoch 2:  26%|██▌       | 133/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.2855315208435059\n",
      "Epoch 2:  26%|██▌       | 134/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.024147868156433\n",
      "Epoch 2:  26%|██▋       | 135/512 [00:52<02:26,  2.58it/s, v_num=0]Training loss: 1.0788605213165283\n",
      "Epoch 2:  27%|██▋       | 136/512 [00:52<02:25,  2.58it/s, v_num=0]Training loss: 1.4281928539276123\n",
      "Epoch 2:  27%|██▋       | 137/512 [00:53<02:25,  2.58it/s, v_num=0]Training loss: 1.1371417045593262\n",
      "Epoch 2:  27%|██▋       | 138/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 0.8689427375793457\n",
      "Epoch 2:  27%|██▋       | 139/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 1.5639008283615112\n",
      "Epoch 2:  27%|██▋       | 140/512 [00:54<02:24,  2.58it/s, v_num=0]Training loss: 1.2643332481384277\n",
      "Epoch 2:  28%|██▊       | 141/512 [00:54<02:23,  2.58it/s, v_num=0]Training loss: 0.8184037804603577\n",
      "Epoch 2:  28%|██▊       | 142/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.5129683017730713\n",
      "Epoch 2:  28%|██▊       | 143/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.0372986793518066\n",
      "Epoch 2:  28%|██▊       | 144/512 [00:55<02:22,  2.58it/s, v_num=0]Training loss: 1.0432435274124146\n",
      "Epoch 2:  28%|██▊       | 145/512 [00:56<02:22,  2.58it/s, v_num=0]Training loss: 1.2457655668258667\n",
      "Epoch 2:  29%|██▊       | 146/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 1.2258518934249878\n",
      "Epoch 2:  29%|██▊       | 147/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 1.0568606853485107\n",
      "Epoch 2:  29%|██▉       | 148/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 1.6457703113555908\n",
      "Epoch 2:  29%|██▉       | 149/512 [00:57<02:20,  2.58it/s, v_num=0]Training loss: 1.3722214698791504\n",
      "Epoch 2:  29%|██▉       | 150/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 0.8909420967102051\n",
      "Epoch 2:  29%|██▉       | 151/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 1.0584776401519775\n",
      "Epoch 2:  30%|██▉       | 152/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 0.9849948883056641\n",
      "Epoch 2:  30%|██▉       | 153/512 [00:59<02:19,  2.58it/s, v_num=0]Training loss: 0.9465528130531311\n",
      "Epoch 2:  30%|███       | 154/512 [00:59<02:18,  2.58it/s, v_num=0]Training loss: 1.2782810926437378\n",
      "Epoch 2:  30%|███       | 155/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.6457079648971558\n",
      "Epoch 2:  30%|███       | 156/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.4177463054656982\n",
      "Epoch 2:  31%|███       | 157/512 [01:00<02:17,  2.58it/s, v_num=0]Training loss: 1.421750545501709\n",
      "Epoch 2:  31%|███       | 158/512 [01:01<02:17,  2.58it/s, v_num=0]Training loss: 1.229053258895874\n",
      "Epoch 2:  31%|███       | 159/512 [01:01<02:16,  2.58it/s, v_num=0]Training loss: 1.1091047525405884\n",
      "Epoch 2:  31%|███▏      | 160/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 0.9507699608802795\n",
      "Epoch 2:  31%|███▏      | 161/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 0.8097696304321289\n",
      "Epoch 2:  32%|███▏      | 162/512 [01:02<02:15,  2.58it/s, v_num=0]Training loss: 1.6988226175308228\n",
      "Epoch 2:  32%|███▏      | 163/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 0.9141731262207031\n",
      "Epoch 2:  32%|███▏      | 164/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 0.895658016204834\n",
      "Epoch 2:  32%|███▏      | 165/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 1.080492377281189\n",
      "Epoch 2:  32%|███▏      | 166/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 1.3384586572647095\n",
      "Epoch 2:  33%|███▎      | 167/512 [01:04<02:13,  2.58it/s, v_num=0]Training loss: 1.3338054418563843\n",
      "Epoch 2:  33%|███▎      | 168/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.7058253288269043\n",
      "Epoch 2:  33%|███▎      | 169/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.4256192445755005\n",
      "Epoch 2:  33%|███▎      | 170/512 [01:05<02:12,  2.58it/s, v_num=0]Training loss: 1.295440912246704\n",
      "Epoch 2:  33%|███▎      | 171/512 [01:06<02:12,  2.58it/s, v_num=0]Training loss: 1.3825082778930664\n",
      "Epoch 2:  34%|███▎      | 172/512 [01:06<02:11,  2.58it/s, v_num=0]Training loss: 1.4221673011779785\n",
      "Epoch 2:  34%|███▍      | 173/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.4253822565078735\n",
      "Epoch 2:  34%|███▍      | 174/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.2869322299957275\n",
      "Epoch 2:  34%|███▍      | 175/512 [01:07<02:10,  2.58it/s, v_num=0]Training loss: 1.0710493326187134\n",
      "Epoch 2:  34%|███▍      | 176/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.6787186861038208\n",
      "Epoch 2:  35%|███▍      | 177/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.058531403541565\n",
      "Epoch 2:  35%|███▍      | 178/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 0.7877302169799805\n",
      "Epoch 2:  35%|███▍      | 179/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 0.9566140174865723\n",
      "Epoch 2:  35%|███▌      | 180/512 [01:09<02:08,  2.58it/s, v_num=0]Training loss: 0.8793796300888062\n",
      "Epoch 2:  35%|███▌      | 181/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 1.4032121896743774\n",
      "Epoch 2:  36%|███▌      | 182/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 0.9872276186943054\n",
      "Epoch 2:  36%|███▌      | 183/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.426499605178833\n",
      "Epoch 2:  36%|███▌      | 184/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.389197826385498\n",
      "Epoch 2:  36%|███▌      | 185/512 [01:11<02:06,  2.58it/s, v_num=0]Training loss: 0.8878473043441772\n",
      "Epoch 2:  36%|███▋      | 186/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 0.9177770614624023\n",
      "Epoch 2:  37%|███▋      | 187/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 1.0471478700637817\n",
      "Epoch 2:  37%|███▋      | 188/512 [01:12<02:05,  2.58it/s, v_num=0]Training loss: 1.2089940309524536\n",
      "Epoch 2:  37%|███▋      | 189/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 1.5048432350158691\n",
      "Epoch 2:  37%|███▋      | 190/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 1.7839884757995605\n",
      "Epoch 2:  37%|███▋      | 191/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.1337366104125977\n",
      "Epoch 2:  38%|███▊      | 192/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.0117692947387695\n",
      "Epoch 2:  38%|███▊      | 193/512 [01:14<02:03,  2.58it/s, v_num=0]Training loss: 1.235072135925293\n",
      "Epoch 2:  38%|███▊      | 194/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 1.48194420337677\n",
      "Epoch 2:  38%|███▊      | 195/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 0.8140459060668945\n",
      "Epoch 2:  38%|███▊      | 196/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 2.6538116931915283\n",
      "Epoch 2:  38%|███▊      | 197/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.247817873954773\n",
      "Epoch 2:  39%|███▊      | 198/512 [01:16<02:01,  2.58it/s, v_num=0]Training loss: 1.1305006742477417\n",
      "Epoch 2:  39%|███▉      | 199/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 2.9616434574127197\n",
      "Epoch 2:  39%|███▉      | 200/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 1.0305061340332031\n",
      "Epoch 2:  39%|███▉      | 201/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 2.2556724548339844\n",
      "Epoch 2:  39%|███▉      | 202/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 2.750293493270874\n",
      "Epoch 2:  40%|███▉      | 203/512 [01:18<01:59,  2.58it/s, v_num=0]Training loss: 1.1702836751937866\n",
      "Epoch 2:  40%|███▉      | 204/512 [01:19<01:59,  2.58it/s, v_num=0]Training loss: 1.1320903301239014\n",
      "Epoch 2:  40%|████      | 205/512 [01:19<01:59,  2.57it/s, v_num=0]Training loss: 1.1678223609924316\n",
      "Epoch 2:  40%|████      | 206/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 0.8844711184501648\n",
      "Epoch 2:  40%|████      | 207/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 1.33529531955719\n",
      "Epoch 2:  41%|████      | 208/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 1.1315829753875732\n",
      "Epoch 2:  41%|████      | 209/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 1.1294035911560059\n",
      "Epoch 2:  41%|████      | 210/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 1.9150558710098267\n",
      "Epoch 2:  41%|████      | 211/512 [01:21<01:56,  2.57it/s, v_num=0]Training loss: 1.1200686693191528\n",
      "Epoch 2:  41%|████▏     | 212/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 0.7183359861373901\n",
      "Epoch 2:  42%|████▏     | 213/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 1.5944795608520508\n",
      "Epoch 2:  42%|████▏     | 214/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.1196719408035278\n",
      "Epoch 2:  42%|████▏     | 215/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.325400948524475\n",
      "Epoch 2:  42%|████▏     | 216/512 [01:23<01:54,  2.57it/s, v_num=0]Training loss: 2.0670535564422607\n",
      "Epoch 2:  42%|████▏     | 217/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.3419039249420166\n",
      "Epoch 2:  43%|████▎     | 218/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.271106243133545\n",
      "Epoch 2:  43%|████▎     | 219/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.9296143651008606\n",
      "Epoch 2:  43%|████▎     | 220/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.8235315680503845\n",
      "Epoch 2:  43%|████▎     | 221/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.2056076526641846\n",
      "Epoch 2:  43%|████▎     | 222/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 1.0760167837142944\n",
      "Epoch 2:  44%|████▎     | 223/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 0.9926599860191345\n",
      "Epoch 2:  44%|████▍     | 224/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.3023234605789185\n",
      "Epoch 2:  44%|████▍     | 225/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.2550841569900513\n",
      "Epoch 2:  44%|████▍     | 226/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.067712426185608\n",
      "Epoch 2:  44%|████▍     | 227/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 1.2359791994094849\n",
      "Epoch 2:  45%|████▍     | 228/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 0.9828822612762451\n",
      "Epoch 2:  45%|████▍     | 229/512 [01:28<01:49,  2.57it/s, v_num=0]Training loss: 0.8780267834663391\n",
      "Epoch 2:  45%|████▍     | 230/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 1.2921398878097534\n",
      "Epoch 2:  45%|████▌     | 231/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 1.4716209173202515\n",
      "Epoch 2:  45%|████▌     | 232/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.7198423147201538\n",
      "Epoch 2:  46%|████▌     | 233/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.1501060724258423\n",
      "Epoch 2:  46%|████▌     | 234/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.0272321701049805\n",
      "Epoch 2:  46%|████▌     | 235/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 0.684639573097229\n",
      "Epoch 2:  46%|████▌     | 236/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.1611741781234741\n",
      "Epoch 2:  46%|████▋     | 237/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 0.9954096078872681\n",
      "Epoch 2:  46%|████▋     | 238/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.6297639608383179\n",
      "Epoch 2:  47%|████▋     | 239/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.008102536201477\n",
      "Epoch 2:  47%|████▋     | 240/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.0229885578155518\n",
      "Epoch 2:  47%|████▋     | 241/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.1782923936843872\n",
      "Epoch 2:  47%|████▋     | 242/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.5951390266418457\n",
      "Epoch 2:  47%|████▋     | 243/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.3117525577545166\n",
      "Epoch 2:  48%|████▊     | 244/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.303982138633728\n",
      "Epoch 2:  48%|████▊     | 245/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 1.4673449993133545\n",
      "Epoch 2:  48%|████▊     | 246/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 2.4731831550598145\n",
      "Epoch 2:  48%|████▊     | 247/512 [01:35<01:42,  2.57it/s, v_num=0]Training loss: 1.3890761137008667\n",
      "Epoch 2:  48%|████▊     | 248/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.3037561178207397\n",
      "Epoch 2:  49%|████▊     | 249/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.2313932180404663\n",
      "Epoch 2:  49%|████▉     | 250/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.339411973953247\n",
      "Epoch 2:  49%|████▉     | 251/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 0.7591617107391357\n",
      "Epoch 2:  49%|████▉     | 252/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.5919873714447021\n",
      "Epoch 2:  49%|████▉     | 253/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 0.9138343930244446\n",
      "Epoch 2:  50%|████▉     | 254/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.1644443273544312\n",
      "Epoch 2:  50%|████▉     | 255/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 0.6776018142700195\n",
      "Epoch 2:  50%|█████     | 256/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.248117446899414\n",
      "Epoch 2:  50%|█████     | 257/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.5352715253829956\n",
      "Epoch 2:  50%|█████     | 258/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 1.079712152481079\n",
      "Epoch 2:  51%|█████     | 259/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 0.9340482354164124\n",
      "Epoch 2:  51%|█████     | 260/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 1.0743533372879028\n",
      "Epoch 2:  51%|█████     | 261/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.9347171187400818\n",
      "Epoch 2:  51%|█████     | 262/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 1.5416046380996704\n",
      "Epoch 2:  51%|█████▏    | 263/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.235086441040039\n",
      "Epoch 2:  52%|█████▏    | 264/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 0.6899455785751343\n",
      "Epoch 2:  52%|█████▏    | 265/512 [01:43<01:36,  2.57it/s, v_num=0]Training loss: 1.0461140871047974\n",
      "Epoch 2:  52%|█████▏    | 266/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 1.3376232385635376\n",
      "Epoch 2:  52%|█████▏    | 267/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.8574602007865906\n",
      "Epoch 2:  52%|█████▏    | 268/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.3673044443130493\n",
      "Epoch 2:  53%|█████▎    | 269/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.0795295238494873\n",
      "Epoch 2:  53%|█████▎    | 270/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.215786099433899\n",
      "Epoch 2:  53%|█████▎    | 271/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 0.9155817627906799\n",
      "Epoch 2:  53%|█████▎    | 272/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 1.644692301750183\n",
      "Epoch 2:  53%|█████▎    | 273/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.0610798597335815\n",
      "Epoch 2:  54%|█████▎    | 274/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.6717735528945923\n",
      "Epoch 2:  54%|█████▎    | 275/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 0.9959180355072021\n",
      "Epoch 2:  54%|█████▍    | 276/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.8046215772628784\n",
      "Epoch 2:  54%|█████▍    | 277/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 0.9218502044677734\n",
      "Epoch 2:  54%|█████▍    | 278/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.0655202865600586\n",
      "Epoch 2:  54%|█████▍    | 279/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.0584344863891602\n",
      "Epoch 2:  55%|█████▍    | 280/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 2.3520331382751465\n",
      "Epoch 2:  55%|█████▍    | 281/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.4850233793258667\n",
      "Epoch 2:  55%|█████▌    | 282/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.2081242799758911\n",
      "Epoch 2:  55%|█████▌    | 283/512 [01:50<01:29,  2.57it/s, v_num=0]Training loss: 1.2168653011322021\n",
      "Epoch 2:  55%|█████▌    | 284/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.0217307806015015\n",
      "Epoch 2:  56%|█████▌    | 285/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.3443795442581177\n",
      "Epoch 2:  56%|█████▌    | 286/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.6180658340454102\n",
      "Epoch 2:  56%|█████▌    | 287/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.3862684965133667\n",
      "Epoch 2:  56%|█████▋    | 288/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 0.7439582943916321\n",
      "Epoch 2:  56%|█████▋    | 289/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.1798734664916992\n",
      "Epoch 2:  57%|█████▋    | 290/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.2115795612335205\n",
      "Epoch 2:  57%|█████▋    | 291/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.1106665134429932\n",
      "Epoch 2:  57%|█████▋    | 292/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.0908427238464355\n",
      "Epoch 2:  57%|█████▋    | 293/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.394757628440857\n",
      "Epoch 2:  57%|█████▋    | 294/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 1.7114888429641724\n",
      "Epoch 2:  58%|█████▊    | 295/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 1.2752974033355713\n",
      "Epoch 2:  58%|█████▊    | 296/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.4975709915161133\n",
      "Epoch 2:  58%|█████▊    | 297/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.237652063369751\n",
      "Epoch 2:  58%|█████▊    | 298/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.1011704206466675\n",
      "Epoch 2:  58%|█████▊    | 299/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.2872307300567627\n",
      "Epoch 2:  59%|█████▊    | 300/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.0106791257858276\n",
      "Epoch 2:  59%|█████▉    | 301/512 [01:57<01:22,  2.57it/s, v_num=0]Training loss: 0.9327877759933472\n",
      "Epoch 2:  59%|█████▉    | 302/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.9975402355194092\n",
      "Epoch 2:  59%|█████▉    | 303/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 1.6754844188690186\n",
      "Epoch 2:  59%|█████▉    | 304/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.8834755420684814\n",
      "Epoch 2:  60%|█████▉    | 305/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.0983319282531738\n",
      "Epoch 2:  60%|█████▉    | 306/512 [01:59<01:20,  2.57it/s, v_num=0]Training loss: 1.0008509159088135\n",
      "Epoch 2:  60%|█████▉    | 307/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.8542336225509644\n",
      "Epoch 2:  60%|██████    | 308/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.0275371074676514\n",
      "Epoch 2:  60%|██████    | 309/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.199405312538147\n",
      "Epoch 2:  61%|██████    | 310/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.2357937097549438\n",
      "Epoch 2:  61%|██████    | 311/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.1340893507003784\n",
      "Epoch 2:  61%|██████    | 312/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 0.9911378622055054\n",
      "Epoch 2:  61%|██████    | 313/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.0165053606033325\n",
      "Epoch 2:  61%|██████▏   | 314/512 [02:02<01:17,  2.57it/s, v_num=0]Training loss: 1.272052526473999\n",
      "Epoch 2:  62%|██████▏   | 315/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.182473063468933\n",
      "Epoch 2:  62%|██████▏   | 316/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.3765922784805298\n",
      "Epoch 2:  62%|██████▏   | 317/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.1240423917770386\n",
      "Epoch 2:  62%|██████▏   | 318/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.2901122570037842\n",
      "Epoch 2:  62%|██████▏   | 319/512 [02:04<01:15,  2.57it/s, v_num=0]Training loss: 1.8777542114257812\n",
      "Epoch 2:  62%|██████▎   | 320/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.4649713039398193\n",
      "Epoch 2:  63%|██████▎   | 321/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.0611181259155273\n",
      "Epoch 2:  63%|██████▎   | 322/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.652266025543213\n",
      "Epoch 2:  63%|██████▎   | 323/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.340529441833496\n",
      "Epoch 2:  63%|██████▎   | 324/512 [02:06<01:13,  2.57it/s, v_num=0]Training loss: 1.4305557012557983\n",
      "Epoch 2:  63%|██████▎   | 325/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.3445276021957397\n",
      "Epoch 2:  64%|██████▎   | 326/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.021507978439331\n",
      "Epoch 2:  64%|██████▍   | 327/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.1533039808273315\n",
      "Epoch 2:  64%|██████▍   | 328/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.3467907905578613\n",
      "Epoch 2:  64%|██████▍   | 329/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.598862886428833\n",
      "Epoch 2:  64%|██████▍   | 330/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.2614901065826416\n",
      "Epoch 2:  65%|██████▍   | 331/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 2.128406524658203\n",
      "Epoch 2:  65%|██████▍   | 332/512 [02:09<01:10,  2.57it/s, v_num=0]Training loss: 0.9565657377243042\n",
      "Epoch 2:  65%|██████▌   | 333/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.2983862161636353\n",
      "Epoch 2:  65%|██████▌   | 334/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.8034878969192505\n",
      "Epoch 2:  65%|██████▌   | 335/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 1.1831693649291992\n",
      "Epoch 2:  66%|██████▌   | 336/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 1.0435806512832642\n",
      "Epoch 2:  66%|██████▌   | 337/512 [02:11<01:08,  2.57it/s, v_num=0]Training loss: 15.1722993850708\n",
      "Epoch 2:  66%|██████▌   | 338/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.011782169342041\n",
      "Epoch 2:  66%|██████▌   | 339/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.1366143226623535\n",
      "Epoch 2:  66%|██████▋   | 340/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.7607866525650024\n",
      "Epoch 2:  67%|██████▋   | 341/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.0290040969848633\n",
      "Epoch 2:  67%|██████▋   | 342/512 [02:13<01:06,  2.57it/s, v_num=0]Training loss: 1.5278559923171997\n",
      "Epoch 2:  67%|██████▋   | 343/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.5093806982040405\n",
      "Epoch 2:  67%|██████▋   | 344/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.4254776239395142\n",
      "Epoch 2:  67%|██████▋   | 345/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.4496031999588013\n",
      "Epoch 2:  68%|██████▊   | 346/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.4393692016601562\n",
      "Epoch 2:  68%|██████▊   | 347/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.2406538724899292\n",
      "Epoch 2:  68%|██████▊   | 348/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 0.6891362071037292\n",
      "Epoch 2:  68%|██████▊   | 349/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 0.9755315184593201\n",
      "Epoch 2:  68%|██████▊   | 350/512 [02:16<01:03,  2.57it/s, v_num=0]Training loss: 1.2403302192687988\n",
      "Epoch 2:  69%|██████▊   | 351/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 0.9441457986831665\n",
      "Epoch 2:  69%|██████▉   | 352/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.0080342292785645\n",
      "Epoch 2:  69%|██████▉   | 353/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.9346768856048584\n",
      "Epoch 2:  69%|██████▉   | 354/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 1.1997079849243164\n",
      "Epoch 2:  69%|██████▉   | 355/512 [02:18<01:01,  2.57it/s, v_num=0]Training loss: 1.0033620595932007\n",
      "Epoch 2:  70%|██████▉   | 356/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.090313196182251\n",
      "Epoch 2:  70%|██████▉   | 357/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.4307548999786377\n",
      "Epoch 2:  70%|██████▉   | 358/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.3462021350860596\n",
      "Epoch 2:  70%|███████   | 359/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 0.9953153133392334\n",
      "Epoch 2:  70%|███████   | 360/512 [02:20<00:59,  2.57it/s, v_num=0]Training loss: 1.0011792182922363\n",
      "Epoch 2:  71%|███████   | 361/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 0.880439281463623\n",
      "Epoch 2:  71%|███████   | 362/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 1.245793342590332\n",
      "Epoch 2:  71%|███████   | 363/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.321751356124878\n",
      "Epoch 2:  71%|███████   | 364/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.2316169738769531\n",
      "Epoch 2:  71%|███████▏  | 365/512 [02:22<00:57,  2.57it/s, v_num=0]Training loss: 0.9564239382743835\n",
      "Epoch 2:  71%|███████▏  | 366/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 1.3901772499084473\n",
      "Epoch 2:  72%|███████▏  | 367/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 1.2362544536590576\n",
      "Epoch 2:  72%|███████▏  | 368/512 [02:23<00:56,  2.57it/s, v_num=0]Training loss: 0.9908387660980225\n",
      "Epoch 2:  72%|███████▏  | 369/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.1677308082580566\n",
      "Epoch 2:  72%|███████▏  | 370/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.3693360090255737\n",
      "Epoch 2:  72%|███████▏  | 371/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.2710282802581787\n",
      "Epoch 2:  73%|███████▎  | 372/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.1141436100006104\n",
      "Epoch 2:  73%|███████▎  | 373/512 [02:25<00:54,  2.57it/s, v_num=0]Training loss: 1.470178246498108\n",
      "Epoch 2:  73%|███████▎  | 374/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 0.9264959692955017\n",
      "Epoch 2:  73%|███████▎  | 375/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.5054911375045776\n",
      "Epoch 2:  73%|███████▎  | 376/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.835718035697937\n",
      "Epoch 2:  74%|███████▎  | 377/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.7197023034095764\n",
      "Epoch 2:  74%|███████▍  | 378/512 [02:27<00:52,  2.57it/s, v_num=0]Training loss: 1.1944090127944946\n",
      "Epoch 2:  74%|███████▍  | 379/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.4761347770690918\n",
      "Epoch 2:  74%|███████▍  | 380/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.0276384353637695\n",
      "Epoch 2:  74%|███████▍  | 381/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.7423982620239258\n",
      "Epoch 2:  75%|███████▍  | 382/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.156184196472168\n",
      "Epoch 2:  75%|███████▍  | 383/512 [02:29<00:50,  2.57it/s, v_num=0]Training loss: 1.3403335809707642\n",
      "Epoch 2:  75%|███████▌  | 384/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.196465253829956\n",
      "Epoch 2:  75%|███████▌  | 385/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.0033020973205566\n",
      "Epoch 2:  75%|███████▌  | 386/512 [02:30<00:49,  2.57it/s, v_num=0]Training loss: 1.35819411277771\n",
      "Epoch 2:  76%|███████▌  | 387/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.0062346458435059\n",
      "Epoch 2:  76%|███████▌  | 388/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 2.012784719467163\n",
      "Epoch 2:  76%|███████▌  | 389/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 0.800292432308197\n",
      "Epoch 2:  76%|███████▌  | 390/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.3852112293243408\n",
      "Epoch 2:  76%|███████▋  | 391/512 [02:32<00:47,  2.57it/s, v_num=0]Training loss: 0.9689403176307678\n",
      "Epoch 2:  77%|███████▋  | 392/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.1260490417480469\n",
      "Epoch 2:  77%|███████▋  | 393/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.2836064100265503\n",
      "Epoch 2:  77%|███████▋  | 394/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 0.8921000361442566\n",
      "Epoch 2:  77%|███████▋  | 395/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.4437183141708374\n",
      "Epoch 2:  77%|███████▋  | 396/512 [02:34<00:45,  2.57it/s, v_num=0]Training loss: 1.2947463989257812\n",
      "Epoch 2:  78%|███████▊  | 397/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.2298015356063843\n",
      "Epoch 2:  78%|███████▊  | 398/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.0806827545166016\n",
      "Epoch 2:  78%|███████▊  | 399/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.9423720240592957\n",
      "Epoch 2:  78%|███████▊  | 400/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.7665684223175049\n",
      "Epoch 2:  78%|███████▊  | 401/512 [02:36<00:43,  2.57it/s, v_num=0]Training loss: 0.7617073655128479\n",
      "Epoch 2:  79%|███████▊  | 402/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.276986002922058\n",
      "Epoch 2:  79%|███████▊  | 403/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.1451387405395508\n",
      "Epoch 2:  79%|███████▉  | 404/512 [02:37<00:42,  2.57it/s, v_num=0]Training loss: 1.0601935386657715\n",
      "Epoch 2:  79%|███████▉  | 405/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 0.8816089034080505\n",
      "Epoch 2:  79%|███████▉  | 406/512 [02:38<00:41,  2.57it/s, v_num=0]Training loss: 1.482271432876587\n",
      "Epoch 2:  79%|███████▉  | 407/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 1.0173957347869873\n",
      "Epoch 2:  80%|███████▉  | 408/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.8481407761573792\n",
      "Epoch 2:  80%|███████▉  | 409/512 [02:39<00:40,  2.57it/s, v_num=0]Training loss: 0.8627734184265137\n",
      "Epoch 2:  80%|████████  | 410/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.914024829864502\n",
      "Epoch 2:  80%|████████  | 411/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.7787103652954102\n",
      "Epoch 2:  80%|████████  | 412/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 1.1721960306167603\n",
      "Epoch 2:  81%|████████  | 413/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.9908860921859741\n",
      "Epoch 2:  81%|████████  | 414/512 [02:41<00:38,  2.57it/s, v_num=0]Training loss: 1.077521800994873\n",
      "Epoch 2:  81%|████████  | 415/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.1847165822982788\n",
      "Epoch 2:  81%|████████▏ | 416/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 0.7990246415138245\n",
      "Epoch 2:  81%|████████▏ | 417/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.2371948957443237\n",
      "Epoch 2:  82%|████████▏ | 418/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.3843096494674683\n",
      "Epoch 2:  82%|████████▏ | 419/512 [02:43<00:36,  2.57it/s, v_num=0]Training loss: 0.9236956834793091\n",
      "Epoch 2:  82%|████████▏ | 420/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 1.372260570526123\n",
      "Epoch 2:  82%|████████▏ | 421/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 0.9040250778198242\n",
      "Epoch 2:  82%|████████▏ | 422/512 [02:44<00:35,  2.57it/s, v_num=0]Training loss: 1.3180807828903198\n",
      "Epoch 2:  83%|████████▎ | 423/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 0.8733298778533936\n",
      "Epoch 2:  83%|████████▎ | 424/512 [02:45<00:34,  2.57it/s, v_num=0]Training loss: 0.9832469820976257\n",
      "Epoch 2:  83%|████████▎ | 425/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 1.0527379512786865\n",
      "Epoch 2:  83%|████████▎ | 426/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 0.9983370900154114\n",
      "Epoch 2:  83%|████████▎ | 427/512 [02:46<00:33,  2.57it/s, v_num=0]Training loss: 1.0420621633529663\n",
      "Epoch 2:  84%|████████▎ | 428/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.0275112390518188\n",
      "Epoch 2:  84%|████████▍ | 429/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.2638517618179321\n",
      "Epoch 2:  84%|████████▍ | 430/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.0425701141357422\n",
      "Epoch 2:  84%|████████▍ | 431/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.3826196193695068\n",
      "Epoch 2:  84%|████████▍ | 432/512 [02:48<00:31,  2.57it/s, v_num=0]Training loss: 0.9415690302848816\n",
      "Epoch 2:  85%|████████▍ | 433/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.9835691452026367\n",
      "Epoch 2:  85%|████████▍ | 434/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 1.3367996215820312\n",
      "Epoch 2:  85%|████████▍ | 435/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.040891408920288\n",
      "Epoch 2:  85%|████████▌ | 436/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.0020259618759155\n",
      "Epoch 2:  85%|████████▌ | 437/512 [02:50<00:29,  2.57it/s, v_num=0]Training loss: 1.62893807888031\n",
      "Epoch 2:  86%|████████▌ | 438/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.2161142826080322\n",
      "Epoch 2:  86%|████████▌ | 439/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 0.7863069772720337\n",
      "Epoch 2:  86%|████████▌ | 440/512 [02:51<00:28,  2.57it/s, v_num=0]Training loss: 0.9697653651237488\n",
      "Epoch 2:  86%|████████▌ | 441/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 0.7766029834747314\n",
      "Epoch 2:  86%|████████▋ | 442/512 [02:52<00:27,  2.57it/s, v_num=0]Training loss: 0.8091093301773071\n",
      "Epoch 2:  87%|████████▋ | 443/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 0.9192154407501221\n",
      "Epoch 2:  87%|████████▋ | 444/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 1.3687608242034912\n",
      "Epoch 2:  87%|████████▋ | 445/512 [02:53<00:26,  2.57it/s, v_num=0]Training loss: 1.1770893335342407\n",
      "Epoch 2:  87%|████████▋ | 446/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 0.8986956477165222\n",
      "Epoch 2:  87%|████████▋ | 447/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 1.3309857845306396\n",
      "Epoch 2:  88%|████████▊ | 448/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 1.1613026857376099\n",
      "Epoch 2:  88%|████████▊ | 449/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 1.2519352436065674\n",
      "Epoch 2:  88%|████████▊ | 450/512 [02:55<00:24,  2.57it/s, v_num=0]Training loss: 1.019884705543518\n",
      "Epoch 2:  88%|████████▊ | 451/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 1.5250321626663208\n",
      "Epoch 2:  88%|████████▊ | 452/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 0.8362988829612732\n",
      "Epoch 2:  88%|████████▊ | 453/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.7249387502670288\n",
      "Epoch 2:  89%|████████▊ | 454/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 1.4831477403640747\n",
      "Epoch 2:  89%|████████▉ | 455/512 [02:57<00:22,  2.57it/s, v_num=0]Training loss: 1.0953949689865112\n",
      "Epoch 2:  89%|████████▉ | 456/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.2591209411621094\n",
      "Epoch 2:  89%|████████▉ | 457/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.277406096458435\n",
      "Epoch 2:  89%|████████▉ | 458/512 [02:58<00:21,  2.57it/s, v_num=0]Training loss: 1.06556236743927\n",
      "Epoch 2:  90%|████████▉ | 459/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 0.666996419429779\n",
      "Epoch 2:  90%|████████▉ | 460/512 [02:59<00:20,  2.57it/s, v_num=0]Training loss: 1.5727640390396118\n",
      "Epoch 2:  90%|█████████ | 461/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 0.9948408603668213\n",
      "Epoch 2:  90%|█████████ | 462/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.047102689743042\n",
      "Epoch 2:  90%|█████████ | 463/512 [03:00<00:19,  2.57it/s, v_num=0]Training loss: 1.3318896293640137\n",
      "Epoch 2:  91%|█████████ | 464/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 1.1194783449172974\n",
      "Epoch 2:  91%|█████████ | 465/512 [03:01<00:18,  2.57it/s, v_num=0]Training loss: 1.156907081604004\n",
      "Epoch 2:  91%|█████████ | 466/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 0.8892983198165894\n",
      "Epoch 2:  91%|█████████ | 467/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 1.2586828470230103\n",
      "Epoch 2:  91%|█████████▏| 468/512 [03:02<00:17,  2.57it/s, v_num=0]Training loss: 0.9222241044044495\n",
      "Epoch 2:  92%|█████████▏| 469/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 0.9850336909294128\n",
      "Epoch 2:  92%|█████████▏| 470/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 1.2519172430038452\n",
      "Epoch 2:  92%|█████████▏| 471/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 0.7020032405853271\n",
      "Epoch 2:  92%|█████████▏| 472/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.1600003242492676\n",
      "Epoch 2:  92%|█████████▏| 473/512 [03:04<00:15,  2.57it/s, v_num=0]Training loss: 1.003646731376648\n",
      "Epoch 2:  93%|█████████▎| 474/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.0441409349441528\n",
      "Epoch 2:  93%|█████████▎| 475/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.5209779739379883\n",
      "Epoch 2:  93%|█████████▎| 476/512 [03:05<00:14,  2.57it/s, v_num=0]Training loss: 0.8385684490203857\n",
      "Epoch 2:  93%|█████████▎| 477/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 1.4378337860107422\n",
      "Epoch 2:  93%|█████████▎| 478/512 [03:06<00:13,  2.57it/s, v_num=0]Training loss: 1.0482640266418457\n",
      "Epoch 2:  94%|█████████▎| 479/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.359757423400879\n",
      "Epoch 2:  94%|█████████▍| 480/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.0004465579986572\n",
      "Epoch 2:  94%|█████████▍| 481/512 [03:07<00:12,  2.57it/s, v_num=0]Training loss: 1.0641801357269287\n",
      "Epoch 2:  94%|█████████▍| 482/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 0.8111063838005066\n",
      "Epoch 2:  94%|█████████▍| 483/512 [03:08<00:11,  2.57it/s, v_num=0]Training loss: 1.0552911758422852\n",
      "Epoch 2:  95%|█████████▍| 484/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.0601972341537476\n",
      "Epoch 2:  95%|█████████▍| 485/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 2.124267816543579\n",
      "Epoch 2:  95%|█████████▍| 486/512 [03:09<00:10,  2.57it/s, v_num=0]Training loss: 1.9582303762435913\n",
      "Epoch 2:  95%|█████████▌| 487/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 1.284061312675476\n",
      "Epoch 2:  95%|█████████▌| 488/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 1.1869808435440063\n",
      "Epoch 2:  96%|█████████▌| 489/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.5882158279418945\n",
      "Epoch 2:  96%|█████████▌| 490/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 0.8548189401626587\n",
      "Epoch 2:  96%|█████████▌| 491/512 [03:11<00:08,  2.57it/s, v_num=0]Training loss: 1.3497751951217651\n",
      "Epoch 2:  96%|█████████▌| 492/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.4727898836135864\n",
      "Epoch 2:  96%|█████████▋| 493/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.0374369621276855\n",
      "Epoch 2:  96%|█████████▋| 494/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 1.0849454402923584\n",
      "Epoch 2:  97%|█████████▋| 495/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 0.782250165939331\n",
      "Epoch 2:  97%|█████████▋| 496/512 [03:13<00:06,  2.57it/s, v_num=0]Training loss: 1.0796805620193481\n",
      "Epoch 2:  97%|█████████▋| 497/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 0.7031455636024475\n",
      "Epoch 2:  97%|█████████▋| 498/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 1.1574629545211792\n",
      "Epoch 2:  97%|█████████▋| 499/512 [03:14<00:05,  2.57it/s, v_num=0]Training loss: 0.6229708790779114\n",
      "Epoch 2:  98%|█████████▊| 500/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 1.2009251117706299\n",
      "Epoch 2:  98%|█████████▊| 501/512 [03:15<00:04,  2.57it/s, v_num=0]Training loss: 1.0670063495635986\n",
      "Epoch 2:  98%|█████████▊| 502/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 0.9566053748130798\n",
      "Epoch 2:  98%|█████████▊| 503/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 1.3893275260925293\n",
      "Epoch 2:  98%|█████████▊| 504/512 [03:16<00:03,  2.57it/s, v_num=0]Training loss: 1.0438508987426758\n",
      "Epoch 2:  99%|█████████▊| 505/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 0.9421201944351196\n",
      "Epoch 2:  99%|█████████▉| 506/512 [03:17<00:02,  2.57it/s, v_num=0]Training loss: 1.3580236434936523\n",
      "Epoch 2:  99%|█████████▉| 507/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.196195125579834\n",
      "Epoch 2:  99%|█████████▉| 508/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 0.8285489678382874\n",
      "Epoch 2:  99%|█████████▉| 509/512 [03:18<00:01,  2.57it/s, v_num=0]Training loss: 1.3444912433624268\n",
      "Epoch 2: 100%|█████████▉| 510/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 0.9642255306243896\n",
      "Epoch 2: 100%|█████████▉| 511/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 1.6224589347839355\n",
      "Epoch 2: 100%|██████████| 512/512 [03:19<00:00,  2.57it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.2271039485931396\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 29.94it/s]\u001b[AValidation loss: 0.8617086410522461\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 25.32it/s]\u001b[AValidation loss: 1.3534660339355469\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 23.28it/s]\u001b[AValidation loss: 1.1814652681350708\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 22.91it/s]\u001b[AValidation loss: 0.9017511606216431\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 22.83it/s]\u001b[AValidation loss: 1.2213146686553955\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 22.70it/s]\u001b[AValidation loss: 0.9694184064865112\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 22.71it/s]\u001b[AValidation loss: 1.1892603635787964\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 22.34it/s]\u001b[AValidation loss: 1.2321393489837646\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.29it/s]\u001b[AValidation loss: 1.0655806064605713\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.27it/s]\u001b[AValidation loss: 0.8577541708946228\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.03it/s]\u001b[AValidation loss: 0.7499175071716309\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.02it/s]\u001b[AValidation loss: 0.9462977051734924\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.02it/s]\u001b[AValidation loss: 1.0888967514038086\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 21.99it/s]\u001b[AValidation loss: 1.087761402130127\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 22.01it/s]\u001b[AValidation loss: 1.26569402217865\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 21.89it/s]\u001b[AValidation loss: 1.032557487487793\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 21.76it/s]\u001b[AValidation loss: 0.7875617742538452\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 21.77it/s]\u001b[AValidation loss: 0.6460860967636108\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 21.79it/s]\u001b[AValidation loss: 0.7511690258979797\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:04, 21.77it/s]\u001b[AValidation loss: 1.477417230606079\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:04, 21.70it/s]\u001b[AValidation loss: 0.7676396369934082\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:01<00:04, 21.61it/s]\u001b[AValidation loss: 1.0651379823684692\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:04, 21.54it/s]\u001b[AValidation loss: 1.0417526960372925\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:04, 21.48it/s]\u001b[AValidation loss: 0.8773037791252136\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 21.49it/s]\u001b[AValidation loss: 0.7791348099708557\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 21.54it/s]\u001b[AValidation loss: 1.431334137916565\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:01<00:03, 21.48it/s]\u001b[AValidation loss: 0.8917557001113892\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:01<00:03, 21.49it/s]\u001b[AValidation loss: 1.1251846551895142\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:01<00:03, 21.53it/s]\u001b[AValidation loss: 2.2900424003601074\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:01<00:03, 21.55it/s]\u001b[AValidation loss: 1.4151064157485962\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:03, 21.57it/s]\u001b[AValidation loss: 1.416159749031067\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:03, 21.58it/s]\u001b[AValidation loss: 1.0366902351379395\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:03, 21.60it/s]\u001b[AValidation loss: 1.110109567642212\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:03, 21.61it/s]\u001b[AValidation loss: 1.099906325340271\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:03, 21.63it/s]\u001b[AValidation loss: 0.7536292672157288\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:03, 21.65it/s]\u001b[AValidation loss: 0.8999382257461548\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:03, 21.67it/s]\u001b[AValidation loss: 1.2466363906860352\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:03, 21.68it/s]\u001b[AValidation loss: 0.9449975490570068\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:03, 21.70it/s]\u001b[AValidation loss: 0.7615637183189392\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:03, 21.65it/s]\u001b[AValidation loss: 0.9940283894538879\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:03, 21.61it/s]\u001b[AValidation loss: 0.9637085199356079\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:03, 21.57it/s]\u001b[AValidation loss: 3.3178791999816895\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:03, 21.58it/s]\u001b[AValidation loss: 1.210787057876587\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:02<00:03, 21.59it/s]\u001b[AValidation loss: 0.7596898674964905\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:02<00:03, 21.60it/s]\u001b[AValidation loss: 0.9983543157577515\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:02<00:02, 21.61it/s]\u001b[AValidation loss: 0.669546365737915\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:02<00:02, 21.63it/s]\u001b[AValidation loss: 1.6288598775863647\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:02<00:02, 21.64it/s]\u001b[AValidation loss: 1.0086907148361206\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:02<00:02, 21.64it/s]\u001b[AValidation loss: 0.7776932716369629\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:02<00:02, 21.66it/s]\u001b[AValidation loss: 1.5522916316986084\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:02<00:02, 21.62it/s]\u001b[AValidation loss: 1.6818156242370605\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:02<00:02, 21.63it/s]\u001b[AValidation loss: 1.2403351068496704\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:02<00:02, 21.64it/s]\u001b[AValidation loss: 1.3902889490127563\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:02<00:02, 21.66it/s]\u001b[AValidation loss: 1.5917459726333618\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:02<00:02, 21.66it/s]\u001b[AValidation loss: 0.9535778760910034\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:02<00:02, 21.67it/s]\u001b[AValidation loss: 0.7868425250053406\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:02<00:02, 21.65it/s]\u001b[AValidation loss: 1.1446549892425537\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:02<00:02, 21.65it/s]\u001b[AValidation loss: 0.8877769708633423\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:02<00:02, 21.66it/s]\u001b[AValidation loss: 1.2594846487045288\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:02<00:02, 21.66it/s]\u001b[AValidation loss: 1.2936826944351196\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:02<00:02, 21.68it/s]\u001b[AValidation loss: 1.0524557828903198\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:02<00:02, 21.69it/s]\u001b[AValidation loss: 1.5976219177246094\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:02, 21.70it/s]\u001b[AValidation loss: 2.218067169189453\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:02, 21.71it/s]\u001b[AValidation loss: 0.7832247018814087\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:02, 21.71it/s]\u001b[AValidation loss: 1.2836787700653076\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:03<00:02, 21.72it/s]\u001b[AValidation loss: 1.77407968044281\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:03<00:01, 21.73it/s]\u001b[AValidation loss: 0.789250910282135\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:03<00:01, 21.74it/s]\u001b[AValidation loss: 1.2015784978866577\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:03<00:01, 21.74it/s]\u001b[AValidation loss: 1.2071025371551514\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:03<00:01, 21.75it/s]\u001b[AValidation loss: 1.0605487823486328\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:03<00:01, 21.76it/s]\u001b[AValidation loss: 0.8896639347076416\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:03<00:01, 21.76it/s]\u001b[AValidation loss: 0.7204945087432861\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:03<00:01, 21.77it/s]\u001b[AValidation loss: 1.1079943180084229\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:03<00:01, 21.78it/s]\u001b[AValidation loss: 1.1861144304275513\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:03<00:01, 21.78it/s]\u001b[AValidation loss: 0.963067889213562\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:03<00:01, 21.79it/s]\u001b[AValidation loss: 0.9080198407173157\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:03<00:01, 21.77it/s]\u001b[AValidation loss: 0.5984165668487549\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:03<00:01, 21.74it/s]\u001b[AValidation loss: 0.6240420341491699\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:03<00:01, 21.74it/s]\u001b[AValidation loss: 1.0875110626220703\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:03<00:01, 21.75it/s]\u001b[AValidation loss: 1.3816099166870117\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:03<00:01, 21.75it/s]\u001b[AValidation loss: 1.0079411268234253\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:03<00:01, 21.76it/s]\u001b[AValidation loss: 0.7543593049049377\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:03<00:01, 21.76it/s]\u001b[AValidation loss: 0.8876407146453857\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:03<00:01, 21.76it/s]\u001b[AValidation loss: 0.7674705982208252\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:03<00:01, 21.77it/s]\u001b[AValidation loss: 0.8831056356430054\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:03<00:01, 21.77it/s]\u001b[AValidation loss: 0.932065486907959\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:03<00:01, 21.77it/s]\u001b[AValidation loss: 0.8278693556785583\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:04<00:01, 21.75it/s]\u001b[AValidation loss: 1.4965007305145264\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:04<00:00, 21.73it/s]\u001b[AValidation loss: 1.881028413772583\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:04<00:00, 21.71it/s]\u001b[AValidation loss: 1.397504210472107\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:04<00:00, 21.71it/s]\u001b[AValidation loss: 0.8016670942306519\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:04<00:00, 21.72it/s]\u001b[AValidation loss: 1.2202376127243042\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:04<00:00, 21.70it/s]\u001b[AValidation loss: 1.6603362560272217\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:04<00:00, 21.71it/s]\u001b[AValidation loss: 1.7655088901519775\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:04<00:00, 21.69it/s]\u001b[AValidation loss: 1.143120527267456\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:04<00:00, 21.69it/s]\u001b[AValidation loss: 1.216491937637329\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:04<00:00, 21.68it/s]\u001b[AValidation loss: 0.745347261428833\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:04<00:00, 21.69it/s]\u001b[AValidation loss: 1.0447757244110107\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:04<00:00, 21.69it/s]\u001b[AValidation loss: 0.8945231437683105\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:04<00:00, 21.67it/s]\u001b[AValidation loss: 0.7965203523635864\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:04<00:00, 21.68it/s]\u001b[AValidation loss: 1.1090608835220337\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:04<00:00, 21.68it/s]\u001b[AValidation loss: 0.9090997576713562\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:04<00:00, 21.68it/s]\u001b[AValidation loss: 1.2429441213607788\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:04<00:00, 21.69it/s]\u001b[AValidation loss: 0.9076043367385864\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:04<00:00, 21.70it/s]\u001b[AValidation loss: 1.135819911956787\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:04<00:00, 21.70it/s]\u001b[AValidation loss: 0.9008262157440186\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:04<00:00, 21.71it/s]\u001b[AValidation loss: 1.0613807439804077\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:04<00:00, 21.71it/s]\u001b[AValidation loss: 0.8460424542427063\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:05<00:00, 21.71it/s]\u001b[AValidation loss: 1.0911864042282104\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:05<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/512 [00:00<?, ?it/s, v_num=0]                 \u001b[ATraining loss: 1.0489146709442139\n",
      "Epoch 3:   0%|          | 1/512 [00:00<00:35, 14.20it/s, v_num=0]Training loss: 0.8920981287956238\n",
      "Epoch 3:   0%|          | 2/512 [00:00<01:55,  4.41it/s, v_num=0]Training loss: 1.072474479675293\n",
      "Epoch 3:   1%|          | 3/512 [00:00<02:23,  3.56it/s, v_num=0]Training loss: 0.95588219165802\n",
      "Epoch 3:   1%|          | 4/512 [00:01<02:36,  3.24it/s, v_num=0]Training loss: 1.603541612625122\n",
      "Epoch 3:   1%|          | 5/512 [00:01<02:44,  3.08it/s, v_num=0]Training loss: 1.1811195611953735\n",
      "Epoch 3:   1%|          | 6/512 [00:02<02:49,  2.98it/s, v_num=0]Training loss: 0.8538500070571899\n",
      "Epoch 3:   1%|▏         | 7/512 [00:02<02:53,  2.91it/s, v_num=0]Training loss: 1.2426022291183472\n",
      "Epoch 3:   2%|▏         | 8/512 [00:02<02:56,  2.86it/s, v_num=0]Training loss: 0.8670870661735535\n",
      "Epoch 3:   2%|▏         | 9/512 [00:03<02:57,  2.83it/s, v_num=0]Training loss: 1.2680999040603638\n",
      "Epoch 3:   2%|▏         | 10/512 [00:03<02:59,  2.80it/s, v_num=0]Training loss: 0.8923635482788086\n",
      "Epoch 3:   2%|▏         | 11/512 [00:03<03:00,  2.78it/s, v_num=0]Training loss: 1.0756248235702515\n",
      "Epoch 3:   2%|▏         | 12/512 [00:04<03:01,  2.76it/s, v_num=0]Training loss: 0.7730788588523865\n",
      "Epoch 3:   3%|▎         | 13/512 [00:04<03:02,  2.74it/s, v_num=0]Training loss: 0.9650810956954956\n",
      "Epoch 3:   3%|▎         | 14/512 [00:05<03:02,  2.73it/s, v_num=0]Training loss: 1.020936369895935\n",
      "Epoch 3:   3%|▎         | 15/512 [00:05<03:02,  2.72it/s, v_num=0]Training loss: 0.9571806788444519\n",
      "Epoch 3:   3%|▎         | 16/512 [00:05<03:03,  2.71it/s, v_num=0]Training loss: 0.8717364072799683\n",
      "Epoch 3:   3%|▎         | 17/512 [00:06<03:03,  2.70it/s, v_num=0]Training loss: 1.1225601434707642\n",
      "Epoch 3:   4%|▎         | 18/512 [00:06<03:03,  2.69it/s, v_num=0]Training loss: 1.4452980756759644\n",
      "Epoch 3:   4%|▎         | 19/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 0.9417051672935486\n",
      "Epoch 3:   4%|▍         | 20/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 1.0571073293685913\n",
      "Epoch 3:   4%|▍         | 21/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 1.5626758337020874\n",
      "Epoch 3:   4%|▍         | 22/512 [00:08<03:03,  2.67it/s, v_num=0]Training loss: 1.4997307062149048\n",
      "Epoch 3:   4%|▍         | 23/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 0.6470742225646973\n",
      "Epoch 3:   5%|▍         | 24/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 1.178912878036499\n",
      "Epoch 3:   5%|▍         | 25/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.0954192876815796\n",
      "Epoch 3:   5%|▌         | 26/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 1.0358703136444092\n",
      "Epoch 3:   5%|▌         | 27/512 [00:10<03:03,  2.65it/s, v_num=0]Training loss: 1.224730134010315\n",
      "Epoch 3:   5%|▌         | 28/512 [00:10<03:03,  2.64it/s, v_num=0]Training loss: 0.807712197303772\n",
      "Epoch 3:   6%|▌         | 29/512 [00:10<03:02,  2.64it/s, v_num=0]Training loss: 1.0622905492782593\n",
      "Epoch 3:   6%|▌         | 30/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 1.1518007516860962\n",
      "Epoch 3:   6%|▌         | 31/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 1.462794542312622\n",
      "Epoch 3:   6%|▋         | 32/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.8335819244384766\n",
      "Epoch 3:   6%|▋         | 33/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.1264760494232178\n",
      "Epoch 3:   7%|▋         | 34/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 1.4104243516921997\n",
      "Epoch 3:   7%|▋         | 35/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 0.9114028811454773\n",
      "Epoch 3:   7%|▋         | 36/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.6252427101135254\n",
      "Epoch 3:   7%|▋         | 37/512 [00:14<03:01,  2.62it/s, v_num=0]Training loss: 0.7364630699157715\n",
      "Epoch 3:   7%|▋         | 38/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.1679575443267822\n",
      "Epoch 3:   8%|▊         | 39/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 0.5897942781448364\n",
      "Epoch 3:   8%|▊         | 40/512 [00:15<03:00,  2.62it/s, v_num=0]Training loss: 1.458708643913269\n",
      "Epoch 3:   8%|▊         | 41/512 [00:15<02:59,  2.62it/s, v_num=0]Training loss: 1.1301391124725342\n",
      "Epoch 3:   8%|▊         | 42/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 15.260175704956055\n",
      "Epoch 3:   8%|▊         | 43/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.5273771286010742\n",
      "Epoch 3:   9%|▊         | 44/512 [00:16<02:58,  2.61it/s, v_num=0]Training loss: 1.3216499090194702\n",
      "Epoch 3:   9%|▉         | 45/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.3793879747390747\n",
      "Epoch 3:   9%|▉         | 46/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.3405205011367798\n",
      "Epoch 3:   9%|▉         | 47/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.2968909740447998\n",
      "Epoch 3:   9%|▉         | 48/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 0.989375650882721\n",
      "Epoch 3:  10%|▉         | 49/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.3204078674316406\n",
      "Epoch 3:  10%|▉         | 50/512 [00:19<02:57,  2.61it/s, v_num=0]Training loss: 0.8974703550338745\n",
      "Epoch 3:  10%|▉         | 51/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.0177627801895142\n",
      "Epoch 3:  10%|█         | 52/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 0.6801337003707886\n",
      "Epoch 3:  10%|█         | 53/512 [00:20<02:56,  2.61it/s, v_num=0]Training loss: 1.3818790912628174\n",
      "Epoch 3:  11%|█         | 54/512 [00:20<02:55,  2.61it/s, v_num=0]Training loss: 1.7138543128967285\n",
      "Epoch 3:  11%|█         | 55/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 0.8176085948944092\n",
      "Epoch 3:  11%|█         | 56/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 1.0059006214141846\n",
      "Epoch 3:  11%|█         | 57/512 [00:21<02:54,  2.60it/s, v_num=0]Training loss: 1.0709761381149292\n",
      "Epoch 3:  11%|█▏        | 58/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 1.077164888381958\n",
      "Epoch 3:  12%|█▏        | 59/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 1.1164731979370117\n",
      "Epoch 3:  12%|█▏        | 60/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.4254390001296997\n",
      "Epoch 3:  12%|█▏        | 61/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.3856393098831177\n",
      "Epoch 3:  12%|█▏        | 62/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.2151682376861572\n",
      "Epoch 3:  12%|█▏        | 63/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 0.9810032844543457\n",
      "Epoch 3:  12%|█▎        | 64/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 0.8645630478858948\n",
      "Epoch 3:  13%|█▎        | 65/512 [00:25<02:52,  2.60it/s, v_num=0]Training loss: 1.1326534748077393\n",
      "Epoch 3:  13%|█▎        | 66/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.3072690963745117\n",
      "Epoch 3:  13%|█▎        | 67/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.222516655921936\n",
      "Epoch 3:  13%|█▎        | 68/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 0.9414016008377075\n",
      "Epoch 3:  13%|█▎        | 69/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.1281378269195557\n",
      "Epoch 3:  14%|█▎        | 70/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 0.9460378289222717\n",
      "Epoch 3:  14%|█▍        | 71/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.113437533378601\n",
      "Epoch 3:  14%|█▍        | 72/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.1099098920822144\n",
      "Epoch 3:  14%|█▍        | 73/512 [00:28<02:49,  2.59it/s, v_num=0]Training loss: 1.6110213994979858\n",
      "Epoch 3:  14%|█▍        | 74/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 0.878488302230835\n",
      "Epoch 3:  15%|█▍        | 75/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.3001192808151245\n",
      "Epoch 3:  15%|█▍        | 76/512 [00:29<02:48,  2.59it/s, v_num=0]Training loss: 1.064092993736267\n",
      "Epoch 3:  15%|█▌        | 77/512 [00:29<02:47,  2.59it/s, v_num=0]Training loss: 1.4251420497894287\n",
      "Epoch 3:  15%|█▌        | 78/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.1275606155395508\n",
      "Epoch 3:  15%|█▌        | 79/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.2375340461730957\n",
      "Epoch 3:  16%|█▌        | 80/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 0.9651841521263123\n",
      "Epoch 3:  16%|█▌        | 81/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 1.2214406728744507\n",
      "Epoch 3:  16%|█▌        | 82/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 1.3474791049957275\n",
      "Epoch 3:  16%|█▌        | 83/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.0778381824493408\n",
      "Epoch 3:  16%|█▋        | 84/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.5137674808502197\n",
      "Epoch 3:  17%|█▋        | 85/512 [00:32<02:44,  2.59it/s, v_num=0]Training loss: 1.4125251770019531\n",
      "Epoch 3:  17%|█▋        | 86/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 0.7680476307868958\n",
      "Epoch 3:  17%|█▋        | 87/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.3996111154556274\n",
      "Epoch 3:  17%|█▋        | 88/512 [00:33<02:43,  2.59it/s, v_num=0]Training loss: 0.7041224837303162\n",
      "Epoch 3:  17%|█▋        | 89/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.2657185792922974\n",
      "Epoch 3:  18%|█▊        | 90/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.046743392944336\n",
      "Epoch 3:  18%|█▊        | 91/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.1705043315887451\n",
      "Epoch 3:  18%|█▊        | 92/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.1033918857574463\n",
      "Epoch 3:  18%|█▊        | 93/512 [00:35<02:41,  2.59it/s, v_num=0]Training loss: 1.7833718061447144\n",
      "Epoch 3:  18%|█▊        | 94/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.5810233354568481\n",
      "Epoch 3:  19%|█▊        | 95/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.2951730489730835\n",
      "Epoch 3:  19%|█▉        | 96/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 0.9956996440887451\n",
      "Epoch 3:  19%|█▉        | 97/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.2777334451675415\n",
      "Epoch 3:  19%|█▉        | 98/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.010941982269287\n",
      "Epoch 3:  19%|█▉        | 99/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 0.6667945981025696\n",
      "Epoch 3:  20%|█▉        | 100/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 0.704755961894989\n",
      "Epoch 3:  20%|█▉        | 101/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 1.178595781326294\n",
      "Epoch 3:  20%|█▉        | 102/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.9498356580734253\n",
      "Epoch 3:  20%|██        | 103/512 [00:39<02:38,  2.58it/s, v_num=0]Training loss: 0.9877886176109314\n",
      "Epoch 3:  20%|██        | 104/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.3814172744750977\n",
      "Epoch 3:  21%|██        | 105/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.4373486042022705\n",
      "Epoch 3:  21%|██        | 106/512 [00:41<02:37,  2.58it/s, v_num=0]Training loss: 1.0439517498016357\n",
      "Epoch 3:  21%|██        | 107/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 0.8616650700569153\n",
      "Epoch 3:  21%|██        | 108/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.1816264390945435\n",
      "Epoch 3:  21%|██▏       | 109/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.2198036909103394\n",
      "Epoch 3:  21%|██▏       | 110/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.6758806705474854\n",
      "Epoch 3:  22%|██▏       | 111/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.0836987495422363\n",
      "Epoch 3:  22%|██▏       | 112/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.1830849647521973\n",
      "Epoch 3:  22%|██▏       | 113/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.0452827215194702\n",
      "Epoch 3:  22%|██▏       | 114/512 [00:44<02:34,  2.58it/s, v_num=0]Training loss: 1.2197285890579224\n",
      "Epoch 3:  22%|██▏       | 115/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.702488362789154\n",
      "Epoch 3:  23%|██▎       | 116/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.9615421295166016\n",
      "Epoch 3:  23%|██▎       | 117/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.19139564037323\n",
      "Epoch 3:  23%|██▎       | 118/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.086605191230774\n",
      "Epoch 3:  23%|██▎       | 119/512 [00:46<02:32,  2.58it/s, v_num=0]Training loss: 0.9259576797485352\n",
      "Epoch 3:  23%|██▎       | 120/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 1.3312751054763794\n",
      "Epoch 3:  24%|██▎       | 121/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.938069224357605\n",
      "Epoch 3:  24%|██▍       | 122/512 [00:47<02:31,  2.58it/s, v_num=0]Training loss: 0.8793106079101562\n",
      "Epoch 3:  24%|██▍       | 123/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 1.0715631246566772\n",
      "Epoch 3:  24%|██▍       | 124/512 [00:48<02:30,  2.58it/s, v_num=0]Training loss: 1.4719256162643433\n",
      "Epoch 3:  24%|██▍       | 125/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.5925782918930054\n",
      "Epoch 3:  25%|██▍       | 126/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.7525572776794434\n",
      "Epoch 3:  25%|██▍       | 127/512 [00:49<02:29,  2.58it/s, v_num=0]Training loss: 0.9568202495574951\n",
      "Epoch 3:  25%|██▌       | 128/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 1.5539695024490356\n",
      "Epoch 3:  25%|██▌       | 129/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 0.7457255125045776\n",
      "Epoch 3:  25%|██▌       | 130/512 [00:50<02:28,  2.58it/s, v_num=0]Training loss: 1.065688967704773\n",
      "Epoch 3:  26%|██▌       | 131/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 0.9395275115966797\n",
      "Epoch 3:  26%|██▌       | 132/512 [00:51<02:27,  2.58it/s, v_num=0]Training loss: 0.920517086982727\n",
      "Epoch 3:  26%|██▌       | 133/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 0.7977656126022339\n",
      "Epoch 3:  26%|██▌       | 134/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.161033034324646\n",
      "Epoch 3:  26%|██▋       | 135/512 [00:52<02:26,  2.58it/s, v_num=0]Training loss: 1.3079558610916138\n",
      "Epoch 3:  27%|██▋       | 136/512 [00:52<02:25,  2.58it/s, v_num=0]Training loss: 0.9808534979820251\n",
      "Epoch 3:  27%|██▋       | 137/512 [00:53<02:25,  2.58it/s, v_num=0]Training loss: 1.3848158121109009\n",
      "Epoch 3:  27%|██▋       | 138/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 0.8668749332427979\n",
      "Epoch 3:  27%|██▋       | 139/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 1.0061925649642944\n",
      "Epoch 3:  27%|██▋       | 140/512 [00:54<02:24,  2.58it/s, v_num=0]Training loss: 1.1347099542617798\n",
      "Epoch 3:  28%|██▊       | 141/512 [00:54<02:23,  2.58it/s, v_num=0]Training loss: 1.1575368642807007\n",
      "Epoch 3:  28%|██▊       | 142/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.1199373006820679\n",
      "Epoch 3:  28%|██▊       | 143/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 0.7462548017501831\n",
      "Epoch 3:  28%|██▊       | 144/512 [00:55<02:22,  2.58it/s, v_num=0]Training loss: 1.0114054679870605\n",
      "Epoch 3:  28%|██▊       | 145/512 [00:56<02:22,  2.58it/s, v_num=0]Training loss: 1.4894614219665527\n",
      "Epoch 3:  29%|██▊       | 146/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 0.81037437915802\n",
      "Epoch 3:  29%|██▊       | 147/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 2.2575936317443848\n",
      "Epoch 3:  29%|██▉       | 148/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 1.0002447366714478\n",
      "Epoch 3:  29%|██▉       | 149/512 [00:57<02:20,  2.58it/s, v_num=0]Training loss: 0.7810066342353821\n",
      "Epoch 3:  29%|██▉       | 150/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 0.8958802819252014\n",
      "Epoch 3:  29%|██▉       | 151/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 0.6591036915779114\n",
      "Epoch 3:  30%|██▉       | 152/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 1.2708760499954224\n",
      "Epoch 3:  30%|██▉       | 153/512 [00:59<02:19,  2.58it/s, v_num=0]Training loss: 0.6303099393844604\n",
      "Epoch 3:  30%|███       | 154/512 [00:59<02:18,  2.58it/s, v_num=0]Training loss: 0.9340366721153259\n",
      "Epoch 3:  30%|███       | 155/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.441767930984497\n",
      "Epoch 3:  30%|███       | 156/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.2298495769500732\n",
      "Epoch 3:  31%|███       | 157/512 [01:00<02:17,  2.58it/s, v_num=0]Training loss: 0.8463214635848999\n",
      "Epoch 3:  31%|███       | 158/512 [01:01<02:17,  2.58it/s, v_num=0]Training loss: 1.6265888214111328\n",
      "Epoch 3:  31%|███       | 159/512 [01:01<02:16,  2.58it/s, v_num=0]Training loss: 1.5941088199615479\n",
      "Epoch 3:  31%|███▏      | 160/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 1.0484732389450073\n",
      "Epoch 3:  31%|███▏      | 161/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 1.0781461000442505\n",
      "Epoch 3:  32%|███▏      | 162/512 [01:02<02:15,  2.58it/s, v_num=0]Training loss: 1.0473666191101074\n",
      "Epoch 3:  32%|███▏      | 163/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 1.4550288915634155\n",
      "Epoch 3:  32%|███▏      | 164/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 0.6706872582435608\n",
      "Epoch 3:  32%|███▏      | 165/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 0.9936233758926392\n",
      "Epoch 3:  32%|███▏      | 166/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 0.8612340092658997\n",
      "Epoch 3:  33%|███▎      | 167/512 [01:04<02:13,  2.58it/s, v_num=0]Training loss: 1.3282912969589233\n",
      "Epoch 3:  33%|███▎      | 168/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 0.8978875875473022\n",
      "Epoch 3:  33%|███▎      | 169/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.006622552871704\n",
      "Epoch 3:  33%|███▎      | 170/512 [01:05<02:12,  2.58it/s, v_num=0]Training loss: 0.7624112963676453\n",
      "Epoch 3:  33%|███▎      | 171/512 [01:06<02:12,  2.58it/s, v_num=0]Training loss: 0.8559145927429199\n",
      "Epoch 3:  34%|███▎      | 172/512 [01:06<02:11,  2.58it/s, v_num=0]Training loss: 1.1907274723052979\n",
      "Epoch 3:  34%|███▍      | 173/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.2656015157699585\n",
      "Epoch 3:  34%|███▍      | 174/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 0.8237701654434204\n",
      "Epoch 3:  34%|███▍      | 175/512 [01:07<02:10,  2.58it/s, v_num=0]Training loss: 1.5036786794662476\n",
      "Epoch 3:  34%|███▍      | 176/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.1649233102798462\n",
      "Epoch 3:  35%|███▍      | 177/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.1510825157165527\n",
      "Epoch 3:  35%|███▍      | 178/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 0.8695040941238403\n",
      "Epoch 3:  35%|███▍      | 179/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 0.8795765042304993\n",
      "Epoch 3:  35%|███▌      | 180/512 [01:09<02:08,  2.58it/s, v_num=0]Training loss: 1.100265622138977\n",
      "Epoch 3:  35%|███▌      | 181/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 0.7263144254684448\n",
      "Epoch 3:  36%|███▌      | 182/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 1.1504249572753906\n",
      "Epoch 3:  36%|███▌      | 183/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 0.8435589671134949\n",
      "Epoch 3:  36%|███▌      | 184/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 0.8961013555526733\n",
      "Epoch 3:  36%|███▌      | 185/512 [01:11<02:06,  2.58it/s, v_num=0]Training loss: 0.7584406733512878\n",
      "Epoch 3:  36%|███▋      | 186/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 0.6405795216560364\n",
      "Epoch 3:  37%|███▋      | 187/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 0.819411039352417\n",
      "Epoch 3:  37%|███▋      | 188/512 [01:12<02:05,  2.58it/s, v_num=0]Training loss: 0.9943445324897766\n",
      "Epoch 3:  37%|███▋      | 189/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 0.6107393503189087\n",
      "Epoch 3:  37%|███▋      | 190/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 1.3055092096328735\n",
      "Epoch 3:  37%|███▋      | 191/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.7544409036636353\n",
      "Epoch 3:  38%|███▊      | 192/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.3504117727279663\n",
      "Epoch 3:  38%|███▊      | 193/512 [01:14<02:03,  2.58it/s, v_num=0]Training loss: 0.9804357290267944\n",
      "Epoch 3:  38%|███▊      | 194/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 0.8328664302825928\n",
      "Epoch 3:  38%|███▊      | 195/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 0.5702407360076904\n",
      "Epoch 3:  38%|███▊      | 196/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 0.7696202993392944\n",
      "Epoch 3:  38%|███▊      | 197/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.194230556488037\n",
      "Epoch 3:  39%|███▊      | 198/512 [01:16<02:01,  2.58it/s, v_num=0]Training loss: 0.8716570138931274\n",
      "Epoch 3:  39%|███▉      | 199/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 0.7534348964691162\n",
      "Epoch 3:  39%|███▉      | 200/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 0.8763691186904907\n",
      "Epoch 3:  39%|███▉      | 201/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 0.8321637511253357\n",
      "Epoch 3:  39%|███▉      | 202/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 1.7467552423477173\n",
      "Epoch 3:  40%|███▉      | 203/512 [01:18<01:59,  2.58it/s, v_num=0]Training loss: 1.0887748003005981\n",
      "Epoch 3:  40%|███▉      | 204/512 [01:19<01:59,  2.57it/s, v_num=0]Training loss: 1.2760978937149048\n",
      "Epoch 3:  40%|████      | 205/512 [01:19<01:59,  2.57it/s, v_num=0]Training loss: 1.9892231225967407\n",
      "Epoch 3:  40%|████      | 206/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 0.9021758437156677\n",
      "Epoch 3:  40%|████      | 207/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 0.8532358407974243\n",
      "Epoch 3:  41%|████      | 208/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 0.7264001369476318\n",
      "Epoch 3:  41%|████      | 209/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 1.6763938665390015\n",
      "Epoch 3:  41%|████      | 210/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 0.6955218315124512\n",
      "Epoch 3:  41%|████      | 211/512 [01:21<01:56,  2.57it/s, v_num=0]Training loss: 0.8845683336257935\n",
      "Epoch 3:  41%|████▏     | 212/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 0.7666506171226501\n",
      "Epoch 3:  42%|████▏     | 213/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 1.5615832805633545\n",
      "Epoch 3:  42%|████▏     | 214/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.3223838806152344\n",
      "Epoch 3:  42%|████▏     | 215/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.0196822881698608\n",
      "Epoch 3:  42%|████▏     | 216/512 [01:23<01:54,  2.57it/s, v_num=0]Training loss: 1.2124921083450317\n",
      "Epoch 3:  42%|████▏     | 217/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.1144741773605347\n",
      "Epoch 3:  43%|████▎     | 218/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 0.9012002944946289\n",
      "Epoch 3:  43%|████▎     | 219/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.1675951480865479\n",
      "Epoch 3:  43%|████▎     | 220/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.9604750871658325\n",
      "Epoch 3:  43%|████▎     | 221/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.6900410652160645\n",
      "Epoch 3:  43%|████▎     | 222/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 0.8048281073570251\n",
      "Epoch 3:  44%|████▎     | 223/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 1.5954790115356445\n",
      "Epoch 3:  44%|████▍     | 224/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.5888925194740295\n",
      "Epoch 3:  44%|████▍     | 225/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.9140825271606445\n",
      "Epoch 3:  44%|████▍     | 226/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.9076325297355652\n",
      "Epoch 3:  44%|████▍     | 227/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 1.1222552061080933\n",
      "Epoch 3:  45%|████▍     | 228/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 0.7751904129981995\n",
      "Epoch 3:  45%|████▍     | 229/512 [01:28<01:49,  2.57it/s, v_num=0]Training loss: 1.173448920249939\n",
      "Epoch 3:  45%|████▍     | 230/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 1.358291506767273\n",
      "Epoch 3:  45%|████▌     | 231/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 0.7198144197463989\n",
      "Epoch 3:  45%|████▌     | 232/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.0598032474517822\n",
      "Epoch 3:  46%|████▌     | 233/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.8440920114517212\n",
      "Epoch 3:  46%|████▌     | 234/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.0226819515228271\n",
      "Epoch 3:  46%|████▌     | 235/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.4156631231307983\n",
      "Epoch 3:  46%|████▌     | 236/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.4828075170516968\n",
      "Epoch 3:  46%|████▋     | 237/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.4640451669692993\n",
      "Epoch 3:  46%|████▋     | 238/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.2112287282943726\n",
      "Epoch 3:  47%|████▋     | 239/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.027845025062561\n",
      "Epoch 3:  47%|████▋     | 240/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 0.9761561155319214\n",
      "Epoch 3:  47%|████▋     | 241/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 0.6332659125328064\n",
      "Epoch 3:  47%|████▋     | 242/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 0.8694041967391968\n",
      "Epoch 3:  47%|████▋     | 243/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.6666983366012573\n",
      "Epoch 3:  48%|████▊     | 244/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.0173354148864746\n",
      "Epoch 3:  48%|████▊     | 245/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 0.9301007390022278\n",
      "Epoch 3:  48%|████▊     | 246/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 1.054347038269043\n",
      "Epoch 3:  48%|████▊     | 247/512 [01:35<01:42,  2.57it/s, v_num=0]Training loss: 1.0806865692138672\n",
      "Epoch 3:  48%|████▊     | 248/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 0.8844345808029175\n",
      "Epoch 3:  49%|████▊     | 249/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 0.7028308510780334\n",
      "Epoch 3:  49%|████▉     | 250/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.1944334506988525\n",
      "Epoch 3:  49%|████▉     | 251/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 0.9181792140007019\n",
      "Epoch 3:  49%|████▉     | 252/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.1749204397201538\n",
      "Epoch 3:  49%|████▉     | 253/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.2212159633636475\n",
      "Epoch 3:  50%|████▉     | 254/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 0.7344001531600952\n",
      "Epoch 3:  50%|████▉     | 255/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 0.7401525974273682\n",
      "Epoch 3:  50%|█████     | 256/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 0.839975118637085\n",
      "Epoch 3:  50%|█████     | 257/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.3675059080123901\n",
      "Epoch 3:  50%|█████     | 258/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 1.3982176780700684\n",
      "Epoch 3:  51%|█████     | 259/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 0.677607536315918\n",
      "Epoch 3:  51%|█████     | 260/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.7716696262359619\n",
      "Epoch 3:  51%|█████     | 261/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.9378744959831238\n",
      "Epoch 3:  51%|█████     | 262/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.9504979848861694\n",
      "Epoch 3:  51%|█████▏    | 263/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 2.5095860958099365\n",
      "Epoch 3:  52%|█████▏    | 264/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.9414921998977661\n",
      "Epoch 3:  52%|█████▏    | 265/512 [01:43<01:36,  2.57it/s, v_num=0]Training loss: 1.1424016952514648\n",
      "Epoch 3:  52%|█████▏    | 266/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.8340701460838318\n",
      "Epoch 3:  52%|█████▏    | 267/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.8431171178817749\n",
      "Epoch 3:  52%|█████▏    | 268/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.388903021812439\n",
      "Epoch 3:  53%|█████▎    | 269/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.1609662771224976\n",
      "Epoch 3:  53%|█████▎    | 270/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 0.9851922988891602\n",
      "Epoch 3:  53%|█████▎    | 271/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 1.3143948316574097\n",
      "Epoch 3:  53%|█████▎    | 272/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 1.0767881870269775\n",
      "Epoch 3:  53%|█████▎    | 273/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 0.7543235421180725\n",
      "Epoch 3:  54%|█████▎    | 274/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.3358081579208374\n",
      "Epoch 3:  54%|█████▎    | 275/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 0.927577793598175\n",
      "Epoch 3:  54%|█████▍    | 276/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.2973854541778564\n",
      "Epoch 3:  54%|█████▍    | 277/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 0.9522305130958557\n",
      "Epoch 3:  54%|█████▍    | 278/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.0236942768096924\n",
      "Epoch 3:  54%|█████▍    | 279/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.2881951332092285\n",
      "Epoch 3:  55%|█████▍    | 280/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.3021481037139893\n",
      "Epoch 3:  55%|█████▍    | 281/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 0.8883995413780212\n",
      "Epoch 3:  55%|█████▌    | 282/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.1704585552215576\n",
      "Epoch 3:  55%|█████▌    | 283/512 [01:50<01:29,  2.57it/s, v_num=0]Training loss: 1.0591892004013062\n",
      "Epoch 3:  55%|█████▌    | 284/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.1131396293640137\n",
      "Epoch 3:  56%|█████▌    | 285/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 0.8909926414489746\n",
      "Epoch 3:  56%|█████▌    | 286/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.3397760391235352\n",
      "Epoch 3:  56%|█████▌    | 287/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.3866420984268188\n",
      "Epoch 3:  56%|█████▋    | 288/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 0.938191294670105\n",
      "Epoch 3:  56%|█████▋    | 289/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.0315101146697998\n",
      "Epoch 3:  57%|█████▋    | 290/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.1029809713363647\n",
      "Epoch 3:  57%|█████▋    | 291/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 0.7171192765235901\n",
      "Epoch 3:  57%|█████▋    | 292/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.0015184879302979\n",
      "Epoch 3:  57%|█████▋    | 293/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.519139051437378\n",
      "Epoch 3:  57%|█████▋    | 294/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 0.7147996425628662\n",
      "Epoch 3:  58%|█████▊    | 295/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 0.9343290328979492\n",
      "Epoch 3:  58%|█████▊    | 296/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.9577906131744385\n",
      "Epoch 3:  58%|█████▊    | 297/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.6893698573112488\n",
      "Epoch 3:  58%|█████▊    | 298/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.6498047113418579\n",
      "Epoch 3:  58%|█████▊    | 299/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 0.799735426902771\n",
      "Epoch 3:  59%|█████▊    | 300/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.29132080078125\n",
      "Epoch 3:  59%|█████▉    | 301/512 [01:57<01:22,  2.57it/s, v_num=0]Training loss: 0.8736817240715027\n",
      "Epoch 3:  59%|█████▉    | 302/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 1.1161763668060303\n",
      "Epoch 3:  59%|█████▉    | 303/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 1.4873332977294922\n",
      "Epoch 3:  59%|█████▉    | 304/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.0146547555923462\n",
      "Epoch 3:  60%|█████▉    | 305/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.9926975965499878\n",
      "Epoch 3:  60%|█████▉    | 306/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.9605866074562073\n",
      "Epoch 3:  60%|█████▉    | 307/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.7319406270980835\n",
      "Epoch 3:  60%|██████    | 308/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.2911245822906494\n",
      "Epoch 3:  60%|██████    | 309/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.1213159561157227\n",
      "Epoch 3:  61%|██████    | 310/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.6398202180862427\n",
      "Epoch 3:  61%|██████    | 311/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.6648544073104858\n",
      "Epoch 3:  61%|██████    | 312/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 0.5487664937973022\n",
      "Epoch 3:  61%|██████    | 313/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 0.731009840965271\n",
      "Epoch 3:  61%|██████▏   | 314/512 [02:02<01:17,  2.57it/s, v_num=0]Training loss: 0.9092777967453003\n",
      "Epoch 3:  62%|██████▏   | 315/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.0036016702651978\n",
      "Epoch 3:  62%|██████▏   | 316/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 0.8037500977516174\n",
      "Epoch 3:  62%|██████▏   | 317/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.2334322929382324\n",
      "Epoch 3:  62%|██████▏   | 318/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 0.9594860076904297\n",
      "Epoch 3:  62%|██████▏   | 319/512 [02:04<01:15,  2.57it/s, v_num=0]Training loss: 1.5526188611984253\n",
      "Epoch 3:  62%|██████▎   | 320/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.0567004680633545\n",
      "Epoch 3:  63%|██████▎   | 321/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 0.5948250889778137\n",
      "Epoch 3:  63%|██████▎   | 322/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.125365972518921\n",
      "Epoch 3:  63%|██████▎   | 323/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 0.8800671100616455\n",
      "Epoch 3:  63%|██████▎   | 324/512 [02:06<01:13,  2.57it/s, v_num=0]Training loss: 1.231963038444519\n",
      "Epoch 3:  63%|██████▎   | 325/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.0488078594207764\n",
      "Epoch 3:  64%|██████▎   | 326/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.3598682880401611\n",
      "Epoch 3:  64%|██████▍   | 327/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.0518486499786377\n",
      "Epoch 3:  64%|██████▍   | 328/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.3229429721832275\n",
      "Epoch 3:  64%|██████▍   | 329/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 0.923413872718811\n",
      "Epoch 3:  64%|██████▍   | 330/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.488430380821228\n",
      "Epoch 3:  65%|██████▍   | 331/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 0.893039882183075\n",
      "Epoch 3:  65%|██████▍   | 332/512 [02:09<01:10,  2.57it/s, v_num=0]Training loss: 0.8063433170318604\n",
      "Epoch 3:  65%|██████▌   | 333/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.0316704511642456\n",
      "Epoch 3:  65%|██████▌   | 334/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.1987038850784302\n",
      "Epoch 3:  65%|██████▌   | 335/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.8858582377433777\n",
      "Epoch 3:  66%|██████▌   | 336/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.6035690903663635\n",
      "Epoch 3:  66%|██████▌   | 337/512 [02:11<01:08,  2.57it/s, v_num=0]Training loss: 1.1337982416152954\n",
      "Epoch 3:  66%|██████▌   | 338/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 0.7361984848976135\n",
      "Epoch 3:  66%|██████▌   | 339/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.290724277496338\n",
      "Epoch 3:  66%|██████▋   | 340/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 0.9163357615470886\n",
      "Epoch 3:  67%|██████▋   | 341/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 0.8847948312759399\n",
      "Epoch 3:  67%|██████▋   | 342/512 [02:13<01:06,  2.57it/s, v_num=0]Training loss: 0.7418739199638367\n",
      "Epoch 3:  67%|██████▋   | 343/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.3537689447402954\n",
      "Epoch 3:  67%|██████▋   | 344/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.1267659664154053\n",
      "Epoch 3:  67%|██████▋   | 345/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 1.1361430883407593\n",
      "Epoch 3:  68%|██████▊   | 346/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.9019783735275269\n",
      "Epoch 3:  68%|██████▊   | 347/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.8620151877403259\n",
      "Epoch 3:  68%|██████▊   | 348/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 1.4813224077224731\n",
      "Epoch 3:  68%|██████▊   | 349/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 1.1047569513320923\n",
      "Epoch 3:  68%|██████▊   | 350/512 [02:16<01:03,  2.57it/s, v_num=0]Training loss: 1.3553177118301392\n",
      "Epoch 3:  69%|██████▊   | 351/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 0.9657074213027954\n",
      "Epoch 3:  69%|██████▉   | 352/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 0.8122215270996094\n",
      "Epoch 3:  69%|██████▉   | 353/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 1.1424453258514404\n",
      "Epoch 3:  69%|██████▉   | 354/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.7179437279701233\n",
      "Epoch 3:  69%|██████▉   | 355/512 [02:18<01:01,  2.57it/s, v_num=0]Training loss: 0.6653579473495483\n",
      "Epoch 3:  70%|██████▉   | 356/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 0.7907304167747498\n",
      "Epoch 3:  70%|██████▉   | 357/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.1153600215911865\n",
      "Epoch 3:  70%|██████▉   | 358/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.276794195175171\n",
      "Epoch 3:  70%|███████   | 359/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.2395063638687134\n",
      "Epoch 3:  70%|███████   | 360/512 [02:20<00:59,  2.57it/s, v_num=0]Training loss: 0.8795729875564575\n",
      "Epoch 3:  71%|███████   | 361/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 1.038617730140686\n",
      "Epoch 3:  71%|███████   | 362/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 0.8615670204162598\n",
      "Epoch 3:  71%|███████   | 363/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.0756585597991943\n",
      "Epoch 3:  71%|███████   | 364/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.1009262800216675\n",
      "Epoch 3:  71%|███████▏  | 365/512 [02:22<00:57,  2.57it/s, v_num=0]Training loss: 1.2325576543807983\n",
      "Epoch 3:  71%|███████▏  | 366/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 1.0233263969421387\n",
      "Epoch 3:  72%|███████▏  | 367/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 0.8912530541419983\n",
      "Epoch 3:  72%|███████▏  | 368/512 [02:23<00:56,  2.57it/s, v_num=0]Training loss: 0.9386674761772156\n",
      "Epoch 3:  72%|███████▏  | 369/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 0.6178491711616516\n",
      "Epoch 3:  72%|███████▏  | 370/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.0019841194152832\n",
      "Epoch 3:  72%|███████▏  | 371/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.1662756204605103\n",
      "Epoch 3:  73%|███████▎  | 372/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.3609042167663574\n",
      "Epoch 3:  73%|███████▎  | 373/512 [02:25<00:54,  2.57it/s, v_num=0]Training loss: 0.9415403008460999\n",
      "Epoch 3:  73%|███████▎  | 374/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.3372288942337036\n",
      "Epoch 3:  73%|███████▎  | 375/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 2.7970921993255615\n",
      "Epoch 3:  73%|███████▎  | 376/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.7392474412918091\n",
      "Epoch 3:  74%|███████▎  | 377/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 1.0372587442398071\n",
      "Epoch 3:  74%|███████▍  | 378/512 [02:27<00:52,  2.57it/s, v_num=0]Training loss: 1.025064468383789\n",
      "Epoch 3:  74%|███████▍  | 379/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.1682368516921997\n",
      "Epoch 3:  74%|███████▍  | 380/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.8975790739059448\n",
      "Epoch 3:  74%|███████▍  | 381/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.0554702281951904\n",
      "Epoch 3:  75%|███████▍  | 382/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.9635130763053894\n",
      "Epoch 3:  75%|███████▍  | 383/512 [02:29<00:50,  2.57it/s, v_num=0]Training loss: 0.8749748468399048\n",
      "Epoch 3:  75%|███████▌  | 384/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.0233370065689087\n",
      "Epoch 3:  75%|███████▌  | 385/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 0.896391749382019\n",
      "Epoch 3:  75%|███████▌  | 386/512 [02:30<00:49,  2.57it/s, v_num=0]Training loss: 1.3471927642822266\n",
      "Epoch 3:  76%|███████▌  | 387/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 0.7168582081794739\n",
      "Epoch 3:  76%|███████▌  | 388/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.059788465499878\n",
      "Epoch 3:  76%|███████▌  | 389/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 0.812329113483429\n",
      "Epoch 3:  76%|███████▌  | 390/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.0092978477478027\n",
      "Epoch 3:  76%|███████▋  | 391/512 [02:32<00:47,  2.57it/s, v_num=0]Training loss: 1.9309948682785034\n",
      "Epoch 3:  77%|███████▋  | 392/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 0.7182187438011169\n",
      "Epoch 3:  77%|███████▋  | 393/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.1692674160003662\n",
      "Epoch 3:  77%|███████▋  | 394/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.9016388654708862\n",
      "Epoch 3:  77%|███████▋  | 395/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.109776258468628\n",
      "Epoch 3:  77%|███████▋  | 396/512 [02:34<00:45,  2.57it/s, v_num=0]Training loss: 0.5957330465316772\n",
      "Epoch 3:  78%|███████▊  | 397/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.410650372505188\n",
      "Epoch 3:  78%|███████▊  | 398/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 0.6271716356277466\n",
      "Epoch 3:  78%|███████▊  | 399/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.6780508160591125\n",
      "Epoch 3:  78%|███████▊  | 400/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 1.2143926620483398\n",
      "Epoch 3:  78%|███████▊  | 401/512 [02:36<00:43,  2.57it/s, v_num=0]Training loss: 1.0114772319793701\n",
      "Epoch 3:  79%|███████▊  | 402/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 3.394286632537842\n",
      "Epoch 3:  79%|███████▊  | 403/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.9185541868209839\n",
      "Epoch 3:  79%|███████▉  | 404/512 [02:37<00:42,  2.57it/s, v_num=0]Training loss: 1.1155240535736084\n",
      "Epoch 3:  79%|███████▉  | 405/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.3568038940429688\n",
      "Epoch 3:  79%|███████▉  | 406/512 [02:38<00:41,  2.57it/s, v_num=0]Training loss: 1.0908721685409546\n",
      "Epoch 3:  79%|███████▉  | 407/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.6872232556343079\n",
      "Epoch 3:  80%|███████▉  | 408/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.8104023933410645\n",
      "Epoch 3:  80%|███████▉  | 409/512 [02:39<00:40,  2.57it/s, v_num=0]Training loss: 1.07841157913208\n",
      "Epoch 3:  80%|████████  | 410/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 1.1639944314956665\n",
      "Epoch 3:  80%|████████  | 411/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.8622748255729675\n",
      "Epoch 3:  80%|████████  | 412/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 1.5807429552078247\n",
      "Epoch 3:  81%|████████  | 413/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.8429729342460632\n",
      "Epoch 3:  81%|████████  | 414/512 [02:41<00:38,  2.57it/s, v_num=0]Training loss: 1.0672388076782227\n",
      "Epoch 3:  81%|████████  | 415/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.5634300708770752\n",
      "Epoch 3:  81%|████████▏ | 416/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.3395946025848389\n",
      "Epoch 3:  81%|████████▏ | 417/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.376676082611084\n",
      "Epoch 3:  82%|████████▏ | 418/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.3413760662078857\n",
      "Epoch 3:  82%|████████▏ | 419/512 [02:43<00:36,  2.57it/s, v_num=0]Training loss: 0.9071577191352844\n",
      "Epoch 3:  82%|████████▏ | 420/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 0.867226779460907\n",
      "Epoch 3:  82%|████████▏ | 421/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 1.169252634048462\n",
      "Epoch 3:  82%|████████▏ | 422/512 [02:44<00:35,  2.57it/s, v_num=0]Training loss: 0.8176013827323914\n",
      "Epoch 3:  83%|████████▎ | 423/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 0.9410767555236816\n",
      "Epoch 3:  83%|████████▎ | 424/512 [02:45<00:34,  2.57it/s, v_num=0]Training loss: 0.7740152478218079\n",
      "Epoch 3:  83%|████████▎ | 425/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 0.909239649772644\n",
      "Epoch 3:  83%|████████▎ | 426/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 1.4187202453613281\n",
      "Epoch 3:  83%|████████▎ | 427/512 [02:46<00:33,  2.57it/s, v_num=0]Training loss: 1.3498504161834717\n",
      "Epoch 3:  84%|████████▎ | 428/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.082527756690979\n",
      "Epoch 3:  84%|████████▍ | 429/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 0.9333198666572571\n",
      "Epoch 3:  84%|████████▍ | 430/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.1897568702697754\n",
      "Epoch 3:  84%|████████▍ | 431/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 0.7762245535850525\n",
      "Epoch 3:  84%|████████▍ | 432/512 [02:48<00:31,  2.57it/s, v_num=0]Training loss: 0.7127314805984497\n",
      "Epoch 3:  85%|████████▍ | 433/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.9306290745735168\n",
      "Epoch 3:  85%|████████▍ | 434/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 1.20313560962677\n",
      "Epoch 3:  85%|████████▍ | 435/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 0.8520652651786804\n",
      "Epoch 3:  85%|████████▌ | 436/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 0.773165762424469\n",
      "Epoch 3:  85%|████████▌ | 437/512 [02:50<00:29,  2.57it/s, v_num=0]Training loss: 0.884663462638855\n",
      "Epoch 3:  86%|████████▌ | 438/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.1611615419387817\n",
      "Epoch 3:  86%|████████▌ | 439/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 0.7880127429962158\n",
      "Epoch 3:  86%|████████▌ | 440/512 [02:51<00:28,  2.57it/s, v_num=0]Training loss: 0.7327064275741577\n",
      "Epoch 3:  86%|████████▌ | 441/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 1.7106196880340576\n",
      "Epoch 3:  86%|████████▋ | 442/512 [02:52<00:27,  2.57it/s, v_num=0]Training loss: 0.8728184103965759\n",
      "Epoch 3:  87%|████████▋ | 443/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 1.046875\n",
      "Epoch 3:  87%|████████▋ | 444/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 1.3783276081085205\n",
      "Epoch 3:  87%|████████▋ | 445/512 [02:53<00:26,  2.57it/s, v_num=0]Training loss: 0.8573094010353088\n",
      "Epoch 3:  87%|████████▋ | 446/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 1.6384592056274414\n",
      "Epoch 3:  87%|████████▋ | 447/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 0.6275060176849365\n",
      "Epoch 3:  88%|████████▊ | 448/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 0.8772852420806885\n",
      "Epoch 3:  88%|████████▊ | 449/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 1.3849365711212158\n",
      "Epoch 3:  88%|████████▊ | 450/512 [02:55<00:24,  2.57it/s, v_num=0]Training loss: 0.9972349405288696\n",
      "Epoch 3:  88%|████████▊ | 451/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 1.2529432773590088\n",
      "Epoch 3:  88%|████████▊ | 452/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 1.1010915040969849\n",
      "Epoch 3:  88%|████████▊ | 453/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.756179928779602\n",
      "Epoch 3:  89%|████████▊ | 454/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.985858142375946\n",
      "Epoch 3:  89%|████████▉ | 455/512 [02:57<00:22,  2.57it/s, v_num=0]Training loss: 0.6070459485054016\n",
      "Epoch 3:  89%|████████▉ | 456/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.344067931175232\n",
      "Epoch 3:  89%|████████▉ | 457/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 1.035491704940796\n",
      "Epoch 3:  89%|████████▉ | 458/512 [02:58<00:21,  2.57it/s, v_num=0]Training loss: 0.9607956409454346\n",
      "Epoch 3:  90%|████████▉ | 459/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 0.7010515928268433\n",
      "Epoch 3:  90%|████████▉ | 460/512 [02:59<00:20,  2.57it/s, v_num=0]Training loss: 1.5417121648788452\n",
      "Epoch 3:  90%|█████████ | 461/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.0940338373184204\n",
      "Epoch 3:  90%|█████████ | 462/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.8465538024902344\n",
      "Epoch 3:  90%|█████████ | 463/512 [03:00<00:19,  2.57it/s, v_num=0]Training loss: 1.081788182258606\n",
      "Epoch 3:  91%|█████████ | 464/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 0.9313623309135437\n",
      "Epoch 3:  91%|█████████ | 465/512 [03:01<00:18,  2.57it/s, v_num=0]Training loss: 1.4875116348266602\n",
      "Epoch 3:  91%|█████████ | 466/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 1.6274296045303345\n",
      "Epoch 3:  91%|█████████ | 467/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 1.1148724555969238\n",
      "Epoch 3:  91%|█████████▏| 468/512 [03:02<00:17,  2.57it/s, v_num=0]Training loss: 1.1528019905090332\n",
      "Epoch 3:  92%|█████████▏| 469/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 0.7446672916412354\n",
      "Epoch 3:  92%|█████████▏| 470/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 1.820794939994812\n",
      "Epoch 3:  92%|█████████▏| 471/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.187738299369812\n",
      "Epoch 3:  92%|█████████▏| 472/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.0349514484405518\n",
      "Epoch 3:  92%|█████████▏| 473/512 [03:04<00:15,  2.57it/s, v_num=0]Training loss: 1.0056332349777222\n",
      "Epoch 3:  93%|█████████▎| 474/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.1218225955963135\n",
      "Epoch 3:  93%|█████████▎| 475/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 0.8355324864387512\n",
      "Epoch 3:  93%|█████████▎| 476/512 [03:05<00:14,  2.57it/s, v_num=0]Training loss: 1.660506248474121\n",
      "Epoch 3:  93%|█████████▎| 477/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 1.2117486000061035\n",
      "Epoch 3:  93%|█████████▎| 478/512 [03:06<00:13,  2.57it/s, v_num=0]Training loss: 2.1410155296325684\n",
      "Epoch 3:  94%|█████████▎| 479/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.0371935367584229\n",
      "Epoch 3:  94%|█████████▍| 480/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.1567338705062866\n",
      "Epoch 3:  94%|█████████▍| 481/512 [03:07<00:12,  2.57it/s, v_num=0]Training loss: 1.082708716392517\n",
      "Epoch 3:  94%|█████████▍| 482/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 0.9590126276016235\n",
      "Epoch 3:  94%|█████████▍| 483/512 [03:08<00:11,  2.57it/s, v_num=0]Training loss: 0.7954738140106201\n",
      "Epoch 3:  95%|█████████▍| 484/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 0.957370936870575\n",
      "Epoch 3:  95%|█████████▍| 485/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.1783318519592285\n",
      "Epoch 3:  95%|█████████▍| 486/512 [03:09<00:10,  2.57it/s, v_num=0]Training loss: 2.041236400604248\n",
      "Epoch 3:  95%|█████████▌| 487/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 1.0359200239181519\n",
      "Epoch 3:  95%|█████████▌| 488/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 0.8166861534118652\n",
      "Epoch 3:  96%|█████████▌| 489/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.1786813735961914\n",
      "Epoch 3:  96%|█████████▌| 490/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.5302445888519287\n",
      "Epoch 3:  96%|█████████▌| 491/512 [03:11<00:08,  2.57it/s, v_num=0]Training loss: 0.9777461290359497\n",
      "Epoch 3:  96%|█████████▌| 492/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 0.8121732473373413\n",
      "Epoch 3:  96%|█████████▋| 493/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 0.6531001925468445\n",
      "Epoch 3:  96%|█████████▋| 494/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 1.2491220235824585\n",
      "Epoch 3:  97%|█████████▋| 495/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 0.9286215901374817\n",
      "Epoch 3:  97%|█████████▋| 496/512 [03:13<00:06,  2.57it/s, v_num=0]Training loss: 1.0902509689331055\n",
      "Epoch 3:  97%|█████████▋| 497/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 1.1448123455047607\n",
      "Epoch 3:  97%|█████████▋| 498/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 0.947694718837738\n",
      "Epoch 3:  97%|█████████▋| 499/512 [03:14<00:05,  2.57it/s, v_num=0]Training loss: 1.0085989236831665\n",
      "Epoch 3:  98%|█████████▊| 500/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 1.2549653053283691\n",
      "Epoch 3:  98%|█████████▊| 501/512 [03:15<00:04,  2.57it/s, v_num=0]Training loss: 0.8380573391914368\n",
      "Epoch 3:  98%|█████████▊| 502/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 0.7625365257263184\n",
      "Epoch 3:  98%|█████████▊| 503/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 0.7990697026252747\n",
      "Epoch 3:  98%|█████████▊| 504/512 [03:16<00:03,  2.57it/s, v_num=0]Training loss: 0.9295433759689331\n",
      "Epoch 3:  99%|█████████▊| 505/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 0.6865597367286682\n",
      "Epoch 3:  99%|█████████▉| 506/512 [03:17<00:02,  2.57it/s, v_num=0]Training loss: 1.0521126985549927\n",
      "Epoch 3:  99%|█████████▉| 507/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 0.8750755786895752\n",
      "Epoch 3:  99%|█████████▉| 508/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 0.9387722015380859\n",
      "Epoch 3:  99%|█████████▉| 509/512 [03:18<00:01,  2.57it/s, v_num=0]Training loss: 1.199690818786621\n",
      "Epoch 3: 100%|█████████▉| 510/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 0.7393360137939453\n",
      "Epoch 3: 100%|█████████▉| 511/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 0.8719576001167297\n",
      "Epoch 3: 100%|██████████| 512/512 [03:19<00:00,  2.57it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.1159744262695312\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 29.90it/s]\u001b[AValidation loss: 0.7930200099945068\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 24.80it/s]\u001b[AValidation loss: 1.2475991249084473\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 22.95it/s]\u001b[AValidation loss: 1.08089017868042\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 22.71it/s]\u001b[AValidation loss: 0.8412272334098816\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 22.60it/s]\u001b[AValidation loss: 1.1184061765670776\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 22.61it/s]\u001b[AValidation loss: 0.8845891952514648\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 22.19it/s]\u001b[AValidation loss: 1.124953031539917\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 22.15it/s]\u001b[AValidation loss: 1.1223198175430298\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.14it/s]\u001b[AValidation loss: 0.985857367515564\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.14it/s]\u001b[AValidation loss: 0.7891018986701965\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.14it/s]\u001b[AValidation loss: 0.6844497919082642\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.17it/s]\u001b[AValidation loss: 0.8700011372566223\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.00it/s]\u001b[AValidation loss: 0.9912741780281067\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 21.85it/s]\u001b[AValidation loss: 1.0081415176391602\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 21.85it/s]\u001b[AValidation loss: 1.1778793334960938\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 21.86it/s]\u001b[AValidation loss: 0.9624504446983337\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 21.88it/s]\u001b[AValidation loss: 0.724339485168457\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 21.89it/s]\u001b[AValidation loss: 0.5977206230163574\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 21.91it/s]\u001b[AValidation loss: 0.683803915977478\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:04, 21.92it/s]\u001b[AValidation loss: 1.38311767578125\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:04, 21.91it/s]\u001b[AValidation loss: 0.6897495985031128\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:01<00:04, 21.82it/s]\u001b[AValidation loss: 0.9995483756065369\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:03, 21.82it/s]\u001b[AValidation loss: 0.9716954231262207\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:03, 21.74it/s]\u001b[AValidation loss: 0.8152526617050171\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 21.75it/s]\u001b[AValidation loss: 0.718743085861206\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 21.76it/s]\u001b[AValidation loss: 1.3347700834274292\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:01<00:03, 21.79it/s]\u001b[AValidation loss: 0.8131178021430969\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:01<00:03, 21.80it/s]\u001b[AValidation loss: 1.0405961275100708\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:01<00:03, 21.81it/s]\u001b[AValidation loss: 2.2052721977233887\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:01<00:03, 21.83it/s]\u001b[AValidation loss: 1.3058552742004395\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:03, 21.84it/s]\u001b[AValidation loss: 1.3338115215301514\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:03, 21.85it/s]\u001b[AValidation loss: 0.9500111937522888\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:03, 21.86it/s]\u001b[AValidation loss: 1.0459026098251343\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:03, 21.88it/s]\u001b[AValidation loss: 1.0097250938415527\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:03, 21.89it/s]\u001b[AValidation loss: 0.6801770925521851\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:03, 21.90it/s]\u001b[AValidation loss: 0.8145943284034729\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:03, 21.91it/s]\u001b[AValidation loss: 1.1484317779541016\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:03, 21.92it/s]\u001b[AValidation loss: 0.8686392307281494\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:03, 21.94it/s]\u001b[AValidation loss: 0.6992485523223877\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:03, 21.94it/s]\u001b[AValidation loss: 0.9205949306488037\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:03, 21.95it/s]\u001b[AValidation loss: 0.8711827993392944\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:03, 21.96it/s]\u001b[AValidation loss: 3.210836410522461\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:03, 21.97it/s]\u001b[AValidation loss: 1.1011128425598145\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:02<00:03, 21.98it/s]\u001b[AValidation loss: 0.694324254989624\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:02<00:02, 21.99it/s]\u001b[AValidation loss: 0.9104917645454407\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 0.6055745482444763\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:02<00:02, 22.00it/s]\u001b[AValidation loss: 1.5150506496429443\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:02<00:02, 22.01it/s]\u001b[AValidation loss: 0.9367696046829224\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:02<00:02, 22.02it/s]\u001b[AValidation loss: 0.7168200016021729\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:02<00:02, 22.02it/s]\u001b[AValidation loss: 1.4723020792007446\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:02<00:02, 22.03it/s]\u001b[AValidation loss: 1.5436489582061768\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:02<00:02, 22.04it/s]\u001b[AValidation loss: 1.1384199857711792\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:02<00:02, 22.04it/s]\u001b[AValidation loss: 1.2631276845932007\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:02<00:02, 22.05it/s]\u001b[AValidation loss: 1.461429476737976\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:02<00:02, 22.06it/s]\u001b[AValidation loss: 0.8708797693252563\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:02<00:02, 22.06it/s]\u001b[AValidation loss: 0.7482517957687378\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:02<00:02, 22.06it/s]\u001b[AValidation loss: 1.0754494667053223\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:02<00:02, 22.07it/s]\u001b[AValidation loss: 0.8364426493644714\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:02<00:02, 22.07it/s]\u001b[AValidation loss: 1.1402320861816406\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:02<00:02, 22.08it/s]\u001b[AValidation loss: 1.176335334777832\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:02<00:02, 22.08it/s]\u001b[AValidation loss: 0.9761334657669067\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:02<00:02, 22.09it/s]\u001b[AValidation loss: 1.4654902219772339\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:02, 22.09it/s]\u001b[AValidation loss: 2.11995005607605\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:02, 22.10it/s]\u001b[AValidation loss: 0.7190922498703003\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:02, 22.10it/s]\u001b[AValidation loss: 1.1761674880981445\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:02<00:01, 22.11it/s]\u001b[AValidation loss: 1.659506916999817\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:03<00:01, 22.11it/s]\u001b[AValidation loss: 0.7362425327301025\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 1.0817029476165771\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 1.102695345878601\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.9650037884712219\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.8316595554351807\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.6687248349189758\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 1.0163886547088623\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 1.0756901502609253\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.8909294605255127\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.8324271440505981\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 0.5482619404792786\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 0.5851314067840576\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 1.0095038414001465\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:03<00:01, 22.16it/s]\u001b[AValidation loss: 1.2679672241210938\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:03<00:01, 22.16it/s]\u001b[AValidation loss: 0.9485189914703369\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:03<00:01, 22.16it/s]\u001b[AValidation loss: 0.6936807036399841\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:03<00:01, 22.17it/s]\u001b[AValidation loss: 0.8037450313568115\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:03<00:01, 22.17it/s]\u001b[AValidation loss: 0.7181428670883179\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:03<00:01, 22.17it/s]\u001b[AValidation loss: 0.8111275434494019\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:03<00:01, 22.17it/s]\u001b[AValidation loss: 0.8746732473373413\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:03<00:01, 22.18it/s]\u001b[AValidation loss: 0.7624762058258057\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:03<00:00, 22.18it/s]\u001b[AValidation loss: 1.4128950834274292\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:04<00:00, 22.18it/s]\u001b[AValidation loss: 1.7329182624816895\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:04<00:00, 22.18it/s]\u001b[AValidation loss: 1.333290696144104\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:04<00:00, 22.19it/s]\u001b[AValidation loss: 0.7444036602973938\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:04<00:00, 22.19it/s]\u001b[AValidation loss: 1.1563730239868164\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:04<00:00, 22.19it/s]\u001b[AValidation loss: 1.5870906114578247\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:04<00:00, 22.19it/s]\u001b[AValidation loss: 1.618057370185852\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:04<00:00, 22.19it/s]\u001b[AValidation loss: 1.0571180582046509\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:04<00:00, 22.20it/s]\u001b[AValidation loss: 1.1333062648773193\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:04<00:00, 22.20it/s]\u001b[AValidation loss: 0.6953558921813965\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:04<00:00, 22.20it/s]\u001b[AValidation loss: 0.9405987858772278\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:04<00:00, 22.20it/s]\u001b[AValidation loss: 0.8296587467193604\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 0.7421671748161316\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 1.0005940198898315\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 0.8262400031089783\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 1.1840940713882446\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 0.8222519755363464\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 1.0319311618804932\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:04<00:00, 22.22it/s]\u001b[AValidation loss: 0.8360154628753662\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:04<00:00, 22.22it/s]\u001b[AValidation loss: 0.9639005064964294\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:04<00:00, 22.22it/s]\u001b[AValidation loss: 0.7786397933959961\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:04<00:00, 22.21it/s]\u001b[AValidation loss: 1.0023589134216309\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:04<00:00, 22.24it/s]\u001b[A\n",
      "Epoch 4:   0%|          | 0/512 [00:00<?, ?it/s, v_num=0]                 \u001b[ATraining loss: 0.9106228351593018\n",
      "Epoch 4:   0%|          | 1/512 [00:00<00:34, 14.61it/s, v_num=0]Training loss: 0.600551426410675\n",
      "Epoch 4:   0%|          | 2/512 [00:00<01:55,  4.42it/s, v_num=0]Training loss: 0.7821058630943298\n",
      "Epoch 4:   1%|          | 3/512 [00:00<02:22,  3.56it/s, v_num=0]Training loss: 0.7948924899101257\n",
      "Epoch 4:   1%|          | 4/512 [00:01<02:36,  3.25it/s, v_num=0]Training loss: 1.5687904357910156\n",
      "Epoch 4:   1%|          | 5/512 [00:01<02:44,  3.08it/s, v_num=0]Training loss: 1.3308039903640747\n",
      "Epoch 4:   1%|          | 6/512 [00:02<02:49,  2.98it/s, v_num=0]Training loss: 1.0095161199569702\n",
      "Epoch 4:   1%|▏         | 7/512 [00:02<02:53,  2.91it/s, v_num=0]Training loss: 0.9217833876609802\n",
      "Epoch 4:   2%|▏         | 8/512 [00:02<02:55,  2.87it/s, v_num=0]Training loss: 0.7901620864868164\n",
      "Epoch 4:   2%|▏         | 9/512 [00:03<02:57,  2.83it/s, v_num=0]Training loss: 0.8722866177558899\n",
      "Epoch 4:   2%|▏         | 10/512 [00:03<02:59,  2.80it/s, v_num=0]Training loss: 1.0789839029312134\n",
      "Epoch 4:   2%|▏         | 11/512 [00:03<03:00,  2.78it/s, v_num=0]Training loss: 0.8267145156860352\n",
      "Epoch 4:   2%|▏         | 12/512 [00:04<03:01,  2.76it/s, v_num=0]Training loss: 0.6697376370429993\n",
      "Epoch 4:   3%|▎         | 13/512 [00:04<03:02,  2.74it/s, v_num=0]Training loss: 0.9176251888275146\n",
      "Epoch 4:   3%|▎         | 14/512 [00:05<03:02,  2.73it/s, v_num=0]Training loss: 0.6761746406555176\n",
      "Epoch 4:   3%|▎         | 15/512 [00:05<03:02,  2.72it/s, v_num=0]Training loss: 0.8553622364997864\n",
      "Epoch 4:   3%|▎         | 16/512 [00:05<03:03,  2.71it/s, v_num=0]Training loss: 0.9116580486297607\n",
      "Epoch 4:   3%|▎         | 17/512 [00:06<03:03,  2.70it/s, v_num=0]Training loss: 1.1880344152450562\n",
      "Epoch 4:   4%|▎         | 18/512 [00:06<03:03,  2.69it/s, v_num=0]Training loss: 1.070826530456543\n",
      "Epoch 4:   4%|▎         | 19/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 0.5745694637298584\n",
      "Epoch 4:   4%|▍         | 20/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 1.6825393438339233\n",
      "Epoch 4:   4%|▍         | 21/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 0.9275306463241577\n",
      "Epoch 4:   4%|▍         | 22/512 [00:08<03:03,  2.67it/s, v_num=0]Training loss: 1.0216838121414185\n",
      "Epoch 4:   4%|▍         | 23/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 0.9140810966491699\n",
      "Epoch 4:   5%|▍         | 24/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 0.9241471290588379\n",
      "Epoch 4:   5%|▍         | 25/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 0.8779621720314026\n",
      "Epoch 4:   5%|▌         | 26/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 0.6421194076538086\n",
      "Epoch 4:   5%|▌         | 27/512 [00:10<03:03,  2.65it/s, v_num=0]Training loss: 0.573468804359436\n",
      "Epoch 4:   5%|▌         | 28/512 [00:10<03:03,  2.64it/s, v_num=0]Training loss: 0.7213443517684937\n",
      "Epoch 4:   6%|▌         | 29/512 [00:10<03:02,  2.64it/s, v_num=0]Training loss: 0.6046873331069946\n",
      "Epoch 4:   6%|▌         | 30/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 0.9119836091995239\n",
      "Epoch 4:   6%|▌         | 31/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 0.747925877571106\n",
      "Epoch 4:   6%|▋         | 32/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.1088542938232422\n",
      "Epoch 4:   6%|▋         | 33/512 [00:12<03:02,  2.63it/s, v_num=0]Training loss: 1.4512156248092651\n",
      "Epoch 4:   7%|▋         | 34/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 0.8486713767051697\n",
      "Epoch 4:   7%|▋         | 35/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.1943342685699463\n",
      "Epoch 4:   7%|▋         | 36/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.0955219268798828\n",
      "Epoch 4:   7%|▋         | 37/512 [00:14<03:01,  2.62it/s, v_num=0]Training loss: 0.9231460690498352\n",
      "Epoch 4:   7%|▋         | 38/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.0947660207748413\n",
      "Epoch 4:   8%|▊         | 39/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 1.1553387641906738\n",
      "Epoch 4:   8%|▊         | 40/512 [00:15<03:00,  2.62it/s, v_num=0]Training loss: 1.5129518508911133\n",
      "Epoch 4:   8%|▊         | 41/512 [00:15<02:59,  2.62it/s, v_num=0]Training loss: 0.8946731090545654\n",
      "Epoch 4:   8%|▊         | 42/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.1729602813720703\n",
      "Epoch 4:   8%|▊         | 43/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 0.9499749541282654\n",
      "Epoch 4:   9%|▊         | 44/512 [00:16<02:59,  2.61it/s, v_num=0]Training loss: 0.8040157556533813\n",
      "Epoch 4:   9%|▉         | 45/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 0.4914247691631317\n",
      "Epoch 4:   9%|▉         | 46/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 0.8667103052139282\n",
      "Epoch 4:   9%|▉         | 47/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 1.2524584531784058\n",
      "Epoch 4:   9%|▉         | 48/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.3752784729003906\n",
      "Epoch 4:  10%|▉         | 49/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 0.6429881453514099\n",
      "Epoch 4:  10%|▉         | 50/512 [00:19<02:57,  2.61it/s, v_num=0]Training loss: 1.1817758083343506\n",
      "Epoch 4:  10%|▉         | 51/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 0.7184381484985352\n",
      "Epoch 4:  10%|█         | 52/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.816406011581421\n",
      "Epoch 4:  10%|█         | 53/512 [00:20<02:56,  2.61it/s, v_num=0]Training loss: 0.9511499404907227\n",
      "Epoch 4:  11%|█         | 54/512 [00:20<02:55,  2.61it/s, v_num=0]Training loss: 0.8764113187789917\n",
      "Epoch 4:  11%|█         | 55/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 1.3254365921020508\n",
      "Epoch 4:  11%|█         | 56/512 [00:21<02:55,  2.60it/s, v_num=0]Training loss: 0.8639190793037415\n",
      "Epoch 4:  11%|█         | 57/512 [00:21<02:54,  2.60it/s, v_num=0]Training loss: 1.0367506742477417\n",
      "Epoch 4:  11%|█▏        | 58/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 0.7234543561935425\n",
      "Epoch 4:  12%|█▏        | 59/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 0.9004566669464111\n",
      "Epoch 4:  12%|█▏        | 60/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 0.9796450138092041\n",
      "Epoch 4:  12%|█▏        | 61/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.2218310832977295\n",
      "Epoch 4:  12%|█▏        | 62/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 2.802457332611084\n",
      "Epoch 4:  12%|█▏        | 63/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.4906435012817383\n",
      "Epoch 4:  12%|█▎        | 64/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 0.9021676778793335\n",
      "Epoch 4:  13%|█▎        | 65/512 [00:25<02:52,  2.60it/s, v_num=0]Training loss: 1.038838267326355\n",
      "Epoch 4:  13%|█▎        | 66/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.1799638271331787\n",
      "Epoch 4:  13%|█▎        | 67/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.1871016025543213\n",
      "Epoch 4:  13%|█▎        | 68/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.9067034721374512\n",
      "Epoch 4:  13%|█▎        | 69/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.106551170349121\n",
      "Epoch 4:  14%|█▎        | 70/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.127151608467102\n",
      "Epoch 4:  14%|█▍        | 71/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.4989042282104492\n",
      "Epoch 4:  14%|█▍        | 72/512 [00:27<02:49,  2.59it/s, v_num=0]Training loss: 0.8948916792869568\n",
      "Epoch 4:  14%|█▍        | 73/512 [00:28<02:49,  2.59it/s, v_num=0]Training loss: 0.9193652272224426\n",
      "Epoch 4:  14%|█▍        | 74/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 0.9662349820137024\n",
      "Epoch 4:  15%|█▍        | 75/512 [00:28<02:48,  2.59it/s, v_num=0]Training loss: 1.0637089014053345\n",
      "Epoch 4:  15%|█▍        | 76/512 [00:29<02:48,  2.59it/s, v_num=0]Training loss: 1.137377142906189\n",
      "Epoch 4:  15%|█▌        | 77/512 [00:29<02:47,  2.59it/s, v_num=0]Training loss: 0.9434385299682617\n",
      "Epoch 4:  15%|█▌        | 78/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 0.9264753460884094\n",
      "Epoch 4:  15%|█▌        | 79/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 1.1140514612197876\n",
      "Epoch 4:  16%|█▌        | 80/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 1.0970139503479004\n",
      "Epoch 4:  16%|█▌        | 81/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 0.9291678071022034\n",
      "Epoch 4:  16%|█▌        | 82/512 [00:31<02:45,  2.59it/s, v_num=0]Training loss: 1.0817326307296753\n",
      "Epoch 4:  16%|█▌        | 83/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 0.9685421586036682\n",
      "Epoch 4:  16%|█▋        | 84/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.0940089225769043\n",
      "Epoch 4:  17%|█▋        | 85/512 [00:32<02:44,  2.59it/s, v_num=0]Training loss: 1.0188595056533813\n",
      "Epoch 4:  17%|█▋        | 86/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.4790513515472412\n",
      "Epoch 4:  17%|█▋        | 87/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 0.8802642822265625\n",
      "Epoch 4:  17%|█▋        | 88/512 [00:33<02:43,  2.59it/s, v_num=0]Training loss: 0.9573974609375\n",
      "Epoch 4:  17%|█▋        | 89/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.167787790298462\n",
      "Epoch 4:  18%|█▊        | 90/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 1.8763508796691895\n",
      "Epoch 4:  18%|█▊        | 91/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 0.7946140766143799\n",
      "Epoch 4:  18%|█▊        | 92/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.144482970237732\n",
      "Epoch 4:  18%|█▊        | 93/512 [00:35<02:41,  2.59it/s, v_num=0]Training loss: 0.943402111530304\n",
      "Epoch 4:  18%|█▊        | 94/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 0.8124553561210632\n",
      "Epoch 4:  19%|█▊        | 95/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.0212249755859375\n",
      "Epoch 4:  19%|█▉        | 96/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.4507299661636353\n",
      "Epoch 4:  19%|█▉        | 97/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 0.8521372675895691\n",
      "Epoch 4:  19%|█▉        | 98/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.1424931287765503\n",
      "Epoch 4:  19%|█▉        | 99/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 0.8487237691879272\n",
      "Epoch 4:  20%|█▉        | 100/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 0.755807101726532\n",
      "Epoch 4:  20%|█▉        | 101/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.9506736993789673\n",
      "Epoch 4:  20%|█▉        | 102/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.9566494226455688\n",
      "Epoch 4:  20%|██        | 103/512 [00:39<02:38,  2.58it/s, v_num=0]Training loss: 1.216802716255188\n",
      "Epoch 4:  20%|██        | 104/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.0400975942611694\n",
      "Epoch 4:  21%|██        | 105/512 [00:40<02:37,  2.58it/s, v_num=0]Training loss: 1.4911071062088013\n",
      "Epoch 4:  21%|██        | 106/512 [00:41<02:37,  2.58it/s, v_num=0]Training loss: 1.4176383018493652\n",
      "Epoch 4:  21%|██        | 107/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 0.7954318523406982\n",
      "Epoch 4:  21%|██        | 108/512 [00:41<02:36,  2.58it/s, v_num=0]Training loss: 1.137718677520752\n",
      "Epoch 4:  21%|██▏       | 109/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.0337238311767578\n",
      "Epoch 4:  21%|██▏       | 110/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 1.3802168369293213\n",
      "Epoch 4:  22%|██▏       | 111/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 0.8861027956008911\n",
      "Epoch 4:  22%|██▏       | 112/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 0.8291383981704712\n",
      "Epoch 4:  22%|██▏       | 113/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.0177377462387085\n",
      "Epoch 4:  22%|██▏       | 114/512 [00:44<02:34,  2.58it/s, v_num=0]Training loss: 1.3423230648040771\n",
      "Epoch 4:  22%|██▏       | 115/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.9165727496147156\n",
      "Epoch 4:  23%|██▎       | 116/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.9547507762908936\n",
      "Epoch 4:  23%|██▎       | 117/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 0.9485976099967957\n",
      "Epoch 4:  23%|██▎       | 118/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.1821537017822266\n",
      "Epoch 4:  23%|██▎       | 119/512 [00:46<02:32,  2.58it/s, v_num=0]Training loss: 1.4880164861679077\n",
      "Epoch 4:  23%|██▎       | 120/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.9068718552589417\n",
      "Epoch 4:  24%|██▎       | 121/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 1.2446285486221313\n",
      "Epoch 4:  24%|██▍       | 122/512 [00:47<02:31,  2.58it/s, v_num=0]Training loss: 0.9322063326835632\n",
      "Epoch 4:  24%|██▍       | 123/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 0.7241804599761963\n",
      "Epoch 4:  24%|██▍       | 124/512 [00:48<02:30,  2.58it/s, v_num=0]Training loss: 1.3599624633789062\n",
      "Epoch 4:  24%|██▍       | 125/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.328330159187317\n",
      "Epoch 4:  25%|██▍       | 126/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.2066795825958252\n",
      "Epoch 4:  25%|██▍       | 127/512 [00:49<02:29,  2.58it/s, v_num=0]Training loss: 1.2536507844924927\n",
      "Epoch 4:  25%|██▌       | 128/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 0.7327800393104553\n",
      "Epoch 4:  25%|██▌       | 129/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 0.619552731513977\n",
      "Epoch 4:  25%|██▌       | 130/512 [00:50<02:28,  2.58it/s, v_num=0]Training loss: 2.05511212348938\n",
      "Epoch 4:  26%|██▌       | 131/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 0.8396193981170654\n",
      "Epoch 4:  26%|██▌       | 132/512 [00:51<02:27,  2.58it/s, v_num=0]Training loss: 0.8718945384025574\n",
      "Epoch 4:  26%|██▌       | 133/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.2292219400405884\n",
      "Epoch 4:  26%|██▌       | 134/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 2.0968897342681885\n",
      "Epoch 4:  26%|██▋       | 135/512 [00:52<02:26,  2.58it/s, v_num=0]Training loss: 0.8754029870033264\n",
      "Epoch 4:  27%|██▋       | 136/512 [00:52<02:25,  2.58it/s, v_num=0]Training loss: 0.9711818695068359\n",
      "Epoch 4:  27%|██▋       | 137/512 [00:53<02:25,  2.58it/s, v_num=0]Training loss: 1.1320654153823853\n",
      "Epoch 4:  27%|██▋       | 138/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 0.6892238855361938\n",
      "Epoch 4:  27%|██▋       | 139/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 1.1555510759353638\n",
      "Epoch 4:  27%|██▋       | 140/512 [00:54<02:24,  2.58it/s, v_num=0]Training loss: 1.1577849388122559\n",
      "Epoch 4:  28%|██▊       | 141/512 [00:54<02:23,  2.58it/s, v_num=0]Training loss: 1.004866361618042\n",
      "Epoch 4:  28%|██▊       | 142/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 0.7384165525436401\n",
      "Epoch 4:  28%|██▊       | 143/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.281381368637085\n",
      "Epoch 4:  28%|██▊       | 144/512 [00:55<02:22,  2.58it/s, v_num=0]Training loss: 0.6450397968292236\n",
      "Epoch 4:  28%|██▊       | 145/512 [00:56<02:22,  2.58it/s, v_num=0]Training loss: 0.7455472946166992\n",
      "Epoch 4:  29%|██▊       | 146/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 0.948032557964325\n",
      "Epoch 4:  29%|██▊       | 147/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 1.7645479440689087\n",
      "Epoch 4:  29%|██▉       | 148/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 0.8338851928710938\n",
      "Epoch 4:  29%|██▉       | 149/512 [00:57<02:20,  2.58it/s, v_num=0]Training loss: 1.1564555168151855\n",
      "Epoch 4:  29%|██▉       | 150/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 0.9220666885375977\n",
      "Epoch 4:  29%|██▉       | 151/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 1.2159574031829834\n",
      "Epoch 4:  30%|██▉       | 152/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 0.6533322334289551\n",
      "Epoch 4:  30%|██▉       | 153/512 [00:59<02:19,  2.58it/s, v_num=0]Training loss: 0.8082600235939026\n",
      "Epoch 4:  30%|███       | 154/512 [00:59<02:18,  2.58it/s, v_num=0]Training loss: 1.091963291168213\n",
      "Epoch 4:  30%|███       | 155/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.1024489402770996\n",
      "Epoch 4:  30%|███       | 156/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 1.0640302896499634\n",
      "Epoch 4:  31%|███       | 157/512 [01:00<02:17,  2.58it/s, v_num=0]Training loss: 1.122984766960144\n",
      "Epoch 4:  31%|███       | 158/512 [01:01<02:17,  2.58it/s, v_num=0]Training loss: 0.8243220448493958\n",
      "Epoch 4:  31%|███       | 159/512 [01:01<02:16,  2.58it/s, v_num=0]Training loss: 0.9416276216506958\n",
      "Epoch 4:  31%|███▏      | 160/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 1.4490504264831543\n",
      "Epoch 4:  31%|███▏      | 161/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 0.9863860011100769\n",
      "Epoch 4:  32%|███▏      | 162/512 [01:02<02:15,  2.58it/s, v_num=0]Training loss: 1.330316424369812\n",
      "Epoch 4:  32%|███▏      | 163/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 0.9564939737319946\n",
      "Epoch 4:  32%|███▏      | 164/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 0.7279343605041504\n",
      "Epoch 4:  32%|███▏      | 165/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 0.8391870856285095\n",
      "Epoch 4:  32%|███▏      | 166/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 0.8206169009208679\n",
      "Epoch 4:  33%|███▎      | 167/512 [01:04<02:13,  2.58it/s, v_num=0]Training loss: 0.9737151265144348\n",
      "Epoch 4:  33%|███▎      | 168/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 0.8671393990516663\n",
      "Epoch 4:  33%|███▎      | 169/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.1380990743637085\n",
      "Epoch 4:  33%|███▎      | 170/512 [01:05<02:12,  2.58it/s, v_num=0]Training loss: 0.9716314077377319\n",
      "Epoch 4:  33%|███▎      | 171/512 [01:06<02:12,  2.58it/s, v_num=0]Training loss: 1.0175073146820068\n",
      "Epoch 4:  34%|███▎      | 172/512 [01:06<02:11,  2.58it/s, v_num=0]Training loss: 1.0896542072296143\n",
      "Epoch 4:  34%|███▍      | 173/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.1016825437545776\n",
      "Epoch 4:  34%|███▍      | 174/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 0.643095076084137\n",
      "Epoch 4:  34%|███▍      | 175/512 [01:07<02:10,  2.58it/s, v_num=0]Training loss: 0.7635441422462463\n",
      "Epoch 4:  34%|███▍      | 176/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.2115241289138794\n",
      "Epoch 4:  35%|███▍      | 177/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.5767501592636108\n",
      "Epoch 4:  35%|███▍      | 178/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 1.0312851667404175\n",
      "Epoch 4:  35%|███▍      | 179/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 1.593827724456787\n",
      "Epoch 4:  35%|███▌      | 180/512 [01:09<02:08,  2.58it/s, v_num=0]Training loss: 1.3182916641235352\n",
      "Epoch 4:  35%|███▌      | 181/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 0.8586336970329285\n",
      "Epoch 4:  36%|███▌      | 182/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 1.1531033515930176\n",
      "Epoch 4:  36%|███▌      | 183/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.137698769569397\n",
      "Epoch 4:  36%|███▌      | 184/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 0.9172785878181458\n",
      "Epoch 4:  36%|███▌      | 185/512 [01:11<02:06,  2.58it/s, v_num=0]Training loss: 0.8857100009918213\n",
      "Epoch 4:  36%|███▋      | 186/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 0.9741929173469543\n",
      "Epoch 4:  37%|███▋      | 187/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 0.6297066807746887\n",
      "Epoch 4:  37%|███▋      | 188/512 [01:12<02:05,  2.58it/s, v_num=0]Training loss: 0.6792058944702148\n",
      "Epoch 4:  37%|███▋      | 189/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 0.827656626701355\n",
      "Epoch 4:  37%|███▋      | 190/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 0.7004568576812744\n",
      "Epoch 4:  37%|███▋      | 191/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 0.7042543888092041\n",
      "Epoch 4:  38%|███▊      | 192/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 0.8277634382247925\n",
      "Epoch 4:  38%|███▊      | 193/512 [01:14<02:03,  2.58it/s, v_num=0]Training loss: 1.1416618824005127\n",
      "Epoch 4:  38%|███▊      | 194/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 1.6310399770736694\n",
      "Epoch 4:  38%|███▊      | 195/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 0.9977092742919922\n",
      "Epoch 4:  38%|███▊      | 196/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.1567997932434082\n",
      "Epoch 4:  38%|███▊      | 197/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 1.1157187223434448\n",
      "Epoch 4:  39%|███▊      | 198/512 [01:16<02:01,  2.58it/s, v_num=0]Training loss: 1.313636064529419\n",
      "Epoch 4:  39%|███▉      | 199/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 0.9319061636924744\n",
      "Epoch 4:  39%|███▉      | 200/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 1.2385603189468384\n",
      "Epoch 4:  39%|███▉      | 201/512 [01:18<02:00,  2.57it/s, v_num=0]Training loss: 1.1561200618743896\n",
      "Epoch 4:  39%|███▉      | 202/512 [01:18<02:00,  2.57it/s, v_num=0]Training loss: 1.0514695644378662\n",
      "Epoch 4:  40%|███▉      | 203/512 [01:18<02:00,  2.57it/s, v_num=0]Training loss: 0.48982295393943787\n",
      "Epoch 4:  40%|███▉      | 204/512 [01:19<01:59,  2.57it/s, v_num=0]Training loss: 1.1840909719467163\n",
      "Epoch 4:  40%|████      | 205/512 [01:19<01:59,  2.57it/s, v_num=0]Training loss: 0.9358936548233032\n",
      "Epoch 4:  40%|████      | 206/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 0.8937426209449768\n",
      "Epoch 4:  40%|████      | 207/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 1.3935774564743042\n",
      "Epoch 4:  41%|████      | 208/512 [01:20<01:58,  2.57it/s, v_num=0]Training loss: 1.2974128723144531\n",
      "Epoch 4:  41%|████      | 209/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 0.7910255789756775\n",
      "Epoch 4:  41%|████      | 210/512 [01:21<01:57,  2.57it/s, v_num=0]Training loss: 1.3896783590316772\n",
      "Epoch 4:  41%|████      | 211/512 [01:21<01:56,  2.57it/s, v_num=0]Training loss: 1.1383163928985596\n",
      "Epoch 4:  41%|████▏     | 212/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 0.8108401298522949\n",
      "Epoch 4:  42%|████▏     | 213/512 [01:22<01:56,  2.57it/s, v_num=0]Training loss: 2.896169424057007\n",
      "Epoch 4:  42%|████▏     | 214/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.1127164363861084\n",
      "Epoch 4:  42%|████▏     | 215/512 [01:23<01:55,  2.57it/s, v_num=0]Training loss: 1.3046808242797852\n",
      "Epoch 4:  42%|████▏     | 216/512 [01:23<01:54,  2.57it/s, v_num=0]Training loss: 1.1359463930130005\n",
      "Epoch 4:  42%|████▏     | 217/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.152248501777649\n",
      "Epoch 4:  43%|████▎     | 218/512 [01:24<01:54,  2.57it/s, v_num=0]Training loss: 1.005429744720459\n",
      "Epoch 4:  43%|████▎     | 219/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.4167706966400146\n",
      "Epoch 4:  43%|████▎     | 220/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 1.0339454412460327\n",
      "Epoch 4:  43%|████▎     | 221/512 [01:25<01:53,  2.57it/s, v_num=0]Training loss: 0.7132362127304077\n",
      "Epoch 4:  43%|████▎     | 222/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 1.3115482330322266\n",
      "Epoch 4:  44%|████▎     | 223/512 [01:26<01:52,  2.57it/s, v_num=0]Training loss: 0.8354814052581787\n",
      "Epoch 4:  44%|████▍     | 224/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 1.308613896369934\n",
      "Epoch 4:  44%|████▍     | 225/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.7345417141914368\n",
      "Epoch 4:  44%|████▍     | 226/512 [01:27<01:51,  2.57it/s, v_num=0]Training loss: 0.7652150392532349\n",
      "Epoch 4:  44%|████▍     | 227/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 0.8524860739707947\n",
      "Epoch 4:  45%|████▍     | 228/512 [01:28<01:50,  2.57it/s, v_num=0]Training loss: 1.1992512941360474\n",
      "Epoch 4:  45%|████▍     | 229/512 [01:28<01:49,  2.57it/s, v_num=0]Training loss: 1.0764498710632324\n",
      "Epoch 4:  45%|████▍     | 230/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 0.8732308745384216\n",
      "Epoch 4:  45%|████▌     | 231/512 [01:29<01:49,  2.57it/s, v_num=0]Training loss: 0.8265461325645447\n",
      "Epoch 4:  45%|████▌     | 232/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 0.7617451548576355\n",
      "Epoch 4:  46%|████▌     | 233/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 1.0084564685821533\n",
      "Epoch 4:  46%|████▌     | 234/512 [01:30<01:48,  2.57it/s, v_num=0]Training loss: 0.8683010339736938\n",
      "Epoch 4:  46%|████▌     | 235/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 0.655380129814148\n",
      "Epoch 4:  46%|████▌     | 236/512 [01:31<01:47,  2.57it/s, v_num=0]Training loss: 1.2993704080581665\n",
      "Epoch 4:  46%|████▋     | 237/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.0535941123962402\n",
      "Epoch 4:  46%|████▋     | 238/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.2383198738098145\n",
      "Epoch 4:  47%|████▋     | 239/512 [01:32<01:46,  2.57it/s, v_num=0]Training loss: 1.0560020208358765\n",
      "Epoch 4:  47%|████▋     | 240/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.1572047472000122\n",
      "Epoch 4:  47%|████▋     | 241/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.0047109127044678\n",
      "Epoch 4:  47%|████▋     | 242/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 0.7143588662147522\n",
      "Epoch 4:  47%|████▋     | 243/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.0939006805419922\n",
      "Epoch 4:  48%|████▊     | 244/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.9013293981552124\n",
      "Epoch 4:  48%|████▊     | 245/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 0.7915430068969727\n",
      "Epoch 4:  48%|████▊     | 246/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 1.0535374879837036\n",
      "Epoch 4:  48%|████▊     | 247/512 [01:35<01:42,  2.57it/s, v_num=0]Training loss: 0.900123655796051\n",
      "Epoch 4:  48%|████▊     | 248/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.1423999071121216\n",
      "Epoch 4:  49%|████▊     | 249/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 1.1921775341033936\n",
      "Epoch 4:  49%|████▉     | 250/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.4737176895141602\n",
      "Epoch 4:  49%|████▉     | 251/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.029097557067871\n",
      "Epoch 4:  49%|████▉     | 252/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.3637243509292603\n",
      "Epoch 4:  49%|████▉     | 253/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.092616319656372\n",
      "Epoch 4:  50%|████▉     | 254/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.4278815984725952\n",
      "Epoch 4:  50%|████▉     | 255/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.1651540994644165\n",
      "Epoch 4:  50%|█████     | 256/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 2.0242247581481934\n",
      "Epoch 4:  50%|█████     | 257/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.3393064737319946\n",
      "Epoch 4:  50%|█████     | 258/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 1.1577122211456299\n",
      "Epoch 4:  51%|█████     | 259/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 0.8815134763717651\n",
      "Epoch 4:  51%|█████     | 260/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.8142354488372803\n",
      "Epoch 4:  51%|█████     | 261/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.9135347008705139\n",
      "Epoch 4:  51%|█████     | 262/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 1.1068392992019653\n",
      "Epoch 4:  51%|█████▏    | 263/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 0.9998071789741516\n",
      "Epoch 4:  52%|█████▏    | 264/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 0.9688418507575989\n",
      "Epoch 4:  52%|█████▏    | 265/512 [01:43<01:36,  2.57it/s, v_num=0]Training loss: 1.3703711032867432\n",
      "Epoch 4:  52%|█████▏    | 266/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 1.2318227291107178\n",
      "Epoch 4:  52%|█████▏    | 267/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.7697299122810364\n",
      "Epoch 4:  52%|█████▏    | 268/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.242117166519165\n",
      "Epoch 4:  53%|█████▎    | 269/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.0664461851119995\n",
      "Epoch 4:  53%|█████▎    | 270/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.183837652206421\n",
      "Epoch 4:  53%|█████▎    | 271/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 0.8297317028045654\n",
      "Epoch 4:  53%|█████▎    | 272/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 0.7222363948822021\n",
      "Epoch 4:  53%|█████▎    | 273/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 0.8234177827835083\n",
      "Epoch 4:  54%|█████▎    | 274/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.6207587718963623\n",
      "Epoch 4:  54%|█████▎    | 275/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.3153759241104126\n",
      "Epoch 4:  54%|█████▍    | 276/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 0.8057392835617065\n",
      "Epoch 4:  54%|█████▍    | 277/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.2898452281951904\n",
      "Epoch 4:  54%|█████▍    | 278/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.1702508926391602\n",
      "Epoch 4:  54%|█████▍    | 279/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 1.0186134576797485\n",
      "Epoch 4:  55%|█████▍    | 280/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 0.8187628388404846\n",
      "Epoch 4:  55%|█████▍    | 281/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.349021077156067\n",
      "Epoch 4:  55%|█████▌    | 282/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 0.731332004070282\n",
      "Epoch 4:  55%|█████▌    | 283/512 [01:50<01:29,  2.57it/s, v_num=0]Training loss: 0.723227858543396\n",
      "Epoch 4:  55%|█████▌    | 284/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 0.9335468411445618\n",
      "Epoch 4:  56%|█████▌    | 285/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 0.8504456281661987\n",
      "Epoch 4:  56%|█████▌    | 286/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 13.969593048095703\n",
      "Epoch 4:  56%|█████▌    | 287/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.3204331398010254\n",
      "Epoch 4:  56%|█████▋    | 288/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 0.7414544224739075\n",
      "Epoch 4:  56%|█████▋    | 289/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 1.4744731187820435\n",
      "Epoch 4:  57%|█████▋    | 290/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 0.6907781362533569\n",
      "Epoch 4:  57%|█████▋    | 291/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.0603489875793457\n",
      "Epoch 4:  57%|█████▋    | 292/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.1392265558242798\n",
      "Epoch 4:  57%|█████▋    | 293/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.0977987051010132\n",
      "Epoch 4:  57%|█████▋    | 294/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 0.8735331892967224\n",
      "Epoch 4:  58%|█████▊    | 295/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 1.109946370124817\n",
      "Epoch 4:  58%|█████▊    | 296/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.9421988725662231\n",
      "Epoch 4:  58%|█████▊    | 297/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.2694411277770996\n",
      "Epoch 4:  58%|█████▊    | 298/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.8437659740447998\n",
      "Epoch 4:  58%|█████▊    | 299/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.3042908906936646\n",
      "Epoch 4:  59%|█████▊    | 300/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.087570309638977\n",
      "Epoch 4:  59%|█████▉    | 301/512 [01:57<01:22,  2.57it/s, v_num=0]Training loss: 1.263292908668518\n",
      "Epoch 4:  59%|█████▉    | 302/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.8967682123184204\n",
      "Epoch 4:  59%|█████▉    | 303/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.8871813416481018\n",
      "Epoch 4:  59%|█████▉    | 304/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.8898327946662903\n",
      "Epoch 4:  60%|█████▉    | 305/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.2823762893676758\n",
      "Epoch 4:  60%|█████▉    | 306/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.1449296474456787\n",
      "Epoch 4:  60%|█████▉    | 307/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.607924461364746\n",
      "Epoch 4:  60%|██████    | 308/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 0.7192725539207458\n",
      "Epoch 4:  60%|██████    | 309/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 1.3723524808883667\n",
      "Epoch 4:  61%|██████    | 310/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.788449764251709\n",
      "Epoch 4:  61%|██████    | 311/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.7769028544425964\n",
      "Epoch 4:  61%|██████    | 312/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.4303157329559326\n",
      "Epoch 4:  61%|██████    | 313/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.305907130241394\n",
      "Epoch 4:  61%|██████▏   | 314/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 0.5727038979530334\n",
      "Epoch 4:  62%|██████▏   | 315/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 0.8593745827674866\n",
      "Epoch 4:  62%|██████▏   | 316/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.3570268154144287\n",
      "Epoch 4:  62%|██████▏   | 317/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 0.7440693378448486\n",
      "Epoch 4:  62%|██████▏   | 318/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.041753888130188\n",
      "Epoch 4:  62%|██████▏   | 319/512 [02:04<01:15,  2.57it/s, v_num=0]Training loss: 0.8195701241493225\n",
      "Epoch 4:  62%|██████▎   | 320/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.2347805500030518\n",
      "Epoch 4:  63%|██████▎   | 321/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.6346886157989502\n",
      "Epoch 4:  63%|██████▎   | 322/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.0966455936431885\n",
      "Epoch 4:  63%|██████▎   | 323/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 0.8350781798362732\n",
      "Epoch 4:  63%|██████▎   | 324/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 0.854533314704895\n",
      "Epoch 4:  63%|██████▎   | 325/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 0.9398033618927002\n",
      "Epoch 4:  64%|██████▎   | 326/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.0808632373809814\n",
      "Epoch 4:  64%|██████▍   | 327/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.0906121730804443\n",
      "Epoch 4:  64%|██████▍   | 328/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.3896056413650513\n",
      "Epoch 4:  64%|██████▍   | 329/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 0.9440336227416992\n",
      "Epoch 4:  64%|██████▍   | 330/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 0.8752446174621582\n",
      "Epoch 4:  65%|██████▍   | 331/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.1754200458526611\n",
      "Epoch 4:  65%|██████▍   | 332/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.351965308189392\n",
      "Epoch 4:  65%|██████▌   | 333/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.360022783279419\n",
      "Epoch 4:  65%|██████▌   | 334/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.648834228515625\n",
      "Epoch 4:  65%|██████▌   | 335/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 1.041538119316101\n",
      "Epoch 4:  66%|██████▌   | 336/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 1.4345349073410034\n",
      "Epoch 4:  66%|██████▌   | 337/512 [02:11<01:08,  2.57it/s, v_num=0]Training loss: 1.3293942213058472\n",
      "Epoch 4:  66%|██████▌   | 338/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.311264157295227\n",
      "Epoch 4:  66%|██████▌   | 339/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.4830536842346191\n",
      "Epoch 4:  66%|██████▋   | 340/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.1957930326461792\n",
      "Epoch 4:  67%|██████▋   | 341/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 0.5650506019592285\n",
      "Epoch 4:  67%|██████▋   | 342/512 [02:13<01:06,  2.57it/s, v_num=0]Training loss: 1.1265289783477783\n",
      "Epoch 4:  67%|██████▋   | 343/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.0036287307739258\n",
      "Epoch 4:  67%|██████▋   | 344/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.3809053897857666\n",
      "Epoch 4:  67%|██████▋   | 345/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.9897890686988831\n",
      "Epoch 4:  68%|██████▊   | 346/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.8614315390586853\n",
      "Epoch 4:  68%|██████▊   | 347/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.8976024985313416\n",
      "Epoch 4:  68%|██████▊   | 348/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 1.3505218029022217\n",
      "Epoch 4:  68%|██████▊   | 349/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 0.6289549469947815\n",
      "Epoch 4:  68%|██████▊   | 350/512 [02:16<01:03,  2.57it/s, v_num=0]Training loss: 1.0800096988677979\n",
      "Epoch 4:  69%|██████▊   | 351/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.24985933303833\n",
      "Epoch 4:  69%|██████▉   | 352/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 0.7017233371734619\n",
      "Epoch 4:  69%|██████▉   | 353/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.5837266445159912\n",
      "Epoch 4:  69%|██████▉   | 354/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.8443005084991455\n",
      "Epoch 4:  69%|██████▉   | 355/512 [02:18<01:01,  2.57it/s, v_num=0]Training loss: 1.1714857816696167\n",
      "Epoch 4:  70%|██████▉   | 356/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 0.8173857927322388\n",
      "Epoch 4:  70%|██████▉   | 357/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.154375672340393\n",
      "Epoch 4:  70%|██████▉   | 358/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.0922530889511108\n",
      "Epoch 4:  70%|███████   | 359/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.5794910192489624\n",
      "Epoch 4:  70%|███████   | 360/512 [02:20<00:59,  2.57it/s, v_num=0]Training loss: 0.7615426778793335\n",
      "Epoch 4:  71%|███████   | 361/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 0.8381909132003784\n",
      "Epoch 4:  71%|███████   | 362/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 1.0466986894607544\n",
      "Epoch 4:  71%|███████   | 363/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 0.9837321639060974\n",
      "Epoch 4:  71%|███████   | 364/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 0.9507431983947754\n",
      "Epoch 4:  71%|███████▏  | 365/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 0.6006896495819092\n",
      "Epoch 4:  71%|███████▏  | 366/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 0.9883258938789368\n",
      "Epoch 4:  72%|███████▏  | 367/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 0.8973382115364075\n",
      "Epoch 4:  72%|███████▏  | 368/512 [02:23<00:56,  2.57it/s, v_num=0]Training loss: 0.7910728454589844\n",
      "Epoch 4:  72%|███████▏  | 369/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 0.8793052434921265\n",
      "Epoch 4:  72%|███████▏  | 370/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 0.8373769521713257\n",
      "Epoch 4:  72%|███████▏  | 371/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.1329646110534668\n",
      "Epoch 4:  73%|███████▎  | 372/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 1.0083086490631104\n",
      "Epoch 4:  73%|███████▎  | 373/512 [02:25<00:54,  2.57it/s, v_num=0]Training loss: 0.7617604732513428\n",
      "Epoch 4:  73%|███████▎  | 374/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.168982744216919\n",
      "Epoch 4:  73%|███████▎  | 375/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.064170479774475\n",
      "Epoch 4:  73%|███████▎  | 376/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 1.1148113012313843\n",
      "Epoch 4:  74%|███████▎  | 377/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.72223961353302\n",
      "Epoch 4:  74%|███████▍  | 378/512 [02:27<00:52,  2.57it/s, v_num=0]Training loss: 0.8838358521461487\n",
      "Epoch 4:  74%|███████▍  | 379/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 0.7438573241233826\n",
      "Epoch 4:  74%|███████▍  | 380/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 0.884337842464447\n",
      "Epoch 4:  74%|███████▍  | 381/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.3043450117111206\n",
      "Epoch 4:  75%|███████▍  | 382/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.8517023324966431\n",
      "Epoch 4:  75%|███████▍  | 383/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.7800888419151306\n",
      "Epoch 4:  75%|███████▌  | 384/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 0.9500167369842529\n",
      "Epoch 4:  75%|███████▌  | 385/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.115464687347412\n",
      "Epoch 4:  75%|███████▌  | 386/512 [02:30<00:49,  2.57it/s, v_num=0]Training loss: 0.8555606007575989\n",
      "Epoch 4:  76%|███████▌  | 387/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.197322964668274\n",
      "Epoch 4:  76%|███████▌  | 388/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.1413074731826782\n",
      "Epoch 4:  76%|███████▌  | 389/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 0.976264476776123\n",
      "Epoch 4:  76%|███████▌  | 390/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.2912379503250122\n",
      "Epoch 4:  76%|███████▋  | 391/512 [02:32<00:47,  2.57it/s, v_num=0]Training loss: 1.0746893882751465\n",
      "Epoch 4:  77%|███████▋  | 392/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 0.9389908909797668\n",
      "Epoch 4:  77%|███████▋  | 393/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.7660764455795288\n",
      "Epoch 4:  77%|███████▋  | 394/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 0.8659886717796326\n",
      "Epoch 4:  77%|███████▋  | 395/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 0.7545178532600403\n",
      "Epoch 4:  77%|███████▋  | 396/512 [02:34<00:45,  2.57it/s, v_num=0]Training loss: 0.719207763671875\n",
      "Epoch 4:  78%|███████▊  | 397/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 0.6858365535736084\n",
      "Epoch 4:  78%|███████▊  | 398/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 0.549802303314209\n",
      "Epoch 4:  78%|███████▊  | 399/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.5269413590431213\n",
      "Epoch 4:  78%|███████▊  | 400/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.7175326943397522\n",
      "Epoch 4:  78%|███████▊  | 401/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.6571186184883118\n",
      "Epoch 4:  79%|███████▊  | 402/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.3285808563232422\n",
      "Epoch 4:  79%|███████▊  | 403/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.2219903469085693\n",
      "Epoch 4:  79%|███████▉  | 404/512 [02:37<00:42,  2.57it/s, v_num=0]Training loss: 1.1269190311431885\n",
      "Epoch 4:  79%|███████▉  | 405/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.1071667671203613\n",
      "Epoch 4:  79%|███████▉  | 406/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.0896937847137451\n",
      "Epoch 4:  79%|███████▉  | 407/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.8587744235992432\n",
      "Epoch 4:  80%|███████▉  | 408/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.981096625328064\n",
      "Epoch 4:  80%|███████▉  | 409/512 [02:39<00:40,  2.57it/s, v_num=0]Training loss: 0.5839593410491943\n",
      "Epoch 4:  80%|████████  | 410/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 1.103492021560669\n",
      "Epoch 4:  80%|████████  | 411/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.9456948637962341\n",
      "Epoch 4:  80%|████████  | 412/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.9288502335548401\n",
      "Epoch 4:  81%|████████  | 413/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 1.3671265840530396\n",
      "Epoch 4:  81%|████████  | 414/512 [02:41<00:38,  2.57it/s, v_num=0]Training loss: 0.9003244042396545\n",
      "Epoch 4:  81%|████████  | 415/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.3206806182861328\n",
      "Epoch 4:  81%|████████▏ | 416/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 0.8035329580307007\n",
      "Epoch 4:  81%|████████▏ | 417/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 0.9531386494636536\n",
      "Epoch 4:  82%|████████▏ | 418/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.271058201789856\n",
      "Epoch 4:  82%|████████▏ | 419/512 [02:43<00:36,  2.57it/s, v_num=0]Training loss: 1.2198002338409424\n",
      "Epoch 4:  82%|████████▏ | 420/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 2.583803176879883\n",
      "Epoch 4:  82%|████████▏ | 421/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 0.8696492314338684\n",
      "Epoch 4:  82%|████████▏ | 422/512 [02:44<00:35,  2.57it/s, v_num=0]Training loss: 0.847062349319458\n",
      "Epoch 4:  83%|████████▎ | 423/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 1.6752886772155762\n",
      "Epoch 4:  83%|████████▎ | 424/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 0.6357697248458862\n",
      "Epoch 4:  83%|████████▎ | 425/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 0.8159141540527344\n",
      "Epoch 4:  83%|████████▎ | 426/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 0.8000177145004272\n",
      "Epoch 4:  83%|████████▎ | 427/512 [02:46<00:33,  2.57it/s, v_num=0]Training loss: 1.0040103197097778\n",
      "Epoch 4:  84%|████████▎ | 428/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.9683361053466797\n",
      "Epoch 4:  84%|████████▍ | 429/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 0.8618513345718384\n",
      "Epoch 4:  84%|████████▍ | 430/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 0.7653910517692566\n",
      "Epoch 4:  84%|████████▍ | 431/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 0.9297112226486206\n",
      "Epoch 4:  84%|████████▍ | 432/512 [02:48<00:31,  2.57it/s, v_num=0]Training loss: 0.6370753049850464\n",
      "Epoch 4:  85%|████████▍ | 433/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.8138885498046875\n",
      "Epoch 4:  85%|████████▍ | 434/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 1.6468371152877808\n",
      "Epoch 4:  85%|████████▍ | 435/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 0.7921236157417297\n",
      "Epoch 4:  85%|████████▌ | 436/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.0071203708648682\n",
      "Epoch 4:  85%|████████▌ | 437/512 [02:50<00:29,  2.57it/s, v_num=0]Training loss: 1.1197841167449951\n",
      "Epoch 4:  86%|████████▌ | 438/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.2135652303695679\n",
      "Epoch 4:  86%|████████▌ | 439/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.208146333694458\n",
      "Epoch 4:  86%|████████▌ | 440/512 [02:51<00:28,  2.57it/s, v_num=0]Training loss: 0.9062681198120117\n",
      "Epoch 4:  86%|████████▌ | 441/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 0.9866911768913269\n",
      "Epoch 4:  86%|████████▋ | 442/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 0.9115729331970215\n",
      "Epoch 4:  87%|████████▋ | 443/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 0.5998413562774658\n",
      "Epoch 4:  87%|████████▋ | 444/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 1.2789504528045654\n",
      "Epoch 4:  87%|████████▋ | 445/512 [02:53<00:26,  2.57it/s, v_num=0]Training loss: 0.5748896598815918\n",
      "Epoch 4:  87%|████████▋ | 446/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 1.1174105405807495\n",
      "Epoch 4:  87%|████████▋ | 447/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 0.8784505128860474\n",
      "Epoch 4:  88%|████████▊ | 448/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 0.7965431809425354\n",
      "Epoch 4:  88%|████████▊ | 449/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 1.0228946208953857\n",
      "Epoch 4:  88%|████████▊ | 450/512 [02:55<00:24,  2.57it/s, v_num=0]Training loss: 1.243194818496704\n",
      "Epoch 4:  88%|████████▊ | 451/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 0.8713330030441284\n",
      "Epoch 4:  88%|████████▊ | 452/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 0.8562221527099609\n",
      "Epoch 4:  88%|████████▊ | 453/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.8556626439094543\n",
      "Epoch 4:  89%|████████▊ | 454/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.9578549265861511\n",
      "Epoch 4:  89%|████████▉ | 455/512 [02:57<00:22,  2.57it/s, v_num=0]Training loss: 0.981478750705719\n",
      "Epoch 4:  89%|████████▉ | 456/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 0.8151035904884338\n",
      "Epoch 4:  89%|████████▉ | 457/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 0.6486825942993164\n",
      "Epoch 4:  89%|████████▉ | 458/512 [02:58<00:21,  2.57it/s, v_num=0]Training loss: 0.9764565825462341\n",
      "Epoch 4:  90%|████████▉ | 459/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 1.2112451791763306\n",
      "Epoch 4:  90%|████████▉ | 460/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 1.2505677938461304\n",
      "Epoch 4:  90%|█████████ | 461/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 0.7161834239959717\n",
      "Epoch 4:  90%|█████████ | 462/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.0868475437164307\n",
      "Epoch 4:  90%|█████████ | 463/512 [03:00<00:19,  2.57it/s, v_num=0]Training loss: 0.9192498326301575\n",
      "Epoch 4:  91%|█████████ | 464/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 0.9100883603096008\n",
      "Epoch 4:  91%|█████████ | 465/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 0.801448404788971\n",
      "Epoch 4:  91%|█████████ | 466/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 0.8047959208488464\n",
      "Epoch 4:  91%|█████████ | 467/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 0.8688408732414246\n",
      "Epoch 4:  91%|█████████▏| 468/512 [03:02<00:17,  2.57it/s, v_num=0]Training loss: 0.8107021450996399\n",
      "Epoch 4:  92%|█████████▏| 469/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 1.1202735900878906\n",
      "Epoch 4:  92%|█████████▏| 470/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 1.159219741821289\n",
      "Epoch 4:  92%|█████████▏| 471/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 0.8321404457092285\n",
      "Epoch 4:  92%|█████████▏| 472/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 0.9512970447540283\n",
      "Epoch 4:  92%|█████████▏| 473/512 [03:04<00:15,  2.57it/s, v_num=0]Training loss: 0.9731996655464172\n",
      "Epoch 4:  93%|█████████▎| 474/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.2142754793167114\n",
      "Epoch 4:  93%|█████████▎| 475/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 1.3238927125930786\n",
      "Epoch 4:  93%|█████████▎| 476/512 [03:05<00:14,  2.57it/s, v_num=0]Training loss: 0.7033113241195679\n",
      "Epoch 4:  93%|█████████▎| 477/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 0.9597980976104736\n",
      "Epoch 4:  93%|█████████▎| 478/512 [03:06<00:13,  2.57it/s, v_num=0]Training loss: 0.8367746472358704\n",
      "Epoch 4:  94%|█████████▎| 479/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 0.8381240367889404\n",
      "Epoch 4:  94%|█████████▍| 480/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 0.8633308410644531\n",
      "Epoch 4:  94%|█████████▍| 481/512 [03:07<00:12,  2.57it/s, v_num=0]Training loss: 0.9085121154785156\n",
      "Epoch 4:  94%|█████████▍| 482/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 0.9020329713821411\n",
      "Epoch 4:  94%|█████████▍| 483/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 1.1832163333892822\n",
      "Epoch 4:  95%|█████████▍| 484/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.100226640701294\n",
      "Epoch 4:  95%|█████████▍| 485/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.073432445526123\n",
      "Epoch 4:  95%|█████████▍| 486/512 [03:09<00:10,  2.57it/s, v_num=0]Training loss: 0.8641178011894226\n",
      "Epoch 4:  95%|█████████▌| 487/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 1.1447280645370483\n",
      "Epoch 4:  95%|█████████▌| 488/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 0.8489050269126892\n",
      "Epoch 4:  96%|█████████▌| 489/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 0.9028434753417969\n",
      "Epoch 4:  96%|█████████▌| 490/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.1253052949905396\n",
      "Epoch 4:  96%|█████████▌| 491/512 [03:11<00:08,  2.57it/s, v_num=0]Training loss: 1.3187817335128784\n",
      "Epoch 4:  96%|█████████▌| 492/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.0054277181625366\n",
      "Epoch 4:  96%|█████████▋| 493/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 0.9710763692855835\n",
      "Epoch 4:  96%|█████████▋| 494/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 0.9455453753471375\n",
      "Epoch 4:  97%|█████████▋| 495/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 1.2986372709274292\n",
      "Epoch 4:  97%|█████████▋| 496/512 [03:13<00:06,  2.57it/s, v_num=0]Training loss: 1.4790222644805908\n",
      "Epoch 4:  97%|█████████▋| 497/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 1.30891752243042\n",
      "Epoch 4:  97%|█████████▋| 498/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 0.7861000299453735\n",
      "Epoch 4:  97%|█████████▋| 499/512 [03:14<00:05,  2.57it/s, v_num=0]Training loss: 0.8422446846961975\n",
      "Epoch 4:  98%|█████████▊| 500/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 0.7774293422698975\n",
      "Epoch 4:  98%|█████████▊| 501/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 0.6303502917289734\n",
      "Epoch 4:  98%|█████████▊| 502/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 1.5040050745010376\n",
      "Epoch 4:  98%|█████████▊| 503/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 1.2867037057876587\n",
      "Epoch 4:  98%|█████████▊| 504/512 [03:16<00:03,  2.57it/s, v_num=0]Training loss: 1.000217318534851\n",
      "Epoch 4:  99%|█████████▊| 505/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 0.667027473449707\n",
      "Epoch 4:  99%|█████████▉| 506/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 0.9535617828369141\n",
      "Epoch 4:  99%|█████████▉| 507/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.1258050203323364\n",
      "Epoch 4:  99%|█████████▉| 508/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.647085428237915\n",
      "Epoch 4:  99%|█████████▉| 509/512 [03:18<00:01,  2.57it/s, v_num=0]Training loss: 0.9286148548126221\n",
      "Epoch 4: 100%|█████████▉| 510/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 1.3145874738693237\n",
      "Epoch 4: 100%|█████████▉| 511/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 1.460190773010254\n",
      "Epoch 4: 100%|██████████| 512/512 [03:19<00:00,  2.57it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.07794189453125\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 30.11it/s]\u001b[AValidation loss: 0.7744613289833069\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 25.31it/s]\u001b[AValidation loss: 1.2123504877090454\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 24.38it/s]\u001b[AValidation loss: 1.0523761510849\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 23.79it/s]\u001b[AValidation loss: 0.8213472366333008\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 23.44it/s]\u001b[AValidation loss: 1.0943888425827026\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 23.23it/s]\u001b[AValidation loss: 0.8647544980049133\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 23.08it/s]\u001b[AValidation loss: 1.1063671112060547\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 23.04it/s]\u001b[AValidation loss: 1.0832256078720093\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.95it/s]\u001b[AValidation loss: 0.9596759676933289\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.88it/s]\u001b[AValidation loss: 0.7740452885627747\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.83it/s]\u001b[AValidation loss: 0.6674032211303711\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.81it/s]\u001b[AValidation loss: 0.8455150723457336\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.77it/s]\u001b[AValidation loss: 0.9546500444412231\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 22.73it/s]\u001b[AValidation loss: 0.9864358305931091\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 22.73it/s]\u001b[AValidation loss: 1.1505860090255737\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 22.70it/s]\u001b[AValidation loss: 0.9389632940292358\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 22.67it/s]\u001b[AValidation loss: 0.7049219608306885\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 22.65it/s]\u001b[AValidation loss: 0.5861755013465881\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 22.65it/s]\u001b[AValidation loss: 0.6650319695472717\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:03, 22.63it/s]\u001b[AValidation loss: 1.3406686782836914\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:03, 22.62it/s]\u001b[AValidation loss: 0.6689559817314148\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:03, 22.62it/s]\u001b[AValidation loss: 0.9816679358482361\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:03, 22.61it/s]\u001b[AValidation loss: 0.9545286297798157\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:03, 22.59it/s]\u001b[AValidation loss: 0.8010256886482239\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 22.60it/s]\u001b[AValidation loss: 0.7026411294937134\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 22.58it/s]\u001b[AValidation loss: 1.3042980432510376\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 27/110 [00:01<00:03, 22.57it/s]\u001b[AValidation loss: 0.7886285781860352\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 28/110 [00:01<00:03, 22.57it/s]\u001b[AValidation loss: 1.0192513465881348\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 29/110 [00:01<00:03, 22.56it/s]\u001b[AValidation loss: 2.177755832672119\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 30/110 [00:01<00:03, 22.55it/s]\u001b[AValidation loss: 1.270440936088562\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 31/110 [00:01<00:03, 22.54it/s]\u001b[AValidation loss: 1.309520959854126\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 32/110 [00:01<00:03, 22.55it/s]\u001b[AValidation loss: 0.9154031276702881\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 33/110 [00:01<00:03, 22.54it/s]\u001b[AValidation loss: 1.0274896621704102\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 34/110 [00:01<00:03, 22.46it/s]\u001b[AValidation loss: 0.9795455932617188\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 35/110 [00:01<00:03, 22.45it/s]\u001b[AValidation loss: 0.6629281044006348\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 36/110 [00:01<00:03, 22.43it/s]\u001b[AValidation loss: 0.7845785021781921\n",
      "\n",
      "Validation DataLoader 0:  34%|███▎      | 37/110 [00:01<00:03, 22.43it/s]\u001b[AValidation loss: 1.1155608892440796\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 38/110 [00:01<00:03, 22.42it/s]\u001b[AValidation loss: 0.8443247079849243\n",
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 39/110 [00:01<00:03, 22.40it/s]\u001b[AValidation loss: 0.6824241280555725\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 40/110 [00:01<00:03, 22.40it/s]\u001b[AValidation loss: 0.9042019844055176\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 41/110 [00:01<00:03, 22.35it/s]\u001b[AValidation loss: 0.8393936157226562\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 42/110 [00:01<00:03, 22.34it/s]\u001b[AValidation loss: 3.205047607421875\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 43/110 [00:01<00:02, 22.35it/s]\u001b[AValidation loss: 1.0683839321136475\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 44/110 [00:01<00:02, 22.29it/s]\u001b[AValidation loss: 0.6727288365364075\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 45/110 [00:02<00:02, 22.28it/s]\u001b[AValidation loss: 0.8835582733154297\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 46/110 [00:02<00:02, 22.29it/s]\u001b[AValidation loss: 0.5823935270309448\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 47/110 [00:02<00:02, 22.24it/s]\u001b[AValidation loss: 1.4748318195343018\n",
      "\n",
      "Validation DataLoader 0:  44%|████▎     | 48/110 [00:02<00:02, 22.23it/s]\u001b[AValidation loss: 0.916278064250946\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 49/110 [00:02<00:02, 22.23it/s]\u001b[AValidation loss: 0.6993420124053955\n",
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 50/110 [00:02<00:02, 22.23it/s]\u001b[AValidation loss: 1.440324306488037\n",
      "\n",
      "Validation DataLoader 0:  46%|████▋     | 51/110 [00:02<00:02, 22.23it/s]\u001b[AValidation loss: 1.497252345085144\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 52/110 [00:02<00:02, 22.24it/s]\u001b[AValidation loss: 1.0989571809768677\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 53/110 [00:02<00:02, 22.24it/s]\u001b[AValidation loss: 1.2186301946640015\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 54/110 [00:02<00:02, 22.24it/s]\u001b[AValidation loss: 1.4279224872589111\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 55/110 [00:02<00:02, 22.24it/s]\u001b[AValidation loss: 0.8452273607254028\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 56/110 [00:02<00:02, 22.25it/s]\u001b[AValidation loss: 0.7417153120040894\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 57/110 [00:02<00:02, 22.21it/s]\u001b[AValidation loss: 1.0562176704406738\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 58/110 [00:02<00:02, 22.20it/s]\u001b[AValidation loss: 0.8167009353637695\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▎    | 59/110 [00:02<00:02, 22.20it/s]\u001b[AValidation loss: 1.107039451599121\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▍    | 60/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 1.1370868682861328\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 61/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 0.9470276832580566\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▋    | 62/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 1.4092918634414673\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 63/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 2.092482328414917\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 64/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 0.7018709182739258\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 65/110 [00:02<00:02, 22.16it/s]\u001b[AValidation loss: 1.1391891241073608\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 66/110 [00:02<00:01, 22.16it/s]\u001b[AValidation loss: 1.616412878036499\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 67/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 0.7242963314056396\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 68/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 1.0408293008804321\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 69/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 1.073186993598938\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 70/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 0.9351712465286255\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▍   | 71/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.8164242506027222\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 72/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.6559552550315857\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▋   | 73/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.9833273887634277\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 74/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 1.0396450757980347\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 75/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.8734833598136902\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 76/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.8093270063400269\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 77/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.5369011759757996\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 78/110 [00:03<00:01, 22.15it/s]\u001b[AValidation loss: 0.5772154331207275\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 79/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 0.9753442406654358\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 80/110 [00:03<00:01, 22.12it/s]\u001b[AValidation loss: 1.2261176109313965\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 81/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.9354477524757385\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 82/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.6753818392753601\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 83/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.7744026780128479\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 84/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.703190803527832\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 85/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.7855955362319946\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 86/110 [00:03<00:01, 22.13it/s]\u001b[AValidation loss: 0.8634529113769531\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 87/110 [00:03<00:01, 22.14it/s]\u001b[AValidation loss: 0.7448744773864746\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 88/110 [00:03<00:00, 22.14it/s]\u001b[AValidation loss: 1.383683443069458\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 89/110 [00:04<00:00, 22.13it/s]\u001b[AValidation loss: 1.677067756652832\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 90/110 [00:04<00:00, 22.13it/s]\u001b[AValidation loss: 1.3167110681533813\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 91/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 0.7261778116226196\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▎ | 92/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 1.137837529182434\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 93/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 1.5696258544921875\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 94/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 1.563736081123352\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 95/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 1.0354100465774536\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 96/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 1.1140466928482056\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 97/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 0.6837816834449768\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 98/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 0.9164614677429199\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 99/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 0.8117978572845459\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 100/110 [00:04<00:00, 22.09it/s]\u001b[AValidation loss: 0.7280706167221069\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 101/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 0.9703494310379028\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 102/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 0.7992956042289734\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▎| 103/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 1.1638654470443726\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 104/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 0.8009305596351624\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 105/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 1.0068295001983643\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▋| 106/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 0.8199623227119446\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 107/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 0.9318984746932983\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 108/110 [00:04<00:00, 22.10it/s]\u001b[AValidation loss: 0.7583925724029541\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 109/110 [00:04<00:00, 22.11it/s]\u001b[AValidation loss: 0.9695327281951904\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 110/110 [00:04<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 5:   0%|          | 0/512 [00:00<?, ?it/s, v_num=0]                 \u001b[ATraining loss: 0.9931336045265198\n",
      "Epoch 5:   0%|          | 1/512 [00:00<00:35, 14.50it/s, v_num=0]Training loss: 0.9656755924224854\n",
      "Epoch 5:   0%|          | 2/512 [00:00<01:55,  4.42it/s, v_num=0]Training loss: 1.3667367696762085\n",
      "Epoch 5:   1%|          | 3/512 [00:00<02:23,  3.56it/s, v_num=0]Training loss: 0.7807345390319824\n",
      "Epoch 5:   1%|          | 4/512 [00:01<02:36,  3.25it/s, v_num=0]Training loss: 0.7834771275520325\n",
      "Epoch 5:   1%|          | 5/512 [00:01<02:44,  3.08it/s, v_num=0]Training loss: 0.7116320133209229\n",
      "Epoch 5:   1%|          | 6/512 [00:02<02:49,  2.98it/s, v_num=0]Training loss: 0.787897527217865\n",
      "Epoch 5:   1%|▏         | 7/512 [00:02<02:53,  2.92it/s, v_num=0]Training loss: 0.8163911700248718\n",
      "Epoch 5:   2%|▏         | 8/512 [00:02<02:55,  2.87it/s, v_num=0]Training loss: 1.2536464929580688\n",
      "Epoch 5:   2%|▏         | 9/512 [00:03<02:57,  2.83it/s, v_num=0]Training loss: 1.5985760688781738\n",
      "Epoch 5:   2%|▏         | 10/512 [00:03<02:59,  2.80it/s, v_num=0]Training loss: 0.790966808795929\n",
      "Epoch 5:   2%|▏         | 11/512 [00:03<03:00,  2.78it/s, v_num=0]Training loss: 1.3068022727966309\n",
      "Epoch 5:   2%|▏         | 12/512 [00:04<03:01,  2.76it/s, v_num=0]Training loss: 1.5989761352539062\n",
      "Epoch 5:   3%|▎         | 13/512 [00:04<03:01,  2.74it/s, v_num=0]Training loss: 0.6977967619895935\n",
      "Epoch 5:   3%|▎         | 14/512 [00:05<03:02,  2.73it/s, v_num=0]Training loss: 1.0886250734329224\n",
      "Epoch 5:   3%|▎         | 15/512 [00:05<03:02,  2.72it/s, v_num=0]Training loss: 0.9572657942771912\n",
      "Epoch 5:   3%|▎         | 16/512 [00:05<03:03,  2.71it/s, v_num=0]Training loss: 0.9093678593635559\n",
      "Epoch 5:   3%|▎         | 17/512 [00:06<03:03,  2.70it/s, v_num=0]Training loss: 1.8810441493988037\n",
      "Epoch 5:   4%|▎         | 18/512 [00:06<03:03,  2.69it/s, v_num=0]Training loss: 1.2169876098632812\n",
      "Epoch 5:   4%|▎         | 19/512 [00:07<03:03,  2.69it/s, v_num=0]Training loss: 1.3214237689971924\n",
      "Epoch 5:   4%|▍         | 20/512 [00:07<03:03,  2.68it/s, v_num=0]Training loss: 1.0224320888519287\n",
      "Epoch 5:   4%|▍         | 21/512 [00:07<03:03,  2.67it/s, v_num=0]Training loss: 1.6760637760162354\n",
      "Epoch 5:   4%|▍         | 22/512 [00:08<03:03,  2.67it/s, v_num=0]Training loss: 1.1178399324417114\n",
      "Epoch 5:   4%|▍         | 23/512 [00:08<03:03,  2.66it/s, v_num=0]Training loss: 1.0126230716705322\n",
      "Epoch 5:   5%|▍         | 24/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 2.430525779724121\n",
      "Epoch 5:   5%|▍         | 25/512 [00:09<03:03,  2.66it/s, v_num=0]Training loss: 0.8166175484657288\n",
      "Epoch 5:   5%|▌         | 26/512 [00:09<03:03,  2.65it/s, v_num=0]Training loss: 0.8288078308105469\n",
      "Epoch 5:   5%|▌         | 27/512 [00:10<03:03,  2.65it/s, v_num=0]Training loss: 1.227288007736206\n",
      "Epoch 5:   5%|▌         | 28/512 [00:10<03:02,  2.65it/s, v_num=0]Training loss: 1.2174797058105469\n",
      "Epoch 5:   6%|▌         | 29/512 [00:10<03:02,  2.64it/s, v_num=0]Training loss: 0.8363378047943115\n",
      "Epoch 5:   6%|▌         | 30/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 0.6996029615402222\n",
      "Epoch 5:   6%|▌         | 31/512 [00:11<03:02,  2.64it/s, v_num=0]Training loss: 1.2220133543014526\n",
      "Epoch 5:   6%|▋         | 32/512 [00:12<03:02,  2.64it/s, v_num=0]Training loss: 1.1883574724197388\n",
      "Epoch 5:   6%|▋         | 33/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 0.9702834486961365\n",
      "Epoch 5:   7%|▋         | 34/512 [00:12<03:01,  2.63it/s, v_num=0]Training loss: 1.1697090864181519\n",
      "Epoch 5:   7%|▋         | 35/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.1522985696792603\n",
      "Epoch 5:   7%|▋         | 36/512 [00:13<03:01,  2.63it/s, v_num=0]Training loss: 1.2062071561813354\n",
      "Epoch 5:   7%|▋         | 37/512 [00:14<03:00,  2.63it/s, v_num=0]Training loss: 0.8729802966117859\n",
      "Epoch 5:   7%|▋         | 38/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 0.6679219007492065\n",
      "Epoch 5:   8%|▊         | 39/512 [00:14<03:00,  2.62it/s, v_num=0]Training loss: 0.7800148725509644\n",
      "Epoch 5:   8%|▊         | 40/512 [00:15<03:00,  2.62it/s, v_num=0]Training loss: 1.77358877658844\n",
      "Epoch 5:   8%|▊         | 41/512 [00:15<02:59,  2.62it/s, v_num=0]Training loss: 1.3095982074737549\n",
      "Epoch 5:   8%|▊         | 42/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.2741378545761108\n",
      "Epoch 5:   8%|▊         | 43/512 [00:16<02:59,  2.62it/s, v_num=0]Training loss: 1.0772640705108643\n",
      "Epoch 5:   9%|▊         | 44/512 [00:16<02:58,  2.62it/s, v_num=0]Training loss: 1.201649785041809\n",
      "Epoch 5:   9%|▉         | 45/512 [00:17<02:58,  2.62it/s, v_num=0]Training loss: 1.4624741077423096\n",
      "Epoch 5:   9%|▉         | 46/512 [00:17<02:58,  2.61it/s, v_num=0]Training loss: 0.741484522819519\n",
      "Epoch 5:   9%|▉         | 47/512 [00:17<02:57,  2.61it/s, v_num=0]Training loss: 0.8682522773742676\n",
      "Epoch 5:   9%|▉         | 48/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 0.9395488500595093\n",
      "Epoch 5:  10%|▉         | 49/512 [00:18<02:57,  2.61it/s, v_num=0]Training loss: 1.101652979850769\n",
      "Epoch 5:  10%|▉         | 50/512 [00:19<02:57,  2.61it/s, v_num=0]Training loss: 0.8598809838294983\n",
      "Epoch 5:  10%|▉         | 51/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.0722590684890747\n",
      "Epoch 5:  10%|█         | 52/512 [00:19<02:56,  2.61it/s, v_num=0]Training loss: 1.3014172315597534\n",
      "Epoch 5:  10%|█         | 53/512 [00:20<02:56,  2.61it/s, v_num=0]Training loss: 0.9488463997840881\n",
      "Epoch 5:  11%|█         | 54/512 [00:20<02:55,  2.61it/s, v_num=0]Training loss: 0.8960005640983582\n",
      "Epoch 5:  11%|█         | 55/512 [00:21<02:55,  2.61it/s, v_num=0]Training loss: 0.6532581448554993\n",
      "Epoch 5:  11%|█         | 56/512 [00:21<02:55,  2.61it/s, v_num=0]Training loss: 1.0687804222106934\n",
      "Epoch 5:  11%|█         | 57/512 [00:21<02:54,  2.60it/s, v_num=0]Training loss: 1.6106771230697632\n",
      "Epoch 5:  11%|█▏        | 58/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 0.9282640218734741\n",
      "Epoch 5:  12%|█▏        | 59/512 [00:22<02:54,  2.60it/s, v_num=0]Training loss: 0.7948552966117859\n",
      "Epoch 5:  12%|█▏        | 60/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 0.8777995109558105\n",
      "Epoch 5:  12%|█▏        | 61/512 [00:23<02:53,  2.60it/s, v_num=0]Training loss: 1.3556206226348877\n",
      "Epoch 5:  12%|█▏        | 62/512 [00:23<02:52,  2.60it/s, v_num=0]Training loss: 0.8849135041236877\n",
      "Epoch 5:  12%|█▏        | 63/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.1944860219955444\n",
      "Epoch 5:  12%|█▎        | 64/512 [00:24<02:52,  2.60it/s, v_num=0]Training loss: 1.1062960624694824\n",
      "Epoch 5:  13%|█▎        | 65/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 1.4714614152908325\n",
      "Epoch 5:  13%|█▎        | 66/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 0.7115808725357056\n",
      "Epoch 5:  13%|█▎        | 67/512 [00:25<02:51,  2.60it/s, v_num=0]Training loss: 0.8595391511917114\n",
      "Epoch 5:  13%|█▎        | 68/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 0.8321796655654907\n",
      "Epoch 5:  13%|█▎        | 69/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.021885633468628\n",
      "Epoch 5:  14%|█▎        | 70/512 [00:26<02:50,  2.60it/s, v_num=0]Training loss: 1.1497571468353271\n",
      "Epoch 5:  14%|█▍        | 71/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 1.1652308702468872\n",
      "Epoch 5:  14%|█▍        | 72/512 [00:27<02:49,  2.60it/s, v_num=0]Training loss: 0.7764773964881897\n",
      "Epoch 5:  14%|█▍        | 73/512 [00:28<02:49,  2.60it/s, v_num=0]Training loss: 1.028346061706543\n",
      "Epoch 5:  14%|█▍        | 74/512 [00:28<02:48,  2.60it/s, v_num=0]Training loss: 0.952555239200592\n",
      "Epoch 5:  15%|█▍        | 75/512 [00:28<02:48,  2.60it/s, v_num=0]Training loss: 1.8656206130981445\n",
      "Epoch 5:  15%|█▍        | 76/512 [00:29<02:48,  2.59it/s, v_num=0]Training loss: 1.0886280536651611\n",
      "Epoch 5:  15%|█▌        | 77/512 [00:29<02:47,  2.59it/s, v_num=0]Training loss: 0.8785967826843262\n",
      "Epoch 5:  15%|█▌        | 78/512 [00:30<02:47,  2.59it/s, v_num=0]Training loss: 0.8982410430908203\n",
      "Epoch 5:  15%|█▌        | 79/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 1.184646487236023\n",
      "Epoch 5:  16%|█▌        | 80/512 [00:30<02:46,  2.59it/s, v_num=0]Training loss: 1.2141711711883545\n",
      "Epoch 5:  16%|█▌        | 81/512 [00:31<02:46,  2.59it/s, v_num=0]Training loss: 0.9331256151199341\n",
      "Epoch 5:  16%|█▌        | 82/512 [00:31<02:45,  2.59it/s, v_num=0]Training loss: 0.740131139755249\n",
      "Epoch 5:  16%|█▌        | 83/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 0.9737973213195801\n",
      "Epoch 5:  16%|█▋        | 84/512 [00:32<02:45,  2.59it/s, v_num=0]Training loss: 1.170379400253296\n",
      "Epoch 5:  17%|█▋        | 85/512 [00:32<02:44,  2.59it/s, v_num=0]Training loss: 1.008496880531311\n",
      "Epoch 5:  17%|█▋        | 86/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 1.4413443803787231\n",
      "Epoch 5:  17%|█▋        | 87/512 [00:33<02:44,  2.59it/s, v_num=0]Training loss: 0.9961596727371216\n",
      "Epoch 5:  17%|█▋        | 88/512 [00:33<02:43,  2.59it/s, v_num=0]Training loss: 0.8417408466339111\n",
      "Epoch 5:  17%|█▋        | 89/512 [00:34<02:43,  2.59it/s, v_num=0]Training loss: 0.9337611198425293\n",
      "Epoch 5:  18%|█▊        | 90/512 [00:34<02:42,  2.59it/s, v_num=0]Training loss: 1.2006908655166626\n",
      "Epoch 5:  18%|█▊        | 91/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.1728241443634033\n",
      "Epoch 5:  18%|█▊        | 92/512 [00:35<02:42,  2.59it/s, v_num=0]Training loss: 1.094435453414917\n",
      "Epoch 5:  18%|█▊        | 93/512 [00:35<02:41,  2.59it/s, v_num=0]Training loss: 1.0341451168060303\n",
      "Epoch 5:  18%|█▊        | 94/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 1.1563317775726318\n",
      "Epoch 5:  19%|█▊        | 95/512 [00:36<02:41,  2.59it/s, v_num=0]Training loss: 0.9802525043487549\n",
      "Epoch 5:  19%|█▉        | 96/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 1.0331895351409912\n",
      "Epoch 5:  19%|█▉        | 97/512 [00:37<02:40,  2.59it/s, v_num=0]Training loss: 0.9521533846855164\n",
      "Epoch 5:  19%|█▉        | 98/512 [00:37<02:39,  2.59it/s, v_num=0]Training loss: 1.0208861827850342\n",
      "Epoch 5:  19%|█▉        | 99/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 1.0063059329986572\n",
      "Epoch 5:  20%|█▉        | 100/512 [00:38<02:39,  2.59it/s, v_num=0]Training loss: 0.9558712244033813\n",
      "Epoch 5:  20%|█▉        | 101/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.7702919244766235\n",
      "Epoch 5:  20%|█▉        | 102/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.7738522887229919\n",
      "Epoch 5:  20%|██        | 103/512 [00:39<02:38,  2.59it/s, v_num=0]Training loss: 0.6704695224761963\n",
      "Epoch 5:  20%|██        | 104/512 [00:40<02:37,  2.59it/s, v_num=0]Training loss: 0.9428144693374634\n",
      "Epoch 5:  21%|██        | 105/512 [00:40<02:37,  2.59it/s, v_num=0]Training loss: 0.7752801775932312\n",
      "Epoch 5:  21%|██        | 106/512 [00:40<02:37,  2.59it/s, v_num=0]Training loss: 1.504089593887329\n",
      "Epoch 5:  21%|██        | 107/512 [00:41<02:36,  2.59it/s, v_num=0]Training loss: 1.6543928384780884\n",
      "Epoch 5:  21%|██        | 108/512 [00:41<02:36,  2.59it/s, v_num=0]Training loss: 0.9447304606437683\n",
      "Epoch 5:  21%|██▏       | 109/512 [00:42<02:35,  2.59it/s, v_num=0]Training loss: 1.274532437324524\n",
      "Epoch 5:  21%|██▏       | 110/512 [00:42<02:35,  2.59it/s, v_num=0]Training loss: 0.8387947678565979\n",
      "Epoch 5:  22%|██▏       | 111/512 [00:42<02:35,  2.58it/s, v_num=0]Training loss: 0.8987666964530945\n",
      "Epoch 5:  22%|██▏       | 112/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 1.0422558784484863\n",
      "Epoch 5:  22%|██▏       | 113/512 [00:43<02:34,  2.58it/s, v_num=0]Training loss: 0.9736368656158447\n",
      "Epoch 5:  22%|██▏       | 114/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.9421027898788452\n",
      "Epoch 5:  22%|██▏       | 115/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 1.1877756118774414\n",
      "Epoch 5:  23%|██▎       | 116/512 [00:44<02:33,  2.58it/s, v_num=0]Training loss: 0.8819255828857422\n",
      "Epoch 5:  23%|██▎       | 117/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.0325316190719604\n",
      "Epoch 5:  23%|██▎       | 118/512 [00:45<02:32,  2.58it/s, v_num=0]Training loss: 1.3688806295394897\n",
      "Epoch 5:  23%|██▎       | 119/512 [00:46<02:32,  2.58it/s, v_num=0]Training loss: 0.6208841800689697\n",
      "Epoch 5:  23%|██▎       | 120/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.9875733256340027\n",
      "Epoch 5:  24%|██▎       | 121/512 [00:46<02:31,  2.58it/s, v_num=0]Training loss: 0.9794681668281555\n",
      "Epoch 5:  24%|██▍       | 122/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 0.9768244624137878\n",
      "Epoch 5:  24%|██▍       | 123/512 [00:47<02:30,  2.58it/s, v_num=0]Training loss: 0.7907119393348694\n",
      "Epoch 5:  24%|██▍       | 124/512 [00:48<02:30,  2.58it/s, v_num=0]Training loss: 0.8251000046730042\n",
      "Epoch 5:  24%|██▍       | 125/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 1.5566827058792114\n",
      "Epoch 5:  25%|██▍       | 126/512 [00:48<02:29,  2.58it/s, v_num=0]Training loss: 0.6299899816513062\n",
      "Epoch 5:  25%|██▍       | 127/512 [00:49<02:29,  2.58it/s, v_num=0]Training loss: 1.5228502750396729\n",
      "Epoch 5:  25%|██▌       | 128/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 1.3843713998794556\n",
      "Epoch 5:  25%|██▌       | 129/512 [00:49<02:28,  2.58it/s, v_num=0]Training loss: 0.8980662822723389\n",
      "Epoch 5:  25%|██▌       | 130/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 0.728643536567688\n",
      "Epoch 5:  26%|██▌       | 131/512 [00:50<02:27,  2.58it/s, v_num=0]Training loss: 0.955604076385498\n",
      "Epoch 5:  26%|██▌       | 132/512 [00:51<02:27,  2.58it/s, v_num=0]Training loss: 0.7374735474586487\n",
      "Epoch 5:  26%|██▌       | 133/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 0.7989621162414551\n",
      "Epoch 5:  26%|██▌       | 134/512 [00:51<02:26,  2.58it/s, v_num=0]Training loss: 1.2054568529129028\n",
      "Epoch 5:  26%|██▋       | 135/512 [00:52<02:26,  2.58it/s, v_num=0]Training loss: 0.8927249312400818\n",
      "Epoch 5:  27%|██▋       | 136/512 [00:52<02:25,  2.58it/s, v_num=0]Training loss: 1.1749399900436401\n",
      "Epoch 5:  27%|██▋       | 137/512 [00:53<02:25,  2.58it/s, v_num=0]Training loss: 0.9813899993896484\n",
      "Epoch 5:  27%|██▋       | 138/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 0.8873971104621887\n",
      "Epoch 5:  27%|██▋       | 139/512 [00:53<02:24,  2.58it/s, v_num=0]Training loss: 1.0988260507583618\n",
      "Epoch 5:  27%|██▋       | 140/512 [00:54<02:24,  2.58it/s, v_num=0]Training loss: 1.0381360054016113\n",
      "Epoch 5:  28%|██▊       | 141/512 [00:54<02:23,  2.58it/s, v_num=0]Training loss: 0.9288124442100525\n",
      "Epoch 5:  28%|██▊       | 142/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.2870163917541504\n",
      "Epoch 5:  28%|██▊       | 143/512 [00:55<02:23,  2.58it/s, v_num=0]Training loss: 1.0066108703613281\n",
      "Epoch 5:  28%|██▊       | 144/512 [00:55<02:22,  2.58it/s, v_num=0]Training loss: 1.2727965116500854\n",
      "Epoch 5:  28%|██▊       | 145/512 [00:56<02:22,  2.58it/s, v_num=0]Training loss: 0.7931069135665894\n",
      "Epoch 5:  29%|██▊       | 146/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 1.6947271823883057\n",
      "Epoch 5:  29%|██▊       | 147/512 [00:56<02:21,  2.58it/s, v_num=0]Training loss: 0.8883915543556213\n",
      "Epoch 5:  29%|██▉       | 148/512 [00:57<02:21,  2.58it/s, v_num=0]Training loss: 0.9018062949180603\n",
      "Epoch 5:  29%|██▉       | 149/512 [00:57<02:20,  2.58it/s, v_num=0]Training loss: 1.7778953313827515\n",
      "Epoch 5:  29%|██▉       | 150/512 [00:58<02:20,  2.58it/s, v_num=0]Training loss: 0.8821602463722229\n",
      "Epoch 5:  29%|██▉       | 151/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 0.8913196921348572\n",
      "Epoch 5:  30%|██▉       | 152/512 [00:58<02:19,  2.58it/s, v_num=0]Training loss: 1.1297595500946045\n",
      "Epoch 5:  30%|██▉       | 153/512 [00:59<02:19,  2.58it/s, v_num=0]Training loss: 1.1851452589035034\n",
      "Epoch 5:  30%|███       | 154/512 [00:59<02:18,  2.58it/s, v_num=0]Training loss: 0.7780870795249939\n",
      "Epoch 5:  30%|███       | 155/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 0.8895907402038574\n",
      "Epoch 5:  30%|███       | 156/512 [01:00<02:18,  2.58it/s, v_num=0]Training loss: 0.9650477170944214\n",
      "Epoch 5:  31%|███       | 157/512 [01:00<02:17,  2.58it/s, v_num=0]Training loss: 0.6827609539031982\n",
      "Epoch 5:  31%|███       | 158/512 [01:01<02:17,  2.58it/s, v_num=0]Training loss: 0.9276092648506165\n",
      "Epoch 5:  31%|███       | 159/512 [01:01<02:16,  2.58it/s, v_num=0]Training loss: 1.4342167377471924\n",
      "Epoch 5:  31%|███▏      | 160/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 1.1222100257873535\n",
      "Epoch 5:  31%|███▏      | 161/512 [01:02<02:16,  2.58it/s, v_num=0]Training loss: 2.652050256729126\n",
      "Epoch 5:  32%|███▏      | 162/512 [01:02<02:15,  2.58it/s, v_num=0]Training loss: 0.7996478080749512\n",
      "Epoch 5:  32%|███▏      | 163/512 [01:03<02:15,  2.58it/s, v_num=0]Training loss: 1.1287275552749634\n",
      "Epoch 5:  32%|███▏      | 164/512 [01:03<02:14,  2.58it/s, v_num=0]Training loss: 0.7073405981063843\n",
      "Epoch 5:  32%|███▏      | 165/512 [01:03<02:14,  2.58it/s, v_num=0]Training loss: 1.0771052837371826\n",
      "Epoch 5:  32%|███▏      | 166/512 [01:04<02:14,  2.58it/s, v_num=0]Training loss: 0.7610934376716614\n",
      "Epoch 5:  33%|███▎      | 167/512 [01:04<02:13,  2.58it/s, v_num=0]Training loss: 1.132502555847168\n",
      "Epoch 5:  33%|███▎      | 168/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.28037428855896\n",
      "Epoch 5:  33%|███▎      | 169/512 [01:05<02:13,  2.58it/s, v_num=0]Training loss: 1.264163613319397\n",
      "Epoch 5:  33%|███▎      | 170/512 [01:05<02:12,  2.58it/s, v_num=0]Training loss: 1.0796490907669067\n",
      "Epoch 5:  33%|███▎      | 171/512 [01:06<02:12,  2.58it/s, v_num=0]Training loss: 1.3031978607177734\n",
      "Epoch 5:  34%|███▎      | 172/512 [01:06<02:11,  2.58it/s, v_num=0]Training loss: 1.5218449831008911\n",
      "Epoch 5:  34%|███▍      | 173/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 1.0761620998382568\n",
      "Epoch 5:  34%|███▍      | 174/512 [01:07<02:11,  2.58it/s, v_num=0]Training loss: 0.7805026173591614\n",
      "Epoch 5:  34%|███▍      | 175/512 [01:07<02:10,  2.58it/s, v_num=0]Training loss: 0.9319809675216675\n",
      "Epoch 5:  34%|███▍      | 176/512 [01:08<02:10,  2.58it/s, v_num=0]Training loss: 1.2003096342086792\n",
      "Epoch 5:  35%|███▍      | 177/512 [01:08<02:09,  2.58it/s, v_num=0]Training loss: 0.8855186700820923\n",
      "Epoch 5:  35%|███▍      | 178/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 1.029876470565796\n",
      "Epoch 5:  35%|███▍      | 179/512 [01:09<02:09,  2.58it/s, v_num=0]Training loss: 0.9333556294441223\n",
      "Epoch 5:  35%|███▌      | 180/512 [01:09<02:08,  2.58it/s, v_num=0]Training loss: 0.7326703667640686\n",
      "Epoch 5:  35%|███▌      | 181/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 0.8130956888198853\n",
      "Epoch 5:  36%|███▌      | 182/512 [01:10<02:08,  2.58it/s, v_num=0]Training loss: 0.9197137951850891\n",
      "Epoch 5:  36%|███▌      | 183/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.2244062423706055\n",
      "Epoch 5:  36%|███▌      | 184/512 [01:11<02:07,  2.58it/s, v_num=0]Training loss: 1.2070246934890747\n",
      "Epoch 5:  36%|███▌      | 185/512 [01:11<02:06,  2.58it/s, v_num=0]Training loss: 0.8499247431755066\n",
      "Epoch 5:  36%|███▋      | 186/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 1.0288037061691284\n",
      "Epoch 5:  37%|███▋      | 187/512 [01:12<02:06,  2.58it/s, v_num=0]Training loss: 1.3067079782485962\n",
      "Epoch 5:  37%|███▋      | 188/512 [01:12<02:05,  2.58it/s, v_num=0]Training loss: 1.0099623203277588\n",
      "Epoch 5:  37%|███▋      | 189/512 [01:13<02:05,  2.58it/s, v_num=0]Training loss: 1.032932162284851\n",
      "Epoch 5:  37%|███▋      | 190/512 [01:13<02:04,  2.58it/s, v_num=0]Training loss: 0.9907642602920532\n",
      "Epoch 5:  37%|███▋      | 191/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 0.9167040586471558\n",
      "Epoch 5:  38%|███▊      | 192/512 [01:14<02:04,  2.58it/s, v_num=0]Training loss: 1.8610011339187622\n",
      "Epoch 5:  38%|███▊      | 193/512 [01:14<02:03,  2.58it/s, v_num=0]Training loss: 0.835620641708374\n",
      "Epoch 5:  38%|███▊      | 194/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 0.8968828916549683\n",
      "Epoch 5:  38%|███▊      | 195/512 [01:15<02:03,  2.58it/s, v_num=0]Training loss: 1.0123974084854126\n",
      "Epoch 5:  38%|███▊      | 196/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 0.6807928085327148\n",
      "Epoch 5:  38%|███▊      | 197/512 [01:16<02:02,  2.58it/s, v_num=0]Training loss: 0.7413601875305176\n",
      "Epoch 5:  39%|███▊      | 198/512 [01:16<02:01,  2.58it/s, v_num=0]Training loss: 15.196195602416992\n",
      "Epoch 5:  39%|███▉      | 199/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 1.0437091588974\n",
      "Epoch 5:  39%|███▉      | 200/512 [01:17<02:01,  2.58it/s, v_num=0]Training loss: 0.8523714542388916\n",
      "Epoch 5:  39%|███▉      | 201/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 1.3319039344787598\n",
      "Epoch 5:  39%|███▉      | 202/512 [01:18<02:00,  2.58it/s, v_num=0]Training loss: 0.9477937817573547\n",
      "Epoch 5:  40%|███▉      | 203/512 [01:18<01:59,  2.58it/s, v_num=0]Training loss: 0.903138279914856\n",
      "Epoch 5:  40%|███▉      | 204/512 [01:19<01:59,  2.58it/s, v_num=0]Training loss: 1.4198771715164185\n",
      "Epoch 5:  40%|████      | 205/512 [01:19<01:59,  2.58it/s, v_num=0]Training loss: 1.5483721494674683\n",
      "Epoch 5:  40%|████      | 206/512 [01:19<01:58,  2.58it/s, v_num=0]Training loss: 1.1399636268615723\n",
      "Epoch 5:  40%|████      | 207/512 [01:20<01:58,  2.58it/s, v_num=0]Training loss: 0.9481041431427002\n",
      "Epoch 5:  41%|████      | 208/512 [01:20<01:58,  2.58it/s, v_num=0]Training loss: 0.8795710802078247\n",
      "Epoch 5:  41%|████      | 209/512 [01:21<01:57,  2.58it/s, v_num=0]Training loss: 1.2145487070083618\n",
      "Epoch 5:  41%|████      | 210/512 [01:21<01:57,  2.58it/s, v_num=0]Training loss: 1.1804773807525635\n",
      "Epoch 5:  41%|████      | 211/512 [01:21<01:56,  2.58it/s, v_num=0]Training loss: 0.8941215872764587\n",
      "Epoch 5:  41%|████▏     | 212/512 [01:22<01:56,  2.58it/s, v_num=0]Training loss: 0.9398599863052368\n",
      "Epoch 5:  42%|████▏     | 213/512 [01:22<01:56,  2.58it/s, v_num=0]Training loss: 1.3039354085922241\n",
      "Epoch 5:  42%|████▏     | 214/512 [01:23<01:55,  2.58it/s, v_num=0]Training loss: 0.8732534646987915\n",
      "Epoch 5:  42%|████▏     | 215/512 [01:23<01:55,  2.58it/s, v_num=0]Training loss: 1.1779567003250122\n",
      "Epoch 5:  42%|████▏     | 216/512 [01:23<01:54,  2.58it/s, v_num=0]Training loss: 0.8381520509719849\n",
      "Epoch 5:  42%|████▏     | 217/512 [01:24<01:54,  2.58it/s, v_num=0]Training loss: 1.0967930555343628\n",
      "Epoch 5:  43%|████▎     | 218/512 [01:24<01:54,  2.58it/s, v_num=0]Training loss: 1.0118646621704102\n",
      "Epoch 5:  43%|████▎     | 219/512 [01:25<01:53,  2.58it/s, v_num=0]Training loss: 1.0048218965530396\n",
      "Epoch 5:  43%|████▎     | 220/512 [01:25<01:53,  2.58it/s, v_num=0]Training loss: 1.1044223308563232\n",
      "Epoch 5:  43%|████▎     | 221/512 [01:25<01:52,  2.58it/s, v_num=0]Training loss: 0.6479832530021667\n",
      "Epoch 5:  43%|████▎     | 222/512 [01:26<01:52,  2.58it/s, v_num=0]Training loss: 0.8575988411903381\n",
      "Epoch 5:  44%|████▎     | 223/512 [01:26<01:52,  2.58it/s, v_num=0]Training loss: 0.6995282173156738\n",
      "Epoch 5:  44%|████▍     | 224/512 [01:26<01:51,  2.58it/s, v_num=0]Training loss: 0.6948176026344299\n",
      "Epoch 5:  44%|████▍     | 225/512 [01:27<01:51,  2.58it/s, v_num=0]Training loss: 0.991805911064148\n",
      "Epoch 5:  44%|████▍     | 226/512 [01:27<01:51,  2.58it/s, v_num=0]Training loss: 0.6518411636352539\n",
      "Epoch 5:  44%|████▍     | 227/512 [01:28<01:50,  2.58it/s, v_num=0]Training loss: 0.9192934036254883\n",
      "Epoch 5:  45%|████▍     | 228/512 [01:28<01:50,  2.58it/s, v_num=0]Training loss: 0.8497007489204407\n",
      "Epoch 5:  45%|████▍     | 229/512 [01:28<01:49,  2.58it/s, v_num=0]Training loss: 3.2756547927856445\n",
      "Epoch 5:  45%|████▍     | 230/512 [01:29<01:49,  2.58it/s, v_num=0]Training loss: 1.2243010997772217\n",
      "Epoch 5:  45%|████▌     | 231/512 [01:29<01:49,  2.58it/s, v_num=0]Training loss: 0.7960274815559387\n",
      "Epoch 5:  45%|████▌     | 232/512 [01:30<01:48,  2.58it/s, v_num=0]Training loss: 1.3899027109146118\n",
      "Epoch 5:  46%|████▌     | 233/512 [01:30<01:48,  2.58it/s, v_num=0]Training loss: 0.6931232213973999\n",
      "Epoch 5:  46%|████▌     | 234/512 [01:30<01:47,  2.58it/s, v_num=0]Training loss: 1.3424052000045776\n",
      "Epoch 5:  46%|████▌     | 235/512 [01:31<01:47,  2.58it/s, v_num=0]Training loss: 0.8017264604568481\n",
      "Epoch 5:  46%|████▌     | 236/512 [01:31<01:47,  2.58it/s, v_num=0]Training loss: 0.956707775592804\n",
      "Epoch 5:  46%|████▋     | 237/512 [01:32<01:46,  2.58it/s, v_num=0]Training loss: 0.8689130544662476\n",
      "Epoch 5:  46%|████▋     | 238/512 [01:32<01:46,  2.58it/s, v_num=0]Training loss: 0.9600443840026855\n",
      "Epoch 5:  47%|████▋     | 239/512 [01:32<01:46,  2.58it/s, v_num=0]Training loss: 0.7306485176086426\n",
      "Epoch 5:  47%|████▋     | 240/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 1.2496508359909058\n",
      "Epoch 5:  47%|████▋     | 241/512 [01:33<01:45,  2.57it/s, v_num=0]Training loss: 0.8039308786392212\n",
      "Epoch 5:  47%|████▋     | 242/512 [01:33<01:44,  2.57it/s, v_num=0]Training loss: 0.8797345757484436\n",
      "Epoch 5:  47%|████▋     | 243/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 0.8495626449584961\n",
      "Epoch 5:  48%|████▊     | 244/512 [01:34<01:44,  2.57it/s, v_num=0]Training loss: 1.3374546766281128\n",
      "Epoch 5:  48%|████▊     | 245/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 0.8661481738090515\n",
      "Epoch 5:  48%|████▊     | 246/512 [01:35<01:43,  2.57it/s, v_num=0]Training loss: 1.0061697959899902\n",
      "Epoch 5:  48%|████▊     | 247/512 [01:35<01:42,  2.57it/s, v_num=0]Training loss: 1.049107551574707\n",
      "Epoch 5:  48%|████▊     | 248/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 0.9233434200286865\n",
      "Epoch 5:  49%|████▊     | 249/512 [01:36<01:42,  2.57it/s, v_num=0]Training loss: 0.7491081357002258\n",
      "Epoch 5:  49%|████▉     | 250/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.0056816339492798\n",
      "Epoch 5:  49%|████▉     | 251/512 [01:37<01:41,  2.57it/s, v_num=0]Training loss: 1.1019539833068848\n",
      "Epoch 5:  49%|████▉     | 252/512 [01:37<01:40,  2.57it/s, v_num=0]Training loss: 0.640174388885498\n",
      "Epoch 5:  49%|████▉     | 253/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 0.5910167694091797\n",
      "Epoch 5:  50%|████▉     | 254/512 [01:38<01:40,  2.57it/s, v_num=0]Training loss: 1.0163342952728271\n",
      "Epoch 5:  50%|████▉     | 255/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 1.1133111715316772\n",
      "Epoch 5:  50%|█████     | 256/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 0.8200669884681702\n",
      "Epoch 5:  50%|█████     | 257/512 [01:39<01:39,  2.57it/s, v_num=0]Training loss: 0.9077029824256897\n",
      "Epoch 5:  50%|█████     | 258/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 0.9507587552070618\n",
      "Epoch 5:  51%|█████     | 259/512 [01:40<01:38,  2.57it/s, v_num=0]Training loss: 0.8325479030609131\n",
      "Epoch 5:  51%|█████     | 260/512 [01:40<01:37,  2.57it/s, v_num=0]Training loss: 0.7513823509216309\n",
      "Epoch 5:  51%|█████     | 261/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.7996141910552979\n",
      "Epoch 5:  51%|█████     | 262/512 [01:41<01:37,  2.57it/s, v_num=0]Training loss: 0.7129586935043335\n",
      "Epoch 5:  51%|█████▏    | 263/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.0035825967788696\n",
      "Epoch 5:  52%|█████▏    | 264/512 [01:42<01:36,  2.57it/s, v_num=0]Training loss: 1.1786391735076904\n",
      "Epoch 5:  52%|█████▏    | 265/512 [01:42<01:35,  2.57it/s, v_num=0]Training loss: 1.043505072593689\n",
      "Epoch 5:  52%|█████▏    | 266/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.9681582450866699\n",
      "Epoch 5:  52%|█████▏    | 267/512 [01:43<01:35,  2.57it/s, v_num=0]Training loss: 0.8548624515533447\n",
      "Epoch 5:  52%|█████▏    | 268/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 1.0290206670761108\n",
      "Epoch 5:  53%|█████▎    | 269/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 0.9366533756256104\n",
      "Epoch 5:  53%|█████▎    | 270/512 [01:44<01:34,  2.57it/s, v_num=0]Training loss: 0.8023684024810791\n",
      "Epoch 5:  53%|█████▎    | 271/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 0.6338135004043579\n",
      "Epoch 5:  53%|█████▎    | 272/512 [01:45<01:33,  2.57it/s, v_num=0]Training loss: 0.5983918905258179\n",
      "Epoch 5:  53%|█████▎    | 273/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.08138906955719\n",
      "Epoch 5:  54%|█████▎    | 274/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 1.219620704650879\n",
      "Epoch 5:  54%|█████▎    | 275/512 [01:46<01:32,  2.57it/s, v_num=0]Training loss: 0.8405705690383911\n",
      "Epoch 5:  54%|█████▍    | 276/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 0.7875162363052368\n",
      "Epoch 5:  54%|█████▍    | 277/512 [01:47<01:31,  2.57it/s, v_num=0]Training loss: 1.2410285472869873\n",
      "Epoch 5:  54%|█████▍    | 278/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 0.7585758566856384\n",
      "Epoch 5:  54%|█████▍    | 279/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 0.7571198344230652\n",
      "Epoch 5:  55%|█████▍    | 280/512 [01:48<01:30,  2.57it/s, v_num=0]Training loss: 0.9761539697647095\n",
      "Epoch 5:  55%|█████▍    | 281/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 1.1100572347640991\n",
      "Epoch 5:  55%|█████▌    | 282/512 [01:49<01:29,  2.57it/s, v_num=0]Training loss: 0.6407504081726074\n",
      "Epoch 5:  55%|█████▌    | 283/512 [01:49<01:28,  2.57it/s, v_num=0]Training loss: 1.0702158212661743\n",
      "Epoch 5:  55%|█████▌    | 284/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 1.285455346107483\n",
      "Epoch 5:  56%|█████▌    | 285/512 [01:50<01:28,  2.57it/s, v_num=0]Training loss: 2.475916862487793\n",
      "Epoch 5:  56%|█████▌    | 286/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 0.727427065372467\n",
      "Epoch 5:  56%|█████▌    | 287/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.1550729274749756\n",
      "Epoch 5:  56%|█████▋    | 288/512 [01:51<01:27,  2.57it/s, v_num=0]Training loss: 1.2909624576568604\n",
      "Epoch 5:  56%|█████▋    | 289/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 0.9798696637153625\n",
      "Epoch 5:  57%|█████▋    | 290/512 [01:52<01:26,  2.57it/s, v_num=0]Training loss: 0.7915399074554443\n",
      "Epoch 5:  57%|█████▋    | 291/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.4021917581558228\n",
      "Epoch 5:  57%|█████▋    | 292/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 1.1622395515441895\n",
      "Epoch 5:  57%|█████▋    | 293/512 [01:53<01:25,  2.57it/s, v_num=0]Training loss: 0.9804310202598572\n",
      "Epoch 5:  57%|█████▋    | 294/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 0.8702578544616699\n",
      "Epoch 5:  58%|█████▊    | 295/512 [01:54<01:24,  2.57it/s, v_num=0]Training loss: 0.8490716814994812\n",
      "Epoch 5:  58%|█████▊    | 296/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.8604795932769775\n",
      "Epoch 5:  58%|█████▊    | 297/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 1.1933236122131348\n",
      "Epoch 5:  58%|█████▊    | 298/512 [01:55<01:23,  2.57it/s, v_num=0]Training loss: 0.8090283274650574\n",
      "Epoch 5:  58%|█████▊    | 299/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 1.1642181873321533\n",
      "Epoch 5:  59%|█████▊    | 300/512 [01:56<01:22,  2.57it/s, v_num=0]Training loss: 0.8234452605247498\n",
      "Epoch 5:  59%|█████▉    | 301/512 [01:56<01:21,  2.57it/s, v_num=0]Training loss: 0.6945077776908875\n",
      "Epoch 5:  59%|█████▉    | 302/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.8220787048339844\n",
      "Epoch 5:  59%|█████▉    | 303/512 [01:57<01:21,  2.57it/s, v_num=0]Training loss: 0.9864054322242737\n",
      "Epoch 5:  59%|█████▉    | 304/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.8686540126800537\n",
      "Epoch 5:  60%|█████▉    | 305/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 1.026516079902649\n",
      "Epoch 5:  60%|█████▉    | 306/512 [01:58<01:20,  2.57it/s, v_num=0]Training loss: 0.8976165056228638\n",
      "Epoch 5:  60%|█████▉    | 307/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 0.9549217224121094\n",
      "Epoch 5:  60%|██████    | 308/512 [01:59<01:19,  2.57it/s, v_num=0]Training loss: 1.0240105390548706\n",
      "Epoch 5:  60%|██████    | 309/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.9110319018363953\n",
      "Epoch 5:  61%|██████    | 310/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.8920755982398987\n",
      "Epoch 5:  61%|██████    | 311/512 [02:00<01:18,  2.57it/s, v_num=0]Training loss: 0.7912899255752563\n",
      "Epoch 5:  61%|██████    | 312/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.6216994524002075\n",
      "Epoch 5:  61%|██████    | 313/512 [02:01<01:17,  2.57it/s, v_num=0]Training loss: 1.0110962390899658\n",
      "Epoch 5:  61%|██████▏   | 314/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 1.6675058603286743\n",
      "Epoch 5:  62%|██████▏   | 315/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 0.9304784536361694\n",
      "Epoch 5:  62%|██████▏   | 316/512 [02:02<01:16,  2.57it/s, v_num=0]Training loss: 0.6244596838951111\n",
      "Epoch 5:  62%|██████▏   | 317/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.0968341827392578\n",
      "Epoch 5:  62%|██████▏   | 318/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 1.0957361459732056\n",
      "Epoch 5:  62%|██████▏   | 319/512 [02:03<01:15,  2.57it/s, v_num=0]Training loss: 0.989409327507019\n",
      "Epoch 5:  62%|██████▎   | 320/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 0.9845048189163208\n",
      "Epoch 5:  63%|██████▎   | 321/512 [02:04<01:14,  2.57it/s, v_num=0]Training loss: 1.2563985586166382\n",
      "Epoch 5:  63%|██████▎   | 322/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.3505923748016357\n",
      "Epoch 5:  63%|██████▎   | 323/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 0.9931756258010864\n",
      "Epoch 5:  63%|██████▎   | 324/512 [02:05<01:13,  2.57it/s, v_num=0]Training loss: 1.2043591737747192\n",
      "Epoch 5:  63%|██████▎   | 325/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 1.32289719581604\n",
      "Epoch 5:  64%|██████▎   | 326/512 [02:06<01:12,  2.57it/s, v_num=0]Training loss: 0.7467640042304993\n",
      "Epoch 5:  64%|██████▍   | 327/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.436706304550171\n",
      "Epoch 5:  64%|██████▍   | 328/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 1.3955551385879517\n",
      "Epoch 5:  64%|██████▍   | 329/512 [02:07<01:11,  2.57it/s, v_num=0]Training loss: 0.8701362609863281\n",
      "Epoch 5:  64%|██████▍   | 330/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.2157737016677856\n",
      "Epoch 5:  65%|██████▍   | 331/512 [02:08<01:10,  2.57it/s, v_num=0]Training loss: 1.0119720697402954\n",
      "Epoch 5:  65%|██████▍   | 332/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 0.6439476013183594\n",
      "Epoch 5:  65%|██████▌   | 333/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 0.7952898740768433\n",
      "Epoch 5:  65%|██████▌   | 334/512 [02:09<01:09,  2.57it/s, v_num=0]Training loss: 1.2762229442596436\n",
      "Epoch 5:  65%|██████▌   | 335/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.7857563495635986\n",
      "Epoch 5:  66%|██████▌   | 336/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.743309736251831\n",
      "Epoch 5:  66%|██████▌   | 337/512 [02:10<01:08,  2.57it/s, v_num=0]Training loss: 0.9851258993148804\n",
      "Epoch 5:  66%|██████▌   | 338/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 0.5687767267227173\n",
      "Epoch 5:  66%|██████▌   | 339/512 [02:11<01:07,  2.57it/s, v_num=0]Training loss: 1.335007905960083\n",
      "Epoch 5:  66%|██████▋   | 340/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.8468774557113647\n",
      "Epoch 5:  67%|██████▋   | 341/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 1.0584028959274292\n",
      "Epoch 5:  67%|██████▋   | 342/512 [02:12<01:06,  2.57it/s, v_num=0]Training loss: 0.8042726516723633\n",
      "Epoch 5:  67%|██████▋   | 343/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 0.979033350944519\n",
      "Epoch 5:  67%|██████▋   | 344/512 [02:13<01:05,  2.57it/s, v_num=0]Training loss: 1.5453394651412964\n",
      "Epoch 5:  67%|██████▋   | 345/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.7426717877388\n",
      "Epoch 5:  68%|██████▊   | 346/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.7763356566429138\n",
      "Epoch 5:  68%|██████▊   | 347/512 [02:14<01:04,  2.57it/s, v_num=0]Training loss: 0.60152268409729\n",
      "Epoch 5:  68%|██████▊   | 348/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 0.9528936147689819\n",
      "Epoch 5:  68%|██████▊   | 349/512 [02:15<01:03,  2.57it/s, v_num=0]Training loss: 0.6270716786384583\n",
      "Epoch 5:  68%|██████▊   | 350/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.2451612949371338\n",
      "Epoch 5:  69%|██████▊   | 351/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.082255244255066\n",
      "Epoch 5:  69%|██████▉   | 352/512 [02:16<01:02,  2.57it/s, v_num=0]Training loss: 1.1203110218048096\n",
      "Epoch 5:  69%|██████▉   | 353/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.7459248900413513\n",
      "Epoch 5:  69%|██████▉   | 354/512 [02:17<01:01,  2.57it/s, v_num=0]Training loss: 0.6333698630332947\n",
      "Epoch 5:  69%|██████▉   | 355/512 [02:18<01:01,  2.57it/s, v_num=0]Training loss: 1.0745811462402344\n",
      "Epoch 5:  70%|██████▉   | 356/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 1.0985651016235352\n",
      "Epoch 5:  70%|██████▉   | 357/512 [02:18<01:00,  2.57it/s, v_num=0]Training loss: 0.7286422848701477\n",
      "Epoch 5:  70%|██████▉   | 358/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 0.5641407370567322\n",
      "Epoch 5:  70%|███████   | 359/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 1.2278423309326172\n",
      "Epoch 5:  70%|███████   | 360/512 [02:19<00:59,  2.57it/s, v_num=0]Training loss: 0.876994788646698\n",
      "Epoch 5:  71%|███████   | 361/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 1.1091583967208862\n",
      "Epoch 5:  71%|███████   | 362/512 [02:20<00:58,  2.57it/s, v_num=0]Training loss: 0.9386535882949829\n",
      "Epoch 5:  71%|███████   | 363/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 0.8642804026603699\n",
      "Epoch 5:  71%|███████   | 364/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.291051983833313\n",
      "Epoch 5:  71%|███████▏  | 365/512 [02:21<00:57,  2.57it/s, v_num=0]Training loss: 1.223989725112915\n",
      "Epoch 5:  71%|███████▏  | 366/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 0.9854880571365356\n",
      "Epoch 5:  72%|███████▏  | 367/512 [02:22<00:56,  2.57it/s, v_num=0]Training loss: 0.8097891807556152\n",
      "Epoch 5:  72%|███████▏  | 368/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.0953677892684937\n",
      "Epoch 5:  72%|███████▏  | 369/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.6208401918411255\n",
      "Epoch 5:  72%|███████▏  | 370/512 [02:23<00:55,  2.57it/s, v_num=0]Training loss: 1.0823328495025635\n",
      "Epoch 5:  72%|███████▏  | 371/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 0.8689135909080505\n",
      "Epoch 5:  73%|███████▎  | 372/512 [02:24<00:54,  2.57it/s, v_num=0]Training loss: 0.6952165365219116\n",
      "Epoch 5:  73%|███████▎  | 373/512 [02:25<00:54,  2.57it/s, v_num=0]Training loss: 0.9017349481582642\n",
      "Epoch 5:  73%|███████▎  | 374/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 0.8689517378807068\n",
      "Epoch 5:  73%|███████▎  | 375/512 [02:25<00:53,  2.57it/s, v_num=0]Training loss: 1.4943807125091553\n",
      "Epoch 5:  73%|███████▎  | 376/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.6048194766044617\n",
      "Epoch 5:  74%|███████▎  | 377/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 1.832351803779602\n",
      "Epoch 5:  74%|███████▍  | 378/512 [02:26<00:52,  2.57it/s, v_num=0]Training loss: 0.8781542778015137\n",
      "Epoch 5:  74%|███████▍  | 379/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.3062390089035034\n",
      "Epoch 5:  74%|███████▍  | 380/512 [02:27<00:51,  2.57it/s, v_num=0]Training loss: 1.273640513420105\n",
      "Epoch 5:  74%|███████▍  | 381/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 1.2671308517456055\n",
      "Epoch 5:  75%|███████▍  | 382/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.9789860248565674\n",
      "Epoch 5:  75%|███████▍  | 383/512 [02:28<00:50,  2.57it/s, v_num=0]Training loss: 0.7892279028892517\n",
      "Epoch 5:  75%|███████▌  | 384/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 0.6285431385040283\n",
      "Epoch 5:  75%|███████▌  | 385/512 [02:29<00:49,  2.57it/s, v_num=0]Training loss: 1.0100518465042114\n",
      "Epoch 5:  75%|███████▌  | 386/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 0.650161862373352\n",
      "Epoch 5:  76%|███████▌  | 387/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.370141625404358\n",
      "Epoch 5:  76%|███████▌  | 388/512 [02:30<00:48,  2.57it/s, v_num=0]Training loss: 1.2188284397125244\n",
      "Epoch 5:  76%|███████▌  | 389/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 0.6917809247970581\n",
      "Epoch 5:  76%|███████▌  | 390/512 [02:31<00:47,  2.57it/s, v_num=0]Training loss: 1.0489521026611328\n",
      "Epoch 5:  76%|███████▋  | 391/512 [02:32<00:47,  2.57it/s, v_num=0]Training loss: 1.3400565385818481\n",
      "Epoch 5:  77%|███████▋  | 392/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.0624704360961914\n",
      "Epoch 5:  77%|███████▋  | 393/512 [02:32<00:46,  2.57it/s, v_num=0]Training loss: 1.1687289476394653\n",
      "Epoch 5:  77%|███████▋  | 394/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 0.7963401079177856\n",
      "Epoch 5:  77%|███████▋  | 395/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 1.2551698684692383\n",
      "Epoch 5:  77%|███████▋  | 396/512 [02:33<00:45,  2.57it/s, v_num=0]Training loss: 0.8735468983650208\n",
      "Epoch 5:  78%|███████▊  | 397/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.1915916204452515\n",
      "Epoch 5:  78%|███████▊  | 398/512 [02:34<00:44,  2.57it/s, v_num=0]Training loss: 1.5214886665344238\n",
      "Epoch 5:  78%|███████▊  | 399/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 1.0344157218933105\n",
      "Epoch 5:  78%|███████▊  | 400/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 0.8337327241897583\n",
      "Epoch 5:  78%|███████▊  | 401/512 [02:35<00:43,  2.57it/s, v_num=0]Training loss: 1.373250126838684\n",
      "Epoch 5:  79%|███████▊  | 402/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 0.839987576007843\n",
      "Epoch 5:  79%|███████▊  | 403/512 [02:36<00:42,  2.57it/s, v_num=0]Training loss: 1.2840781211853027\n",
      "Epoch 5:  79%|███████▉  | 404/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 0.8983869552612305\n",
      "Epoch 5:  79%|███████▉  | 405/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.0348784923553467\n",
      "Epoch 5:  79%|███████▉  | 406/512 [02:37<00:41,  2.57it/s, v_num=0]Training loss: 1.1524732112884521\n",
      "Epoch 5:  79%|███████▉  | 407/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.8951613903045654\n",
      "Epoch 5:  80%|███████▉  | 408/512 [02:38<00:40,  2.57it/s, v_num=0]Training loss: 0.9314839243888855\n",
      "Epoch 5:  80%|███████▉  | 409/512 [02:39<00:40,  2.57it/s, v_num=0]Training loss: 1.1827776432037354\n",
      "Epoch 5:  80%|████████  | 410/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.9018276333808899\n",
      "Epoch 5:  80%|████████  | 411/512 [02:39<00:39,  2.57it/s, v_num=0]Training loss: 0.9836833477020264\n",
      "Epoch 5:  80%|████████  | 412/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.7117871046066284\n",
      "Epoch 5:  81%|████████  | 413/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.5364962220191956\n",
      "Epoch 5:  81%|████████  | 414/512 [02:40<00:38,  2.57it/s, v_num=0]Training loss: 0.6449934840202332\n",
      "Epoch 5:  81%|████████  | 415/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 1.0761146545410156\n",
      "Epoch 5:  81%|████████▏ | 416/512 [02:41<00:37,  2.57it/s, v_num=0]Training loss: 0.8542202711105347\n",
      "Epoch 5:  81%|████████▏ | 417/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.1016203165054321\n",
      "Epoch 5:  82%|████████▏ | 418/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 0.8667174577713013\n",
      "Epoch 5:  82%|████████▏ | 419/512 [02:42<00:36,  2.57it/s, v_num=0]Training loss: 1.1862587928771973\n",
      "Epoch 5:  82%|████████▏ | 420/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 0.7394864559173584\n",
      "Epoch 5:  82%|████████▏ | 421/512 [02:43<00:35,  2.57it/s, v_num=0]Training loss: 1.0646533966064453\n",
      "Epoch 5:  82%|████████▏ | 422/512 [02:44<00:35,  2.57it/s, v_num=0]Training loss: 0.9878525733947754\n",
      "Epoch 5:  83%|████████▎ | 423/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 1.1747404336929321\n",
      "Epoch 5:  83%|████████▎ | 424/512 [02:44<00:34,  2.57it/s, v_num=0]Training loss: 0.9466582536697388\n",
      "Epoch 5:  83%|████████▎ | 425/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 1.4430288076400757\n",
      "Epoch 5:  83%|████████▎ | 426/512 [02:45<00:33,  2.57it/s, v_num=0]Training loss: 1.2775615453720093\n",
      "Epoch 5:  83%|████████▎ | 427/512 [02:46<00:33,  2.57it/s, v_num=0]Training loss: 1.5699245929718018\n",
      "Epoch 5:  84%|████████▎ | 428/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 1.4235519170761108\n",
      "Epoch 5:  84%|████████▍ | 429/512 [02:46<00:32,  2.57it/s, v_num=0]Training loss: 0.8995101451873779\n",
      "Epoch 5:  84%|████████▍ | 430/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 0.7895803451538086\n",
      "Epoch 5:  84%|████████▍ | 431/512 [02:47<00:31,  2.57it/s, v_num=0]Training loss: 1.1228549480438232\n",
      "Epoch 5:  84%|████████▍ | 432/512 [02:48<00:31,  2.57it/s, v_num=0]Training loss: 0.9481258392333984\n",
      "Epoch 5:  85%|████████▍ | 433/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 1.1458178758621216\n",
      "Epoch 5:  85%|████████▍ | 434/512 [02:48<00:30,  2.57it/s, v_num=0]Training loss: 0.9479098916053772\n",
      "Epoch 5:  85%|████████▍ | 435/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 0.9537103176116943\n",
      "Epoch 5:  85%|████████▌ | 436/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.03603994846344\n",
      "Epoch 5:  85%|████████▌ | 437/512 [02:49<00:29,  2.57it/s, v_num=0]Training loss: 1.1248552799224854\n",
      "Epoch 5:  86%|████████▌ | 438/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 1.000609040260315\n",
      "Epoch 5:  86%|████████▌ | 439/512 [02:50<00:28,  2.57it/s, v_num=0]Training loss: 0.8278400301933289\n",
      "Epoch 5:  86%|████████▌ | 440/512 [02:51<00:28,  2.57it/s, v_num=0]Training loss: 0.7502972483634949\n",
      "Epoch 5:  86%|████████▌ | 441/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 1.426953673362732\n",
      "Epoch 5:  86%|████████▋ | 442/512 [02:51<00:27,  2.57it/s, v_num=0]Training loss: 1.174309253692627\n",
      "Epoch 5:  87%|████████▋ | 443/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 0.9348105788230896\n",
      "Epoch 5:  87%|████████▋ | 444/512 [02:52<00:26,  2.57it/s, v_num=0]Training loss: 0.9885730743408203\n",
      "Epoch 5:  87%|████████▋ | 445/512 [02:53<00:26,  2.57it/s, v_num=0]Training loss: 0.7351709604263306\n",
      "Epoch 5:  87%|████████▋ | 446/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 1.0188254117965698\n",
      "Epoch 5:  87%|████████▋ | 447/512 [02:53<00:25,  2.57it/s, v_num=0]Training loss: 1.0155208110809326\n",
      "Epoch 5:  88%|████████▊ | 448/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 2.371286630630493\n",
      "Epoch 5:  88%|████████▊ | 449/512 [02:54<00:24,  2.57it/s, v_num=0]Training loss: 0.7561212778091431\n",
      "Epoch 5:  88%|████████▊ | 450/512 [02:55<00:24,  2.57it/s, v_num=0]Training loss: 1.0850974321365356\n",
      "Epoch 5:  88%|████████▊ | 451/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 0.7914508581161499\n",
      "Epoch 5:  88%|████████▊ | 452/512 [02:55<00:23,  2.57it/s, v_num=0]Training loss: 0.7703431844711304\n",
      "Epoch 5:  88%|████████▊ | 453/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.9209370017051697\n",
      "Epoch 5:  89%|████████▊ | 454/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 1.060437560081482\n",
      "Epoch 5:  89%|████████▉ | 455/512 [02:56<00:22,  2.57it/s, v_num=0]Training loss: 0.7556657791137695\n",
      "Epoch 5:  89%|████████▉ | 456/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 0.6742886304855347\n",
      "Epoch 5:  89%|████████▉ | 457/512 [02:57<00:21,  2.57it/s, v_num=0]Training loss: 0.9561626315116882\n",
      "Epoch 5:  89%|████████▉ | 458/512 [02:58<00:21,  2.57it/s, v_num=0]Training loss: 1.0640755891799927\n",
      "Epoch 5:  90%|████████▉ | 459/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 0.680303156375885\n",
      "Epoch 5:  90%|████████▉ | 460/512 [02:58<00:20,  2.57it/s, v_num=0]Training loss: 0.8706661462783813\n",
      "Epoch 5:  90%|█████████ | 461/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 0.9858864545822144\n",
      "Epoch 5:  90%|█████████ | 462/512 [02:59<00:19,  2.57it/s, v_num=0]Training loss: 1.0087003707885742\n",
      "Epoch 5:  90%|█████████ | 463/512 [03:00<00:19,  2.57it/s, v_num=0]Training loss: 0.8237769603729248\n",
      "Epoch 5:  91%|█████████ | 464/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 0.9800634384155273\n",
      "Epoch 5:  91%|█████████ | 465/512 [03:00<00:18,  2.57it/s, v_num=0]Training loss: 0.7613661289215088\n",
      "Epoch 5:  91%|█████████ | 466/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 0.9035975933074951\n",
      "Epoch 5:  91%|█████████ | 467/512 [03:01<00:17,  2.57it/s, v_num=0]Training loss: 0.9793359041213989\n",
      "Epoch 5:  91%|█████████▏| 468/512 [03:02<00:17,  2.57it/s, v_num=0]Training loss: 0.9728190898895264\n",
      "Epoch 5:  92%|█████████▏| 469/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 0.8778352737426758\n",
      "Epoch 5:  92%|█████████▏| 470/512 [03:02<00:16,  2.57it/s, v_num=0]Training loss: 0.8578687906265259\n",
      "Epoch 5:  92%|█████████▏| 471/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 1.2996094226837158\n",
      "Epoch 5:  92%|█████████▏| 472/512 [03:03<00:15,  2.57it/s, v_num=0]Training loss: 0.6223821043968201\n",
      "Epoch 5:  92%|█████████▏| 473/512 [03:04<00:15,  2.57it/s, v_num=0]Training loss: 0.9260120987892151\n",
      "Epoch 5:  93%|█████████▎| 474/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 0.8514437079429626\n",
      "Epoch 5:  93%|█████████▎| 475/512 [03:04<00:14,  2.57it/s, v_num=0]Training loss: 0.8813586831092834\n",
      "Epoch 5:  93%|█████████▎| 476/512 [03:05<00:14,  2.57it/s, v_num=0]Training loss: 1.3189877271652222\n",
      "Epoch 5:  93%|█████████▎| 477/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 0.6649411916732788\n",
      "Epoch 5:  93%|█████████▎| 478/512 [03:05<00:13,  2.57it/s, v_num=0]Training loss: 1.192307472229004\n",
      "Epoch 5:  94%|█████████▎| 479/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 1.053501009941101\n",
      "Epoch 5:  94%|█████████▍| 480/512 [03:06<00:12,  2.57it/s, v_num=0]Training loss: 0.8425448536872864\n",
      "Epoch 5:  94%|█████████▍| 481/512 [03:07<00:12,  2.57it/s, v_num=0]Training loss: 0.6352587938308716\n",
      "Epoch 5:  94%|█████████▍| 482/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 0.781238317489624\n",
      "Epoch 5:  94%|█████████▍| 483/512 [03:07<00:11,  2.57it/s, v_num=0]Training loss: 0.5775534510612488\n",
      "Epoch 5:  95%|█████████▍| 484/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 0.7105574011802673\n",
      "Epoch 5:  95%|█████████▍| 485/512 [03:08<00:10,  2.57it/s, v_num=0]Training loss: 1.2919225692749023\n",
      "Epoch 5:  95%|█████████▍| 486/512 [03:09<00:10,  2.57it/s, v_num=0]Training loss: 0.8241280317306519\n",
      "Epoch 5:  95%|█████████▌| 487/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 1.0080902576446533\n",
      "Epoch 5:  95%|█████████▌| 488/512 [03:09<00:09,  2.57it/s, v_num=0]Training loss: 0.8304331302642822\n",
      "Epoch 5:  96%|█████████▌| 489/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 0.7354955077171326\n",
      "Epoch 5:  96%|█████████▌| 490/512 [03:10<00:08,  2.57it/s, v_num=0]Training loss: 1.4054197072982788\n",
      "Epoch 5:  96%|█████████▌| 491/512 [03:11<00:08,  2.57it/s, v_num=0]Training loss: 1.2115488052368164\n",
      "Epoch 5:  96%|█████████▌| 492/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.0272365808486938\n",
      "Epoch 5:  96%|█████████▋| 493/512 [03:11<00:07,  2.57it/s, v_num=0]Training loss: 1.1489640474319458\n",
      "Epoch 5:  96%|█████████▋| 494/512 [03:12<00:07,  2.57it/s, v_num=0]Training loss: 0.7911015748977661\n",
      "Epoch 5:  97%|█████████▋| 495/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 0.9686103463172913\n",
      "Epoch 5:  97%|█████████▋| 496/512 [03:12<00:06,  2.57it/s, v_num=0]Training loss: 0.8950133323669434\n",
      "Epoch 5:  97%|█████████▋| 497/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 0.658223569393158\n",
      "Epoch 5:  97%|█████████▋| 498/512 [03:13<00:05,  2.57it/s, v_num=0]Training loss: 0.7011949419975281\n",
      "Epoch 5:  97%|█████████▋| 499/512 [03:14<00:05,  2.57it/s, v_num=0]Training loss: 0.5546450614929199\n",
      "Epoch 5:  98%|█████████▊| 500/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 1.0864001512527466\n",
      "Epoch 5:  98%|█████████▊| 501/512 [03:14<00:04,  2.57it/s, v_num=0]Training loss: 0.5043836832046509\n",
      "Epoch 5:  98%|█████████▊| 502/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 0.9903090000152588\n",
      "Epoch 5:  98%|█████████▊| 503/512 [03:15<00:03,  2.57it/s, v_num=0]Training loss: 1.0356096029281616\n",
      "Epoch 5:  98%|█████████▊| 504/512 [03:16<00:03,  2.57it/s, v_num=0]Training loss: 0.6750988364219666\n",
      "Epoch 5:  99%|█████████▊| 505/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 1.8478338718414307\n",
      "Epoch 5:  99%|█████████▉| 506/512 [03:16<00:02,  2.57it/s, v_num=0]Training loss: 1.077800989151001\n",
      "Epoch 5:  99%|█████████▉| 507/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 1.0431748628616333\n",
      "Epoch 5:  99%|█████████▉| 508/512 [03:17<00:01,  2.57it/s, v_num=0]Training loss: 0.8088544607162476\n",
      "Epoch 5:  99%|█████████▉| 509/512 [03:18<00:01,  2.57it/s, v_num=0]Training loss: 1.0742107629776\n",
      "Epoch 5: 100%|█████████▉| 510/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 1.2958080768585205\n",
      "Epoch 5: 100%|█████████▉| 511/512 [03:18<00:00,  2.57it/s, v_num=0]Training loss: 0.7801079750061035\n",
      "Epoch 5: 100%|██████████| 512/512 [03:19<00:00,  2.57it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[AValidation loss: 1.059625267982483\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/110 [00:00<00:03, 30.14it/s]\u001b[AValidation loss: 0.762542724609375\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/110 [00:00<00:04, 25.34it/s]\u001b[AValidation loss: 1.1901599168777466\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/110 [00:00<00:04, 24.32it/s]\u001b[AValidation loss: 1.0358120203018188\n",
      "\n",
      "Validation DataLoader 0:   4%|▎         | 4/110 [00:00<00:04, 23.76it/s]\u001b[AValidation loss: 0.8135499954223633\n",
      "\n",
      "Validation DataLoader 0:   5%|▍         | 5/110 [00:00<00:04, 23.50it/s]\u001b[AValidation loss: 1.0731889009475708\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/110 [00:00<00:04, 23.29it/s]\u001b[AValidation loss: 0.8534315824508667\n",
      "\n",
      "Validation DataLoader 0:   6%|▋         | 7/110 [00:00<00:04, 23.12it/s]\u001b[AValidation loss: 1.0979684591293335\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/110 [00:00<00:04, 23.02it/s]\u001b[AValidation loss: 1.0653891563415527\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/110 [00:00<00:04, 22.93it/s]\u001b[AValidation loss: 0.9489937424659729\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 10/110 [00:00<00:04, 22.89it/s]\u001b[AValidation loss: 0.7615377902984619\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 11/110 [00:00<00:04, 22.84it/s]\u001b[AValidation loss: 0.6546808481216431\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 12/110 [00:00<00:04, 22.79it/s]\u001b[AValidation loss: 0.8325425982475281\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 13/110 [00:00<00:04, 22.77it/s]\u001b[AValidation loss: 0.9430738091468811\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 14/110 [00:00<00:04, 22.74it/s]\u001b[AValidation loss: 0.9769268035888672\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 15/110 [00:00<00:04, 22.71it/s]\u001b[AValidation loss: 1.1385916471481323\n",
      "\n",
      "Validation DataLoader 0:  15%|█▍        | 16/110 [00:00<00:04, 22.68it/s]\u001b[AValidation loss: 0.9276241660118103\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 17/110 [00:00<00:04, 22.65it/s]\u001b[AValidation loss: 0.6934599280357361\n",
      "\n",
      "Validation DataLoader 0:  16%|█▋        | 18/110 [00:00<00:04, 22.63it/s]\u001b[AValidation loss: 0.577576756477356\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 19/110 [00:00<00:04, 22.63it/s]\u001b[AValidation loss: 0.6535196304321289\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 20/110 [00:00<00:03, 22.60it/s]\u001b[AValidation loss: 1.3308062553405762\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 21/110 [00:00<00:03, 22.59it/s]\u001b[AValidation loss: 0.6549140810966492\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 22/110 [00:00<00:03, 22.58it/s]\u001b[AValidation loss: 0.9702777862548828\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 23/110 [00:01<00:03, 22.58it/s]\u001b[AValidation loss: 0.940913736820221\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 24/110 [00:01<00:03, 22.57it/s]\u001b[AValidation loss: 0.7919363975524902\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 25/110 [00:01<00:03, 22.55it/s]\u001b[AValidation loss: 0.6931434273719788\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 26/110 [00:01<00:03, 22.45it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "\n",
    "# Create a logger\n",
    "logger = loggers.CSVLogger('lightning_logs/', name='m1_vae')\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Initialize the VAE Lightning model\n",
    "input_dim = X_train_tensor.shape[1]  # The number of input features\n",
    "latent_dim = 256  # Latent dimension size, can be tuned\n",
    "hidden_dims = [2048, 1024, 512]\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-6\n",
    "\n",
    "model = VAE_Lightning(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    dropout_rate=dropout_rate,\n",
    "    lr=lr)\n",
    "\n",
    "# Training\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    dirpath=f'{logger.save_dir}/{logger.name}/version_{logger.version}/checkpoints/',\n",
    "    filename='m1-vae-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    gradient_clip_val=0.5,  # Clip gradients to avoid explosion\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, loss_history_callback],\n",
    "    precision=32,\n",
    "    accelerator='gpu',          # Use 'gpu' or 'cpu'\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',  # Use 1 GPU or CPU ('auto' will pick the appropriate one)\n",
    "    deterministic=True,  # Ensure reproducibility\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8c82f-6050-40f5-a048-725a3a1b641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c431f599-80a3-41d2-94f0-d91882af4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': 0.2, 'hidden_dims': [2048, 1024, 512], 'input_dim': 451747, 'latent_dim': 256, 'lr': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the hyperparameters from the hparams.yaml file\n",
    "hparams_path = 'lightning_logs/version_3/hparams.yaml'  # Replace with the correct path\n",
    "with open(hparams_path) as file:\n",
    "    hparams = yaml.safe_load(file)\n",
    "\n",
    "print(hparams)  # To inspect the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb373265-2411-49c0-ab8e-b739615c35c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_Lightning(\n",
       "  (model): VAE(\n",
       "    (encoder_layers): Sequential(\n",
       "      (0): Linear(in_features=451747, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_logvar): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc_output): Linear(in_features=2048, out_features=451747, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# checkpoint_path = \"lightning_logs/version_11/checkpoints/epoch=9-step=5120.ckpt\"\n",
    "checkpoint_path = \"lightning_logs/version_1/checkpoints/epoch=24-step=12800.ckpt\"\n",
    "\n",
    "# input_dim = X_train_tensor.shape[1]  # The number of input features\n",
    "# latent_dim = 128  # Latent dimension size, can be tuned\n",
    "# hidden_dims = [2048, 1024, 512]\n",
    "# dropout_rate = 0.2\n",
    "# # hidden_dims = [1000]\n",
    "# # dropout_rate = 0.0\n",
    "# lr = 1e-6\n",
    "\n",
    "loaded_model = VAE_Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    **hparams\n",
    "    )\n",
    "\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151a777-f314-4cc2-8456-aa01e7b220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_embeddings(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x,y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Replace NaNs with zero or another neutral value for forward pass\n",
    "            x_filled = replace_nan_with_mean(x)\n",
    "            # x_filled = torch.nan_to_num(x, nan=0.0)\n",
    "            \n",
    "            z, _, _ = model.forward(x_filled)\n",
    "            embeddings.append(z)\n",
    "            labels.append(y)\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = get_latent_embeddings(model, train_loader)\n",
    "val_embeddings, val_labels = get_latent_embeddings(model, val_loader)\n",
    "test_embeddings, test_labels = get_latent_embeddings(model, test_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbab96-e2f0-44ee-bf0c-63e74b8659dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a6bf4-15d5-4bf3-a6c9-2f113fc5db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Combine train and validation embeddings\n",
    "# combined_embeddings = torch.cat([train_embeddings, val_embeddings], dim=0).cpu().numpy()\n",
    "# combined_labels = torch.cat([train_labels, val_labels], dim=0).cpu().numpy()\n",
    "# # Convert one-hot encoded labels to class indices using argmax\n",
    "# combined_labels_1d = combined_labels.argmax(axis=1)  # Find the index of the maximum (1) in each one-hot vector\n",
    "\n",
    "# # Fit UMAP on the combined embeddings\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "# combined_umap = umap_model.fit_transform(combined_embeddings)\n",
    "\n",
    "# # Plot UMAP embeddings (train + validation)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(combined_umap[:, 0], combined_umap[:, 1], c=combined_labels_1d, cmap='Spectral', s=10, alpha=0.8)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title('UMAP Projection of Train + Validation Embeddings')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7886a9d-eaf0-4e58-9cfe-0b60d1f45b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23994b9c-c8ce-4f8e-b360-270180d0dab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9009f-37a4-45b7-a04b-e8a27ae23e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3529e-6a6a-4ab5-9103-6c50c6b83cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9933dc-318e-4ba3-838f-87836ef4e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert test one-hot encoded labels to class indices\n",
    "# test_labels_1d = test_labels.cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# torch.cat([train_embeddings, val_embeddings], dim=0).cpu().numpy()\n",
    "\n",
    "# # Project test embeddings onto the UMAP space\n",
    "# test_umap = umap_model.transform(test_embeddings.cpu().numpy())\n",
    "\n",
    "# # Plot the test UMAP embeddings\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(test_umap[:, 0], test_umap[:, 1], c=test_labels_1d, cmap='Spectral', s=10, alpha=0.8)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title('UMAP Projection of Test Embeddings')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bc319-9669-41cd-927e-778ddc8ff8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d939d-d920-4f44-9ba6-6a3dfde75331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ba6f-2fe3-4b94-a544-41c527fb8a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f916b22f-7888-485b-a7d3-517aaba4f247",
   "metadata": {},
   "source": [
    "# MLP building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f8621-94ab-4914-b610-5de0314770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_dim, layer_dims, num_classes, dropout_rate=0.2, lr=1e-3):\n",
    "        \"\"\"\n",
    "        :param input_dim: Number of input features (e.g., from VAE embeddings)\n",
    "        :param layer_dims: List of integers defining the number of units in each layer\n",
    "        :param num_classes: Number of output classes for classification (should match the number of columns in one-hot labels)\n",
    "        :param dropout_rate: Dropout rate to be applied after each layer\n",
    "        :param lr: Learning rate\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()  # Save hyperparameters for checkpointing\n",
    "\n",
    "        # Store learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        for dim in layer_dims:\n",
    "            layers.append(nn.Linear(current_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_dim = dim\n",
    "\n",
    "        # Output layer for one-hot encoded labels (num_classes)\n",
    "        layers.append(nn.Linear(current_dim, num_classes))  # Output layer with num_classes\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y is one-hot encoded\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Use BCEWithLogitsLoss for one-hot encoded classification\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y is one-hot encoded\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        \n",
    "        print(f\"Validation loss: {val_loss.item()}\")\n",
    "\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cd9ea-7148-4851-ae21-7e3980937df6",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d5733-062b-4850-b83b-090edbaea82b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming train_embeddings, val_embeddings, test_embeddings are the VAE embeddings\n",
    "# and train_labels, val_labels, test_labels are one-hot encoded (not converting them back to class indices)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=lambda _: np.random.seed(42))\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Create a logger\n",
    "logger = loggers.CSVLogger('lightning_logs/', name='m1_mlp')\n",
    "\n",
    "\n",
    "# Define MLP layer dimensions\n",
    "layer_dims = [128, 64, 32]  # Example of multiple hidden layers\n",
    "\n",
    "# Initialize the MLP model with dropout, L1, and L2 regularization\n",
    "mlp_model = MLP(input_dim=train_embeddings.shape[1],  # Input dimension should match VAE embeddings size\n",
    "                layer_dims=layer_dims, \n",
    "                num_classes=train_labels.shape[1],  # Number of output classes (same as number of columns in one-hot encoded labels)\n",
    "                dropout_rate=0.3, \n",
    "                lr=1e-3)\n",
    "\n",
    "# Create the loss history callback\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    dirpath=f'{logger.save_dir}/{logger.name}/version_{logger.version}/checkpoints/',\n",
    "    filename='m1-mlp-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Initialize PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, loss_history_callback],\n",
    "    accelerator='gpu',\n",
    "    precision=32,\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    deterministic=True,  # Ensure reproducibility\n",
    "    logger=logger\n",
    "    )\n",
    "\n",
    "# Train the MLP model\n",
    "trainer.fit(mlp_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a4398-7b5c-4d72-9bdd-8d42201460fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(train_losses), len(val_losses))\n",
    "\n",
    "# Trim the losses to the same length\n",
    "train_losses = train_losses[:min_len]\n",
    "val_losses = val_losses[:min_len]\n",
    "\n",
    "# Generate the plot\n",
    "epochs = range(1, min_len + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5d362-cecf-476f-874d-cf0c01c9043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get predictions from the MLP model\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)  # Move model to the correct device (e.g., GPU)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, labels = batch\n",
    "            \n",
    "            # Move data to the same device as the model\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get logits from the model\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Get predicted classes\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Append predictions and labels\n",
    "            all_preds.append(preds.cpu())  # Move predictions back to CPU\n",
    "            all_labels.append(labels.cpu())  # Move labels back to CPU\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Define the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get predictions and labels on the test set\n",
    "test_preds, test_labels = get_predictions(mlp_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f173a5-3673-4b1d-b531-5950ef4f0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert one-hot encoded labels back to class indices\n",
    "test_labels_argmax = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Generate class names for 27 classes\n",
    "target_names = [f\"Class {i}\" for i in range(27)]\n",
    "\n",
    "# Generate the classification report using class labels\n",
    "report = classification_report(test_labels_argmax, test_preds, target_names=target_names)\n",
    "\n",
    "\n",
    "# Print the report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7ca9d-0f27-4a05-8aae-448d05623a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85318797-2e74-42ef-995c-6e337790a433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meth]",
   "language": "python",
   "name": "conda-env-meth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
